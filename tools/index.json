{
    "http://arxiv.org/abs/2512.19606v1": {
        "title": "RAPID-LLM: Resilience-Aware Performance analysis of Infrastructure for Distributed LLM Training and Inference",
        "link": "http://arxiv.org/abs/2512.19606v1",
        "abstract": "RAPID-LLM is a unified performance modeling framework for large language model (LLM) training and inference on GPU clusters. It couples a DeepFlow-based frontend that generates hardware-aware, operator-level Chakra execution traces from an abstract LLM specification (model shape, batch/sequence settings, training vs. inference, and hybrid parallelism choices) with an extended Astra-Sim backend that executes those traces on explicit multi-dimensional network topologies with congestion-aware routing and support for degraded and faulty links. The frontend assigns per-operator latency using a tile-based model that accounts for SM under-utilization and multi-level memory traffic (SRAM/ L2/ HBM), and prunes memory-infeasible configurations using an activation-liveness traversal under recomputation, parallelism and ZeRO/FDSP sharding policies.\n  Across A100-based validation cases, RAPID-LLM predicts Llama inference step latency and GPT-scale training time per batch within 10.4\\% relative to published measurements, and matches ns-3 packet-level results within 8\\% on representative communication workloads. Case studies demonstrate how RAPID-LLM enables fast, exhaustive sweeps over hybrid-parallel configurations, quantifies sensitivity to soft link faults under realistic routing and congestion, and evaluates hypothetical GPU design variants including HBM bandwidth throttling effects.",
        "authors": [
            "George Karfakis",
            "Faraz Tahmasebi",
            "Binglu Chen",
            "Lime Yao",
            "Saptarshi Mitra",
            "Tianyue Pan",
            "Hyoukjun Kwon",
            "Puneet Gupta"
        ],
        "categories": [
            "cs.PF",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.19606v1",
        "relevant": true,
        "tags": [
            "training",
            "serving",
            "networking"
        ],
        "tldr": "Proposes RAPID-LLM, a unified performance modeling framework for LLM training and inference on GPU clusters. Combines DeepFlow-based frontend and Astra-Sim backend to simulate hardware-aware execution with network faults. Predicts latency within 10.4% of measurements, enabling configuration sweeps and resilience analysis.",
        "indexed_date": "2025-12-23"
    }
}