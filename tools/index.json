{
    "http://arxiv.org/abs/2512.19606v1": {
        "id": "http://arxiv.org/abs/2512.19606v1",
        "title": "RAPID-LLM: Resilience-Aware Performance analysis of Infrastructure for Distributed LLM Training and Inference",
        "link": "http://arxiv.org/abs/2512.19606v1",
        "tags": [
            "training",
            "serving",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-12-20",
        "tldr": "Proposes RAPID-LLM, a unified performance modeling framework for LLM training and inference on GPU clusters. Combines DeepFlow-based frontend and Astra-Sim backend to simulate hardware-aware execution with network faults. Predicts latency within 10.4% of measurements, enabling configuration sweeps and resilience analysis.",
        "abstract": "RAPID-LLM is a unified performance modeling framework for large language model (LLM) training and inference on GPU clusters. It couples a DeepFlow-based frontend that generates hardware-aware, operator-level Chakra execution traces from an abstract LLM specification (model shape, batch/sequence settings, training vs. inference, and hybrid parallelism choices) with an extended Astra-Sim backend that executes those traces on explicit multi-dimensional network topologies with congestion-aware routing and support for degraded and faulty links. The frontend assigns per-operator latency using a tile-based model that accounts for SM under-utilization and multi-level memory traffic (SRAM/ L2/ HBM), and prunes memory-infeasible configurations using an activation-liveness traversal under recomputation, parallelism and ZeRO/FDSP sharding policies.   Across A100-based validation cases, RAPID-LLM predicts Llama inference step latency and GPT-scale training time per batch within 10.4\\% relative to published measurements, and matches ns-3 packet-level results within 8\\% on representative communication workloads. Case studies demonstrate how RAPID-LLM enables fast, exhaustive sweeps over hybrid-parallel configurations, quantifies sensitivity to soft link faults under realistic routing and congestion, and evaluates hypothetical GPU design variants including HBM bandwidth throttling effects.",
        "authors": [
            "George Karfakis",
            "Faraz Tahmasebi",
            "Binglu Chen",
            "Lime Yao",
            "Saptarshi Mitra",
            "Tianyue Pan",
            "Hyoukjun Kwon",
            "Puneet Gupta"
        ],
        "categories": [
            "cs.PF",
            "cs.DC"
        ],
        "submit_date": "2025-12-22"
    },
    "http://arxiv.org/abs/2512.19342v1": {
        "id": "http://arxiv.org/abs/2512.19342v1",
        "title": "Faster Distributed Inference-Only Recommender Systems via Bounded Lag Synchronous Collectives",
        "link": "http://arxiv.org/abs/2512.19342v1",
        "tags": [
            "serving",
            "offline",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-12-20",
        "tldr": "Addresses communication bottlenecks in distributed recommender system inference. Proposes bounded lag synchronous (BLS) alltoallv collective with adjustable lag bounds to mask process delays. Achieves improved latency and throughput in unbalanced scenarios, masking delays entirely in best cases.",
        "abstract": "Recommender systems are enablers of personalized content delivery, and therefore revenue, for many large companies. In the last decade, deep learning recommender models (DLRMs) are the de-facto standard in this field. The main bottleneck in DLRM inference is the lookup of sparse features across huge embedding tables, which are usually partitioned across the aggregate RAM of many nodes. In state-of-the-art recommender systems, the distributed lookup is implemented via irregular all-to-all (alltoallv) communication, and often presents the main bottleneck. Today, most related work sees this operation as a given; in addition, every collective is synchronous in nature. In this work, we propose a novel bounded lag synchronous (BLS) version of the alltoallv operation. The bound can be a parameter allowing slower processes to lag behind entire iterations before the fastest processes block. In special applications such as inference-only DLRM, the accuracy of the application is fully preserved. We implement BLS alltoallv in a new PyTorch Distributed backend and evaluate it with a BLS version of the reference DLRM code. We show that for well balanced, homogeneous-access DLRM runs our BLS technique does not offer notable advantages. But for unbalanced runs, e.g. runs with strongly irregular embedding table accesses or with delays across different processes, our BLS technique improves both the latency and throughput of inference-only DLRM. In the best-case scenario, the proposed reduced synchronisation can mask the delays across processes altogether.",
        "authors": [
            "Kiril Dichev",
            "Filip Pawlowski",
            "Albert-Jan Yzelman"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-12-22"
    },
    "http://arxiv.org/abs/2512.19326v1": {
        "title": "Simulations between Strongly Sublinear MPC and Node-Capacitated Clique",
        "link": "http://arxiv.org/abs/2512.19326v1",
        "abstract": "We study how the strongly sublinear MPC model relates to the classic, graph-centric distributed models, focusing on the Node-Capacitated Clique (NCC), a bandwidth-parametrized generalization of the Congested Clique. In MPC, $M$ machines with per-machine memory $S$ hold a partition of the input graph, in NCC, each node knows its full neighborhood but can send/receive only a bounded number of $C$ words per round. We are particularly interested in the strongly sublinear regime where $S=C=n^δ$ for some constant $0 < δ<1$.   Our goal is determine when round-preserving simulations between these models are possible and when they are not, when total memory and total bandwidth $SM=nC$ in both models are matched, for different problem families and graph classes. On the positive side, we provide techniques that allow us to replicate the specific behavior regarding input representation, number of machines and local memory from one model to the other to obtain simulations with only constant overhead. On the negative side, we prove simulation impossibility results, which show that the limitations of our simulations are necessary.",
        "authors": [
            "Philipp Schneider",
            "Julian Werthmann"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.19326v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-22"
    },
    "http://arxiv.org/abs/2512.19179v1": {
        "id": "http://arxiv.org/abs/2512.19179v1",
        "title": "L4: Low-Latency and Load-Balanced LLM Serving via Length-Aware Scheduling",
        "link": "http://arxiv.org/abs/2512.19179v1",
        "tags": [
            "serving",
            "offloading",
            "scheduling"
        ],
        "relevant": true,
        "indexed_date": "2025-12-20",
        "tldr": "Addresses GPU underutilization and latency in LLM serving due to request-length heterogeneity. Proposes L4, a runtime system for dynamic request rescheduling and instance partitioning based on length groups. Achieves up to 69% lower tail latency and 2.89× throughput improvement over state-of-the-art schedulers.",
        "abstract": "Efficiently harnessing GPU compute is critical to improving user experience and reducing operational costs in large language model (LLM) services. However, current inference engine schedulers overlook the attention backend's sensitivity to request-length heterogeneity within a batch. As state-of-the-art models now support context windows exceeding 128K tokens, this once-tolerable inefficiency has escalated into a primary system bottleneck, causing severe performance degradation through GPU underutilization and increased latency. We present L4, a runtime system that dynamically reschedules requests across multiple instances serving the same LLM to mitigate per-instance length heterogeneity. L4 partitions these instances into length-specialized groups, each handling requests within a designated length range, naturally forming a pipeline as requests flow through them. L4 devises a dynamic programming algorithm to efficiently find the stage partition with the best QoE, employs runtime range refinement together with decentralized load (re)balance both across and within groups, achieving a balanced and efficient multi-instance service. Our evaluation shows that, under the same configuration, L4 reduces end-to-end latency by up to 67% and tail latency by up to 69%, while improving overall system throughput by up to 2.89 times compared to the state-of-the-art multi-instance scheduling systems.",
        "authors": [
            "Yitao Yuan",
            "Chenqi Zhao",
            "Bohan Zhao",
            "Zane Cao",
            "Yongchao He",
            "Wenfei Wu"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-12-22"
    },
    "http://arxiv.org/abs/2512.19131v1": {
        "id": "http://arxiv.org/abs/2512.19131v1",
        "title": "Evidential Trust-Aware Model Personalization in Decentralized Federated Learning for Wearable IoT",
        "link": "http://arxiv.org/abs/2512.19131v1",
        "tags": [
            "edge",
            "offline",
            "RL"
        ],
        "relevant": true,
        "indexed_date": "2025-12-20",
        "tldr": "Proposes Murmura, a trust-aware model personalization framework for decentralized federated learning on edge devices. Uses evidential deep learning to compute compatibility scores via cross-evaluation and adaptive aggregation. Achieves 7.4× faster convergence vs IID degradation in wearable IoT datasets.",
        "abstract": "Decentralized federated learning (DFL) enables collaborative model training across edge devices without centralized coordination, offering resilience against single points of failure. However, statistical heterogeneity arising from non-identically distributed local data creates a fundamental challenge: nodes must learn personalized models adapted to their local distributions while selectively collaborating with compatible peers. Existing approaches either enforce a single global model that fits no one well, or rely on heuristic peer selection mechanisms that cannot distinguish between peers with genuinely incompatible data distributions and those with valuable complementary knowledge. We present Murmura, a framework that leverages evidential deep learning to enable trust-aware model personalization in DFL. Our key insight is that epistemic uncertainty from Dirichlet-based evidential models directly indicates peer compatibility: high epistemic uncertainty when a peer's model evaluates local data reveals distributional mismatch, enabling nodes to exclude incompatible influence while maintaining personalized models through selective collaboration. Murmura introduces a trust-aware aggregation mechanism that computes peer compatibility scores through cross-evaluation on local validation samples and personalizes model aggregation based on evidential trust with adaptive thresholds. Evaluation on three wearable IoT datasets (UCI HAR, PAMAP2, PPG-DaLiA) demonstrates that Murmura reduces performance degradation from IID to non-IID conditions compared to baseline (0.9% vs. 19.3%), achieves 7.4$\\times$ faster convergence, and maintains stable accuracy across hyperparameter choices. These results establish evidential uncertainty as a principled foundation for compatibility-aware personalization in decentralized heterogeneous environments.",
        "authors": [
            "Murtaza Rangwala",
            "Richard O. Sinnott",
            "Rajkumar Buyya"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-12-22"
    },
    "http://arxiv.org/abs/2512.19103v1": {
        "title": "Timely Parameter Updating in Over-the-Air Federated Learning",
        "link": "http://arxiv.org/abs/2512.19103v1",
        "abstract": "Incorporating over-the-air computations (OAC) into the model training process of federated learning (FL) is an effective approach to alleviating the communication bottleneck in FL systems. Under OAC-FL, every client modulates its intermediate parameters, such as gradient, onto the same set of orthogonal waveforms and simultaneously transmits the radio signal to the edge server. By exploiting the superposition property of multiple-access channels, the edge server can obtain an automatically aggregated global gradient from the received signal. However, the limited number of orthogonal waveforms available in practical systems is fundamentally mismatched with the high dimensionality of modern deep learning models. To address this issue, we propose Freshness Freshness-mAgnItude awaRe top-k (FAIR-k), an algorithm that selects, in each communication round, the most impactful subset of gradients to be updated over the air. In essence, FAIR-k combines the complementary strengths of the Round-Robin and Top-k algorithms, striking a delicate balance between timeliness (freshness of parameter updates) and importance (gradient magnitude). Leveraging tools from Markov analysis, we characterize the distribution of parameter staleness under FAIR-k. Building on this, we establish the convergence rate of OAC-FL with FAIR-k, which discloses the joint effect of data heterogeneity, channel noise, and parameter staleness on the training efficiency. Notably, as opposed to conventional analyses that assume a universal Lipschitz constant across all the clients, our framework adopts a finer-grained model of the data heterogeneity. The analysis demonstrates that since FAIR-k promotes fresh (and fair) parameter updates, it not only accelerates convergence but also enhances communication efficiency by enabling an extended period of local training without significantly affecting overall training efficiency.",
        "authors": [
            "Jiaqi Zhu",
            "Zhongyuan Zhao",
            "Xiao Li",
            "Ruihao Du",
            "Shi Jin",
            "Howard H. Yang"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.19103v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-22"
    },
    "http://arxiv.org/abs/2512.18915v1": {
        "id": "http://arxiv.org/abs/2512.18915v1",
        "title": "QoS-Aware Load Balancing in the Computing Continuum via Multi-Player Bandits",
        "link": "http://arxiv.org/abs/2512.18915v1",
        "tags": [
            "edge",
            "networking",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-12-20",
        "tldr": "Proposes QEdgeProxy, a decentralized QoS-aware load balancer for edge computing, modeled as a Multi-Player MAB with kernel density estimation for per-client QoS. Kubernetes implementation achieves improved per-client QoS satisfaction in latency-sensitive workloads.",
        "abstract": "As computation shifts from the cloud to the edge to reduce processing latency and network traffic, the resulting Computing Continuum (CC) creates a dynamic environment where it is challenging to meet strict Quality of Service (QoS) requirements and avoid service instance overload. Existing methods often prioritize global metrics, overlooking per-client QoS, which is crucial for latency-sensitive and reliability-critical applications. We propose QEdgeProxy, a decentralized QoS-aware load balancer that acts as a proxy between IoT devices and service instances in CC. We formulate the load balancing problem as a Multi-Player Multi-Armed Bandit (MP-MAB) with heterogeneous rewards, where each load balancer autonomously selects service instances that maximize the probability of meeting its clients' QoS targets by using Kernel Density Estimation (KDE) to estimate QoS success probabilities. It also incorporates an adaptive exploration mechanism to recover rapidly from performance shifts and non-stationary conditions. We present a Kubernetes-native QEdgeProxy implementation and evaluate it on an emulated CC testbed deployed on a K3s cluster with realistic network conditions and a latency-sensitive edge-AI workload. Results show that QEdgeProxy significantly outperforms proximity-based and reinforcement-learning baselines in per-client QoS satisfaction, while adapting effectively to load surges and instance availability changes.",
        "authors": [
            "Ivan Čilić",
            "Ivana Podnar Žarko",
            "Pantelis Frangoudis",
            "Schahram Dustdar"
        ],
        "categories": [
            "cs.NI",
            "cs.DC"
        ],
        "submit_date": "2025-12-21"
    },
    "http://arxiv.org/abs/2512.18894v1": {
        "title": "A Real-Time Digital Twin for Adaptive Scheduling",
        "link": "http://arxiv.org/abs/2512.18894v1",
        "abstract": "High-performance computing (HPC) workloads are becoming increasingly diverse, exhibiting wide variability in job characteristics, yet cluster scheduling has long relied on static, heuristic-based policies. In this work we present SchedTwin, a real-time digital twin designed to adaptively guide scheduling decisions using predictive simulation. SchedTwin periodically ingests runtime events from the physical scheduler, performs rapid what-if evaluations of multiple policies using a high-fidelity discrete-event simulator, and dynamically selects the one satisfying the administrator configured optimization goal. We implement SchedTwin as an open-source software and integrate it with the production PBS scheduler. Preliminary results show that SchedTwin consistently outperforms widely used static scheduling policies, while maintaining low overhead (a few seconds per scheduling cycle). These results demonstrate that real-time digital twins offer a practical and effective path toward adaptive HPC scheduling.",
        "authors": [
            "Yihe Zhang",
            "Yash Kurkure",
            "Yiheng Tao",
            "Michael E. Papka",
            "Zhiling Lan"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.18894v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-21"
    },
    "http://arxiv.org/abs/2512.18883v1": {
        "title": "EuroHPC SPACE CoE: Redesigning Scalable Parallel Astrophysical Codes for Exascale",
        "link": "http://arxiv.org/abs/2512.18883v1",
        "abstract": "High Performance Computing (HPC) based simulations are crucial in Astrophysics and Cosmology (A&C), helping scientists investigate and understand complex astrophysical phenomena. Taking advantage of exascale computing capabilities is essential for these efforts. However, the unprecedented architectural complexity of exascale systems impacts legacy codes. The SPACE Centre of Excellence (CoE) aims to re-engineer key astrophysical codes to tackle new computational challenges by adopting innovative programming paradigms and software (SW) solutions. SPACE brings together scientists, code developers, HPC experts, hardware (HW) manufacturers, and SW developers. This collaboration enhances exascale A&C applications, promoting the use of exascale and post-exascale computing capabilities. Additionally, SPACE addresses high-performance data analysis for the massive data outputs from exascale simulations and modern observations, using machine learning (ML) and visualisation tools. The project facilitates application deployment across platforms by focusing on code repositories and data sharing, integrating European astrophysical communities around exascale computing with standardised SW and data protocols.",
        "authors": [
            "Nitin Shukla",
            "Alessandro Romeo",
            "Caterina Caravita",
            "Lubomir Riha",
            "Ondrej Vysocky",
            "Petr Strakos",
            "Milan Jaros",
            "João Barbosa",
            "Radim Vavrik",
            "Andrea Mignone",
            "Marco Rossazza",
            "Stefano Truzzi",
            "Vittoria Berta",
            "Iacopo Colonnelli",
            "Doriana Medić",
            "Elisabetta Boella",
            "Daniele Gregori",
            "Eva Sciacca",
            "Luca Tornatore",
            "Giuliano Taffoni",
            "Pranab J. Deka",
            "Fabio Bacchini",
            "Rostislav-Paul Wilhelm",
            "Georgios Doulis",
            "Khalil Pierre",
            "Luciano Rezzolla",
            "Tine Colman",
            "Benoît Commerçon",
            "Othman Bouizi",
            "Matthieu Kuhn",
            "Erwan Raffin",
            "Marc Sergent",
            "Robert Wissing",
            "Guillermo Marin",
            "Klaus Dolag",
            "Geray S. Karademir",
            "Gino Perna",
            "Marisa Zanotti",
            "Sebastian Trujillo-Gomez"
        ],
        "categories": [
            "astro-ph.IM",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.18883v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-21"
    },
    "http://arxiv.org/abs/2512.18674v1": {
        "id": "http://arxiv.org/abs/2512.18674v1",
        "title": "Remoe: Towards Efficient and Low-Cost MoE Inference in Serverless Computing",
        "link": "http://arxiv.org/abs/2512.18674v1",
        "tags": [
            "MoE",
            "offloading",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-12-20",
        "tldr": "Addresses high cost and memory overhead in MoE inference for serverless computing. Proposes Remoe, a heterogeneous system that assigns non-expert modules to GPUs and experts to CPUs, with SPS for activation prediction and MMP for SLO compliance. Reduces cost by 57% and cold start latency by 47%.",
        "abstract": "Mixture-of-Experts (MoE) has become a dominant architecture in large language models (LLMs) due to its ability to scale model capacity via sparse expert activation. Meanwhile, serverless computing, with its elasticity and pay-per-use billing, is well-suited for deploying MoEs with bursty workloads. However, the large number of experts in MoE models incurs high inference costs due to memory-intensive parameter caching. These costs are difficult to mitigate via simple model partitioning due to input-dependent expert activation. To address these issues, we propose Remoe, a heterogeneous MoE inference system tailored for serverless computing. Remoe assigns non-expert modules to GPUs and expert modules to CPUs, and further offloads infrequently activated experts to separate serverless functions to reduce memory overhead and enable parallel execution. We incorporate three key techniques: (1) a Similar Prompts Searching (SPS) algorithm to predict expert activation patterns based on semantic similarity of inputs; (2) a Main Model Pre-allocation (MMP) algorithm to ensure service-level objectives (SLOs) via worst-case memory estimation; and (3) a joint memory and replica optimization framework leveraging Lagrangian duality and the Longest Processing Time (LPT) algorithm. We implement Remoe on Kubernetes and evaluate it across multiple LLM benchmarks. Experimental results show that Remoe reduces inference cost by up to 57% and cold start latency by 47% compared to state-of-the-art baselines.",
        "authors": [
            "Wentao Liu",
            "Yuhao Hu",
            "Ruiting Zhou",
            "Baochun Li",
            "Ne Wang"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-12-21"
    },
    "http://arxiv.org/abs/2512.18444v1": {
        "title": "Snowveil: A Framework for Decentralised Preference Discovery",
        "link": "http://arxiv.org/abs/2512.18444v1",
        "abstract": "Aggregating subjective preferences of a large group is a fundamental challenge in computational social choice, traditionally reliant on central authorities. To address the limitations of this model, this paper introduces Decentralised Preference Discovery (DPD), the problem of determining the collective will of an electorate under constraints of censorship resistance, partial information, and asynchronous communication. We propose Snowveil, a novel framework for this task. Snowveil uses an iterative, gossip-based protocol where voters repeatedly sample the preferences of a small, random subset of the electorate to progressively converge on a collective outcome. We demonstrate the framework's modularity by designing the Constrained Hybrid Borda (CHB), a novel aggregation rule engineered to balance broad consensus with strong plurality support, and provide a rigorous axiomatic analysis of its properties. By applying a potential function and submartingale theory, we develop a multi-level analytical method to show that the system almost surely converges to a stable, single-winner in finite time, a process that can then be iterated to construct a set of winning candidates for multi-winner scenarios. This technique is largely agnostic to the specific aggregation rule, requiring only that it satisfies core social choice axioms like Positive Responsiveness, thus offering a formal toolkit for a wider class of DPD protocols. Furthermore, we present a comprehensive empirical analysis through extensive simulation, validating Snowveil's $O(n)$ scalability. Overall, this work advances the understanding of how a stable consensus can emerge from subjective, complex, and diverse preferences in decentralised systems for large electorates.",
        "authors": [
            "Grammateia Kotsialou"
        ],
        "categories": [
            "cs.GT",
            "cs.AI",
            "cs.DC",
            "cs.MA"
        ],
        "id": "http://arxiv.org/abs/2512.18444v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-20"
    },
    "http://arxiv.org/abs/2512.18334v1": {
        "title": "Faster Vertex Cover Algorithms on GPUs with Component-Aware Parallel Branching",
        "link": "http://arxiv.org/abs/2512.18334v1",
        "abstract": "Algorithms for finding minimum or bounded vertex covers in graphs use a branch-and-reduce strategy, which involves exploring a highly imbalanced search tree. Prior GPU solutions assign different thread blocks to different sub-trees, while using a shared worklist to balance the load. However, these prior solutions do not scale to large and complex graphs because their unawareness of when the graph splits into components causes them to solve these components redundantly. Moreover, their high memory footprint limits the number of workers that can execute concurrently. We propose a novel GPU solution for vertex cover problems that detects when a graph splits into components and branches on the components independently. Although the need to aggregate the solutions of different components introduces non-tail-recursive branches which interfere with load balancing, we overcome this challenge by delegating the post-processing to the last descendant of each branch. We also reduce the memory footprint by reducing the graph and inducing a subgraph before exploring the search tree. Our solution substantially outperforms the state-of-the-art GPU solution, finishing in seconds when the state-of-the-art solution exceeds 6 hours. To the best of our knowledge, our work is the first to parallelize non-tail-recursive branching patterns on GPUs in a load balanced manner.",
        "authors": [
            "Hussein Amro",
            "Basel Fakhri",
            "Amer E. Mouawad",
            "Izzat El Hajj"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.18334v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-20"
    },
    "http://arxiv.org/abs/2512.18318v1": {
        "id": "http://arxiv.org/abs/2512.18318v1",
        "title": "Asynchronous Pipeline Parallelism for Real-Time Multilingual Lip Synchronization in Video Communication Systems",
        "link": "http://arxiv.org/abs/2512.18318v1",
        "tags": [
            "serving",
            "offloading",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-12-20",
        "tldr": "Proposes an asynchronous pipeline-parallel Transformer framework for real-time multilingual lip synchronization. Employs message-queue decoupling, low-level graph compilation, mixed-precision quantization, and kernel fusion to reduce latency. Achieves up to 3.1× lower end-to-end latency than sequential approaches.",
        "abstract": "This paper introduces a parallel and asynchronous Transformer framework designed for efficient and accurate multilingual lip synchronization in real-time video conferencing systems. The proposed architecture integrates translation, speech processing, and lip-synchronization modules within a pipeline-parallel design that enables concurrent module execution through message-queue-based decoupling, reducing end-to-end latency by up to 3.1 times compared to sequential approaches. To enhance computational efficiency and throughput, the inference workflow of each module is optimized through low-level graph compilation, mixed-precision quantization, and hardware-accelerated kernel fusion. These optimizations provide substantial gains in efficiency while preserving model accuracy and visual quality. In addition, a context-adaptive silence-detection component segments the input speech stream at semantically coherent boundaries, improving translation consistency and temporal alignment across languages. Experimental results demonstrate that the proposed parallel architecture outperforms conventional sequential pipelines in processing speed, synchronization stability, and resource utilization. The modular, message-oriented design makes this work applicable to resource-constrained IoT communication scenarios including telemedicine, multilingual kiosks, and remote assistance systems. Overall, this work advances the development of low-latency, resource-efficient multimodal communication frameworks for next-generation AIoT systems.",
        "authors": [
            "Eren Caglar",
            "Amirkia Rafiei Oskooei",
            "Mehmet Kutanoglu",
            "Mustafa Keles",
            "Mehmet S. Aktas"
        ],
        "categories": [
            "cs.MM",
            "cs.AI",
            "cs.CV",
            "cs.DC",
            "cs.NI"
        ],
        "submit_date": "2025-12-20"
    },
    "http://arxiv.org/abs/2512.18194v1": {
        "id": "http://arxiv.org/abs/2512.18194v1",
        "title": "TraCT: Disaggregated LLM Serving with CXL Shared Memory KV Cache at Rack-Scale",
        "link": "http://arxiv.org/abs/2512.18194v1",
        "tags": [
            "serving",
            "offloading",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-12-20",
        "tldr": "Proposes TraCT, a rack-scale LLM serving system using CXL shared memory for KV cache, avoiding RDMA networks. Addresses synchronization and consistency via software solutions like two-tier synchronization. Achieves 9.8x lower TTFT, 6.2x lower P99 latency, and 1.6x higher throughput.",
        "abstract": "Disaggregated LLM serving improves resource efficiency by separating the compute-intensive prefill phase from the latency-critical decode phase. However, this architecture introduces a fundamental bottleneck: key/value (KV) tensors generated during prefill must be transferred to decode workers, and existing systems rely on RDMA-based network paths for this exchange. As model sizes and context lengths increase, KV transfer dominates both time-to-first-token (TTFT) and peak throughput, and remains highly sensitive to network contention even when prefix reuse is high. This paper presents TraCT, a rack-scale LLM serving system that uses CXL shared memory as both a KV-transfer substrate and a rack-wide prefix-aware KV cache. TraCT enables GPUs to write and read KV blocks directly through CXL load/store and DMA operations, eliminating the NIC hop that constrains existing disaggregated pipelines. However, to realize this design, multiple new challenges such as synchronization, consistency, and data management on non-coherent CXL memory need to be addressed. TraCT proposes various software solutions such as the two-tier inter-node synchronization mechanism to address these challenges. We implement TraCT on the Dynamo LLM inference framework and show that, across static and synthetic workloads, TraCT reduces average TTFT by up to 9.8x, lowers P99 latency by up to 6.2x, and improves peak throughput by up to 1.6x compared to RDMA and DRAM-based caching baselines.",
        "authors": [
            "Dongha Yoon",
            "Younghoon Min",
            "Hoshik Kim",
            "Sam H. Noh",
            "Jongryool Kim"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-12-20"
    },
    "http://arxiv.org/abs/2512.18141v1": {
        "title": "Constrained Cuts, Flows, and Lattice-Linearity",
        "link": "http://arxiv.org/abs/2512.18141v1",
        "abstract": "In a capacitated directed graph, it is known that the set of all min-cuts forms a distributive lattice [1], [2]. Here, we describe this lattice as a regular predicate whose forbidden elements can be advanced in constant parallel time after precomputing a max-flow, so as to obtain parallel algorithms for min-cut problems with additional constraints encoded by lattice-linear predicates [3]. Some nice algorithmic applications follow. First, we use these methods to compute the irreducibles of the sublattice of min-cuts satisfying a regular predicate. By Birkhoff's theorem [4] this gives a succinct representation of such cuts, and so we also obtain a general algorithm for enumerating this sublattice. Finally, though we prove computing min-cuts satisfying additional constraints is NP-hard in general, we use poset slicing [5], [6] for exact algorithms with constraints not necessarily encoded by lattice-linear predicates) with better complexity than exhaustive search. We also introduce $k$-transition predicates and strong advancement for improved complexity analyses of lattice-linear predicate algorithms in parallel settings, which is of independent interest.",
        "authors": [
            "Robert Streit",
            "Vijay K. Garg"
        ],
        "categories": [
            "cs.DS",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.18141v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-19"
    },
    "http://arxiv.org/abs/2512.18127v1": {
        "id": "http://arxiv.org/abs/2512.18127v1",
        "title": "ACE-Sync: An Adaptive Cloud-Edge Synchronization Framework for Communication-Efficient Large-Scale Distributed Model Training",
        "link": "http://arxiv.org/abs/2512.18127v1",
        "tags": [
            "training",
            "networking",
            "sparse"
        ],
        "relevant": true,
        "indexed_date": "2025-12-20",
        "tldr": "Proposes ACE-Sync, an adaptive cloud-edge synchronization framework with attention-based gradient importance prediction, differentiated compression, and hierarchical coordination to reduce communication in distributed training. Reduces communication cost from 112.5 GB to 44.7 GB (60% reduction) while maintaining model accuracy within 0.3%.",
        "abstract": "Large-scale deep learning models impose substantial communication overh ead in distributed training, particularly in bandwidth-constrained or heterogeneous clo ud-edge environments. Conventional synchronous or fixed-compression techniques o ften struggle to balance communication cost, convergence stability, and model accura cy. To address these challenges, we propose ACE-Sync, an Adaptive Cloud-Edge Sy nchronization Framework that integrates (1) an attention-based gradient importance p redictor, (2) a differentiated parameter compression strategy, and (3) a hierarchical cl oud-edge coordination mechanism. ACE-Sync dynamically selects which parameter groups to synchronize and determines appropriate compression levels under per-devic e bandwidth budgets. A knapsack-based optimization strategy is adopted to maximize important gradient preservation while reducing redundant communication. Furthermo re, residual-based error compensation and device clustering ensure long-term converg ence and cross-device personalization. Experiments show that ACE-Sync substantiall y reduces communication overhead while maintaining competitive accuracy. Compar ed with FullSync, ACE-Sync lowers communication cost from 112.5 GB to 44.7 GB (a 60% reduction) and shortens convergence from 41 to 39 epochs. Despite aggressiv e communication reduction, ACE-Sync preserves high model quality, achieving 82. 1% Top-1 accuracy-only 0.3% below the full-synchronization baseline-demonstrating its efficiency and scalability for large-scale distributed training. These results indicate that ACE-Sync provides a scalable, communication-efficient, and accuracy-preservin g solution for large-scale cloud-edge distributed model training.",
        "authors": [
            "Yi Yang",
            "Ziyu Lin",
            "Liesheng Wei"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-12-19"
    },
    "http://arxiv.org/abs/2512.17885v1": {
        "title": "Asymptotic behaviour of galactic small-scale dynamos at modest magnetic Prandtl number",
        "link": "http://arxiv.org/abs/2512.17885v1",
        "abstract": "Magnetic fields are critical at many scales to galactic dynamics and structure, including multiphase pressure balance, dust processing, and star formation. Dynamo action determines their dynamical structure and strength. Simulations of combined large- and small-scale dynamos have successfully developed mean fields with strength and topology consistent with observations but with turbulent fields much weaker than observed, while simulations of small-scale dynamos with parameters relevant to the interstellar medium yield turbulent fields an order of magnitude below the values observed or expected theoretically. We use the Pencil Code accelerated on GPUs with Astaroth to perform high-resolution simulations of a supernova-driven galactic dynamo including heating and cooling in a periodic domain. Our models show that the strength of the turbulent field produced by the small-scale dynamo approaches an asymptote at only modest magnetic Prandtl numbers. This allows us to use these models to suggest the essential characteristics of this constituent of the magnetic field for inclusion in global galactic models. The asymptotic limit occurs already at magnetic Prandtl number of only a few hundred, many orders of magnitude below physical values in the the interstellar medium and consistent with previous findings for isothermal compressible flows.",
        "authors": [
            "Frederick A. Gent",
            "Mordecai-Mark Mac Low",
            "Maarit J. Korpi-Lagg",
            "Touko Puro",
            "Matthias Reinhardt"
        ],
        "categories": [
            "astro-ph.GA",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.17885v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-19"
    },
    "http://arxiv.org/abs/2512.17589v1": {
        "title": "Torrent: A Distributed DMA for Efficient and Flexible Point-to-Multipoint Data Movement",
        "link": "http://arxiv.org/abs/2512.17589v1",
        "abstract": "The growing disparity between computational power and on-chip communication bandwidth is a critical bottleneck in modern Systems-on-Chip (SoCs), especially for data-parallel workloads like AI. Efficient point-to-multipoint (P2MP) data movement, such as multicast, is essential for high performance. However, native multicast support is lacking in standard interconnect protocols. Existing P2MP solutions, such as multicast-capable Network-on-Chip (NoC), impose additional overhead to the network hardware and require modifications to the interconnect protocol, compromising scalability and compatibility.   This paper introduces Torrent, a novel distributed DMA architecture that enables efficient P2MP data transfers without modifying NoC hardware and interconnect protocol. Torrent conducts P2MP data transfers by forming logical chains over the NoC, where the data traverses through targeted destinations resembling a linked list. This Chainwrite mechanism preserves the P2P nature of every data transfer while enabling flexible data transfers to an unlimited number of destinations. To optimize the performance and energy consumption of Chainwrite, two scheduling algorithms are developed to determine the optimal chain order based on NoC topology.   Our RTL and FPGA prototype evaluations using both synthetic and real workloads demonstrate significant advantages in performance, flexibility, and scalability over network-layer multicast. Compared to the unicast baseline, Torrent achieves up to a 7.88x speedup. ASIC synthesis on 16nm technology confirms the architecture's minimal footprint in area (1.2%) and power (2.3%). Thanks to the Chainwrite, Torrent delivers scalable P2MP data transfers with a small cycle overhead of 82CC and area overhead of 207um2 per destination.",
        "authors": [
            "Yunhao Deng",
            "Fanchen Kong",
            "Xiaoling Yi",
            "Ryan Antonio",
            "Marian Verhelst"
        ],
        "categories": [
            "cs.AR",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.17589v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-19"
    },
    "http://arxiv.org/abs/2512.17574v1": {
        "id": "http://arxiv.org/abs/2512.17574v1",
        "title": "Enabling Disaggregated Multi-Stage MLLM Inference via GPU-Internal Scheduling and Resource Sharing",
        "link": "http://arxiv.org/abs/2512.17574v1",
        "tags": [
            "serving",
            "multi-modal",
            "video"
        ],
        "relevant": true,
        "indexed_date": "2025-12-19",
        "tldr": "Addresses latency and throughput bottlenecks in multi-stage multimodal LLM (MLLM) serving. Proposes FlashCodec for GPU-accelerated video decoding and UnifiedServe for resource sharing and inter-stage optimization. Achieves up to 4.4× higher throughput and 3.0× more requests served vs. SOTA.",
        "abstract": "Multimodal large language models (MLLMs) extend LLMs with visual understanding through a three-stage pipeline: multimodal preprocessing, vision encoding, and LLM inference. While these stages enhance capability, they introduce significant system bottlenecks. First, multimodal preprocessing-especially video decoding-often dominates Time-to-First-Token (TTFT). Most systems rely on CPU-based decoding, which severely limits throughput, while existing GPU-based approaches prioritize throughput-oriented parallelism and fail to meet the latency-sensitive requirements of MLLM inference. Second, the vision encoder is a standalone, compute-intensive stage that produces visual embeddings and cannot be co-batched with LLM prefill or decoding. This heterogeneity forces inter-stage blocking and increases token-generation latency. Even when deployed on separate GPUs, these stages underutilize available compute and memory resources, reducing overall utilization and constraining system throughput.   To address these challenges, we present FlashCodec and UnifiedServe, two complementary designs that jointly optimize the end-to-end MLLM pipeline. FlashCodec accelerates the multimodal preprocessing stage through collaborative multi-GPU video decoding, reducing decoding latency while preserving high throughput. UnifiedServe optimizes the vision-to-text and inference stages using a logically decoupled their execution to eliminate inter-stage blocking, yet physically sharing GPU resources to maximize GPU system utilization. By carefully orchestrating execution across stages and minimizing interference, UnifiedServe Together, our proposed framework forms an end-to-end optimized stack that can serve up to 3.0$\\times$ more requests or enforce 1.5$\\times$ tighter SLOs, while achieving up to 4.4$\\times$ higher throughput compared to state-of-the-art systems.",
        "authors": [
            "Lingxiao Zhao",
            "Haoran Zhou",
            "Yuezhi Che",
            "Dazhao Cheng"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-12-19"
    },
    "http://arxiv.org/abs/2512.17506v1": {
        "title": "The HEAL Data Platform",
        "link": "http://arxiv.org/abs/2512.17506v1",
        "abstract": "Objective: The objective was to develop a cloud-based, federated system to serve as a single point of search, discovery and analysis for data generated under the NIH Helping to End Addiction Long-term (HEAL) Initiative.   Materials and methods: The HEAL Data Platform is built on the open source Gen3 platform, utilizing a small set of framework services and exposed APIs to interoperate with both NIH and non-NIH data repositories. Framework services include those for authentication and authorization, creating persistent identifiers for data objects, and adding and updating metadata.   Results: The HEAL Data Platform serves as a single point of discovery of over one thousand studies funded under the HEAL Initiative. With hundreds of users per month, the HEAL Data Platform provides rich metadata and interoperates with data repositories and commons to provide access to shared datasets. Secure, cloud-based compute environments that are integrated with STRIDES facilitate secondary analysis of HEAL data. The HEAL Data Platform currently interoperates with nineteen data repositories.   Discussion: Studies funded under the HEAL Initiative generate a wide variety of data types, which are deposited across multiple NIH and third-party data repositories. The mesh architecture of the HEAL Data Platform provides a single point of discovery of these data resources, accelerating and facilitating secondary use.   Conclusion: The HEAL Data Platform enables search, discovery, and analysis of data that are deposited in connected data repositories and commons. By ensuring that these data are fully Findable, Accessible, Interoperable and Reusable (FAIR), the HEAL Data Platform maximizes the value of data generated under the HEAL Initiative.",
        "authors": [
            "Brienna M. Larrick",
            "L. Philip Schumm",
            "Mingfei Shao",
            "Craig Barnes",
            "Anthony Juehne",
            "Hara Prasad Juvvla",
            "Michael B. Kranz",
            "Michael Lukowski",
            "Clint Malson",
            "Jessica N. Mazerik",
            "Christopher G. Meyer",
            "Jawad Qureshi",
            "Erin Spaniol",
            "Andrea Tentner",
            "Alexander VanTol",
            "Peter Vassilatos",
            "Sara Volk de Garcia",
            "Robert L. Grossman"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.17506v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-19"
    },
    "http://arxiv.org/abs/2512.17429v1": {
        "title": "Democratizing Scalable Cloud Applications: Transactional Stateful Functions on Streaming Dataflows",
        "link": "http://arxiv.org/abs/2512.17429v1",
        "abstract": "Web applications underpin much of modern digital life, yet building scalable and consistent cloud applications remains difficult, requiring expertise across cloud computing, distributed systems, databases, and software engineering. These demands restrict development to a small number of highly specialized engineers. This thesis aims to democratize cloud application development by addressing three challenges: programmability, high-performance fault-tolerant serializable transactions, and serverless semantics.   The thesis identifies strong parallels between cloud applications and the streaming dataflow execution model. It first explores this connection through T-Statefun, a transactional extension of Apache Flink Statefun, demonstrating that dataflow systems can support transactional cloud applications via a stateful functions-as-a-service API. However, this approach revealed significant limitations in programmability and performance.   To overcome these issues, the thesis introduces Stateflow, a high-level object-oriented programming model that compiles applications into stateful dataflow graphs with minimal boilerplate. Building on this model, the thesis presents Styx, a distributed streaming dataflow engine that provides deterministic, multi-partition, serializable transactions with strong fault tolerance guarantees. Styx eliminates explicit transaction failure handling and significantly outperforms state-of-the-art systems.   Finally, the thesis extends Styx with transactional state migration to support elasticity under dynamic workloads.",
        "authors": [
            "Kyriakos Psarakis"
        ],
        "categories": [
            "cs.DB",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.17429v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-19"
    },
    "http://arxiv.org/abs/2512.17352v1": {
        "id": "http://arxiv.org/abs/2512.17352v1",
        "title": "Adaptive Graph Pruning with Sudden-Events Evaluation for Traffic Prediction using Online Semi-Decentralized ST-GNNs",
        "link": "http://arxiv.org/abs/2512.17352v1",
        "tags": [
            "offline",
            "network",
            "edge"
        ],
        "relevant": true,
        "indexed_date": "2025-12-19",
        "tldr": "Proposes adaptive graph pruning to reduce communication in distributed ST-GNNs for traffic prediction. Dynamically adjusts pruning based on performance and introduces SEPA metric. Achieves reduced communication costs by 27% while maintaining accuracy in semi-decentralized edge settings.",
        "abstract": "Spatio-Temporal Graph Neural Networks (ST-GNNs) are well-suited for processing high-frequency data streams from geographically distributed sensors in smart mobility systems. However, their deployment at the edge across distributed compute nodes (cloudlets) createssubstantial communication overhead due to repeated transmission of overlapping node features between neighbouring cloudlets. To address this, we propose an adaptive pruning algorithm that dynamically filters redundant neighbour features while preserving the most informative spatial context for prediction. The algorithm adjusts pruning rates based on recent model performance, allowing each cloudlet to focus on regions experiencing traffic changes without compromising accuracy. Additionally, we introduce the Sudden Event Prediction Accuracy (SEPA), a novel event-focused metric designed to measure responsiveness to traffic slowdowns and recoveries, which are often missed by standard error metrics. We evaluate our approach in an online semi-decentralized setting with traditional FL, server-free FL, and Gossip Learning on two large-scale traffic datasets, PeMS-BAY and PeMSD7-M, across short-, mid-, and long-term prediction horizons. Experiments show that, in contrast to standard metrics, SEPA exposes the true value of spatial connectivity in predicting dynamic and irregular traffic. Our adaptive pruning algorithm maintains prediction accuracy while significantly lowering communication cost in all online semi-decentralized settings, demonstrating that communication can be reduced without compromising responsiveness to critical traffic events.",
        "authors": [
            "Ivan Kralj",
            "Lodovico Giaretta",
            "Gordan Ježić",
            "Ivana Podnar Žarko",
            "Šarūnas Girdzijauskas"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC"
        ],
        "submit_date": "2025-12-19"
    },
    "http://arxiv.org/abs/2512.17264v1": {
        "id": "http://arxiv.org/abs/2512.17264v1",
        "title": "Scalable Distributed Vector Search via Accuracy Preserving Index Construction",
        "link": "http://arxiv.org/abs/2512.17264v1",
        "tags": [
            "storage",
            "offline",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-12-19",
        "tldr": "Scaling distributed ANN search for billion-scale vectors. Proposes SPIRE with balanced partition granularity and recursive accuracy-preserving index construction. Achieves up to 9.64x higher throughput vs. SOTA.",
        "abstract": "Scaling Approximate Nearest Neighbor Search (ANNS) to billions of vectors requires distributed indexes that balance accuracy, latency, and throughput. Yet existing index designs struggle with this tradeoff. This paper presents SPIRE, a scalable vector index based on two design decisions. First, it identifies a balanced partition granularity that avoids read-cost explosion. Second, it introduces an accuracy-preserving recursive construction that builds a multi-level index with predictable search cost and stable accuracy. In experiments with up to 8 billion vectors across 46 nodes, SPIRE achieves high scalability and up to 9.64X higher throughput than state-of-the-art systems.",
        "authors": [
            "Yuming Xu",
            "Qianxi Zhang",
            "Qi Chen",
            "Baotong Lu",
            "Menghao Li",
            "Philip Adams",
            "Mingqin Li",
            "Zengzhong Li",
            "Jing Liu",
            "Cheng Li",
            "Fan Yang"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-12-19"
    },
    "http://arxiv.org/abs/2512.17254v1": {
        "title": "Practical Framework for Privacy-Preserving and Byzantine-robust Federated Learning",
        "link": "http://arxiv.org/abs/2512.17254v1",
        "abstract": "Federated Learning (FL) allows multiple clients to collaboratively train a model without sharing their private data. However, FL is vulnerable to Byzantine attacks, where adversaries manipulate client models to compromise the federated model, and privacy inference attacks, where adversaries exploit client models to infer private data. Existing defenses against both backdoor and privacy inference attacks introduce significant computational and communication overhead, creating a gap between theory and practice. To address this, we propose ABBR, a practical framework for Byzantine-robust and privacy-preserving FL. We are the first to utilize dimensionality reduction to speed up the private computation of complex filtering rules in privacy-preserving FL. Additionally, we analyze the accuracy loss of vector-wise filtering in low-dimensional space and introduce an adaptive tuning strategy to minimize the impact of malicious models that bypass filtering on the global model. We implement ABBR with state-of-the-art Byzantine-robust aggregation rules and evaluate it on public datasets, showing that it runs significantly faster, has minimal communication overhead, and maintains nearly the same Byzantine-resilience as the baselines.",
        "authors": [
            "Baolei Zhang",
            "Minghong Fang",
            "Zhuqing Liu",
            "Biao Yi",
            "Peizhao Zhou",
            "Yuan Wang",
            "Tong Li",
            "Zheli Liu"
        ],
        "categories": [
            "cs.CR",
            "cs.DC",
            "cs.LG"
        ],
        "id": "http://arxiv.org/abs/2512.17254v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-19"
    },
    "http://arxiv.org/abs/2512.17077v1": {
        "id": "http://arxiv.org/abs/2512.17077v1",
        "title": "Taming the Memory Footprint Crisis: System Design for Production Diffusion LLM Serving",
        "link": "http://arxiv.org/abs/2512.17077v1",
        "tags": [
            "serving",
            "diffusion",
            "sparse"
        ],
        "relevant": true,
        "indexed_date": "2025-12-20",
        "tldr": "Addresses the memory footprint crisis in serving diffusion LLMs. Proposes dLLM-Serve with Logit-Aware Activation Budgeting, Phase-Multiplexed Scheduler, and Head-Centric Sparse Attention. Achieves up to 1.81× higher throughput and 4× lower tail latency versus baselines.",
        "abstract": "Diffusion Large Language Models (dLLMs) have emerged as a promising alternative to Autoregressive Models (ARMs), utilizing parallel decoding to overcome sequential bottlenecks. However, existing research focuses primarily on kernel-level optimizations, lacking a holistic serving framework that addresses the unique memory dynamics of diffusion processes in production. We identify a critical \"memory footprint crisis\" specific to dLLMs, driven by monolithic logit tensors and the severe resource oscillation between compute-bound \"Refresh\" phases and bandwidth-bound \"Reuse\" phases. To bridge this gap, we present dLLM-Serve, an efficient dLLM serving system that co-optimizes memory footprint, computational scheduling, and generation quality. dLLM-Serve introduces Logit-Aware Activation Budgeting to decompose transient tensor peaks, a Phase-Multiplexed Scheduler to interleave heterogeneous request phases, and Head-Centric Sparse Attention to decouple logical sparsity from physical storage. We evaluate dLLM-Serve on diverse workloads (LiveBench, Burst, OSC) and GPUs (RTX 4090, L40S). Relative to the state-of-the-art baseline, dLLM-Serve improves throughput by 1.61$\\times$-1.81$\\times$ on the consumer-grade RTX 4090 and 1.60$\\times$-1.74$\\times$ on the server-grade NVIDIA L40S, while reducing tail latency by nearly 4$\\times$ under heavy contention. dLLM-Serve establishes the first blueprint for scalable dLLM inference, converting theoretical algorithmic sparsity into tangible wall-clock acceleration across heterogeneous hardware.",
        "authors": [
            "Jiakun Fan",
            "Yanglin Zhang",
            "Xiangchen Li",
            "Dimitrios S. Nikolopoulos"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-12-18"
    },
    "http://arxiv.org/abs/2512.17045v1": {
        "title": "Sedna: Sharding transactions in multiple concurrent proposer blockchains",
        "link": "http://arxiv.org/abs/2512.17045v1",
        "abstract": "Modern blockchains increasingly adopt multi-proposer (MCP) consensus to remove single-leader bottlenecks and improve censorship resistance. However, MCP alone does not resolve how users should disseminate transactions to proposers. Today, users either naively replicate full transactions to many proposers, sacrificing goodput and exposing payloads to MEV, or target few proposers and accept weak censorship and latency guarantees. This yields a practical trilemma among censorship resistance, low latency, and reasonable cost (in fees or system goodput).   We present Sedna, a user-facing protocol that replaces naive transaction replication with verifiable, rateless coding. Users privately deliver addressed symbol bundles to subsets of proposers; execution follows a deterministic order once enough symbols are finalized to decode. We prove Sedna guarantees liveness and \\emph{until-decode privacy}, significantly reducing MEV exposure. Analytically, the protocol approaches the information-theoretic lower bound for bandwidth overhead, yielding a 2-3x efficiency improvement over naive replication. Sedna requires no consensus modifications, enabling incremental deployment.",
        "authors": [
            "Alejandro Ranchal-Pedrosa",
            "Benjamin Marsh",
            "Lefteris Kokoris-Kogias",
            "Alberto Sonnino"
        ],
        "categories": [
            "cs.CR",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.17045v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-18"
    },
    "http://arxiv.org/abs/2512.17023v1": {
        "id": "http://arxiv.org/abs/2512.17023v1",
        "title": "LLM-HPC++: Evaluating LLM-Generated Modern C++ and MPI+OpenMP Codes for Scalable Mandelbrot Set Computation",
        "link": "http://arxiv.org/abs/2512.17023v1",
        "tags": [
            "multi-modal"
        ],
        "relevant": false,
        "indexed_date": "2025-12-20",
        "tldr": "",
        "abstract": "Parallel programming remains one of the most challenging aspects of High-Performance Computing (HPC), requiring deep knowledge of synchronization, communication, and memory models. While modern C++ standards and frameworks like OpenMP and MPI have simplified parallelism, mastering these paradigms is still complex. Recently, Large Language Models (LLMs) have shown promise in automating code generation, but their effectiveness in producing correct and efficient HPC code is not well understood. In this work, we systematically evaluate leading LLMs including ChatGPT 4 and 5, Claude, and LLaMA on the task of generating C++ implementations of the Mandelbrot set using shared-memory, directive-based, and distributed-memory paradigms. Each generated program is compiled and executed with GCC 11.5.0 to assess its correctness, robustness, and scalability. Results show that ChatGPT-4 and ChatGPT-5 achieve strong syntactic precision and scalable performance.",
        "authors": [
            "Patrick Diehl",
            "Noujoud Nader",
            "Deepti Gupta"
        ],
        "categories": [
            "cs.DC",
            "cs.SE"
        ],
        "submit_date": "2025-12-18"
    },
    "http://arxiv.org/abs/2512.16876v1": {
        "title": "Training Together, Diagnosing Better: Federated Learning for Collagen VI-Related Dystrophies",
        "link": "http://arxiv.org/abs/2512.16876v1",
        "abstract": "The application of Machine Learning (ML) to the diagnosis of rare diseases, such as collagen VI-related dystrophies (COL6-RD), is fundamentally limited by the scarcity and fragmentation of available data. Attempts to expand sampling across hospitals, institutions, or countries with differing regulations face severe privacy, regulatory, and logistical obstacles that are often difficult to overcome. The Federated Learning (FL) provides a promising solution by enabling collaborative model training across decentralized datasets while keeping patient data local and private. Here, we report a novel global FL initiative using the Sherpa.ai FL platform, which leverages FL across distributed datasets in two international organizations for the diagnosis of COL6-RD, using collagen VI immunofluorescence microscopy images from patient-derived fibroblast cultures. Our solution resulted in an ML model capable of classifying collagen VI patient images into the three primary pathogenic mechanism groups associated with COL6-RD: exon skipping, glycine substitution, and pseudoexon insertion. This new approach achieved an F1-score of 0.82, outperforming single-organization models (0.57-0.75). These results demonstrate that FL substantially improves diagnostic utility and generalizability compared to isolated institutional models. Beyond enabling more accurate diagnosis, we anticipate that this approach will support the interpretation of variants of uncertain significance and guide the prioritization of sequencing strategies to identify novel pathogenic variants.",
        "authors": [
            "Astrid Brull",
            "Sara Aguti",
            "Véronique Bolduc",
            "Ying Hu",
            "Daniel M. Jimenez-Gutierrez",
            "Enrique Zuazua",
            "Joaquin Del-Rio",
            "Oleksii Sliusarenko",
            "Haiyan Zhou",
            "Francesco Muntoni",
            "Carsten G. Bönnemann",
            "Xabi Uribe-Etxebarria"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.16876v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-18"
    },
    "http://arxiv.org/abs/2512.16813v1": {
        "title": "Coordinated Anti-Jamming Resilience in Swarm Networks via Multi-Agent Reinforcement Learning",
        "link": "http://arxiv.org/abs/2512.16813v1",
        "abstract": "Reactive jammers pose a severe security threat to robotic-swarm networks by selectively disrupting inter-agent communications and undermining formation integrity and mission success. Conventional countermeasures such as fixed power control or static channel hopping are largely ineffective against such adaptive adversaries. This paper presents a multi-agent reinforcement learning (MARL) framework based on the QMIX algorithm to improve the resilience of swarm communications under reactive jamming. We consider a network of multiple transmitter-receiver pairs sharing channels while a reactive jammer with Markovian threshold dynamics senses aggregate power and reacts accordingly. Each agent jointly selects transmit frequency (channel) and power, and QMIX learns a centralized but factorizable action-value function that enables coordinated yet decentralized execution. We benchmark QMIX against a genie-aided optimal policy in a no-channel-reuse setting, and against local Upper Confidence Bound (UCB) and a stateless reactive policy in a more general fading regime with channel reuse enabled. Simulation results show that QMIX rapidly converges to cooperative policies that nearly match the genie-aided bound, while achieving higher throughput and lower jamming incidence than the baselines, thereby demonstrating MARL's effectiveness for securing autonomous swarms in contested environments.",
        "authors": [
            "Bahman Abolhassani",
            "Tugba Erpek",
            "Kemal Davaslioglu",
            "Yalin E. Sagduyu",
            "Sastry Kompella"
        ],
        "categories": [
            "cs.NI",
            "cs.AI",
            "cs.DC",
            "cs.LG",
            "eess.SP"
        ],
        "id": "http://arxiv.org/abs/2512.16813v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-18"
    },
    "http://arxiv.org/abs/2512.16792v1": {
        "id": "http://arxiv.org/abs/2512.16792v1",
        "title": "Delay-Aware Multi-Stage Edge Server Upgrade with Budget Constraint",
        "link": "http://arxiv.org/abs/2512.16792v1",
        "tags": [
            "edge",
            "offloading",
            "training"
        ],
        "relevant": true,
        "indexed_date": "2025-12-18",
        "tldr": "Proposes M-ESU algorithm for multi-stage edge server upgrade and task offloading under budget constraints. Optimizes deployment/upgrade decisions and offloading to maximize tasks meeting delay requirements. Achieves 21.57% higher task satisfaction with efficient heuristic for large networks.",
        "abstract": "In this paper, the Multi-stage Edge Server Upgrade (M-ESU) is proposed as a new network planning problem, involving the upgrading of an existing multi-access edge computing (MEC) system through multiple stages (e.g., over several years). More precisely, the problem considers two key decisions: (i) whether to deploy additional edge servers or upgrade those already installed, and (ii) how tasks should be offloaded so that the average number of tasks that meet their delay requirement is maximized. The framework specifically involves: (i) deployment of new servers combined with capacity upgrades for existing servers, and (ii) the optimal task offloading to maximize the average number of tasks with a delay requirement. It also considers the following constraints: (i) budget per stage, (ii) server deployment and upgrade cost (in $) and cost depreciation rate, (iii) computation resource of servers, (iv) number of tasks and their growth rate (in %), and (v) the increase in task sizes and stricter delay requirements over time. We present two solutions: a Mixed Integer Linear Programming (MILP) model and an efficient heuristic algorithm (M-ESU/H). MILP yields the optimal solution for small networks, whereas M-ESU/H is used in large-scale networks. For small networks, the simulation results show that the solution computed by M-ESU/H is within 1.25% of the optimal solution while running several orders of magnitude faster. For large networks, M-ESU/H is compared against three alternative heuristic solutions that consider only server deployment, or giving priority to server deployment or upgrade. Our experiments show that M-ESU/H yields up to 21.57% improvement in task satisfaction under identical budget and demand growth conditions, confirming its scalability and practical value for long-term MEC systems.",
        "authors": [
            "Endar Suprih Wihidayat",
            "Sieteng Soh",
            "Kwan-Wu Chin",
            "Duc-son Pham"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-12-18"
    },
    "http://arxiv.org/abs/2512.16683v2": {
        "title": "Efficient Bitcoin Meta-Protocol Transaction and Data Discovery Through nLockTime Field Repurposing",
        "link": "http://arxiv.org/abs/2512.16683v2",
        "abstract": "We describe the Lockchain Protocol, a lightweight Bitcoin meta-protocol that enables highly efficient transaction discovery at zero marginal block space cost, and data verification without introducing any new on-chain storage mechanism. The protocol repurposes the mandatory 4-byte nLockTime field of every Bitcoin transaction as a compact metadata header. By constraining values to an unused range of past Unix timestamps greater than or equal to 500,000,000, the field can encode a protocol signal, type, variant, and sequence identifier while remaining fully valid under Bitcoin consensus and policy rules. The primary contribution of the protocol is an efficient discovery layer. Indexers can filter candidate transactions by examining a fixed-size header field, independent of transaction payload size, and only then selectively inspect heavier data such as OP RETURN outputs or witness fields. The Lockchain Protocol applies established protocol design patterns to an under-optimised problem domain, namely transaction discovery at scale, and does not claim new cryptographic primitives or storage methods.",
        "authors": [
            "Nikodem Tomczak"
        ],
        "categories": [
            "cs.CR",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.16683v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-18"
    },
    "http://arxiv.org/abs/2512.16473v1": {
        "id": "http://arxiv.org/abs/2512.16473v1",
        "title": "Efficient CPU-GPU Collaborative Inference for MoE-based LLMs on Memory-Limited Systems",
        "link": "http://arxiv.org/abs/2512.16473v1",
        "tags": [
            "MoE",
            "offloading",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-12-18",
        "tldr": "Proposes a CPU-GPU collaborative inference framework for memory-limited systems running MoE-based LLMs. Uses expert caching on GPU and CPU offloading with multithreading to minimize data transfer. Achieves faster inference with reduced latency compared to traditional offloading methods.",
        "abstract": "Large Language Models (LLMs) have achieved impressive results across various tasks, yet their high computational demands pose deployment challenges, especially on consumer-grade hardware. Mixture of Experts (MoE) models provide an efficient solution through selective activation of parameter subsets, which reduces computation requirements. Despite this efficiency, state-of-the-art MoE models still require substantial memory beyond typical consumer GPU capacities. Traditional offloading methods that transfer model weights between CPU and GPU introduce latency, limiting inference performance. This paper presents a novel CPU-GPU collaborative inference framework that incorporates an expert caching mechanism on the GPU to reduce data transfer requirements and enable faster inference through cache hits. Computations are offloaded to CPU for efficient cache miss handling, which benefits from CPU multithreading optimizations. The evaluations of our framework demonstrate performance improvements and highlight the potential of CPU-GPU collaboration to maximize hardware utilization for single-request inference scenarios on consumer-grade systems. The implementation of our framework is available at https://github.com/elsa-lab/MoE-CPU-GPU-Collaborative-Inference.",
        "authors": [
            "En-Ming Huang",
            "Li-Shang Lin",
            "Chun-Yi Lee"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-12-18"
    },
    "http://arxiv.org/abs/2512.16455v1": {
        "id": "http://arxiv.org/abs/2512.16455v1",
        "title": "AI4EOSC: a Federated Cloud Platform for Artificial Intelligence in Scientific Research",
        "link": "http://arxiv.org/abs/2512.16455v1",
        "tags": [
            "training",
            "offline",
            "storage"
        ],
        "relevant": true,
        "indexed_date": "2025-12-18",
        "tldr": "Presents a federated cloud platform for AI in science, offering integrated tools for the ML lifecycle including distributed GPU training, model deployment, and storage resources. Achieves reproducible deployments across distributed infrastructures with unified service catalog.",
        "abstract": "In this paper, we describe a federated compute platform dedicated to support Artificial Intelligence in scientific workloads. Putting the effort into reproducible deployments, it delivers consistent, transparent access to a federation of physically distributed e-Infrastructures. Through a comprehensive service catalogue, the platform is able to offer an integrated user experience covering the full Machine Learning lifecycle, including model development (with dedicated interactive development environments), training (with GPU resources, annotation tools, experiment tracking, and federated learning support) and deployment (covering a wide range of deployment options all along the Cloud Continuum). The platform also provides tools for traceability and reproducibility of AI models, integrates with different Artificial Intelligence model providers, datasets and storage resources, allowing users to interact with the broader Machine Learning ecosystem. Finally, it is easily customizable to lower the adoption barrier by external communities.",
        "authors": [
            "Ignacio Heredia",
            "Álvaro López García",
            "Germán Moltó",
            "Amanda Calatrava",
            "Valentin Kozlov",
            "Alessandro Costantini",
            "Viet Tran",
            "Mario David",
            "Daniel San Martín",
            "Marcin Płóciennik",
            "Marta Obregón Ruiz",
            "Saúl Fernandez",
            "Judith Sáinz-Pardo Díaz",
            "Miguel Caballer",
            "Caterina Alarcón Marín",
            "Stefan Dlugolinsky",
            "Martin Šeleng",
            "Lisana Berberi",
            "Khadijeh Alibabaei",
            "Borja Esteban Sanchis",
            "Pedro Castro",
            "Giacinto Donvito",
            "Diego Aguirre",
            "Sergio Langarita",
            "Vicente Rodriguez",
            "Leonhard Duda",
            "Andrés Heredia Canales",
            "Susana Rebolledo Ruiz",
            "João Machado",
            "Giang Nguyen",
            "Fernando Aguilar Gómez",
            "Jaime Díez"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-12-18"
    },
    "http://arxiv.org/abs/2512.16391v1": {
        "id": "http://arxiv.org/abs/2512.16391v1",
        "title": "Kascade: A Practical Sparse Attention Method for Long-Context LLM Inference",
        "link": "http://arxiv.org/abs/2512.16391v1",
        "tags": [
            "serving",
            "kernel",
            "sparse"
        ],
        "relevant": true,
        "indexed_date": "2025-12-18",
        "tldr": "Proposes Kascade, a training-free sparse attention method for long-context LLM inference. It reuses top-k indices across anchor and reuse layers with head-aware selection, optimizing tile-level operations. Achieves up to 4.1x decode attention speedup over FlashAttention-3 on H100 GPUs with minimal accuracy loss.",
        "abstract": "Attention is the dominant source of latency during long-context LLM inference, an increasingly popular workload with reasoning models and RAG. We propose Kascade, a training-free sparse attention method that leverages known observations such as 1) post-softmax attention is intrinsically sparse, and 2) the identity of high-weight keys is stable across nearby layers. Kascade computes exact Top-k indices in a small set of anchor layers, then reuses those indices in intermediate reuse layers. The anchor layers are selected algorithmically, via a dynamic-programming objective that maximizes cross-layer similarity over a development set, allowing easy deployment across models. The method incorporates efficient implementation constraints (e.g. tile-level operations), across both prefill and decode attention. The Top-k selection and reuse in Kascade is head-aware and we show in our experiments that this is critical for high accuracy. Kascade achieves up to 4.1x speedup in decode attention and 2.2x speedup in prefill attention over FlashAttention-3 baseline on H100 GPUs while closely matching dense attention accuracy on long-context benchmarks such as LongBench and AIME-24.",
        "authors": [
            "Dhruv Deshmukh",
            "Saurabh Goyal",
            "Nipun Kwatra",
            "Ramachandran Ramjee"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC"
        ],
        "submit_date": "2025-12-18"
    },
    "http://arxiv.org/abs/2512.16148v1": {
        "id": "http://arxiv.org/abs/2512.16148v1",
        "title": "FlexKV: Flexible Index Offloading for Memory-Disaggregated Key-Value Store",
        "link": "http://arxiv.org/abs/2512.16148v1",
        "tags": [
            "offloading",
            "storage",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-12-20",
        "tldr": "Proposes FlexKV, a memory-disaggregated key-value store with index proxying to address poor performance. Dynamically offloads index to compute nodes with load balancing, memory optimization, and RPC-aggregated cache coherence. Achieves up to 2.94× higher throughput and 85.2% lower latency.",
        "abstract": "Disaggregated memory (DM) is a promising data center architecture that decouples CPU and memory into independent resource pools to improve resource utilization. Building on DM, memory-disaggregated key-value (KV) stores are adopted to efficiently manage remote data. Unfortunately, existing approaches suffer from poor performance due to two critical issues: 1) the overdependence on one-sided atomic operations in index processing, and 2) the constrained efficiency in compute-side caches. To address these issues, we propose FlexKV, a memory-disaggregated KV store with index proxying. Our key idea is to dynamically offload the index to compute nodes, leveraging their powerful CPUs to accelerate index processing and maintain high-performance compute-side caches. Three challenges have to be addressed to enable efficient index proxying on DM, i.e., the load imbalance across compute nodes, the limited memory of compute nodes, and the expensive cache coherence overhead. FlexKV proposes: 1) a rank-aware hotness detection algorithm to continuously balance index load across compute nodes, 2) a two-level CN memory optimization scheme to efficiently utilize compute node memory, and 3) an RPC-aggregated cache management mechanism to reduce cache coherence overhead. The experimental results show that FlexKV improves throughput by up to 2.94$\\times$ and reduces latency by up to 85.2%, compared with the state-of-the-art memory-disaggregated KV stores.",
        "authors": [
            "Zhisheng Hu",
            "Jiacheng Shen",
            "Ming-Chang Yang"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-12-18"
    },
    "http://arxiv.org/abs/2512.16146v1": {
        "title": "Analysis of Design Patterns and Benchmark Practices in Apache Kafka Event-Streaming Systems",
        "link": "http://arxiv.org/abs/2512.16146v1",
        "abstract": "Apache Kafka has become a foundational platform for high throughput event streaming, enabling real time analytics, financial transaction processing, industrial telemetry, and large scale data driven systems. Despite its maturity and widespread adoption, consolidated research on reusable architectural design patterns and reproducible benchmarking methodologies remains fragmented across academic and industrial publications. This paper presents a structured synthesis of forty two peer reviewed studies published between 2015 and 2025, identifying nine recurring Kafka design patterns including log compaction, CQRS bus, exactly once pipelines, change data capture, stream table joins, saga orchestration, tiered storage, multi tenant topics, and event sourcing replay. The analysis examines co usage trends, domain specific deployments, and empirical benchmarking practices using standard suites such as TPCx Kafka and the Yahoo Streaming Benchmark, as well as custom workloads. The study highlights significant inconsistencies in configuration disclosure, evaluation rigor, and reproducibility that limit cross study comparison and practical replication. By providing a unified taxonomy, pattern benchmark matrix, and actionable decision heuristics, this work offers practical guidance for architects and researchers designing reproducible, high performance, and fault tolerant Kafka based event streaming systems.",
        "authors": [
            "Muzeeb Mohammad"
        ],
        "categories": [
            "cs.SE",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.16146v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-18"
    },
    "http://arxiv.org/abs/2512.16136v1": {
        "id": "http://arxiv.org/abs/2512.16136v1",
        "title": "Lotus: Optimizing Disaggregated Transactions with Disaggregated Locks",
        "link": "http://arxiv.org/abs/2512.16136v1",
        "tags": [
            "storage",
            "networking",
            "hardware"
        ],
        "relevant": true,
        "indexed_date": "2025-12-18",
        "tldr": "Addresses the bottleneck of RDMA NICs in disaggregated memory transaction systems by disaggregating locks to compute nodes. Introduces Lotus with application-aware lock sharding, lock-first protocol, and lock-rebuild-free recovery. Achieves 2.1× higher throughput and 49.4% lower latency.",
        "abstract": "Disaggregated memory (DM) separates compute and memory resources, allowing flexible scaling to achieve high resource utilization. To ensure atomic and consistent data access on DM, distributed transaction systems have been adapted, where compute nodes (CNs) rely on one-sided RDMA operations to access remote data in memory nodes (MNs). However, we observe that in existing transaction systems, the RDMA network interface cards at MNs become a primary performance bottleneck. This bottleneck arises from the high volume of one-sided atomic operations used for locks, which hinders the system's ability to scale efficiently.   To address this issue, this paper presents Lotus, a scalable distributed transaction system with lock disaggregation on DM. The key innovation of Lotus is to disaggregate locks from data and execute all locks on CNs, thus eliminating the bottleneck at MN RNICs. To achieve efficient lock management on CNs, Lotus employs an application-aware lock management mechanism that leverages the locality of the OLTP workloads to shard locks while maintaining load balance. To ensure consistent transaction processing with lock disaggregation, Lotus introduces a lock-first transaction protocol, which separates the locking phase as the first step in each read-write transaction execution. This protocol allows the system to determine the success of lock acquisitions early and proactively abort conflicting transactions, improving overall efficiency. To tolerate lock loss during CN failures, Lotus employs a lock-rebuild-free recovery mechanism that treats locks as ephemeral and avoids their reconstruction, ensuring lightweight recovery for CN failures. Experimental results demonstrate that Lotus improves transaction throughput by up to 2.1$\\times$ and reduces latency by up to 49.4% compared to state-of-the-art transaction systems on DM.",
        "authors": [
            "Zhisheng Hu",
            "Pengfei Zuo",
            "Junliang Hu",
            "Yizou Chen",
            "Yingjia Wang",
            "Ming-Chang Yang"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-12-18"
    },
    "http://arxiv.org/abs/2512.16134v1": {
        "id": "http://arxiv.org/abs/2512.16134v1",
        "title": "Staggered Batch Scheduling: Co-optimizing Time-to-First-Token and Throughput for High-Efficiency LLM Inference",
        "link": "http://arxiv.org/abs/2512.16134v1",
        "tags": [
            "serving",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-12-18",
        "tldr": "Optimizes scheduling in P/D-separated LLM inference systems to reduce Time-to-First-Token (TTFT) and boost throughput. Proposes Staggered Batch Scheduling (SBS) with Load-Aware Global Allocation to eliminate queuing bubbles and balance DP load. Achieves 30-40% TTFT reduction and 15-20% throughput gain.",
        "abstract": "The evolution of Large Language Model (LLM) serving towards complex, distributed architectures--specifically the P/D-separated, large-scale DP+EP paradigm--introduces distinct scheduling challenges. Unlike traditional deployments where schedulers can treat instances as black boxes, DP+EP architectures exhibit high internal synchronization costs. We identify that immediate request dispatching in such systems leads to severe in-engine queuing and parallelization bubbles, degrading Time-to-First-Token (TTFT). To address this, we propose Staggered Batch Scheduling (SBS), a mechanism that deliberately buffers requests to form optimal execution batches. This temporal decoupling eliminates internal queuing bubbles without compromising throughput. Furthermore, leveraging the scheduling window created by buffering, we introduce a Load-Aware Global Allocation strategy that balances computational load across DP units for both Prefill and Decode phases. Deployed on a production H800 cluster serving Deepseek-V3, our system reduces TTFT by 30%-40% and improves throughput by 15%-20% compared to state-of-the-art immediate scheduling baselines.",
        "authors": [
            "Jian Tian",
            "Shuailong Li",
            "Yang Cao",
            "Wenbo Cui",
            "Minghan Zhu",
            "Wenkang Wu",
            "Jianming Zhang",
            "Yanpeng Wang",
            "Zhiwen Xiao",
            "Zhenyu Hou",
            "Dou Shen"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-12-18"
    },
    "http://arxiv.org/abs/2512.16099v1": {
        "id": "http://arxiv.org/abs/2512.16099v1",
        "title": "An Online Fragmentation-Aware Scheduler for Managing GPU-Sharing Workloads on Multi-Instance GPUs",
        "link": "http://arxiv.org/abs/2512.16099v1",
        "tags": [
            "serving",
            "offline",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-12-18",
        "tldr": "Addresses GPU fragmentation and resource contention in MIG-enabled clusters. Proposes an online scheduler integrating load balancing, dynamic partitioning, and migration for efficient GPU-sharing. Achieves up to 35% makespan improvement.",
        "abstract": "Modern GPU workloads increasingly demand efficient resource sharing, as many jobs do not require the full capacity of a GPU. Among sharing techniques, NVIDIA's Multi-Instance GPU (MIG) offers strong resource isolation by enabling hardware-level GPU partitioning. However, leveraging MIG effectively introduces new challenges. First, resource contention persists due to shared components such as PCIe bandwidth. Second, GPU fragmentation becomes a critical issue, which is different from prior fine-grained GPU sharing work due to MIG's limited number of valid MIG configurations. Fragmentation arises not only from spatial discontinuity but also from rigid profile placement constraints, especially after job arrivals and terminations. To address these issues, we propose an online scheduling framework that integrates conditional load balancing, dynamic partitioning, and job migration. Our approach dynamically adapts job placement to minimize contention and reorganizes GPU allocations to combat both internal and external fragmentation. Experimental results show that our method significantly improves system efficiency. When all techniques are applied, the makespan improves by up to 35%.",
        "authors": [
            "Hsu-Tzu Ting",
            "Jerry Chou",
            "Ming-Hung Chen",
            "I-Hsin Chung"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-12-18"
    },
    "http://arxiv.org/abs/2512.16066v1": {
        "title": "Cold-Start Anti-Patterns and Refactorings in Serverless Systems: An Empirical Study",
        "link": "http://arxiv.org/abs/2512.16066v1",
        "abstract": "Serverless computing simplifies deployment and scaling, yet cold-start latency remains a major performance bottleneck. Unlike prior work that treats mitigation as a black-box optimization, we study cold starts as a developer-visible design problem. From 81 adjudicated issue reports across open-source serverless systems, we derive taxonomies of initialization anti-patterns, remediation strategies, and diagnostic challenges spanning design, packaging, and runtime layers. Building on these insights, we introduce SCABENCH, a reproducible benchmark, and INITSCOPE, a lightweight analysis framework linking what code is loaded with what is executed. On SCABENCH, INITSCOPE improved localization accuracy by up to 40% and reduced diagnostic effort by 64% compared with prior tools, while a developer study showed higher task accuracy and faster diagnosis. Together, these results advance evidence-driven, performance-aware practices for cold-start mitigation in serverless design. Availability: The research artifact is publicly accessible for future studies and improvements.",
        "authors": [
            "Syed Salauddin Mohammad Tariq",
            "Foyzul Hassan",
            "Amiangshu Bosu",
            "Probir Roy"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.16066v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-18"
    },
    "http://arxiv.org/abs/2512.16058v1": {
        "title": "Twinning for Space-Air-Ground-Sea Integrated Networks: Beyond Conventional Digital Twin Towards Goal-Oriented Semantic Twin",
        "link": "http://arxiv.org/abs/2512.16058v1",
        "abstract": "A space-air-ground-sea integrated network (SAGSIN) has emerged as a cornerstone of 6G systems, establishing a unified global architecture by integrating multi-domain network resources. Motivated by the demand for real-time situational awareness and intelligent operational maintenance, digital twin (DT) technology was initially regarded as a promising solution, owing to its capability to create virtual replicas and emulate physical system behaviors. However, in the context of SAGSIN, the high-fidelity, full-scale modeling paradigm inherent to conventional DTs encounters fundamental limitations, including prohibitive computational overhead, delayed model synchronization, and cross-system semantic gaps. To address these limitations, this survey paper proposes a novel twinning framework: goal-oriented semantic twin (GOST). Unlike DTs that pursue physical mirroring, GOST prioritizes ``utility'' over ``fidelity,'' leveraging semantic technologies and goal-oriented principles to construct lightweight, task-specific representations. This paper systematically articulates the GOST framework through three layers: knowledge-based semantics, data-driven semantics, and goal-oriented principles. Furthermore, we provide a comprehensive tutorial on constructing GOST by detailing its core enabling technologies and introduce a multidimensional evaluation framework for GOST. We present a case study targeting collaborative tracking tasks in remote satellite-UAV networks, demonstrating that GOST significantly outperforms conventional DTs in timeliness of perceptual data and collaborative tracking. Finally, we outline research directions, establishing GOST as a transformative twinning paradigm to guide the development of SAGSIN.",
        "authors": [
            "Yifei Qiu",
            "Tianle Liao",
            "Xin Jin",
            "Shaohua Wu",
            "Dusit Niyato",
            "Qinyu Zhang"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.16058v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-18"
    },
    "http://arxiv.org/abs/2512.16056v1": {
        "id": "http://arxiv.org/abs/2512.16056v1",
        "title": "MultiPath Transfer Engine: Breaking GPU and Host-Memory Bandwidth Bottlenecks in LLM Services",
        "link": "http://arxiv.org/abs/2512.16056v1",
        "tags": [
            "serving",
            "offloading",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-12-18",
        "tldr": "Proposes Multipath Memory Access (MMA) to overcome PCIe bandwidth bottlenecks for GPU-host data transfer in LLM serving. Utilizes dynamic library injection for deployment transparency. Achieves up to 4.62x higher bandwidth and reduces TTFT by up to 2.38x in vLLM.",
        "abstract": "The limited bandwidth of PCIe has emerged as the critical bottleneck for large language model (LLM) performance, such as prefix cache fetching and model switching. Although intra-server multipath data transfer between GPU and host memory is theoretically possible, heterogeneous protocols such as PCIe and NVLink currently limit the bandwidth between host memory and GPUs to that of a single PICe link. This limitation resuals in underutilized intra-server bandwidth. To address this issue, we propose Multipath Memory Access (MMA), a scheme that, to the best of our knowledge, is the first to enalbe efficient multipath data transfer between GPU and host memory. MMA supports seamless deployment via dynamic library injection, enabling LLM applications to benefit from MMA without requiring any code modification. In our testbed, MMA significantly improves the data transfer bandwidth between the GPU and memory, achieving a peak bandwidth of 245 GB/s-representing a 4.62x speedup compared to the natice single-path bandwidth. End-to-end evaluations demonstrate that MMA reduces the time-to-first-token (TTFT) for LLM serving by 1.14x to 2.38x and decreases model-switching latency in vLLM's sleep mode by 1.12x to 2.48x.",
        "authors": [
            "Lingfeng Tang",
            "Daoping Zhang",
            "Junjie Chen",
            "Peihao Huang",
            "Feng Jin",
            "Chengguang Xu",
            "Yuxin Chen",
            "Feiqiang Sun",
            "Guo Chen"
        ],
        "categories": [
            "cs.DC",
            "cs.NI",
            "cs.PF"
        ],
        "submit_date": "2025-12-18"
    },
    "http://arxiv.org/abs/2512.16038v1": {
        "id": "http://arxiv.org/abs/2512.16038v1",
        "title": "LOG.io: Unified Rollback Recovery and Data Lineage Capture for Distributed Data Pipelines",
        "link": "http://arxiv.org/abs/2512.16038v1",
        "tags": [
            "training",
            "storage",
            "offline"
        ],
        "relevant": true,
        "indexed_date": "2025-12-20",
        "tldr": "Introduces LOG.io, a log-based system for rollback recovery and data lineage in distributed data pipelines. It supports non-deterministic operators and dynamic scaling, with non-blocking recovery. Achieves marginal overhead (≤1.5%) for lineage capture and outperforms ABS in straggler scenarios.",
        "abstract": "This paper introduces LOG.io, a comprehensive solution designed for correct rollback recovery and fine-grain data lineage capture in distributed data pipelines. It is tailored for serverless scalable architectures and uses a log-based rollback recovery protocol. LOG.io supports a general programming model, accommodating non-deterministic operators, interactions with external systems, and arbitrary custom code. It is non-blocking, allowing failed operators to recover independently without interrupting other active operators, thereby leveraging data parallelization, and it facilitates dynamic scaling of operators during pipeline execution. Performance evaluations, conducted within the SAP Data Intelligence system, compare LOG.io with the Asynchronous Barrier Snapshotting (ABS) protocol, originally implemented in Flink. Our experiments show that when there are straggler operators in a data pipeline and the throughput of events is moderate (e.g., 1 event every 100 ms), LOG.io performs as well as ABS during normal processing and outperforms ABS during recovery. Otherwise, ABS performs better than LOG.io for both normal processing and recovery. However, we show that in these cases, data parallelization can largely reduce the overhead of LOG.io while ABS does not improve. Finally, we show that the overhead of data lineage capture, at the granularity of the event and between any two operators in a pipeline, is marginal, with less than 1.5% in all our experiments.",
        "authors": [
            "Eric Simon",
            "Renato B. Hoffmann",
            "Lucas Alf",
            "Dalvan Griebler"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-12-17"
    },
    "http://arxiv.org/abs/2512.15915v1": {
        "title": "Private Virtual Tree Networks for Secure Multi-Tenant Environments Based on the VIRGO Overlay Network",
        "link": "http://arxiv.org/abs/2512.15915v1",
        "abstract": "Hierarchical organization is a fundamental structure in real-world society, where authority and responsibility are delegated from managers to subordinates. The VIRGO network (Virtual Hierarchical Overlay Network for scalable grid computing) provides a scalable overlay for organizing distributed systems but lacks intrinsic security and privacy mechanisms. This paper proposes Private Virtual Tree Networks (PVTNs), a cryptographically enforced extension that leverages the VIRGO overlay to mirror real organizational hierarchies. In PVTNs, join requests are encrypted with the manager's public key to ensure confidentiality, while membership authorization is enforced through manager-signed delegation certificates. Public keys are treated as organizational secrets and are disclosed only within direct manager-member relationships, resulting in a private, non-enumerable virtual tree. Our work demonstrates, through the system model, protocols, security analysis, and design rationale, that PVTNs achieve scalability, dynamic management, and strong security guarantees without relying on global public key infrastructures.",
        "authors": [
            "Lican Huang"
        ],
        "categories": [
            "cs.CR",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.15915v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-17"
    },
    "http://arxiv.org/abs/2512.15705v1": {
        "id": "http://arxiv.org/abs/2512.15705v1",
        "title": "Dynamic Rebatching for Efficient Early-Exit Inference with DREX",
        "link": "http://arxiv.org/abs/2512.15705v1",
        "tags": [
            "serving",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-12-17",
        "tldr": "Addresses inefficiency in Early-Exit LLM inference due to static batching. Proposes DREX system with copy-free dynamic rebatching and scheduler to optimally regroup requests at exit points. Achieves 2-12% higher throughput while eliminating involuntary exits and preserving quality.",
        "abstract": "Early-Exit (EE) is a Large Language Model (LLM) architecture that accelerates inference by allowing easier tokens to be generated using only a subset of the model's layers. However, traditional batching frameworks are ill-suited for EE LLMs, as not all requests in a batch may be ready to exit at the same time. Existing solutions either force a uniform decision on the batch, which overlooks EE opportunities, or degrade output quality by forcing premature exits. We propose Dynamic Rebatching, a solution where we dynamically reorganize the batch at each early-exit point. Requests that meet the exit criteria are immediately processed, while those that continue are held in a buffer, re-grouped into a new batch, and forwarded to deeper layers. We introduce DREX, an early-exit inference system that implements Dynamic Rebatching with two key optimizations: 1) a copy-free rebatching buffer that avoids physical data movement, and 2) an EE and SLA-aware scheduler that analytically predicts whether a given rebatching operation will be profitable. DREX also efficiently handles the missing KV cache from skipped layers using memory-efficient state-copying. Our evaluation shows that DREX improves throughput by 2-12% compared to baseline approaches while maintaining output quality. Crucially, DREX completely eliminates involuntary exits, providing a key guarantee for preserving the output quality intended by the EE model.",
        "authors": [
            "Xuting Liu",
            "Daniel Alexander",
            "Siva Kesava Reddy Kakarla",
            "Behnaz Arzani",
            "Vincent Liu"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-12-17"
    },
    "http://arxiv.org/abs/2512.15834v1": {
        "id": "http://arxiv.org/abs/2512.15834v1",
        "title": "Optimizing Agentic Language Model Inference via Speculative Tool Calls",
        "link": "http://arxiv.org/abs/2512.15834v1",
        "tags": [
            "offline",
            "agentic",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-12-20",
        "tldr": "Addresses performance bottlenecks in tool-using agentic LMs via speculative tool calls and sequence residency. Optimizations include speculative execution and tool caching to reduce inference overheads. Achieves hundreds of tokens per second throughput improvement over baselines.",
        "abstract": "Language models (LMs) are becoming increasingly dependent on external tools. LM-based agentic frameworks frequently interact with their environment via such tools to search files, run code, call APIs, etc. Further, modern reasoning-based LMs use tools such as web search and Python code execution to enhance their reasoning capabilities. While tools greatly improve the capabilities of LMs, they also introduce performance bottlenecks during the inference process. In this paper, we introduce novel systems optimizations to address such performance bottlenecks by speculating tool calls and forcing sequences to remain resident in the inference engine to minimize overheads. Our optimizations lead to throughput improvements of several hundred tokens per second when hosting inference for LM agents. We provide a theoretical analysis of our algorithms to provide insights into speculation configurations that will yield the best performance. Further, we recommend a new \"tool cache\" API endpoint to enable LM providers to easily adopt these optimizations.",
        "authors": [
            "Daniel Nichols",
            "Prajwal Singhania",
            "Charles Jekel",
            "Abhinav Bhatele",
            "Harshitha Menon"
        ],
        "categories": [
            "cs.PL",
            "cs.AI",
            "cs.DC",
            "cs.PF",
            "cs.SE"
        ],
        "submit_date": "2025-12-17"
    },
    "http://arxiv.org/abs/2512.15659v2": {
        "title": "LeaseGuard: Raft Leases Done Right",
        "link": "http://arxiv.org/abs/2512.15659v2",
        "abstract": "Raft is a leading consensus algorithm for replicating writes in distributed databases. However, distributed databases also require consistent reads. To guarantee read consistency, a Raft-based system must either accept the high communication overhead of a safety check for each read, or implement leader leases. Prior lease protocols are vaguely specified and hurt availability, so most Raft systems implement them incorrectly or not at all. We introduce LeaseGuard, a novel lease algorithm that relies on guarantees specific to Raft elections. LeaseGuard is simple, rigorously specified in TLA+, and includes two novel optimizations that maximize availability during leader failover. The first optimization restores write throughput quickly, and the second improves read availability. We evaluate LeaseGuard with a simulation in Python and an implementation in LogCabin, the C++ reference implementation of Raft. By replacing LogCabin's default consistency mechanism (quorum checks), LeaseGuard reduces the overhead of consistent reads from one to zero network roundtrips. It also improves write throughput from ~1000 to ~10,000 writes per second, by eliminating contention between writes and quorum reads. Whereas traditional leases ban all reads on a new leader while it waits for a lease, in our LeaseGuard test the new leader instantly allows 99% of reads to succeed.",
        "authors": [
            "A. Jesse Jiryu Davis",
            "Murat Demirbas",
            "Lingzhi Deng"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.15659v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-17"
    },
    "http://arxiv.org/abs/2512.15595v1": {
        "id": "http://arxiv.org/abs/2512.15595v1",
        "title": "Optimizing Bloom Filters for Modern GPU Architectures",
        "link": "http://arxiv.org/abs/2512.15595v1",
        "tags": [
            "serving",
            "kernel",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-12-17",
        "tldr": "Explores GPU-optimized Bloom filters to accelerate approximate membership queries. Introduces designs leveraging vectorization, thread cooperation, and compute latency optimization. Achieves up to 15.4× faster construction and 92% of practical speed-of-light throughput on B200 GPU.",
        "abstract": "Bloom filters are a fundamental data structure for approximate membership queries, with applications ranging from data analytics to databases and genomics. Several variants have been proposed to accommodate parallel architectures. GPUs, with massive thread-level parallelism and high-bandwidth memory, are a natural fit for accelerating these Bloom filter variants potentially to billions of operations per second. Although CPU-optimized implementations have been well studied, GPU designs remain underexplored. We close this gap by exploring the design space on GPUs along three dimensions: vectorization, thread cooperation, and compute latency.   Our evaluation shows that the combination of these optimization points strongly affects throughput, with the largest gains achieved when the filter fits within the GPU's cache domain. We examine how the hardware responds to different parameter configurations and relate these observations to measured performance trends. Crucially, our optimized design overcomes the conventional trade-off between speed and precision, delivering the throughput typically restricted to high-error variants while maintaining the superior accuracy of high-precision configurations. At iso error rate, the proposed method outperforms the state-of-the-art by $11.35\\times$ ($15.4\\times$) for bulk filter lookup (construction), respectively, achieving above $92\\%$ of the practical speed-of-light across a wide range of configurations on a B200 GPU. We propose a modular CUDA/C++ implementation, which will be openly available soon.",
        "authors": [
            "Daniel Jünger",
            "Kevin Kristensen",
            "Yunsong Wang",
            "Xiangyao Yu",
            "Bertil Schmidt"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-12-17"
    },
    "http://arxiv.org/abs/2512.15306v1": {
        "id": "http://arxiv.org/abs/2512.15306v1",
        "title": "LLMQ: Efficient Lower-Precision Pretraining for Consumer GPUs",
        "link": "http://arxiv.org/abs/2512.15306v1",
        "tags": [
            "training",
            "offloading",
            "quantization"
        ],
        "relevant": true,
        "indexed_date": "2025-12-17",
        "tldr": "Proposes LLMQ, a CUDA/C++ system for efficient 8-bit LLM training on consumer GPUs with limited memory. Uses activation checkpointing, weight offloading, and copy-engine collectives to handle memory and communication bottlenecks. Trains a 7B model on a single 16GB GPU and maintains 50% FLOP utilization.",
        "abstract": "We present LLMQ, an end-to-end CUDA/C++ implementation for medium-sized language-model training, e.g. 3B to 32B parameters, on affordable, commodity GPUs. These devices are characterized by low memory availability and slow communication compared to datacentre-grade GPUs. Consequently, we showcase a range of optimizations that target these bottlenecks, including activation checkpointing, offloading, and copy-engine based collectives. LLMQ is able to train or fine-tune a 7B model on a single 16GB mid-range gaming card, or a 32B model on a workstation equipped with 4 RTX 4090s. This is achieved while executing a standard 8-bit training pipeline, without additional algorithmic approximations, and maintaining FLOP utilization of around 50%. The efficiency of LLMQ rivals that of production-scale systems on much more expensive cloud-grade GPUs.",
        "authors": [
            "Erik Schultheis",
            "Dan Alistarh"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-12-17"
    },
    "http://arxiv.org/abs/2512.15028v1": {
        "id": "http://arxiv.org/abs/2512.15028v1",
        "title": "Reexamining Paradigms of End-to-End Data Movement",
        "link": "http://arxiv.org/abs/2512.15028v1",
        "tags": [
            "hardware",
            "storage",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-12-17",
        "tldr": "Examines end-to-end data movement bottlenecks beyond raw bandwidth. Proposes holistic hardware-software co-design addressing host CPU, virtualization, and congestion control. Achieves consistent performance across 1-100 Gbps links, reducing latency and throughput disparities in edge-to-core transfers.",
        "abstract": "The pursuit of high-performance data transfer often focuses on raw network bandwidth, and international links of 100 Gbps or higher are frequently considered the primary enabler. While necessary, this network-centric view is incomplete, equating provisioned link speeds with practical, sustainable data movement capabilities across the entire edge-to-core spectrum. This paper investigates six common paradigms, from the often-cited constraints of network latency and TCP congestion control algorithms to host-side factors such as CPU performance and virtualization that critically impact data movement workflows. We validated our findings using a latency-emulation-capable testbed for high-speed WAN performance prediction and through extensive production measurements from resource-constrained edge environments to a 100 Gbps operational link connecting Switzerland and California, U.S. These results show that the principal bottlenecks often reside outside the network core, and that a holistic hardware-software co-design ensures consistent performance, whether moving data at 1 Gbps or 100 Gbps and faster. This approach effectively closes the fidelity gap between benchmark results and diverse and complex production environments.",
        "authors": [
            "Chin Fang",
            "Timothy Stitt",
            "Michael J. McManus",
            "Toshio Moriya"
        ],
        "categories": [
            "cs.DC",
            "cs.NI",
            "cs.OS",
            "cs.PF"
        ],
        "submit_date": "2025-12-17"
    },
    "http://arxiv.org/abs/2512.14971v1": {
        "title": "Optimizing Sensor Node Localization for Achieving Sustainable Smart Agriculture System Connectivity",
        "link": "http://arxiv.org/abs/2512.14971v1",
        "abstract": "The innovative agriculture system is revolutionizing how we farm, making it one of the most critical innovations of our time! Yet it faces significant connectivity challenges, particularly with the sensors that power this technology. An efficient sensor deployment solution is still required to maximize the network's detection capabilities and efficiency while minimizing resource consumption and operational costs. This paper introduces an innovative sensor allocation optimization method that employs a Gradient-Based Iteration with Lagrange. The proposed method enhances coverage by utilizing a hybrid approach while minimizing the number of sensor nodes required under grid-based allocation. The proposed sensor distribution outperformed the classic deterministic deployment across coverage, number of sensors, cost, and power consumption. Furthermore, scalability is enhanced by extending sensing coverage to the remaining area via Bluetooth, which has a shorter communication range. Moreover, the proposed algorithm achieved 98.5% wireless sensor coverage, compared with 95% for the particle swarm distribution.",
        "authors": [
            "Mohamed Naeem"
        ],
        "categories": [
            "eess.SY",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.14971v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-16"
    },
    "http://arxiv.org/abs/2512.14628v1": {
        "id": "http://arxiv.org/abs/2512.14628v1",
        "title": "PruneX: A Hierarchical Communication-Efficient System for Distributed CNN Training with Structured Pruning",
        "link": "http://arxiv.org/abs/2512.14628v1",
        "tags": [
            "training",
            "sparse",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-12-20",
        "tldr": "Addresses high communication overhead in distributed CNN training. Proposes hierarchical structured pruning with buffer compaction for reduced inter-node transmissions. Reduces inter-node communication volume by 60% and achieves 6.75x speedup on ResNet at 64 GPUs.",
        "abstract": "Inter-node communication bandwidth increasingly constrains distributed training at scale on multi-node GPU clusters. While compact models are the ultimate deployment target, conventional pruning-aware distributed training systems typically fail to reduce communication overhead because unstructured sparsity cannot be efficiently exploited by highly optimized dense collective primitives. We present PruneX, a distributed data-parallel training system that co-designs pruning algorithms with cluster hierarchy to reduce inter-node bandwidth usage. PruneX introduces the Hierarchical Structured ADMM (H-SADMM) algorithm, which enforces node-level structured sparsity before inter-node synchronization, enabling dynamic buffer compaction that eliminates both zero-valued transmissions and indexing overhead. The system adopts a leader-follower execution model with separated intra-node and inter-node process groups, performing dense collectives on compacted tensors over bandwidth-limited links while confining full synchronization to high-bandwidth intra-node interconnects. Evaluation on ResNet architectures across 64 GPUs demonstrates that PruneX reduces inter-node communication volume by approximately 60% and achieves 6.75x strong scaling speedup, outperforming the dense baseline (5.81x) and Top-K gradient compression (3.71x) on the Puhti supercomputer at CSC - IT Center for Science (Finland).",
        "authors": [
            "Alireza Olama",
            "Andreas Lundell",
            "Izzat El Hajj",
            "Johan Lilius",
            "Jerker Björkqvist"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-12-16"
    },
    "http://arxiv.org/abs/2512.14522v1": {
        "title": "Improving Slow Transfer Predictions: Generative Methods Compared",
        "link": "http://arxiv.org/abs/2512.14522v1",
        "abstract": "Monitoring data transfer performance is a crucial task in scientific computing networks. By predicting performance early in the communication phase, potentially sluggish transfers can be identified and selectively monitored, optimizing network usage and overall performance. A key bottleneck to improving the predictive power of machine learning (ML) models in this context is the issue of class imbalance. This project focuses on addressing the class imbalance problem to enhance the accuracy of performance predictions. In this study, we analyze and compare various augmentation strategies, including traditional oversampling methods and generative techniques. Additionally, we adjust the class imbalance ratios in training datasets to evaluate their impact on model performance. While augmentation may improve performance, as the imbalance ratio increases, the performance does not significantly improve. We conclude that even the most advanced technique, such as CTGAN, does not significantly improve over simple stratified sampling.",
        "authors": [
            "Jacob Taegon Kim",
            "Alex Sim",
            "Kesheng Wu",
            "Jinoh Kim"
        ],
        "categories": [
            "cs.LG",
            "cs.DC",
            "cs.NI"
        ],
        "id": "http://arxiv.org/abs/2512.14522v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-16"
    },
    "http://arxiv.org/abs/2512.14445v1": {
        "id": "http://arxiv.org/abs/2512.14445v1",
        "title": "Performance and Stability of Barrier Mode Parallel Systems with Heterogeneous and Redundant Jobs",
        "link": "http://arxiv.org/abs/2512.14445v1",
        "tags": [
            "training",
            "sparse",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-12-16",
        "tldr": "Analyzes stability and performance overhead of barrier synchronization in parallel ML training systems. Models (s,k,l) barrier systems that allow partial task completion and hybrid workloads. Validated against Apache Spark, showing overhead from dual event/polling mechanisms with quantified performance bounds.",
        "abstract": "In some models of parallel computation, jobs are split into smaller tasks and can be executed completely asynchronously. In other situations the parallel tasks have constraints that require them to synchronize their start and possibly departure times. This is true of many parallelized machine learning workloads, and the popular Apache Spark processing engine has recently added support for Barrier Execution Mode, which allows users to add such barriers to their jobs. These barriers necessarily result in idle periods on some of the workers, which reduces their stability and performance, compared to equivalent workloads with no barriers.   In this paper we will consider and analyze the stability and performance penalties resulting from barriers. We include an analysis of the stability of $(s,k,l)$ barrier systems that allow jobs to depart after $l$ out of $k$ of their tasks complete. We also derive and evaluate performance bounds for hybrid barrier systems servicing a mix of jobs, both with and without barriers, and with varying degrees of parallelism. For the purely 1-barrier case we compare the bounds and simulation results to benchmark data from a standalone Spark system. We study the overhead in the real system, and based on its distribution we attribute it to the dual event and polling-driven mechanism used to schedule barrier-mode jobs. We develop a model for this type of overhead and validate it against the real system through simulation.",
        "authors": [
            "Brenton Walker",
            "Markus Fidler"
        ],
        "categories": [
            "cs.DC",
            "cs.NI",
            "cs.PF"
        ],
        "submit_date": "2025-12-16"
    },
    "http://arxiv.org/abs/2512.14290v1": {
        "id": "http://arxiv.org/abs/2512.14290v1",
        "title": "A Hybrid Reactive-Proactive Auto-scaling Algorithm for SLA-Constrained Edge Computing",
        "link": "http://arxiv.org/abs/2512.14290v1",
        "tags": [
            "edge",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-12-16",
        "tldr": "Proposes a hybrid reactive-proactive auto-scaling algorithm for SLA-constrained edge computing. Combines ML-based demand forecasting with reactive resource adjustment, integrated into Kubernetes. Reduces SLA violation rate from 23% to 6% compared to baselines.",
        "abstract": "Edge computing decentralizes computing resources, allowing for novel applications in domains such as the Internet of Things (IoT) in healthcare and agriculture by reducing latency and improving performance. This decentralization is achieved through the implementation of microservice architectures, which require low latencies to meet stringent service level agreements (SLA) such as performance, reliability, and availability metrics. While cloud computing offers the large data storage and computation resources necessary to handle peak demands, a hybrid cloud and edge environment is required to ensure SLA compliance. This is achieved by sophisticated orchestration strategies such as Kubernetes, which help facilitate resource management. The orchestration strategies alone do not guarantee SLA adherence due to the inherent delay of scaling resources. Existing auto-scaling algorithms have been proposed to address these challenges, but they suffer from performance issues and configuration complexity. In this paper, a novel auto-scaling algorithm is proposed for SLA-constrained edge computing applications. This approach combines a Machine Learning (ML) based proactive auto-scaling algorithm, capable of predicting incoming resource requests to forecast demand, with a reactive autoscaler which considers current resource utilization and SLA constraints for immediate adjustments. The algorithm is integrated into Kubernetes as an extension, and its performance is evaluated through extensive experiments in an edge environment with real applications. The results demonstrate that existing solutions have an SLA violation rate of up to 23%, whereas the proposed hybrid solution outperforms the baselines with an SLA violation rate of only 6%, ensuring stable SLA compliance across various applications.",
        "authors": [
            "Suhrid Gupta",
            "Muhammed Tawfiqul Islam",
            "Rajkumar Buyya"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-12-16"
    },
    "http://arxiv.org/abs/2512.14767v1": {
        "title": "Privacy-Preserving Feature Valuation in Vertical Federated Learning Using Shapley-CMI and PSI Permutation",
        "link": "http://arxiv.org/abs/2512.14767v1",
        "abstract": "Federated Learning (FL) is an emerging machine learning paradigm that enables multiple parties to collaboratively train models without sharing raw data, ensuring data privacy. In Vertical FL (VFL), where each party holds different features for the same users, a key challenge is to evaluate the feature contribution of each party before any model is trained, particularly in the early stages when no model exists. To address this, the Shapley-CMI method was recently proposed as a model-free, information-theoretic approach to feature valuation using Conditional Mutual Information (CMI). However, its original formulation did not provide a practical implementation capable of computing the required permutations and intersections securely. This paper presents a novel privacy-preserving implementation of Shapley-CMI for VFL. Our system introduces a private set intersection (PSI) server that performs all necessary feature permutations and computes encrypted intersection sizes across discretized and encrypted ID groups, without the need for raw data exchange. Each party then uses these intersection results to compute Shapley-CMI values, computing the marginal utility of their features. Initial experiments confirm the correctness and privacy of the proposed system, demonstrating its viability for secure and efficient feature contribution estimation in VFL. This approach ensures data confidentiality, scales across multiple parties, and enables fair data valuation without requiring the sharing of raw data or training models.",
        "authors": [
            "Unai Laskurain",
            "Aitor Aguirre-Ortuzar",
            "Urko Zurutuza"
        ],
        "categories": [
            "cs.CR",
            "cs.AI",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.14767v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-16"
    },
    "http://arxiv.org/abs/2512.14098v2": {
        "id": "http://arxiv.org/abs/2512.14098v2",
        "title": "Cornserve: Efficiently Serving Any-to-Any Multimodal Models",
        "link": "http://arxiv.org/abs/2512.14098v2",
        "tags": [
            "serving",
            "multi-modal"
        ],
        "relevant": true,
        "indexed_date": "2025-12-16",
        "tldr": "Proposes Cornserve, a serving system for any-to-any multimodal models that automatically disaggregates computation graphs. Includes a planner for optimized deployment and a distributed runtime to handle heterogeneity. Achieves 3.81× throughput gain and 5.79× tail latency reduction.",
        "abstract": "We present Cornserve, an efficient online serving system for an emerging class of multimodal models called Any-to-Any models. Any-to-Any models accept combinations of text and multimodal data (e.g., image, video, audio) as input and also generate combinations of text and multimodal data as output, introducing request type, computation path, and computation scaling heterogeneity in model serving.   Cornserve allows model developers to describe the computation graph of generic Any-to-Any models, which consists of heterogeneous components such as multimodal encoders, autoregressive models like Large Language Models (LLMs), and multimodal generators like Diffusion Transformers (DiTs). Given this, Cornserve's planner automatically finds an optimized deployment plan for the model, including whether and how to disaggregate the model into smaller components based on model and workload characteristics. Cornserve's distributed runtime then executes the model per the plan, efficiently handling Any-to-Any model heterogeneity during online serving. Evaluations show that Cornserve can efficiently serve diverse Any-to-Any models and workloads, delivering up to 3.81$\\times$ throughput improvement and up to 5.79$\\times$ tail latency reduction over existing solutions.",
        "authors": [
            "Jeff J. Ma",
            "Jae-Won Chung",
            "Jisang Ahn",
            "Yizhuo Liang",
            "Akshay Jajoo",
            "Myungjin Lee",
            "Mosharaf Chowdhury"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "submit_date": "2025-12-16"
    },
    "http://arxiv.org/abs/2512.14002v1": {
        "title": "Real-Time Service Subscription and Adaptive Offloading Control in Vehicular Edge Computing",
        "link": "http://arxiv.org/abs/2512.14002v1",
        "abstract": "Vehicular Edge Computing (VEC) has emerged as a promising paradigm for enhancing the computational efficiency and service quality in intelligent transportation systems by enabling vehicles to wirelessly offload computation-intensive tasks to nearby Roadside Units. However, efficient task offloading and resource allocation for time-critical applications in VEC remain challenging due to constrained network bandwidth and computational resources, stringent task deadlines, and rapidly changing network conditions. To address these challenges, we formulate a Deadline-Constrained Task Offloading and Resource Allocation Problem (DOAP), denoted as $\\mathbf{P}$, in VEC with both bandwidth and computational resource constraints, aiming to maximize the total vehicle utility. To solve $\\mathbf{P}$, we propose $\\mathtt{SARound}$, an approximation algorithm based on Linear Program rounding and local-ratio techniques, that improves the best-known approximation ratio for DOAP from $\\frac{1}{6}$ to $\\frac{1}{4}$. Additionally, we design an online service subscription and offloading control framework to address the challenges of short task deadlines and rapidly changing wireless network conditions. To validate our approach, we develop a comprehensive VEC simulator, VecSim, using the open-source simulation libraries OMNeT++ and Simu5G. VecSim integrates our designed framework to manage the full life-cycle of real-time vehicular tasks. Experimental results, based on profiled object detection applications and real-world taxi trace data, show that $\\mathtt{SARound}$ consistently outperforms state-of-the-art baselines under varying network conditions while maintaining runtime efficiency.",
        "authors": [
            "Chuanchao Gao",
            "Arvind Easwaran"
        ],
        "categories": [
            "cs.DC",
            "cs.DM"
        ],
        "id": "http://arxiv.org/abs/2512.14002v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-16"
    },
    "http://arxiv.org/abs/2512.18436v1": {
        "title": "VeruSAGE: A Study of Agent-Based Verification for Rust Systems",
        "link": "http://arxiv.org/abs/2512.18436v1",
        "abstract": "Large language models (LLMs) have shown impressive capability to understand and develop code. However, their capability to rigorously reason about and prove code correctness remains in question. This paper offers a comprehensive study of LLMs' capability to develop correctness proofs for system software written in Rust. We curate a new system-verification benchmark suite, VeruSAGE-Bench, which consists of 849 proof tasks extracted from eight open-source Verus-verified Rust systems. Furthermore, we design different agent systems to match the strengths and weaknesses of different LLMs (o4-mini, GPT-5, Sonnet 4, and Sonnet 4.5). Our study shows that different tools and agent settings are needed to stimulate the system-verification capability of different types of LLMs. The best LLM-agent combination in our study completes over 80% of system-verification tasks in VeruSAGE-Bench. It also completes over 90% of a set of system proof tasks not part of VeruSAGE-Bench because they had not yet been finished by human experts. This result shows the great potential for LLM-assisted development of verified system software.",
        "authors": [
            "Chenyuan Yang",
            "Natalie Neamtu",
            "Chris Hawblitzel",
            "Jacob R. Lorch",
            "Shan Lu"
        ],
        "categories": [
            "cs.OS",
            "cs.AI",
            "cs.FL",
            "cs.SE"
        ],
        "id": "http://arxiv.org/abs/2512.18436v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-20"
    },
    "http://arxiv.org/abs/2512.16238v2": {
        "id": "http://arxiv.org/abs/2512.16238v2",
        "title": "Trustworthy and Controllable Professional Knowledge Utilization in Large Language Models with TEE-GPU Execution",
        "link": "http://arxiv.org/abs/2512.16238v2",
        "tags": [
            "serving",
            "RAG",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-12-18",
        "tldr": "Addresses trustworthy utilization of professional knowledge in LLMs. Proposes PKUS system with knowledge encoded as TEE-executed adapters, GPU-TEE split execution, and scheduling. Achieves 8.1-11.9x lower latency vs CPU-TEE while matching full fine-tuning accuracy.",
        "abstract": "Future improvements in large language model (LLM) services increasingly hinge on access to high-value professional knowledge rather than more generic web data. However, the data providers of this knowledge face a skewed tradeoff between income and risk: they receive little share of downstream value yet retain copyright and privacy liability, making them reluctant to contribute their assets to LLM services. Existing techniques do not offer a trustworthy and controllable way to use professional knowledge, because they keep providers in the dark and combine knowledge parameters with the underlying LLM backbone.   In this paper, we present PKUS, the Professional Knowledge Utilization System, which treats professional knowledge as a first-class, separable artifact. PKUS keeps the backbone model on GPUs and encodes each provider's contribution as a compact adapter that executes only inside an attested Trusted Execution Environment (TEE). A hardware-rooted lifecycle protocol, adapter pruning, multi-provider aggregation, and split-execution scheduling together make this design practical at serving time. On SST-2, MNLI, and SQuAD with GPT-2 Large and Llama-3.2-1B, PKUS preserves model utility, matching the accuracy and F1 of full fine-tuning and plain LoRA, while achieving the lowest per-request latency with 8.1-11.9x speedup over CPU-only TEE inference and naive CPU-GPU co-execution.",
        "authors": [
            "Yifeng Cai",
            "Zhida An",
            "Yuhan Meng",
            "Houqian Liu",
            "Pengli Wang",
            "Hanwen Lei",
            "Yao Guo",
            "Ding Li"
        ],
        "categories": [
            "cs.OS"
        ],
        "submit_date": "2025-12-18"
    },
    "http://arxiv.org/abs/2512.14946v1": {
        "id": "http://arxiv.org/abs/2512.14946v1",
        "title": "EVICPRESS: Joint KV-Cache Compression and Eviction for Efficient LLM Serving",
        "link": "http://arxiv.org/abs/2512.14946v1",
        "tags": [
            "serving",
            "offloading",
            "compression"
        ],
        "relevant": true,
        "indexed_date": "2025-12-20",
        "tldr": "Addresses KV-cache management inefficiencies in LLM inference. Proposes EVICPRESS, which jointly optimizes lossy compression and adaptive eviction across storage tiers via a unified utility function. Achieves up to 2.19x faster time-to-first-token while maintaining generation quality.",
        "abstract": "Reusing KV cache is essential for high efficiency of Large Language Model (LLM) inference systems. With more LLM users, the KV cache footprint can easily exceed GPU memory capacity, so prior work has proposed to either evict KV cache to lower-tier storage devices, or compress KV cache so that more KV cache can be fit in the fast memory. However, prior work misses an important opportunity: jointly optimizing the eviction and compression decisions across all KV caches to minimize average generation latency without hurting quality.   We propose EVICPRESS, a KV-cache management system that applies lossy compression and adaptive eviction to KV cache across multiple storage tiers. Specifically, for each KV cache of a context, EVICPRESS considers the effect of compression and eviction of the KV cache on the average generation quality and delay across all contexts as a whole. To achieve this, EVICPRESS proposes a unified utility function that quantifies the effect of quality and delay of the lossy compression or eviction. To this end, EVICPRESS's profiling module periodically updates the utility function scores on all possible eviction-compression configurations for all contexts and places KV caches using a fast heuristic to rearrange KV caches on all storage tiers, with the goal of maximizing the utility function scores on each storage tier. Compared to the baselines that evict KV cache or compress KV cache, EVICPRESS achieves higher KV-cache hit rates on fast devices, i.e., lower delay, while preserving high generation quality by applying conservative compression to contexts that are sensitive to compression errors. Evaluation on 12 datasets and 5 models demonstrates that EVICPRESS achieves up to 2.19x faster time-to-first-token (TTFT) at equivalent generation quality.",
        "authors": [
            "Shaoting Feng",
            "Yuhan Liu",
            "Hanchen Li",
            "Xiaokun Chen",
            "Samuel Shen",
            "Kuntai Du",
            "Zhuohan Gu",
            "Rui Zhang",
            "Yuyang Huang",
            "Yihua Cheng",
            "Jiayi Yao",
            "Qizheng Zhang",
            "Ganesh Ananthanarayanan",
            "Junchen Jiang"
        ],
        "categories": [
            "cs.OS",
            "cs.AI",
            "cs.LG"
        ],
        "submit_date": "2025-12-16"
    },
    "http://arxiv.org/abs/2512.13931v1": {
        "id": "http://arxiv.org/abs/2512.13931v1",
        "title": "Q-IRIS: The Evolution of the IRIS Task-Based Runtime to Enable Classical-Quantum Workflows",
        "link": "http://arxiv.org/abs/2512.13931v1",
        "tags": [
            "serving",
            "RL",
            "edge"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes Q-IRIS, a hybrid runtime for classical-quantum workflows integrating IRIS task-based runtime with XACC quantum framework. Enables asynchronous scheduling of QIR programs across heterogeneous backends (including quantum simulators). Demonstrates circuit cutting to reduce per-task simulation load, improving throughput and reducing queueing behavior.",
        "abstract": "Extreme heterogeneity in emerging HPC systems are starting to include quantum accelerators, motivating runtimes that can coordinate between classical and quantum workloads. We present a proof-of-concept hybrid execution framework integrating the IRIS asynchronous task-based runtime with the XACC quantum programming framework via the Quantum Intermediate Representation Execution Engine (QIR-EE). IRIS orchestrates multiple programs written in the quantum intermediate representation (QIR) across heterogeneous backends (including multiple quantum simulators), enabling concurrent execution of classical and quantum tasks. Although not a performance study, we report measurable outcomes through the successful asynchronous scheduling and execution of multiple quantum workloads. To illustrate practical runtime implications, we decompose a four-qubit circuit into smaller subcircuits through a process known as quantum circuit cutting, reducing per-task quantum simulation load and demonstrating how task granularity can improve simulator throughput and reduce queueing behavior -- effects directly relevant to early quantum hardware environments. We conclude by outlining key challenges for scaling hybrid runtimes, including coordinated scheduling, classical-quantum interaction management, and support for diverse backend resources in heterogeneous systems.",
        "authors": [
            "Narasinga Rao Miniskar",
            "Mohammad Alaul Haque Monil",
            "Elaine Wong",
            "Vicente Leyton-Ortega",
            "Jeffrey S. Vetter",
            "Seth R. Johnson",
            "Travis S. Humble"
        ],
        "categories": [
            "quant-ph",
            "cs.DC"
        ],
        "submit_date": "2025-12-15"
    },
    "http://arxiv.org/abs/2512.13666v1": {
        "title": "SEDULity: A Proof-of-Learning Framework for Distributed and Secure Blockchains with Efficient Useful Work",
        "link": "http://arxiv.org/abs/2512.13666v1",
        "abstract": "The security and decentralization of Proof-of-Work (PoW) have been well-tested in existing blockchain systems. However, its tremendous energy waste has raised concerns about sustainability. Proof-of-Useful-Work (PoUW) aims to redirect the meaningless computation to meaningful tasks such as solving machine learning (ML) problems, giving rise to the branch of Proof-of-Learning (PoL). While previous studies have proposed various PoLs, they all, to some degree, suffer from security, decentralization, or efficiency issues. In this paper, we propose a PoL framework that trains ML models efficiently while maintaining blockchain security in a fully distributed manner. We name the framework SEDULity, which stands for a Secure, Efficient, Distributed, and Useful Learning-based blockchain system. Specifically, we encode the template block into the training process and design a useful function that is difficult to solve but relatively easy to verify, as a substitute for the PoW puzzle. We show that our framework is distributed, secure, and efficiently trains ML models. We further demonstrate that the proposed PoL framework can be extended to other types of useful work and design an incentive mechanism to incentivize task verification. We show theoretically that a rational miner is incentivized to train fully honestly with well-designed system parameters. Finally, we present simulation results to demonstrate the performance of our framework and validate our analysis.",
        "authors": [
            "Weihang Cao",
            "Mustafa Doger",
            "Sennur Ulukus"
        ],
        "categories": [
            "cs.CR",
            "cs.DC",
            "cs.IT",
            "cs.LG"
        ],
        "id": "http://arxiv.org/abs/2512.13666v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-15"
    },
    "http://arxiv.org/abs/2512.13638v1": {
        "id": "http://arxiv.org/abs/2512.13638v1",
        "title": "Design in Tiles: Automating GEMM Deployment on Tile-Based Many-PE Accelerators",
        "link": "http://arxiv.org/abs/2512.13638v1",
        "tags": [
            "hardware",
            "training",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Automates GEMM deployment on tile-based many-PE accelerators via DiT framework, connecting deployment toolchain with configurable executable model. Achieves 1.2-2.0x speedup over NVIDIA GH200 with higher PE utilization for diverse matrix shapes.",
        "abstract": "Tile-based many-Processing Element (PE) accelerators can achieve competitive performance on General Matrix Multiplication (GEMM), but they are extremely hard to program, as their optimal software mapping is deeply coupled with hardware design which is unwieldy to manual deployment. We propose \"Design in Tiles (DiT)\", an automated framework connecting a deployment toolchain with a configurable executable model for these accelerators. For evaluation, we apply our framework to GEMM targeting a large acceleration configuration (e.g., 32x32 tiles, 1979 TFLOPS@FP8, 4 TB/s Bandwidth) comparable to an NVIDIA GH200. We achieve higher PE utilization than GH200 with its expert-tuned GEMM libraries, achieving 1.2-2.0x speedup across diverse matrix shapes.",
        "authors": [
            "Aofeng Shen",
            "Chi Zhang",
            "Yakup Budanaz",
            "Alexandru Calotoiu",
            "Torsten Hoefler",
            "Luca Benini"
        ],
        "categories": [
            "cs.DC",
            "cs.AR"
        ],
        "submit_date": "2025-12-15"
    },
    "http://arxiv.org/abs/2512.13591v1": {
        "title": "astroCAMP: A Community Benchmark and Co-Design Framework for Sustainable SKA-Scale Radio Imaging",
        "link": "http://arxiv.org/abs/2512.13591v1",
        "abstract": "The Square Kilometre Array (SKA) project will operate one of the world's largest continuous scientific data systems, sustaining petascale imaging under strict power caps. Yet, current radio-interferometric pipelines utilize only a small fraction of hardware peak performance, typically 4-14%, due to memory and I/O bottlenecks, resulting in poor energy efficiency and high operational and carbon costs. Progress is further limited by the absence of standardised metrics and fidelity tolerances, preventing principled hardware-software co-design and rigorous exploration of quality-efficiency trade-offs. We introduce astroCAMP, a framework for guiding the co-design of next-generation imaging pipelines and sustainable HPC architectures that maximise scientific return within SKA's operational and environmental limits. astroCAMP provides: (1) a unified, extensible metric suite covering scientific fidelity, computational performance, sustainability, and lifecycle economics; (2) standardised SKA-representative datasets and reference outputs enabling reproducible benchmarking across CPUs, GPUs, and emerging accelerators; and (3) a multi-objective co-design formulation linking scientific-quality constraints to time-, energy-, carbon-to-solution, and total cost of ownership. We release datasets, benchmarking results, and a reproducibility kit, and evaluate co-design metrics for WSClean and IDG on an AMD EPYC 9334 processor and an NVIDIA H100 GPU. Further, we illustrate the use of astroCAMP for heterogeneous CPU-FPGA design-space exploration, and its potential to facilitate the identification of Pareto-optimal operating points for SKA-scale imaging deployments. Last, we make a call to the SKA community to define quantifiable fidelity metrics and thresholds to accelerate principled optimisation for SKA-scale imaging.",
        "authors": [
            "Denisa-Andreea Constantinescu",
            "Rubén Rodríguez Álvarez",
            "Jacques Morin",
            "Etienne Orliac",
            "Mickaël Dardaillon",
            "Sunrise Wang",
            "Hugo Miomandre",
            "Miguel Peón-Quirós",
            "Jean-François Nezan",
            "David Atienza"
        ],
        "categories": [
            "cs.DC",
            "astro-ph.IM",
            "cs.PF"
        ],
        "id": "http://arxiv.org/abs/2512.13591v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-15"
    },
    "http://arxiv.org/abs/2512.13525v2": {
        "id": "http://arxiv.org/abs/2512.13525v2",
        "title": "Janus: Disaggregating Attention and Experts for Scalable MoE Inference",
        "link": "http://arxiv.org/abs/2512.13525v2",
        "tags": [
            "MoE",
            "serving",
            "disaggregation"
        ],
        "relevant": true,
        "indexed_date": "2025-12-15",
        "tldr": "Investigates scalable MoE inference under dynamic workloads by disaggregating attention and experts onto separate GPU sub-clusters. Key designs include adaptive communication, lightweight scheduling via GPU kernel, and fine-grained resource management. Achieves up to 3.9× higher per-GPU throughput versus SOTA.",
        "abstract": "Large Mixture-of-Experts (MoE) model inference is challenging due to high resource demands and dynamic workloads. Existing solutions often deploy the entire model as a single monolithic unit, which applies a unified resource configuration to both attention and expert modules despite their different requirements, leading to limited scalability and resource inefficiency. In this paper, we propose Janus, a scalable MoE inference system that disaggregates attention and experts on separate GPU sub-clusters, enabling each module to be managed and scaled independently. Janus incorporates three key designs for efficient, disaggregated MoE inference. First, it proposes an adaptive two-phase communication scheme that exploits intra- and inter-node bandwidth hierarchies for low-latency data exchange. Second, motivated by the memory-bound nature of MoE modules, Janus introduces a lightweight scheduler and implements it as a GPU kernel to balance the number of activated experts across GPUs at minimal overhead, thereby reducing inference latency. Third, Janus performs fine-grained resource management to dynamically adjust expert placement and independently scale attention and MoE resources to improve overall efficiency. Evaluation shows Janus achieves up to 3.9 higher perGPU throughput than state-of-the-art systems while meeting per-token latency requirements.",
        "authors": [
            "Zhexiang Zhang",
            "Ye Wang",
            "Xiangyu Wang",
            "Yumiao Zhao",
            "Jingzhe Jiang",
            "Qizhen Weng",
            "Shaohuai Shi",
            "Yin Chen",
            "Minchen Yu"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-12-15"
    },
    "http://arxiv.org/abs/2512.13488v1": {
        "id": "http://arxiv.org/abs/2512.13488v1",
        "title": "SIGMA: An AI-Empowered Training Stack on Early-Life Hardware",
        "link": "http://arxiv.org/abs/2512.13488v1",
        "tags": [
            "training",
            "MoE",
            "storage"
        ],
        "relevant": true,
        "indexed_date": "2025-12-15",
        "tldr": "Proposes SIGMA, a training stack for large-scale distributed training on early-life AI accelerators. Combines LTP platform for reliability and LTF framework for efficient MoE model training. Achieves 94.45% accelerator utilization and trains 200B MoE model with 21.08% MFU.",
        "abstract": "An increasing variety of AI accelerators is being considered for large-scale training. However, enabling large-scale training on early-life AI accelerators faces three core challenges: frequent system disruptions and undefined failure modes that undermine reliability; numerical errors and training instabilities that threaten correctness and convergence; and the complexity of parallelism optimization combined with unpredictable local noise that degrades efficiency. To address these challenges, SIGMA is an open-source training stack designed to improve the reliability, stability, and efficiency of large-scale distributed training on early-life AI hardware. The core of this initiative is the LUCIA TRAINING PLATFORM (LTP), the system optimized for clusters with early-life AI accelerators. Since its launch in March 2025, LTP has significantly enhanced training reliability and operational productivity. Over the past five months, it has achieved an impressive 94.45% effective cluster accelerator utilization, while also substantially reducing node recycling and job-recovery times. Building on the foundation of LTP, the LUCIA TRAINING FRAMEWORK (LTF) successfully trained SIGMA-MOE, a 200B MoE model, using 2,048 AI accelerators. This effort delivered remarkable stability and efficiency outcomes, achieving 21.08% MFU, state-of-the-art downstream accuracy, and encountering only one stability incident over a 75-day period. Together, these advances establish SIGMA, which not only tackles the critical challenges of large-scale training but also establishes a new benchmark for AI infrastructure and platform innovation, offering a robust, cost-effective alternative to prevailing established accelerator stacks and significantly advancing AI capabilities and scalability. The source code of SIGMA is available at https://github.com/microsoft/LuciaTrainingPlatform.",
        "authors": [
            "Lei Qu",
            "Lianhai Ren",
            "Peng Cheng",
            "Rui Gao",
            "Ruizhe Wang",
            "Tianyu Chen",
            "Xiao Liu",
            "Xingjian Zhang",
            "Yeyun Gong",
            "Yifan Xiong",
            "Yucheng Ding",
            "Yuting Jiang",
            "Zhenghao Lin",
            "Zhongxin Guo",
            "Ziyue Yang"
        ],
        "categories": [
            "cs.DC",
            "cs.CL"
        ],
        "submit_date": "2025-12-15"
    },
    "http://arxiv.org/abs/2512.13319v1": {
        "id": "http://arxiv.org/abs/2512.13319v1",
        "title": "Temporal parallelisation of continuous-time maximum-a-posteriori trajectory estimation",
        "link": "http://arxiv.org/abs/2512.13319v1",
        "tags": [
            "training",
            "offline",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes a parallel-in-time method for continuous-time MAP trajectory estimation to accelerate computations on parallel architectures like GPUs. Uses a reformulation into an optimal control problem and associative scan algorithms for parallelization. Achieves significant speedup in GPU experiments while maintaining sequential algorithm accuracy.",
        "abstract": "This paper proposes a parallel-in-time method for computing continuous-time maximum-a-posteriori (MAP) trajectory estimates of the states of partially observed stochastic differential equations (SDEs), with the goal of improving computational speed on parallel architectures. The MAP estimation problem is reformulated as a continuous-time optimal control problem based on the Onsager-Machlup functional. This reformulation enables the use of a previously proposed parallel-in-time solution for optimal control problems, which we adapt to the current problem. The structure of the resulting optimal control problem admits a parallel solution based on parallel associative scan algorithms. In the linear Gaussian special case, it yields a parallel Kalman-Bucy filter and a parallel continuous-time Rauch-Tung-Striebel smoother. These linear computational methods are further extended to nonlinear continuous-time state-space models through Taylor expansions. We also present the corresponding parallel two-filter smoother. The graphics processing unit (GPU) experiments on linear and nonlinear models demonstrate that the proposed framework achieves a significant speedup in computations while maintaining the accuracy of sequential algorithms.",
        "authors": [
            "Hassan Razavi",
            "Ángel F. García-Fernández",
            "Simo Särkkä"
        ],
        "categories": [
            "cs.DC",
            "eess.SP",
            "eess.SY",
            "stat.CO"
        ],
        "submit_date": "2025-12-15"
    },
    "http://arxiv.org/abs/2512.13268v1": {
        "title": "SPARS: A Reinforcement Learning-Enabled Simulator for Power Management in HPC Job Scheduling",
        "link": "http://arxiv.org/abs/2512.13268v1",
        "abstract": "High-performance computing (HPC) clusters consume enormous amounts of energy, with idle nodes as a major source of waste. Powering down unused nodes can mitigate this problem, but poorly timed transitions introduce long delays and reduce overall performance. To address this trade-off, we present SPARS, a reinforcement learning-enabled simulator for power management in HPC job scheduling. SPARS integrates job scheduling and node power state management within a discrete-event simulation framework. It supports traditional scheduling policies such as First Come First Served and EASY Backfilling, along with enhanced variants that employ reinforcement learning agents to dynamically decide when nodes should be powered on or off. Users can configure workloads and platforms in JSON format, specifying job arrivals, execution times, node power models, and transition delays. The simulator records comprehensive metrics-including energy usage, wasted power, job waiting times, and node utilization-and provides Gantt chart visualizations to analyze scheduling dynamics and power transitions. Unlike widely used Batsim-based frameworks that rely on heavy inter-process communication, SPARS provides lightweight event handling and consistent simulation results, making experiments easier to reproduce and extend. Its modular design allows new scheduling heuristics or learning algorithms to be integrated with minimal effort. By providing a flexible, reproducible, and extensible platform, SPARS enables researchers and practitioners to systematically evaluate power-aware scheduling strategies, explore the trade-offs between energy efficiency and performance, and accelerate the development of sustainable HPC operations.",
        "authors": [
            "Muhammad Alfian Amrizal",
            "Raka Satya Prasasta",
            "Santana Yuda Pradata",
            "Kadek Gemilang Santiyuda",
            "Reza Pulungan",
            "Hiroyuki Takizawa"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.13268v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-15"
    },
    "http://arxiv.org/abs/2512.13213v1": {
        "title": "Towards Secure Decentralized Applications and Consensus Protocols in Blockchains (on Selfish Mining, Undercutting Attacks, DAG-Based Blockchains, E-Voting, Cryptocurrency Wallets, Secure-Logging, and CBDC)",
        "link": "http://arxiv.org/abs/2512.13213v1",
        "abstract": "With the rise of cryptocurrencies, many new applications built on decentralized blockchains have emerged. Blockchains are full-stack distributed systems where multiple sub-systems interact. While many deployed blockchains and decentralized applications need better scalability and performance, security is also critical. Due to their complexity, assessing blockchain and DAPP security requires a more holistic view than for traditional distributed or centralized systems.   In this thesis, we summarize our contributions to blockchain and decentralized application security. We propose a security reference architecture to support standardized vulnerability and threat analysis. We study consensus security in single-chain Proof-of-Work blockchains, including resistance to selfish mining, undercutting, and greedy transaction selection, as well as related issues in DAG-based systems. We contribute to wallet security with a new classification of authentication schemes and a two-factor method based on One-Time Passwords. We advance e-voting with a practical boardroom voting protocol, extend it to a scalable version for millions of participants while preserving security and privacy, and introduce a repetitive voting framework that enables vote changes between elections while avoiding peak-end effects. Finally, we improve secure logging using blockchains and trusted computing through a centralized ledger that guarantees non-equivocation, integrity, and censorship evidence, then build on it to propose an interoperability protocol for central bank digital currencies that ensures atomic transfers.",
        "authors": [
            "Ivan Homoliak"
        ],
        "categories": [
            "cs.CR",
            "cs.DC",
            "cs.GT"
        ],
        "id": "http://arxiv.org/abs/2512.13213v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-15"
    },
    "http://arxiv.org/abs/2512.13096v1": {
        "title": "Toward Self-Healing Networks-on-Chip: RL-Driven Routing in 2D Torus Architectures",
        "link": "http://arxiv.org/abs/2512.13096v1",
        "abstract": "We investigate adaptive minimal routing in 2D torus networks on chip NoCs under node fault conditions comparing a reinforcement learning RL based strategy to an adaptive routing baseline A torus topology is used for its low diameter high connectivity properties The RL approach models each router as an agent that learns to forward packets based on network state while the adaptive scheme uses fixed minimal paths with simple rerouting around faults We implement both methods in simulation injecting up to 50 node faults uniformly at random Key metrics are measured 1 throughput vs offered load at fault density 02 2 packet delivery ratio PDR vs fault density and 3 a fault adaptive score FT vs fault density Experimental results show the RL method achieves significantly higher throughput at high load approximately 2030 gain and maintains higher reliability under increasing faults The RL router delivers more packets per cycle and adapts to faults by exploiting path diversity whereas the adaptive scheme degrades sharply as faults accumulate In particular the RL approach preserves end to end connectivity longer PDR remains above 90 until approximately 3040 faults while adaptive PDR drops to approximately 70 at the same point The fault adaptive score likewise favors RL routing Thus RL based adaptive routing demonstrates clear advantages in throughput and fault resilience for torus NoCs",
        "authors": [
            "Mohammad Walid Charrwi",
            "Zaid Hussain"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.13096v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-15"
    },
    "http://arxiv.org/abs/2512.12949v1": {
        "id": "http://arxiv.org/abs/2512.12949v1",
        "title": "FlashFuser: Expanding the Scale of Kernel Fusion for Compute-Intensive Operators via Inter-Core Connection",
        "link": "http://arxiv.org/abs/2512.12949v1",
        "tags": [
            "kernel",
            "quantization",
            "hardware"
        ],
        "relevant": true,
        "indexed_date": "2025-12-15",
        "tldr": "Proposes FlashFuser, a compiler framework for kernel fusion using GPU inter-core connections to overcome memory limits. Utilizes Distributed Shared Memory (DSM) for complex operators, optimizing data movement and tile selection. Achieves 58% less memory access, 3.3x kernel speedup against tuned libraries.",
        "abstract": "The scaling of computation throughput continues to outpace improvements in memory bandwidth, making many deep learning workloads memory-bound. Kernel fusion is a key technique to alleviate this problem, but the fusion strategies of existing compilers and frameworks are limited to using local scratchpad memory. When the intermediate results exceed the limited capacity (such as FFN), the fusion fails. Although modern GPUs (like the NVIDIA H100) now incorporate an inter-core connection mechanism known as Distributed Shared Memory(DSM)--providing a larger, high-bandwidth, and low-latency on-chip memory pool--this hardware potential has yet to be exploited by software frameworks. To bridge this gap, we present FlashFuser, the first compiler framework to utilize inter-core connection for kernel fusion on modern GPUs. FlashFuser extends established fusion techniques to the DSM domain through three core contributions. First, we propose a powerful DSM-based communication abstraction that formalizes complex cluster-based data exchange patterns, such as reduce, shuffle and multiply. Second, we introduce a dataflow analyzer that generalizes loop scheduling, resource mapping, and tile selection to the distributed memory hierarchy; it determines the optimal execution order and tile sizes by quantifying data movement across memory levels. Finally, FlashFuser integrates these components into a unified search engine that employs analytical cost modeling and DSM-aware pruning strategies to efficiently discover the optimal execution plan. Our evaluation on an NVIDIA H100 GPU shows that FlashFuser reduces memory access by 58% and delivers kernel speedups of 3.3x against highly-tuned libraries and 4.1x against state-of-the-art compilers, resulting in a 1.24x end-to-end speedup.",
        "authors": [
            "Ziyu Huang",
            "Yangjie Zhou",
            "Zihan Liu",
            "Xinhao Luo",
            "Yijia Diao",
            "Minyi Guo",
            "Jidong Zhai",
            "Yu Feng",
            "Chen Zhang",
            "Anbang Wu",
            "Jingwen Leng"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-12-15"
    },
    "http://arxiv.org/abs/2512.12928v1": {
        "id": "http://arxiv.org/abs/2512.12928v1",
        "title": "PROSERVE: Unified Multi-Priority Request Scheduling for LLM Serving",
        "link": "http://arxiv.org/abs/2512.12928v1",
        "tags": [
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-12-15",
        "tldr": "Addresses multi-priority request scheduling in LLM serving to maximize service gain balancing SLO attainment and client priority. Proposes PROSERVE, a two-tier scheduler with SlideBatching for batch formation and GoRouting for request dispatching. Achieves up to 35% higher system gain and 52% better SLO attainment.",
        "abstract": "The widespread deployment of large language models (LLMs) for interactive applications necessitates serving systems that can handle thousands of concurrent requests with diverse Service Level Objective (SLO) requirements. A critical yet often overlooked dimension in this context is the inherent priority difference among clients; for instance, business-critical functions demand higher performance guarantees, as fulfilling such requests yields significantly greater business value. However, existing LLM serving schedulers fail to jointly optimize for both SLO attainment and client-level priorities.   To bridge this gap, we first \\textit{formalize multi-priority request scheduling as a service gain maximization problem}, where satisfying latency requirements for requests of different priorities contributes varying levels of gain. We then propose PROSERVE, a unified two-tier scheduling framework designed to maximize overall service gain. At the engine level, SlideBatching dynamically adapts batch formation and request ordering under varying load conditions, employing a sliding boundary mechanism to balance deadline-first and density-first strategies. At the service level, GoRouting performs gain-oriented and capability-aware dispatching across distributed instances, proactively reserving capacity for future high-priority or long requests. Extensive evaluation across four open-source datasets and a real-world industrial trace demonstrates that \\systemname{} consistently outperforms state-of-the-art baselines, improving system gain by up to 35% and boosting SLO attainment by up to 52%.",
        "authors": [
            "Weizhe Huang",
            "Tao Peng",
            "Tongxuan Liu",
            "Donghe Jin",
            "Xianzhe Dong",
            "Ke Zhang"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-12-15"
    },
    "http://arxiv.org/abs/2512.12801v1": {
        "id": "http://arxiv.org/abs/2512.12801v1",
        "title": "Fine-Grained Energy Prediction For Parallellized LLM Inference With PIE-P",
        "link": "http://arxiv.org/abs/2512.12801v1",
        "tags": [
            "serving",
            "offline",
            "training"
        ],
        "relevant": true,
        "indexed_date": "2025-12-14",
        "tldr": "Proposes PIE-P, a framework for fine-grained energy prediction in multi-GPU parallellized LLM inference. Uses precise sampling and modeling of inter-GPU communication to account for parallelism overhead. Achieves accurate energy predictions, significantly outperforming baselines in parallelized settings.",
        "abstract": "With the widespread adoption of Large Language Models (LLMs), energy costs of running LLMs is quickly becoming a critical concern. However, precisely measuring the energy consumption of LLMs is often infeasible because hardware-based power monitors are not always accessible and software-based energy measurement tools are not accurate. While various prediction techniques have been developed to estimate LLM energy consumption, these approaches are limited to single-GPU environments and thus are not applicable to modern LLM inference which is typically parallelized across multiple GPUs. In this work, we remedy this gap and introduce PIE-P, a fine-grained energy prediction framework for multi-GPU inference, including tensor, pipeline, and data parallelism. Predicting the energy under parallelized inference is complicated by the non-determinism in inter-GPU communication, additional communication overheads, and difficulties in isolating energy during the communication/synchronization phase. We develop a scalable prediction framework that addresses these issues via precise sampling, fine-grained modeling of inter-GPU communication, and careful accounting of parallelization overhead. Our evaluation results show that PIE-P yields accurate and fine-grained energy predictions across parallelism strategies, significantly outperforming baselines.",
        "authors": [
            "Anurag Dutt",
            "Young Won Choi",
            "Avirup Sil",
            "Anshul Gandhi",
            "Aruna Balasubramanian",
            "Niranjan Balasubramanian"
        ],
        "categories": [
            "cs.DC",
            "cs.PF"
        ],
        "submit_date": "2025-12-14"
    },
    "http://arxiv.org/abs/2512.12737v1": {
        "id": "http://arxiv.org/abs/2512.12737v1",
        "title": "SPARK: Igniting Communication-Efficient Decentralized Learning via Stage-wise Projected NTK and Accelerated Regularization",
        "link": "http://arxiv.org/abs/2512.12737v1",
        "tags": [
            "training",
            "networking",
            "quantization"
        ],
        "relevant": true,
        "indexed_date": "2025-12-14",
        "tldr": "Addresses communication overhead in decentralized federated learning with statistical heterogeneity. Proposes SPARK, integrating Jacobian compression via random projection, stage-wise distillation, and Nesterov momentum. Reduces communication by 98.7% compared to NTK-DFL with 3× faster convergence.",
        "abstract": "Decentralized federated learning (DFL) faces critical challenges from statistical heterogeneity and communication overhead. While NTK-based methods achieve faster convergence, transmitting full Jacobian matrices is impractical for bandwidth-constrained edge networks. We propose SPARK, synergistically integrating random projection-based Jacobian compression, stage-wise annealed distillation, and Nesterov momentum acceleration. Random projections compress Jacobians while preserving spectral properties essential for convergence. Stage-wise annealed distillation transitions from pure NTK evolution to neighbor-regularized learning, counteracting compression noise. Nesterov momentum accelerates convergence through stable accumulation enabled by distillation smoothing. SPARK achieves 98.7% communication reduction compared to NTK-DFL while maintaining convergence speed and superior accuracy. With momentum, SPARK reaches target performance 3 times faster, establishing state-of-the-art results for communication-efficient decentralized learning and enabling practical deployment in bandwidth-limited edge environments.",
        "authors": [
            "Li Xia"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "submit_date": "2025-12-14"
    },
    "http://arxiv.org/abs/2512.12732v1": {
        "title": "Ethical Risk Analysis of L2 Rollups",
        "link": "http://arxiv.org/abs/2512.12732v1",
        "abstract": "Layer 2 rollups improve throughput and fees, but can reintroduce risk through operator discretion and information asymmetry. We ask which operator and governance designs produce ethically problematic user risk. We adapt Ethical Risk Analysis to rollup architectures, build a role-based taxonomy of decision authority and exposure, and pair the framework with two empirical signals, a cross sectional snapshot of 129 projects from L2BEAT and a hand curated incident set covering 2022 to 2025. We analyze mechanisms that affect risks to users funds, including upgrade timing and exit windows, proposer liveness and whitelisting, forced inclusion usability, and data availability choices. We find that ethical hazards rooted in L2 components control arrangements are widespread: instant upgrades without exit windows appear in about 86 percent of projects, and proposer controls that can freeze withdrawals in about 50 percent. Reported incidents concentrate in sequencer liveness and inclusion, consistent with these dependencies. We translate these findings into ethically grounded suggestions on mitigation strategies including technical components and governance mechanisms.",
        "authors": [
            "Georgy Ishmaev",
            "Emmanuelle Anceaume",
            "Davide Frey",
            "François Taïani"
        ],
        "categories": [
            "cs.DC",
            "cs.CY"
        ],
        "id": "http://arxiv.org/abs/2512.12732v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-14"
    },
    "http://arxiv.org/abs/2512.12617v1": {
        "title": "Spectral Sentinel: Scalable Byzantine-Robust Decentralized Federated Learning via Sketched Random Matrix Theory on Blockchain",
        "link": "http://arxiv.org/abs/2512.12617v1",
        "abstract": "Decentralized federated learning (DFL) enables collaborative model training without centralized trust, but it remains vulnerable to Byzantine clients that poison gradients under heterogeneous (Non-IID) data. Existing defenses face a scalability trilemma: distance-based filtering (e.g., Krum) can reject legitimate Non-IID updates, geometric-median methods incur prohibitive $O(n^2 d)$ cost, and many certified defenses are evaluated only on models below 100M parameters. We propose Spectral Sentinel, a Byzantine detection and aggregation framework that leverages a random-matrix-theoretic signature: honest Non-IID gradients produce covariance eigenspectra whose bulk follows the Marchenko-Pastur law, while Byzantine perturbations induce detectable tail anomalies. Our algorithm combines Frequent Directions sketching with data-dependent MP tracking, enabling detection on models up to 1.5B parameters using $O(k^2)$ memory with $k \\ll d$. Under a $(σ,f)$ threat model with coordinate-wise honest variance bounded by $σ^2$ and $f < 1/2$ adversaries, we prove $(ε,δ)$-Byzantine resilience with convergence rate $O(σf / \\sqrt{T} + f^2 / T)$, and we provide a matching information-theoretic lower bound $Ω(σf / \\sqrt{T})$, establishing minimax optimality. We implement the full system with blockchain integration on Polygon networks and validate it across 144 attack-aggregator configurations, achieving 78.4 percent average accuracy versus 48-63 percent for baseline methods.",
        "authors": [
            "Animesh Mishra"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.12617v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-14"
    },
    "http://arxiv.org/abs/2512.12532v1": {
        "id": "http://arxiv.org/abs/2512.12532v1",
        "title": "Strategic Server Deployment under Uncertainty in Mobile Edge Computing",
        "link": "http://arxiv.org/abs/2512.12532v1",
        "tags": [
            "edge",
            "networking",
            "storage"
        ],
        "relevant": true,
        "indexed_date": "2025-12-14",
        "tldr": "Addresses strategic server deployment in mobile edge computing under uncertainty. Formulates as stochastic bilevel optimization and uses submodular approximation with greedy algorithms. Achieves up to 55% improvement over alternatives in real-world evaluations.",
        "abstract": "Server deployment is a fundamental task in mobile edge computing: where to place the edge servers and what user cells to assign to them. To make this decision is context-specific, but common goals are 1) computing efficiency: maximize the amount of workload processed by the edge, and 2) communication efficiency: minimize the communication cost between the cells and their assigned servers. We focus on practical scenarios where the user workload in each cell is unknown and time-varying, and so are the effective capacities of the servers. Our research problem is to choose a subset of candidate servers and assign them to the user cells such that the above goals are sustainably achieved under the above uncertainties. We formulate this problem as a stochastic bilevel optimization, which is strongly NP-hard and unseen in the literature. By approximating the objective function with submodular functions, we can utilize state-of-the-art greedy algorithms for submodular maximization to effectively solve our problem. We evaluate the proposed algorithm using real-world data, showing its superiority to alternative methods; the improvement can be as high as 55%",
        "authors": [
            "Duc A. Tran",
            "Dung Truong",
            "Duy Le"
        ],
        "categories": [
            "cs.DC",
            "cs.NI"
        ],
        "submit_date": "2025-12-14"
    },
    "http://arxiv.org/abs/2512.12476v1": {
        "id": "http://arxiv.org/abs/2512.12476v1",
        "title": "HetRL: Efficient Reinforcement Learning for LLMs in Heterogeneous Environments",
        "link": "http://arxiv.org/abs/2512.12476v1",
        "tags": [
            "training",
            "RL",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-12-13",
        "tldr": "Addresses efficient RL training for LLMs in heterogeneous GPU environments. Proposes HetRL system with a constrained joint optimization formulation and a multi-level search scheduling algorithm. Achieves up to 9.17x higher throughput over state-of-the-art systems.",
        "abstract": "As large language models (LLMs) continue to scale and new GPUs are released even more frequently, there is an increasing demand for LLM post-training in heterogeneous environments to fully leverage underutilized mid-range or previous-generation GPUs across regions and alleviate the shortage of homogeneous high-end GPUs within a single region. However, achieving high-performance reinforcement learning (RL) training for LLMs on such computing resources remains challenging because the workflow involves multiple models and tasks with complex computation and data dependencies. In this paper, we present HetRL, a distributed system for efficient RL training in infrastructures with heterogeneous GPUs and networks. HetRL formulates the scheduling of RL training in heterogeneous environments as a constrained joint optimization problem and introduces a novel scheduling algorithm that (1) decomposes the complex search space with a multi-level search framework; and (2) allocates the search budget via successive halving. Our extensive evaluation, consuming 20,000 GPU-hours, shows that HetRL delivers up to 9.17x the throughput of state-of-the-art systems, and 3.17x on average, under various workloads and settings.",
        "authors": [
            "Yongjun He",
            "Shuai Zhang",
            "Jiading Gai",
            "Xiyuan Zhang",
            "Boran Han",
            "Bernie Wang",
            "Huzefa Rangwala",
            "George Karypis"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-12-13"
    },
    "http://arxiv.org/abs/2512.12409v2": {
        "id": "http://arxiv.org/abs/2512.12409v2",
        "title": "Reputation-Based Leader Election under Partial Synchrony: Towards a Protocol-Independent Abstraction with Enhanced Guarantees",
        "link": "http://arxiv.org/abs/2512.12409v2",
        "tags": [
            "training",
            "RL"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes Sliding Window Leader Election (SWLE), a reputation-based abstraction for leader-based BFT protocols under partial synchrony. Enhances leader election via Byzantine-cost amplification and consensus-behavior reputation. Achieves 4.2x throughput, 75% lower latency vs state-of-the-art in geo-distributed deployment.",
        "abstract": "Leader election serves a well-defined role in leader-based Byzantine Fault Tolerant (BFT) protocols. Existing reputation-based leader election frameworks for partially synchronous BFTs suffer from either protocol-specific proofs, narrow applicability, or unbounded recovery after network stabilization, leaving an open problem. This paper presents a novel protocol-independent abstraction formalizing generic correctness properties and effectiveness guarantees for leader election under partial synchrony, enabling protocol-independent analysis and design. Building on this, we design the Sliding Window Leader Election (SWLE) mechanism. SWLE dynamically adjusts leader nominations via consensus-behavior-based reputation scores, enforcing Byzantine-cost amplification. We demonstrate SWLE introduces minimal extra overhead to the base protocol and prove it satisfies all abstraction properties and provides superior effectiveness. We show, with a 16-server deployment across 4 different regions in northern China, SWLE achieves up to 4.2x higher throughput, 75% lower latency and 27% Byzantine leader frequency compared to the state-of-the-art solution under common Byzantine faults, while maintaining efficiency in fault-free scenarios.",
        "authors": [
            "Xuyang Liu",
            "Zijian Zhang",
            "Zhen Li",
            "Jiahang Sun",
            "Jiamou Liu",
            "Peng Jiang"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-12-13"
    },
    "http://arxiv.org/abs/2512.12314v1": {
        "title": "Evaluating Asynchronous Semantics in Trace-Discovered Resilience Models: A Case Study on the OpenTelemetry Demo",
        "link": "http://arxiv.org/abs/2512.12314v1",
        "abstract": "While distributed tracing and chaos engineering are becoming standard for microservices, resilience models remain largely manual and bespoke. We revisit a trace-discovered connectivity model that derives a service dependency graph from traces and uses Monte Carlo simulation to estimate endpoint availability under fail-stop service failures. Compared to earlier work, we (i) derive the graph directly from raw OpenTelemetry traces, (ii) attach endpoint-specific success predicates, and (iii) add a simple asynchronous semantics that treats Kafka edges as non-blocking for immediate HTTP success. We apply this model to the OpenTelemetry Demo (\"Astronomy Shop\") using a GitHub Actions workflow that discovers the graph, runs simulations, and executes chaos experiments that randomly kill microservices in a Docker Compose deployment. Across the studied failure fractions, the model reproduces the overall availability degradation curve, while asynchronous semantics for Kafka edges change predicted availabilities by at most about 10^(-5) (0.001 percentage points). This null result suggests that for immediate HTTP availability in this case study, explicitly modeling asynchronous dependencies is not warranted, and a simpler connectivity-only model is sufficient.",
        "authors": [
            "Anatoly A. Krasnovsky"
        ],
        "categories": [
            "cs.SE",
            "cs.DC",
            "cs.PF",
            "eess.SY"
        ],
        "id": "http://arxiv.org/abs/2512.12314v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-13"
    },
    "http://arxiv.org/abs/2512.12299v1": {
        "id": "http://arxiv.org/abs/2512.12299v1",
        "title": "A Conflict-Aware Resource Management Framework for the Computing Continuum",
        "link": "http://arxiv.org/abs/2512.12299v1",
        "tags": [
            "edge",
            "RL",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-12-13",
        "tldr": "Proposes a DRL-based framework for conflict-aware resource orchestration across edge, fog, and cloud. Integrates real-time performance feedback and historical data to mediate resource conflicts. Achieves efficient resource reallocation and adaptive learning in dynamic Kubernetes environments.",
        "abstract": "The increasing device heterogeneity and decentralization requirements in the computing continuum (i.e., spanning edge, fog, and cloud) introduce new challenges in resource orchestration. In such environments, agents are often responsible for optimizing resource usage across deployed services. However, agent decisions can lead to persistent conflict loops, inefficient resource utilization, and degraded service performance. To overcome such challenges, we propose a novel framework for adaptive conflict resolution in resource-oriented orchestration using a Deep Reinforcement Learning (DRL) approach. The framework enables handling resource conflicts across deployments and integrates a DRL model trained to mediate such conflicts based on real-time performance feedback and historical state information. The framework has been prototyped and validated on a Kubernetes-based testbed, illustrating its methodological feasibility and architectural resilience. Preliminary results show that the framework achieves efficient resource reallocation and adaptive learning in dynamic scenarios, thus providing a scalable and resilient solution for conflict-aware orchestration in the computing continuum.",
        "authors": [
            "Vlad Popescu-Vifor",
            "Ilir Murturi",
            "Praveen Kumar Donta",
            "Schahram Dustdar"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-12-13"
    },
    "http://arxiv.org/abs/2512.12295v2": {
        "id": "http://arxiv.org/abs/2512.12295v2",
        "title": "Near-Zero-Overhead Freshness for Recommendation Systems via Inference-Side Model Updates",
        "link": "http://arxiv.org/abs/2512.12295v2",
        "tags": [
            "recommendation",
            "serving",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-12-13",
        "tldr": "Proposes LiveUpdate to eliminate inter-cluster synchronization overhead for recommendation models by colocating LoRA-based training within underutilized inference nodes. Uses dynamic rank adaptation and NUMA-aware scheduling to constrain memory and latency impacts. Achieves 2x better update cost and 0.24% higher accuracy versus baselines.",
        "abstract": "Deep Learning Recommendation Models (DLRMs) underpin personalized services but face a critical freshness-accuracy tradeoff due to massive parameter synchronization overheads. Production DLRMs deploy decoupled training/inference clusters, where synchronizing petabyte-scale embedding tables (EMTs) causes multi-minute staleness, degrading recommendation quality and revenue. We observe that (1) inference nodes exhibit sustained CPU underutilization (peak <= 20%), and (2) EMT gradients possess intrinsic low-rank structure, enabling compact update representation. We present LiveUpdate, a system that eliminates inter-cluster synchronization by colocating Low-Rank Adaptation (LoRA) trainers within inference nodes. LiveUpdate addresses two core challenges: (1) dynamic rank adaptation via singular value monitoring to constrain memory overhead (<2% of EMTs), and (2) NUMA-aware resource scheduling with hardware-enforced QoS to eliminate update inference contention (P99 latency impact <20ms). Evaluations show LiveUpdate reduces update costs by 2x versus delta-update baselines while achieving higher accuracy within 1-hour windows. By transforming idle inference resources into freshness engines, LiveUpdate delivers online model updates while outperforming state-of-the-art delta-update methods by 0.04% to 0.24% in accuracy.",
        "authors": [
            "Wenjun Yu",
            "Sitian Chen",
            "Cheng Chen",
            "Amelie Chi Zhou"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-12-13"
    },
    "http://arxiv.org/abs/2512.17942v1": {
        "id": "http://arxiv.org/abs/2512.17942v1",
        "title": "Fast Online Digital Twinning on FPGA for Mission Critical Applications",
        "link": "http://arxiv.org/abs/2512.17942v1",
        "tags": [
            "edge",
            "offloading",
            "hardware"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes an FPGA-accelerated digital twinning framework for low-latency mission-critical applications. Offloads GRU and dense layers to reconfigurable hardware for parallel execution. Achieves 5x faster operation than human reaction time, enabling real-time edge deployment.",
        "abstract": "Digital twinning enables real-time simulation and predictive modeling by maintaining a continuously updated virtual representation of a physical system. In mission-critical applications, such as mid-air collision avoidance, these models must operate online with extremely low latency to ensure safety. However, executing complex Model Recovery (MR) pipelines on edge devices is limited by computational and memory bandwidth constraints. This paper introduces a fast, FPGA-accelerated digital twinning framework that offloads key neural components, including gated recurrent units (GRU) and dense layers, to reconfigurable hardware for efficient parallel execution. Our system achieves real-time responsiveness, operating five times faster than typical human reaction time, and demonstrates the practical viability of deploying digital twins on edge platforms for time-sensitive, safety-critical environments.",
        "authors": [
            "Bin Xu",
            "Ayan Banerjee",
            "Sandeep K. S. Gupta"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-12-13"
    },
    "http://arxiv.org/abs/2512.17941v1": {
        "id": "http://arxiv.org/abs/2512.17941v1",
        "title": "Accelerated Digital Twin Learning for Edge AI: A Comparison of FPGA and Mobile GPU",
        "link": "http://arxiv.org/abs/2512.17941v1",
        "tags": [
            "edge",
            "hardware",
            "quantization"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes a digital twin learning framework accelerated on FPGAs for edge AI healthcare applications. Leverages reconfigurable hardware optimizations for compute/memory efficiency. Achieves 8.8x performance-per-watt gain and 28.5x DRAM reduction over cloud GPU baselines.",
        "abstract": "Digital twins (DTs) can enable precision healthcare by continually learning a mathematical representation of patient-specific dynamics. However, mission critical healthcare applications require fast, resource-efficient DT learning, which is often infeasible with existing model recovery (MR) techniques due to their reliance on iterative solvers and high compute/memory demands. In this paper, we present a general DT learning framework that is amenable to acceleration on reconfigurable hardware such as FPGAs, enabling substantial speedup and energy efficiency. We compare our FPGA-based implementation with a multi-processing implementation in mobile GPU, which is a popular choice for AI in edge devices. Further, we compare both edge AI implementations with cloud GPU baseline. Specifically, our FPGA implementation achieves an 8.8x improvement in \\text{performance-per-watt} for the MR task, a 28.5x reduction in DRAM footprint, and a 1.67x runtime speedup compared to cloud GPU baselines. On the other hand, mobile GPU achieves 2x better performance per watts but has 2x increase in runtime and 10x more DRAM footprint than FPGA. We show the usage of this technique in DT guided synthetic data generation for Type 1 Diabetes and proactive coronary artery disease detection.",
        "authors": [
            "Bin Xu",
            "Ayan Banerjee",
            "Midhat Urooj",
            "Sandeep K. S. Gupta"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-12-13"
    },
    "http://arxiv.org/abs/2512.12131v1": {
        "id": "http://arxiv.org/abs/2512.12131v1",
        "title": "BOOST: BOttleneck-Optimized Scalable Training Framework for Low-Rank Large Language Models",
        "link": "http://arxiv.org/abs/2512.12131v1",
        "tags": [
            "training",
            "sparse"
        ],
        "relevant": true,
        "indexed_date": "2025-12-13",
        "tldr": "Proposes BOOST, a training framework for low-rank LLMs with bottleneck-aware tensor parallelism and other optimizations. Addresses poor scalability of low-rank architectures by reducing communication and improving GPU utilization. Achieves 1.46-1.91× speedup over full-rank baselines.",
        "abstract": "The scale of transformer model pre-training is constrained by the increasing computation and communication cost. Low-rank bottleneck architectures offer a promising solution to significantly reduce the training time and memory footprint with minimum impact on accuracy. Despite algorithmic efficiency, bottleneck architectures scale poorly under standard tensor parallelism. Simply applying 3D parallelism designed for full-rank methods leads to excessive communication and poor GPU utilization. To address this limitation, we propose BOOST, an efficient training framework tailored for large-scale low-rank bottleneck architectures. BOOST introduces a novel Bottleneck-aware Tensor Parallelism, and combines optimizations such as online-RMSNorm, linear layer grouping, and low-rank activation checkpointing to achieve end-to-end training speedup. Evaluations on different low-rank bottleneck architectures demonstrate that BOOST achieves 1.46-1.91$\\times$ speedup over full-rank model baselines and 1.87-2.27$\\times$ speedup over low-rank model with naively integrated 3D parallelism, with improved GPU utilization and reduced communication overhead.",
        "authors": [
            "Zhengyang Wang",
            "Ziyue Liu",
            "Ruijie Zhang",
            "Avinash Maurya",
            "Paul Hovland",
            "Bogdan Nicolae",
            "Franck Cappello",
            "Zheng Zhang"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "submit_date": "2025-12-13"
    },
    "http://arxiv.org/abs/2512.12105v1": {
        "title": "Beyond right or wrong: towards redefining adaptive learning indicators in virtual learning environments",
        "link": "http://arxiv.org/abs/2512.12105v1",
        "abstract": "Student learning development must involve more than just correcting or incorrect questions. However, most adaptive learning methods in Virtual Learning Environments are based on whether the student's response is incorrect or correct. This perspective is limited in assessing the student's learning level, as it does not consider other elements that can be crucial in this process. The objective of this work is to conduct a Systematic Literature Review (SLR) to elucidate which learning indicators influence student learning and which can be implemented in a VLE to assist in adaptive learning. The works selected and filtered by qualitative assessment reveal a comprehensive approach to assessing different aspects of the learning in virtual environments, such as motivation, emotions, physiological responses, brain imaging, and the students' prior knowledge. The discussion of these new indicators allows adaptive technology developers to implement more appropriate solutions to students' realities, resulting in more complete training.",
        "authors": [
            "Andreia dos Santos Sachete",
            "Alba Valeria de SantAnna de Freitas Loiola",
            "Fabio Diniz Rossi",
            "Jose Valdeni de Lima",
            "Raquel Salcedo Gomes"
        ],
        "categories": [
            "cs.CY",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.12105v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-13"
    },
    "http://arxiv.org/abs/2512.12068v2": {
        "title": "TreeVQA: A Tree-Structured Execution Framework for Shot Reduction in Variational Quantum Algorithms",
        "link": "http://arxiv.org/abs/2512.12068v2",
        "abstract": "Variational Quantum Algorithms (VQAs) are promising for near- and intermediate-term quantum computing, but their execution cost is substantial. Each task requires many iterations and numerous circuits per iteration, and real-world applications often involve multiple tasks, scaling with the precision needed to explore the application's energy landscape. This demands an enormous number of execution shots, making practical use prohibitively expensive. We observe that VQA costs can be significantly reduced by exploiting execution similarities across an application's tasks. Based on this insight, we propose TreeVQA, a tree-based execution framework that begins by executing tasks jointly and progressively branches only as their quantum executions diverge. Implemented as a VQA wrapper, TreeVQA integrates with typical VQA applications. Evaluations on scientific and combinatorial benchmarks show shot count reductions of $25.9\\times$ on average and over $100\\times$ for large-scale problems at the same target accuracy. The benefits grow further with increasing problem size and precision requirements.",
        "authors": [
            "Yuewen Hou",
            "Dhanvi Bharadwaj",
            "Gokul Subramanian Ravi"
        ],
        "categories": [
            "quant-ph",
            "cs.AR",
            "cs.DC",
            "cs.ET"
        ],
        "id": "http://arxiv.org/abs/2512.12068v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-12"
    },
    "http://arxiv.org/abs/2512.12036v1": {
        "id": "http://arxiv.org/abs/2512.12036v1",
        "title": "Accelerating Sparse Matrix-Matrix Multiplication on GPUs with Processing Near HBMs",
        "link": "http://arxiv.org/abs/2512.12036v1",
        "tags": [
            "kernel",
            "sparse",
            "hardware"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Optimizes SpGEMM on GPUs for irregular memory access via hardware-software co-design with HBM processing. Presents Hash-based Multi-phase approach and AIA technique, achieving up to 4.18x speedup in GNN training over cuSPARSE on large datasets.",
        "abstract": "Sparse General Matrix-Matrix Multiplication (SpGEMM) is a fundamental operation in numerous scientific computing and data analytics applications, often bottlenecked by irregular memory access patterns. This paper presents Hash based Multi-phase SpGEMM on GPU and the Acceleration of Indirect Memory Access (AIA) technique, a novel custom near-memory processing approach to optimizing SpGEMM on GPU HBM. Our hardware-software co-designed framework for SpGEMM demonstrates significant performance improvements over state-of-the-art methods, particularly in handling complex, application-specific workloads. We evaluate our approach on various graph workloads, including graph contraction, Markov clustering, and Graph Neural Networks (GNNs), showcasing its practical applicability. For graph analytics applications, AIA demonstrates up to 17.3% time reduction from the software-only implementation, while achieving time reduction of 76.5% for Graph Contraction and 58.4% for Markov Clustering compared to cuSPARSE. For GNN training applications with structured global pruning, our hybrid approach delivers an average of 1.43x speedup over software-only implementation across six benchmark datasets and three architectures (GCN, GIN, GraphSAGE), and shows 1.95x speedup for GNN workloads when compared to cuSPARSE, with up to 4.18x gains on large-scale datasets.",
        "authors": [
            "Shiju Li",
            "Younghoon Min",
            "Hane Yie",
            "Hoshik Kim",
            "Soohong Ahn",
            "Joonseop Sim",
            "Chul-Ho Lee",
            "Jongryool Kim"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-12-12"
    },
    "http://arxiv.org/abs/2512.12006v1": {
        "title": "MVP-ORAM: a Wait-free Concurrent ORAM for Confidential BFT Storage",
        "link": "http://arxiv.org/abs/2512.12006v1",
        "abstract": "It is well known that encryption alone is not enough to protect data privacy. Access patterns, revealed when operations are performed, can also be leveraged in inference attacks. Oblivious RAM (ORAM) hides access patterns by making client requests oblivious. However, existing protocols are still limited in supporting concurrent clients and Byzantine fault tolerance (BFT). We present MVP-ORAM, the first wait-free ORAM protocol that supports concurrent fail-prone clients. In contrast to previous works, MVP-ORAM avoids using trusted proxies, which require additional security assumptions, and concurrency control mechanisms based on inter-client communication or distributed locks, which limit overall throughput and the capability of tolerating faulty clients. Instead, MVP-ORAM enables clients to perform concurrent requests and merge conflicting updates as they happen, satisfying wait-freedom, i.e., clients make progress independently of the performance or failures of other clients. Since wait and collision freedom are fundamentally contradictory goals that cannot be achieved simultaneously in an asynchronous concurrent ORAM service, we define a weaker notion of obliviousness that depends on the application workload and number of concurrent clients, and prove MVP-ORAM is secure in practical scenarios where clients perform skewed block accesses. By being wait-free, MVP-ORAM can be seamlessly integrated into existing confidential BFT data stores, creating the first BFT ORAM construction. We implement MVP-ORAM on top of a confidential BFT data store and show our prototype can process hundreds of 4KB accesses per second in modern clouds.",
        "authors": [
            "Robin Vassantlal",
            "Hasan Heydari",
            "Bernardo Ferreira",
            "Alysson Bessani"
        ],
        "categories": [
            "cs.CR",
            "cs.DC",
            "cs.DS"
        ],
        "id": "http://arxiv.org/abs/2512.12006v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-12"
    },
    "http://arxiv.org/abs/2512.11775v1": {
        "title": "Hypergraph based Multi-Party Payment Channel",
        "link": "http://arxiv.org/abs/2512.11775v1",
        "abstract": "Public blockchains inherently offer low throughput and high latency, motivating off-chain scalability solutions such as Payment Channel Networks (PCNs). However, existing PCNs suffer from liquidity fragmentation-funds locked in one channel cannot be reused elsewhere-and channel depletion, both of which limit routing efficiency and reduce transaction success rates. Multi-party channel (MPC) constructions mitigate these issues, but they typically rely on leaders or coordinators, creating single points of failure and providing only limited flexibility for inter-channel payments.   We introduce Hypergraph-based Multi-Party Payment Channels (H-MPCs), a new off-chain construction that replaces bilateral channels with collectively funded hyperedges. These hyperedges enable fully concurrent, leaderless intra- and inter-hyperedge payments through verifiable, proposer-ordered DAG updates, offering significantly greater flexibility and concurrency than prior designs.   Our implementation on a 150-node network demonstrates a transaction success rate of approximately 94% without HTLC expiry or routing failures, highlighting the robustness of H-MPCs.",
        "authors": [
            "Ayush Nainwal",
            "Atharva Kamble",
            "Nitin Awathare"
        ],
        "categories": [
            "cs.DC",
            "cs.CR",
            "cs.NI"
        ],
        "id": "http://arxiv.org/abs/2512.11775v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-12"
    },
    "http://arxiv.org/abs/2512.11727v1": {
        "id": "http://arxiv.org/abs/2512.11727v1",
        "title": "ECCO: Leveraging Cross-Camera Correlations for Efficient Live Video Continuous Learning",
        "link": "http://arxiv.org/abs/2512.11727v1",
        "tags": [
            "video",
            "training",
            "RAG"
        ],
        "relevant": true,
        "indexed_date": "2025-12-12",
        "tldr": "Introduces ECCO, a framework for efficient continuous learning in multi-camera systems by grouping cameras with correlated data drift for shared model retraining. ECCO includes dynamic grouping, GPU allocation, and transmission control. Achieves 6.7%-18.1% higher accuracy or supports 3.3× more cameras at same resource.",
        "abstract": "Recent advances in video analytics address real-time data drift by continuously retraining specialized, lightweight DNN models for individual cameras. However, the current practice of retraining a separate model for each camera suffers from high compute and communication costs, making it unscalable. We present ECCO, a new video analytics framework designed for resource-efficient continuous learning. The key insight is that the data drift, which necessitates model retraining, often shows temporal and spatial correlations across nearby cameras. By identifying cameras that experience similar drift and retraining a shared model for them, ECCO can substantially reduce the associated compute and communication costs. Specifically, ECCO introduces: (i) a lightweight grouping algorithm that dynamically forms and updates camera groups; (ii) a GPU allocator that dynamically assigns GPU resources across different groups to improve retraining accuracy and ensure fairness; and (iii) a transmission controller at each camera that configures frame sampling and coordinates bandwidth sharing with other cameras based on its assigned GPU resources. We conducted extensive evaluations on three distinctive datasets for two vision tasks. Compared to leading baselines, ECCO improves retraining accuracy by 6.7%-18.1% using the same compute and communication resources, or supports 3.3 times more concurrent cameras at the same accuracy.",
        "authors": [
            "Yuze He",
            "Ferdi Kossmann",
            "Srinivasan Seshan",
            "Peter Steenkiste"
        ],
        "categories": [
            "cs.DC",
            "cs.LG",
            "cs.NI"
        ],
        "submit_date": "2025-12-12"
    },
    "http://arxiv.org/abs/2512.11643v1": {
        "title": "Stateless Snowflake: A Cloud-Agnostic Distributed ID Generator Using Network-Derived Identity",
        "link": "http://arxiv.org/abs/2512.11643v1",
        "abstract": "Snowflake-style distributed ID generators are the industry standard for producing k-ordered, unique identifiers at scale. However, the traditional requirement for manually assigned or centrally coordinated worker IDs introduces significant friction in modern container-orchestrated environments (e.g., Kubernetes), where workloads are ephemeral and autoscaled. In such systems, maintaining stable worker identities requires complex stateful sets or external coordination services (e.g., ZooKeeper), negating the operational benefits of stateless microservices.   This paper presents a cloud-agnostic, container-native ID generation protocol that eliminates the dependency on explicit worker IDs. By deriving node uniqueness deterministically from ephemeral network properties - specifically the container's private IPv4 address - the proposed method removes the need for centralized coordination. We introduce a modified bit-allocation scheme (1-41-16-6) that accommodates 16 bits of network-derived entropy while preserving strict monotonicity. We validate the approach across AWS, GCP, and Azure environments. Evaluation results demonstrate that while the design has a theoretical single-node ceiling of approximately 64,000 TPS, in practical microservice deployments the network I/O dominates latency, resulting in end-to-end performance (approximately 31,000 TPS on a 3-node cluster) comparable to classic stateful generators while offering effectively unbounded horizontal scalability.",
        "authors": [
            "Manideep Reddy Chinthareddy"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.11643v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-12-23",
        "submit_date": "2025-12-12"
    },
    "http://arxiv.org/abs/2512.11634v1": {
        "title": "FirecREST v2: lessons learned from redesigning an API for scalable HPC resource access",
        "link": "http://arxiv.org/abs/2512.11634v1",
        "abstract": "Introducing FirecREST v2, the next generation of our open-source RESTful API for programmatic access to HPC resources. FirecREST v2 delivers a 100x performance improvement over its predecessor. This paper explores the lessons learned from redesigning FirecREST from the ground up, with a focus on integrating enhanced security and high throughput as core requirements.   We provide a detailed account of our systematic performance testing methodology, highlighting common bottlenecks in proxy-based APIs with intensive I/O operations. Key design and architectural changes that enabled these performance gains are presented. Finally, we demonstrate the impact of these improvements, supported by independent peer validation, and discuss opportunities for further improvements.",
        "authors": [
            "Elia Palme",
            "Juan Pablo Dorsch",
            "Ali Khosravi",
            "Giovanni Pizzi",
            "Francesco Pagnamenta",
            "Andrea Ceriani",
            "Eirini Koutsaniti",
            "Rafael Sarmiento",
            "Ivano Bonesana",
            "Alejandro Dabin"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.11634v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-12"
    },
    "http://arxiv.org/abs/2512.11532v1": {
        "id": "http://arxiv.org/abs/2512.11532v1",
        "title": "Parallax: Runtime Parallelization for Operator Fallbacks in Heterogeneous Edge Systems",
        "link": "http://arxiv.org/abs/2512.11532v1",
        "tags": [
            "serving",
            "edge",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-12-12",
        "tldr": "Addresses inefficient CPU fallbacks for unsupported DNN operators on edge devices. Proposes Parallax, a framework with DAG partitioning, branch-aware memory management, and an adaptive scheduler. Achieves up to 46% latency reduction compared to state-of-the-art frameworks.",
        "abstract": "The growing demand for real-time DNN applications on edge devices necessitates faster inference of increasingly complex models. Although many devices include specialized accelerators (e.g., mobile GPUs), dynamic control-flow operators and unsupported kernels often fall back to CPU execution. Existing frameworks handle these fallbacks poorly, leaving CPU cores idle and causing high latency and memory spikes. We introduce Parallax, a framework that accelerates mobile DNN inference without model refactoring or custom operator implementations. Parallax first partitions the computation DAG to expose parallelism, then employs branch-aware memory management with dedicated arenas and buffer reuse to reduce runtime footprint. An adaptive scheduler executes branches according to device memory constraints, meanwhile, fine-grained subgraph control enables heterogeneous inference of dynamic models. By evaluating on five representative DNNs across three different mobile devices, Parallax achieves up to 46% latency reduction, maintains controlled memory overhead (26.5% on average), and delivers up to 30% energy savings compared with state-of-the-art frameworks, offering improvements aligned with the responsiveness demands of real-time mobile inference.",
        "authors": [
            "Chong Tang",
            "Hao Dai",
            "Jagmohan Chauhan"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.CV"
        ],
        "submit_date": "2025-12-12"
    },
    "http://arxiv.org/abs/2512.11512v1": {
        "title": "Enhanced Pruning for Distributed Closeness Centrality under Multi-Packet Messaging",
        "link": "http://arxiv.org/abs/2512.11512v1",
        "abstract": "Identifying central nodes using closeness centrality is a critical task in analyzing large-scale complex networks, yet its decentralized computation remains challenging due to high communication overhead. Existing distributed approximation techniques, such as pruning, often fail to fully mitigate the cost of exchanging numerous data packets in large network settings. In this paper, we introduce a novel enhancement to the distributed pruning method specifically designed to overcome this communication bottleneck. Our core contribution is a technique that leverages multi-packet messaging, allowing nodes to batch and transmit larger, consolidated data blocks. This approach significantly reduces the number of exchanged messages and minimizes data loss without compromising the accuracy of the centrality estimates. We demonstrate that our multi-packet approach substantially outperforms the original pruning technique in both message efficiency (fewer overall messages) and computation time, preserving the core approximation properties of the baseline method. While we observe a manageable trade-off in increased per-node memory usage and local overhead, our findings show that this is outweighed by the gains in communication efficiency, particularly for very large networks and complex packet structures. Our work offers a more scalable and efficient solution for decentralized closeness centrality computation, promising a significant step forward for large-scale network analysis.",
        "authors": [
            "Patrick D. Manya",
            "Eugene M. Mbuyi",
            "Gothy T. Ngoie",
            "Jordan F. Masakuna"
        ],
        "categories": [
            "cs.DC",
            "cs.SI"
        ],
        "id": "http://arxiv.org/abs/2512.11512v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-12"
    },
    "http://arxiv.org/abs/2512.15766v1": {
        "title": "LOOPRAG: Enhancing Loop Transformation Optimization with Retrieval-Augmented Large Language Models",
        "link": "http://arxiv.org/abs/2512.15766v1",
        "abstract": "Loop transformations are semantics-preserving optimization techniques, widely used to maximize objectives such as parallelism. Despite decades of research, applying the optimal composition of loop transformations remains challenging due to inherent complexities, including cost modeling for optimization objectives. Recent studies have explored the potential of Large Language Models (LLMs) for code optimization. However, our key observation is that LLMs often struggle with effective loop transformation optimization, frequently leading to errors or suboptimal optimization, thereby missing opportunities for performance improvements. To bridge this gap, we propose LOOPRAG, a novel retrieval-augmented generation framework designed to guide LLMs in performing effective loop optimization on Static Control Part. We introduce a parameter-driven method to harness loop properties, which trigger various loop transformations, and generate diverse yet legal example codes serving as a demonstration source. To effectively obtain the most informative demonstrations, we propose a loop-aware algorithm based on loop features, which balances similarity and diversity for code retrieval. To enhance correct and efficient code generation, we introduce a feedback-based iterative mechanism that incorporates compilation, testing and performance results as feedback to guide LLMs. Each optimized code undergoes mutation, coverage and differential testing for equivalence checking. We evaluate LOOPRAG on PolyBench, TSVC and LORE benchmark suites, and compare it against compilers (GCC-Graphite, Clang-Polly, Perspective and ICX) and representative LLMs (DeepSeek and GPT-4). The results demonstrate average speedups over base compilers of up to 11.20$\\times$, 14.34$\\times$, and 9.29$\\times$ for PolyBench, TSVC, and LORE, respectively, and speedups over base LLMs of up to 11.97$\\times$, 5.61$\\times$, and 11.59$\\times$.",
        "authors": [
            "Yijie Zhi",
            "Yayu Cao",
            "Jianhua Dai",
            "Xiaoyang Han",
            "Jingwen Pu",
            "Qingran Wu",
            "Sheng Cheng",
            "Ming Cai"
        ],
        "categories": [
            "cs.PL",
            "cs.AI",
            "cs.DC",
            "cs.PF"
        ],
        "id": "http://arxiv.org/abs/2512.15766v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-12"
    },
    "http://arxiv.org/abs/2512.11306v2": {
        "id": "http://arxiv.org/abs/2512.11306v2",
        "title": "RollMux: Phase-Level Multiplexing for Disaggregated RL Post-Training",
        "link": "http://arxiv.org/abs/2512.11306v2",
        "tags": [
            "RL",
            "disaggregation",
            "training"
        ],
        "relevant": true,
        "indexed_date": "2025-12-12",
        "tldr": "Addresses dependency bubbles in disaggregated RL post-training. Proposes RollMux, a cluster scheduler with co-execution groups and two-tier scheduling. Achieves 1.84x cost efficiency over baseline disaggregation with 100% SLO attainment.",
        "abstract": "Rollout-training disaggregation is emerging as the standard architecture for Reinforcement Learning (RL) post-training, where memory-bound rollout and compute-bound training are physically disaggregated onto purpose-built clusters to maximize hardware efficiency. However, the strict synchronization required by on-policy algorithms introduces severe dependency bubbles, forcing one cluster to idle while the dependent phase is running on the other. We present RollMux, a cluster scheduling framework that reclaims these bubbles through cross-cluster orchestration. RollMux is built on the insight that the structural idleness of one job can be effectively utilized by the active phase of another. To realize this, we introduce the co-execution group abstraction, which partitions the cluster into isolated locality domains. This abstraction enables a two-tier scheduling architecture: an inter-group scheduler that optimizes job placement using conservative stochastic planning, and an intra-group scheduler that orchestrates a provably optimal round-robin schedule. The group abstraction also imposes a residency constraint, ensuring that massive model states remain cached in host memory to enable \"warm-star\" context switching. We evaluate RollMux on a production-scale testbed with 328 H20 and 328 H800 GPUs. RollMux improves cost efficiency by 1.84x over standard disaggregation and 1.38x over state-of-the-art co-located baselines, all while achieving 100% SLO attainment.",
        "authors": [
            "Tianyuan Wu",
            "Lunxi Cao",
            "Yining Wei",
            "Wei Gao",
            "Yuheng Zhao",
            "Dakai An",
            "Shaopan Xiong",
            "Zhiqiang Lv",
            "Ju Huang",
            "Siran Yang",
            "Yinghao Yu",
            "Jiamang Wang",
            "Lin Qu",
            "Wei Wang"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-12-12"
    },
    "http://arxiv.org/abs/2512.11200v1": {
        "id": "http://arxiv.org/abs/2512.11200v1",
        "title": "Theoretical Foundations of GPU-Native Compilation for Rapid Code Iteration",
        "link": "http://arxiv.org/abs/2512.11200v1",
        "tags": [
            "kernel",
            "hardware",
            "training"
        ],
        "relevant": true,
        "indexed_date": "2025-12-12",
        "tldr": "Proposes GPU-native compilation to eliminate CPU-GPU data transfer bottlenecks. Introduces parallel traditional, neural, and hybrid compilation strategies with probabilistic verification. Achieves 10-100x speedups in code iteration cycles through massive parallelism and transfer elimination.",
        "abstract": "Current AI code generation systems suffer from significant latency bottlenecks due to CPU-GPU data transfers during compilation, execution, and testing phases. We establish theoretical foundations for three complementary approaches to GPU-native compilation that eliminate these transfers: (1) parallel traditional compilation adapted for GPU execution, (2) neural compilation using learned sequence-to-sequence translation with probabilistic verification, and (3) hybrid architectures combining both strategies. We derive latency and energy bounds demonstrating potential speedups of 10-100x for code iteration cycles. Our analysis shows that traditional GPU compilation provides 2-5x improvements through transfer elimination, neural compilation achieves 10-100x speedups via massive parallelism, and hybrid approaches offer practical deployment paths with guaranteed correctness. We formalize the probabilistic verification framework that enables trading compilation accuracy for parallel exploration, and discuss implications for self-improving AI systems and future analog computing substrates.",
        "authors": [
            "Adilet Metinov",
            "Gulida M. Kudakeeva",
            "Gulnara D. Kabaeva"
        ],
        "categories": [
            "cs.DC",
            "cs.LG",
            "cs.PL"
        ],
        "submit_date": "2025-12-12"
    },
    "http://arxiv.org/abs/2512.11112v1": {
        "title": "An LLVM-Based Optimization Pipeline for SPDZ",
        "link": "http://arxiv.org/abs/2512.11112v1",
        "abstract": "Actively secure arithmetic MPC is now practical for real applications, but performance and usability are still limited by framework-specific compilation stacks, the need for programmers to explicitly express parallelism, and high communication overhead. We design and implement a proof-of-concept LLVM-based optimization pipeline for the SPDZ protocol that addresses these bottlenecks. Our front end accepts a subset of C with lightweight privacy annotations and lowers it to LLVM IR, allowing us to reuse mature analyses and transformations to automatically batch independent arithmetic operations. Our back end performs data-flow and control-flow analysis on the optimized IR to drive a non-blocking runtime scheduler that overlaps independent operations and aggressively overlaps communication with computation; when enabled, it can map batched operations to GPU kernels. This design preserves a low learning curve by using a mainstream language and hiding optimization and hardware-specific mechanics from programmers. We evaluate the system on controlled microbenchmarks against MP-SPDZ, focusing on online phase performance. Our CPU back end achieves up to 5.56 times speedup under intermediate and heavy algebraic workloads, shows strong scaling with thread count, and our GPU back end scales better as the input size increases. Overall, these results indicate that leveraging LLVM with protocol-aware scheduling is an effective architectural direction for extracting parallelism without sacrificing usability.",
        "authors": [
            "Tianye Dai",
            "Hammurabi Mendes",
            "Heuichan Lim"
        ],
        "categories": [
            "cs.CR",
            "cs.DC",
            "cs.SE"
        ],
        "id": "http://arxiv.org/abs/2512.11112v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-11"
    },
    "http://arxiv.org/abs/2512.10732v1": {
        "title": "TriHaRd: Higher Resilience for TEE Trusted Time",
        "link": "http://arxiv.org/abs/2512.10732v1",
        "abstract": "Accurately measuring time passing is critical for many applications. However, in Trusted Execution Environments (TEEs) such as Intel SGX, the time source is outside the Trusted Computing Base: a malicious host can manipulate the TEE's notion of time, jumping in time or affecting perceived time speed. Previous work (Triad) proposes protocols for TEEs to maintain a trustworthy time source by building a cluster of TEEs that collaborate with each other and with a remote Time Authority to maintain a continuous notion of passing time. However, such approaches still allow an attacker to control the operating system and arbitrarily manipulate their own TEE's perceived clock speed. An attacker can even propagate faster passage of time to honest machines participating in Triad's trusted time protocol, causing them to skip to timestamps arbitrarily far in the future. We propose TriHaRd, a TEE trusted time protocol achieving high resilience against clock speed and offset manipulations, notably through Byzantine-resilient clock updates and consistency checks. We empirically show that TriHaRd mitigates known attacks against Triad.",
        "authors": [
            "Matthieu Bettinger",
            "Sonia Ben Mokhtar",
            "Pascal Felber",
            "Etienne Rivière",
            "Valerio Schiavoni",
            "Anthony Simonet-Boulogne"
        ],
        "categories": [
            "cs.CR",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.10732v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-11"
    },
    "http://arxiv.org/abs/2512.10667v1": {
        "title": "A Proof of Success and Reward Distribution Protocol for Multi-bridge Architecture in Cross-chain Communication",
        "link": "http://arxiv.org/abs/2512.10667v1",
        "abstract": "Single-bridge blockchain solutions enable cross-chain communication. However, they are associated with centralization and single-point-of-failure risks. This paper proposes Proof of Success and Reward Distribution (PSCRD), a novel multi-bridge response coordination and incentive distribution protocol designed to address the challenges. PSCRD introduces a fair reward distribution system that equitably distributes the transfer fee among participating bridges, incentivizing honest behavior and sustained commitment. The purpose is to encourage bridge participation for higher decentralization and lower single-point-of-failure risks. The mathematical analysis and simulation results validate the effectiveness of PSCRD using two key metrics: the Gini index, which demonstrates a progressive improvement in the fairness of the reward distribution as new bridge groups joined the network; and the Nakamoto coefficient, which shows a significant improvement in decentralization over time. These findings highlight that PSCRD provides a more resilient and secure cross-chain bridge system without substantially increasing user costs.",
        "authors": [
            "Damilare Peter Oyinloye",
            "Mohd Sameen Chishti",
            "Jingyue Li"
        ],
        "categories": [
            "cs.CR",
            "cs.DC",
            "cs.ET"
        ],
        "id": "http://arxiv.org/abs/2512.10667v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-11"
    },
    "http://arxiv.org/abs/2512.10576v1": {
        "id": "http://arxiv.org/abs/2512.10576v1",
        "title": "ESS: An Offload-Centric Latent-Cache Management Architecture for DeepSeek-V3.2-Exp",
        "link": "http://arxiv.org/abs/2512.10576v1",
        "tags": [
            "serving",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-12-11",
        "tldr": "Addresses GPU memory bottleneck in DeepSeek-V3.2-Exp's decode stage caused by linear growth of latent-cache. Proposes ESS, an offload-centric system that selectively moves latent-cache to CPU memory to enable larger batch sizes. Achieves up to 123% throughput improvement at 128K context length.",
        "abstract": "DeepSeek-V3.2-Exp introduces a sparse attention mechanism that significantly reduces inference latency in long-context scenarios. Although the overall throughput has improved greatly, the Decode-stage of PD disaggregation remains to be a major bottleneck. This bottleneck primarily stems from the conflict between linear growth of Latent-Cache with sequence length and the limited GPU memory capacity, which constrains the feasible batch-size and thereby suppresses Decode-stage throughput.   To address this challenge, we propose ESS (Extended Sparse Server), an offload-centric system design tailored for DeepSeek-V3.2-Exp. ESS selectively offloads Latent-Cache to CPU memory while preserving latency-critical components on GPU. By freeing up GPU memory, ESS effectively decoupling batch-size scaling from GPU memory constraints. This design significantly improves Decode-stage throughput, thereby reducing deployment costs in real-world settings.   Our high-fidelity simulations show that ESS delivers 69.4\\% throughput improvement at 32K context length and up to 123\\% throughput improvement at 128K, demonstrating its effectiveness for large-context inference workloads. These results highlight ESS as a practical and scalable solution for long-context LLM serving.",
        "authors": [
            "Xinhang Chen",
            "Chao Zhang",
            "Jiahuan He",
            "Wei Liu",
            "Jianming Zhang",
            "Wenlong Zhou",
            "Xiao Li",
            "Pai Zeng",
            "Shiyong Li",
            "Yuanpan Qian",
            "Dong Li",
            "Zhaogeng Li"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-12-11"
    },
    "http://arxiv.org/abs/2512.10443v1": {
        "title": "Clustered Federated Learning with Hierarchical Knowledge Distillation",
        "link": "http://arxiv.org/abs/2512.10443v1",
        "abstract": "Clustered Federated Learning (CFL) has emerged as a powerful approach for addressing data heterogeneity and ensuring privacy in large distributed IoT environments. By clustering clients and training cluster-specific models, CFL enables personalized models tailored to groups of heterogeneous clients. However, conventional CFL approaches suffer from fragmented learning for training independent global models for each cluster and fail to take advantage of collective cluster insights. This paper advocates a shift to hierarchical CFL, allowing bi-level aggregation to train cluster-specific models at the edge and a unified global model at the cloud. This shift improves training efficiency yet might introduce communication challenges. To this end, we propose CFLHKD, a novel personalization scheme for integrating hierarchical cluster knowledge into CFL. Built upon multi-teacher knowledge distillation, CFLHKD enables inter-cluster knowledge sharing while preserving cluster-specific personalization. CFLHKD adopts a bi-level aggregation to bridge the gap between local and global learning. Extensive evaluations of standard benchmark datasets demonstrate that CFLHKD outperforms representative baselines in cluster-specific and global model accuracy and achieves a performance improvement of 3.32-7.57\\%.",
        "authors": [
            "Sabtain Ahmad",
            "Meerzhan Kanatbekova",
            "Ivona Brandic",
            "Atakan Aral"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.LG"
        ],
        "id": "http://arxiv.org/abs/2512.10443v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-11"
    },
    "http://arxiv.org/abs/2512.10426v1": {
        "title": "Differential Privacy for Secure Machine Learning in Healthcare IoT-Cloud Systems",
        "link": "http://arxiv.org/abs/2512.10426v1",
        "abstract": "Healthcare has become exceptionally sophisticated, as wearables and connected medical devices are revolutionising remote patient monitoring, emergency response, medication management, diagnosis, and predictive and prescriptive analytics. Internet of Things and Cloud computing integrated systems (IoT-Cloud) facilitate sensing, automation, and processing for these healthcare applications. While real-time response is crucial for alleviating patient emergencies, protecting patient privacy is extremely important in data-driven healthcare. In this paper, we propose a multi-layer IoT, Edge and Cloud architecture to enhance the speed of response for emergency healthcare by distributing tasks based on response criticality and permanence of storage. Privacy of patient data is assured by proposing a Differential Privacy framework across several machine learning models such as K-means, Logistic Regression, Random Forest and Naive Bayes. We establish a comprehensive threat model identifying three adversary classes and evaluate Laplace, Gaussian, and hybrid noise mechanisms across varying privacy budgets, with supervised algorithms achieving up to 86% accuracy. The proposed hybrid Laplace-Gaussian noise mechanism with adaptive budget allocation provides a balanced approach, offering moderate tails and better privacy-utility trade-offs for both low and high dimension datasets. At the practical threshold of $\\varepsilon = 5.0$, supervised algorithms achieve 82-84% accuracy while reducing attribute inference attacks by up to 18% and data reconstruction correlation by 70%. Blockchain security further ensures trusted communication through time-stamping, traceability, and immutability for analytics applications. Edge computing demonstrates 8$\\times$ latency reduction for emergency scenarios, validating the hierarchical architecture for time-critical operations.",
        "authors": [
            "N Mangala",
            "Murtaza Rangwala",
            "S Aishwarya",
            "B Eswara Reddy",
            "Rajkumar Buyya",
            "KR Venugopal",
            "SS Iyengar",
            "LM Patnaik"
        ],
        "categories": [
            "cs.CR",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.10426v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-11"
    },
    "http://arxiv.org/abs/2512.10425v1": {
        "title": "Making Wide Stripes Practical: Cascaded Parity LRCs for Efficient Repair and High Reliability",
        "link": "http://arxiv.org/abs/2512.10425v1",
        "abstract": "Erasure coding with wide stripes is increasingly adopted to reduce storage overhead in large-scale storage systems. However, existing Locally Repairable Codes (LRCs) exhibit structural limitations in this setting: inflated local groups increase single-node repair cost, multi-node failures frequently trigger expensive global repair, and reliability degrades sharply. We identify a key root cause: local and global parity blocks are designed independently, preventing them from cooperating during repair. We present Cascaded Parity LRCs (CP-LRCs), a new family of wide stripe LRCs that embed structured dependency between parity blocks by decomposing a global parity block across all local parity blocks. This creates a cascaded parity group that preserves MDS-level fault tolerance while enabling low-bandwidth single-node and multi-node repairs. We provide a general coefficient-generation framework, develop repair algorithms exploiting cascading, and instantiate the design with CP-Azure and CP-Uniform. Evaluations on Alibaba Cloud show reductions in repair time of up to 41% for single-node failures and 26% for two-node failures.",
        "authors": [
            "Fan Yu",
            "Guodong Li",
            "Si Wu",
            "Weijun Fang",
            "Sihuang Hu"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.10425v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-11"
    },
    "http://arxiv.org/abs/2512.10372v1": {
        "title": "D2M: A Decentralized, Privacy-Preserving, Incentive-Compatible Data Marketplace for Collaborative Learning",
        "link": "http://arxiv.org/abs/2512.10372v1",
        "abstract": "The rising demand for collaborative machine learning and data analytics calls for secure and decentralized data sharing frameworks that balance privacy, trust, and incentives. Existing approaches, including federated learning (FL) and blockchain-based data markets, fall short: FL often depends on trusted aggregators and lacks Byzantine robustness, while blockchain frameworks struggle with computation-intensive training and incentive integration.   We present \\prot, a decentralized data marketplace that unifies federated learning, blockchain arbitration, and economic incentives into a single framework for privacy-preserving data sharing. \\prot\\ enables data buyers to submit bid-based requests via blockchain smart contracts, which manage auctions, escrow, and dispute resolution. Computationally intensive training is delegated to \\cone\\ (\\uline{Co}mpute \\uline{N}etwork for \\uline{E}xecution), an off-chain distributed execution layer. To safeguard against adversarial behavior, \\prot\\ integrates a modified YODA protocol with exponentially growing execution sets for resilient consensus, and introduces Corrected OSMD to mitigate malicious or low-quality contributions from sellers. All protocols are incentive-compatible, and our game-theoretic analysis establishes honesty as the dominant strategy.   We implement \\prot\\ on Ethereum and evaluate it over benchmark datasets -- MNIST, Fashion-MNIST, and CIFAR-10 -- under varying adversarial settings. \\prot\\ achieves up to 99\\% accuracy on MNIST and 90\\% on Fashion-MNIST, with less than 3\\% degradation up to 30\\% Byzantine nodes, and 56\\% accuracy on CIFAR-10 despite its complexity. Our results show that \\prot\\ ensures privacy, maintains robustness under adversarial conditions, and scales efficiently with the number of participants, making it a practical foundation for real-world decentralized data sharing.",
        "authors": [
            "Yash Srivastava",
            "Shalin Jain",
            "Sneha Awathare",
            "Nitin Awathare"
        ],
        "categories": [
            "cs.CR",
            "cs.AI",
            "cs.DC",
            "cs.LG"
        ],
        "id": "http://arxiv.org/abs/2512.10372v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-11"
    },
    "http://arxiv.org/abs/2512.10361v2": {
        "title": "Bit of a Close Talker: A Practical Guide to Serverless Cloud Co-Location Attacks",
        "link": "http://arxiv.org/abs/2512.10361v2",
        "abstract": "Serverless computing has revolutionized cloud computing by offering users an efficient, cost-effective way to develop and deploy applications without managing infrastructure details. However, serverless cloud users remain vulnerable to various types of attacks, including micro-architectural side-channel attacks. These attacks typically rely on the physical co-location of victim and attacker instances, and attackers need to exploit cloud schedulers to achieve co-location with victims. Therefore, it is crucial to study vulnerabilities in serverless cloud schedulers and assess the security of different serverless scheduling algorithms. This study addresses the gap in understanding and constructing co-location attacks in serverless clouds. We present a comprehensive methodology to uncover exploitable features in serverless scheduling algorithms and to devise strategies for constructing co-location attacks via normal user interfaces. In our experiments, we successfully reveal exploitable vulnerabilities and achieve instance co-location on prevalent open-source infrastructures and Microsoft Azure Functions. We also present a mitigation strategy, the Double-Dip scheduler, to defend against co-location attacks in serverless clouds. Our work highlights critical areas for security enhancements in current cloud schedulers, offering insights to fortify serverless computing environments against potential co-location attacks.",
        "authors": [
            "Wei Shao",
            "Najmeh Nazari",
            "Behnam Omidi",
            "Setareh Rafatirad",
            "Houman Homayoun",
            "Khaled N. Khasawneh",
            "Chongzhou Fang"
        ],
        "categories": [
            "cs.CR",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.10361v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-11"
    },
    "http://arxiv.org/abs/2512.10312v1": {
        "title": "High-Dimensional Data Processing: Benchmarking Machine Learning and Deep Learning Architectures in Local and Distributed Environments",
        "link": "http://arxiv.org/abs/2512.10312v1",
        "abstract": "This document reports the sequence of practices and methodologies implemented during the Big Data course. It details the workflow beginning with the processing of the Epsilon dataset through group and individual strategies, followed by text analysis and classification with RestMex and movie feature analysis with IMDb. Finally, it describes the technical implementation of a distributed computing cluster with Apache Spark on Linux using Scala.",
        "authors": [
            "Julian Rodriguez",
            "Piotr Lopez",
            "Emiliano Lerma",
            "Rafael Medrano",
            "Jacobo Hernandez"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "id": "http://arxiv.org/abs/2512.10312v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-11"
    },
    "http://arxiv.org/abs/2512.10271v1": {
        "id": "http://arxiv.org/abs/2512.10271v1",
        "title": "Hybrid Learning and Optimization-Based Dynamic Scheduling for DL Workloads on Heterogeneous GPU Clusters",
        "link": "http://arxiv.org/abs/2512.10271v1",
        "tags": [
            "training",
            "RL",
            "offline"
        ],
        "relevant": true,
        "indexed_date": "2025-12-11",
        "tldr": "Proposes RLTune, an RL-based scheduling framework with MILP job mapping for DL workloads on heterogeneous GPU clusters. Aims to optimize GPU utilization, queueing delay, and JCT without per-job profiling. Achieves up to 20% higher GPU utilization, 81% lower queueing delay, and 70% shorter JCT.",
        "abstract": "Modern cloud platforms increasingly host large-scale deep learning (DL) workloads, demanding high-throughput, low-latency GPU scheduling. However, the growing heterogeneity of GPU clusters and limited visibility into application characteristics pose major challenges for existing schedulers, which often rely on offline profiling or application-specific assumptions. We present RLTune, an application-agnostic reinforcement learning (RL)-based scheduling framework that dynamically prioritizes and allocates DL jobs on heterogeneous GPU clusters. RLTune integrates RL-driven prioritization with MILP-based job-to-node mapping to optimize system-wide objectives such as job completion time (JCT), queueing delay, and resource utilization. Trained on large-scale production traces from Microsoft Philly, Helios, and Alibaba, RLTune improves GPU utilization by up to 20%, reduces queueing delay by up to 81%, and shortens JCT by as much as 70 percent. Unlike prior approaches, RLTune generalizes across diverse workloads without requiring per-job profiling, making it practical for cloud providers to deploy at scale for more efficient, fair, and sustainable DL workload management.",
        "authors": [
            "Shruti Dongare",
            "Redwan Ibne Seraj Khan",
            "Hadeel Albahar",
            "Nannan Zhao",
            "Diego Melendez Maita",
            "Ali R. Butt"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.LG"
        ],
        "submit_date": "2025-12-11"
    },
    "http://arxiv.org/abs/2512.10236v1": {
        "id": "http://arxiv.org/abs/2512.10236v1",
        "title": "Design Space Exploration of DMA based Finer-Grain Compute Communication Overlap",
        "link": "http://arxiv.org/abs/2512.10236v1",
        "tags": [
            "training",
            "networking",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-12-11",
        "tldr": "Proposes FiCCO, a finer-grain compute-communication overlap method using DMA offloading for distributed ML training. Introduces heuristics to select optimal schedules by characterizing inefficiencies, achieving up to 1.6× speedup and 81% accuracy in unseen scenarios.",
        "abstract": "As both ML training and inference are increasingly distributed, parallelization techniques that shard (divide) ML model across GPUs of a distributed system, are often deployed. With such techniques, there is a high prevalence of data-dependent communication and computation operations where communication is exposed, leaving as high as 1.7x ideal performance on the table. Prior works harness the fact that ML model state and inputs are already sharded, and employ careful overlap of individual computation/communication shards. While such coarse-grain overlap is promising, in this work, we instead make a case for finer-grain compute-communication overlap which we term FiCCO, where we argue for finer-granularity, one-level deeper overlap than at shard-level, to unlock compute/communication overlap for a wider set of network topologies, finer-grain dataflow and more. We show that FiCCO opens up a wider design space of execution schedules than possible at shard-level alone. At the same time, decomposition of ML operations into smaller operations (done in both shard-based and finer-grain techniques) causes operation-level inefficiency losses. To balance the two, we first present a detailed characterization of these inefficiency losses, then present a design space of FiCCO schedules, and finally overlay the schedules with concomitant inefficiency signatures. Doing so helps us design heuristics that frameworks and runtimes can harness to select bespoke FiCCO schedules based on the nature of underlying ML operations. Finally, to further minimize contention inefficiencies inherent with operation overlap, we offload communication to GPU DMA engines. We evaluate several scenarios from realistic ML deployments and demonstrate that our proposed bespoke schedules deliver up to 1.6x speedup and our heuristics provide accurate guidance in 81% of unseen scenarios.",
        "authors": [
            "Shagnik Pal",
            "Shaizeen Aga",
            "Suchita Pati",
            "Mahzabeen Islam",
            "Lizy K. John"
        ],
        "categories": [
            "cs.DC",
            "cs.AR",
            "cs.LG"
        ],
        "submit_date": "2025-12-11"
    },
    "http://arxiv.org/abs/2512.10020v1": {
        "title": "A Comparative Analysis of zk-SNARKs and zk-STARKs: Theory and Practice",
        "link": "http://arxiv.org/abs/2512.10020v1",
        "abstract": "Zero-knowledge proofs (ZKPs) are central to secure and privacy-preserving computation, with zk-SNARKs and zk-STARKs emerging as leading frameworks offering distinct trade-offs in efficiency, scalability, and trust assumptions. While their theoretical foundations are well studied, practical performance under real-world conditions remains less understood.   In this work, we present a systematic, implementation-level comparison of zk-SNARKs (Groth16) and zk-STARKs using publicly available reference implementations on a consumer-grade ARM platform. Our empirical evaluation covers proof generation time, verification latency, proof size, and CPU profiling. Results show that zk-SNARKs generate proofs 68x faster with 123x smaller proof size, but verify slower and require trusted setup, whereas zk-STARKs, despite larger proofs and slower generation, verify faster and remain transparent and post-quantum secure. Profiling further identifies distinct computational bottlenecks across the two systems, underscoring how execution models and implementation details significantly affect real-world performance. These findings provide actionable insights for developers, protocol designers, and researchers in selecting and optimizing proof systems for applications such as privacy-preserving transactions, verifiable computation, and scalable rollups.",
        "authors": [
            "Ayush Nainwal",
            "Atharva Kamble",
            "Nitin Awathare"
        ],
        "categories": [
            "cs.CR",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.10020v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-10"
    },
    "http://arxiv.org/abs/2512.09902v1": {
        "title": "Link-Sharing Backpressure Routing In Wireless Multi-Hop Networks",
        "link": "http://arxiv.org/abs/2512.09902v1",
        "abstract": "Backpressure (BP) routing and scheduling is an established resource allocation method for wireless multi-hop networks, noted for its fully distributed operation and maximum queue stability. Recent advances in shortest path-biased BP routing (SP-BP) mitigate shortcomings such as slow startup and random walks, yet exclusive link-level commodity selection still causes last-packet problem and bandwidth underutilization. By revisiting the Lyapunov drift theory underlying BP, we show that the legacy exclusive commodity selection is unnecessary, and propose a Maximum Utility (MaxU) link-sharing method to expand its performance envelope without increasing control message overhead. Numerical results show that MaxU SP-BP substantially mitigates the last-packet problem and slightly expands the network capacity region.",
        "authors": [
            "Zhongyuan Zhao",
            "Yujun Ming",
            "Ananthram Swami",
            "Kevin Chan",
            "Fikadu Dagefu",
            "Santiago Segarra"
        ],
        "categories": [
            "cs.NI",
            "cs.DC",
            "eess.SY"
        ],
        "id": "http://arxiv.org/abs/2512.09902v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-10"
    },
    "http://arxiv.org/abs/2512.09800v1": {
        "id": "http://arxiv.org/abs/2512.09800v1",
        "title": "Ariel-ML: Computing Parallelization with Embedded Rust for Neural Networks on Heterogeneous Multi-core Microcontrollers",
        "link": "http://arxiv.org/abs/2512.09800v1",
        "tags": [
            "edge",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-12-10",
        "tldr": "Presents Ariel-ML, a Rust toolkit for parallelized ANN inference on multi-core microcontrollers. Combines a generic TinyML pipeline with embedded Rust to utilize multi-core capabilities. Achieves lower inference latency than prior art while maintaining comparable memory footprint to C/C++ toolkits.",
        "abstract": "Low-power microcontroller (MCU) hardware is currently evolving from single-core architectures to predominantly multi-core architectures. In parallel, new embedded software building blocks are more and more written in Rust, while C/C++ dominance fades in this domain. On the other hand, small artificial neural networks (ANN) of various kinds are increasingly deployed in edge AI use cases, thus deployed and executed directly on low-power MCUs. In this context, both incremental improvements and novel innovative services will have to be continuously retrofitted using ANNs execution in software embedded on sensing/actuating systems already deployed in the field. However, there was so far no Rust embedded software platform automating parallelization for inference computation on multi-core MCUs executing arbitrary TinyML models. This paper thus fills this gap by introducing Ariel-ML, a novel toolkit we designed combining a generic TinyML pipeline and an embedded Rust software platform which can take full advantage of multi-core capabilities of various 32bit microcontroller families (Arm Cortex-M, RISC-V, ESP-32). We published the full open source code of its implementation, which we used to benchmark its capabilities using a zoo of various TinyML models. We show that Ariel-ML outperforms prior art in terms of inference latency as expected, and we show that, compared to pre-existing toolkits using embedded C/C++, Ariel-ML achieves comparable memory footprints. Ariel-ML thus provides a useful basis for TinyML practitioners and resource-constrained embedded Rust developers.",
        "authors": [
            "Zhaolan Huang",
            "Kaspar Schleiser",
            "Gyungmin Myung",
            "Emmanuel Baccelli"
        ],
        "categories": [
            "cs.LG",
            "cs.DC",
            "cs.PF"
        ],
        "submit_date": "2025-12-10"
    },
    "http://arxiv.org/abs/2512.09710v1": {
        "title": "Recoverable Lock-Free Locks",
        "link": "http://arxiv.org/abs/2512.09710v1",
        "abstract": "This paper presents the first transformation that introduces both lock-freedom and recoverability. Our transformation starts with a lock-based implementation, and provides a recoverable, lock-free substitution to lock acquire and lock release operations. The transformation supports nested locks for generality and ensures recoverability without jeopardising the correctness of the lock-based implementation it is applied on.",
        "authors": [
            "Hagit Attiya",
            "Panagiota Fatourou",
            "Eleftherios Kosmas",
            "Yuanhao Wei"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.09710v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-10"
    },
    "http://arxiv.org/abs/2512.09685v1": {
        "id": "http://arxiv.org/abs/2512.09685v1",
        "title": "Straggler Tolerant and Resilient DL Training on Homogeneous GPUs",
        "link": "http://arxiv.org/abs/2512.09685v1",
        "tags": [
            "training",
            "scheduling"
        ],
        "relevant": true,
        "indexed_date": "2025-12-10",
        "tldr": "Investigate stragglers in distributed training on GPU clusters. Propose STAR with adaptive synchronization modes and resource reallocation to mitigate stragglers caused by CPU/bandwidth imbalance. Reduces Time-To-Accuracy by 70% compared to state-of-the-art.",
        "abstract": "Despite the popularity of homogeneous GPU-based deep learning (DL) training, the prevalence, causes and impact of stragglers and the effectiveness of existing straggler mitigation approaches are still not well understood in this scenario due to limited research on these questions. To fill this gap, we conducted comprehensive experiments and found that stragglers remain widespread due to CPU and bandwidth usage imbalances. Additionally, existing mitigation methods that switch from synchronous stochastic gradient descent (SSGD) to asynchronous SGD (ASGD) may not improve Time-To-Accuracy (TTA) and can even generate more stragglers due to its higher resource consumption. To address these newly found problems, we propose the Straggler Tolerant And Resilient DL training system (STAR). STAR includes new synchronization modes that group workers for each parameter updating. It has a heuristic and an ML method to choose the optimal synchronization mode for minimizing TTA, and reallocates resources to support the selected mode while minimizing the impact on co-located jobs. Moreover, it proactively prevents stragglers by avoiding overloading the CPU and bandwidth resources in allocating PSs (which consume high CPU and bandwidth) and in gradient transmission. Our trace-driven evaluation on AWS shows that STAR generates 48-84% and 51-70% lower TTA than state-of-the-art systems in the PS and all-reduce architectures, respectively, while maintaining the converged accuracy of SSGD. The code for STAR is open-sourced.",
        "authors": [
            "Zeyu Zhang",
            "Haiying Shen"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-12-10"
    },
    "http://arxiv.org/abs/2512.09664v1": {
        "title": "SynthPix: A lightspeed PIV images generator",
        "link": "http://arxiv.org/abs/2512.09664v1",
        "abstract": "We describe SynthPix, a synthetic image generator for Particle Image Velocimetry (PIV) with a focus on performance and parallelism on accelerators, implemented in JAX. SynthPix supports the same configuration parameters as existing tools but achieves a throughput several orders of magnitude higher in image-pair generation per second. SynthPix was developed to enable the training of data-hungry reinforcement learning methods for flow estimation and for reducing the iteration times during the development of fast flow estimation methods used in recent active fluids control studies with real-time PIV feedback. We believe SynthPix to be useful for the fluid dynamics community, and in this paper we describe the main ideas behind this software package.",
        "authors": [
            "Antonio Terpin",
            "Alan Bonomi",
            "Francesco Banelli",
            "Raffaello D'Andrea"
        ],
        "categories": [
            "cs.DC",
            "cs.CV",
            "cs.LG",
            "eess.IV"
        ],
        "id": "http://arxiv.org/abs/2512.09664v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-10"
    },
    "http://arxiv.org/abs/2512.09568v1": {
        "title": "PHWSOA: A Pareto-based Hybrid Whale-Seagull Scheduling for Multi-Objective Tasks in Cloud Computing",
        "link": "http://arxiv.org/abs/2512.09568v1",
        "abstract": "Task scheduling is a critical research challenge in cloud computing, a transformative technology widely adopted across industries. Although numerous scheduling solutions exist, they predominantly optimize singular or limited metrics such as execution time or resource utilization often neglecting the need for comprehensive multi-objective optimization. To bridge this gap, this paper proposes the Pareto-based Hybrid Whale-Seagull Optimization Algorithm (PHWSOA). This algorithm synergistically combines the strengths of the Whale Optimization Algorithm (WOA) and the Seagull Optimization Algorithm (SOA), specifically mitigating WOA's limitations in local exploitation and SOA's constraints in global exploration. Leveraging Pareto dominance principles, PHWSOA simultaneously optimizes three key objectives: makespan, virtual machine (VM) load balancing, and economic cost. Key enhancements include: Halton sequence initialization for superior population diversity, a Pareto-guided mutation mechanism to avert premature convergence, and parallel processing for accelerated convergence. Furthermore, a dynamic VM load redistribution mechanism is integrated to improve load balancing during task execution. Extensive experiments conducted on the CloudSim simulator, utilizing real-world workload traces from NASA-iPSC and HPC2N, demonstrate that PHWSOA delivers substantial performance gains. Specifically, it achieves up to a 72.1% reduction in makespan, a 36.8% improvement in VM load balancing, and 23.5% cost savings. These results substantially outperform baseline methods including WOA, GA, PEWOA, and GCWOA underscoring PHWSOA's strong potential for enabling efficient resource management in practical cloud environments.",
        "authors": [
            "Zhi Zhao",
            "Hang Xiao",
            "Wei Rang"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.09568v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-10"
    },
    "http://arxiv.org/abs/2512.09502v1": {
        "title": "Scalable Construction of Spiking Neural Networks using up to thousands of GPUs",
        "link": "http://arxiv.org/abs/2512.09502v1",
        "abstract": "Diverse scientific and engineering research areas deal with discrete, time-stamped changes in large systems of interacting delay differential equations. Simulating such complex systems at scale on high-performance computing clusters demands efficient management of communication and memory. Inspired by the human cerebral cortex -- a sparsely connected network of $\\mathcal{O}(10^{10})$ neurons, each forming $\\mathcal{O}(10^{3})$--$\\mathcal{O}(10^{4})$ synapses and communicating via short electrical pulses called spikes -- we study the simulation of large-scale spiking neural networks for computational neuroscience research. This work presents a novel network construction method for multi-GPU clusters and upcoming exascale supercomputers using the Message Passing Interface (MPI), where each process builds its local connectivity and prepares the data structures for efficient spike exchange across the cluster during state propagation. We demonstrate scaling performance of two cortical models using point-to-point and collective communication, respectively.",
        "authors": [
            "Bruno Golosio",
            "Gianmarco Tiddia",
            "José Villamar",
            "Luca Pontisso",
            "Luca Sergi",
            "Francesco Simula",
            "Pooja Babu",
            "Elena Pastorelli",
            "Abigail Morrison",
            "Markus Diesmann",
            "Alessandro Lonardo",
            "Pier Stanislao Paolucci",
            "Johanna Senk"
        ],
        "categories": [
            "cs.DC",
            "cs.NE",
            "physics.comp-ph",
            "q-bio.NC"
        ],
        "id": "http://arxiv.org/abs/2512.09502v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-10"
    },
    "http://arxiv.org/abs/2512.09472v1": {
        "id": "http://arxiv.org/abs/2512.09472v1",
        "title": "WarmServe: Enabling One-for-Many GPU Prewarming for Multi-LLM Serving",
        "link": "http://arxiv.org/abs/2512.09472v1",
        "tags": [
            "serving",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-12-10",
        "tldr": "Addresses performance degradation in multi-LLM GPU serving due to cold starts. Proposes WarmServe with universal GPU workers, evict-aware placement, proactive prewarming, and zero-overhead memory switching. Achieves 50.8× TTFT improvement and 2.5× request capacity.",
        "abstract": "Deploying multiple models within shared GPU clusters is promising for improving resource efficiency in large language model (LLM) serving. Existing multi-LLM serving systems optimize GPU utilization at the cost of worse inference performance, especially time-to-first-token (TTFT). We identify the root cause of such compromise as their unawareness of future workload characteristics. In contrast, recent analysis on real-world traces has shown the high periodicity and long-term predictability of LLM serving workloads.   We propose universal GPU workers to enable one-for-many GPU prewarming that loads models with knowledge of future workloads. Based on universal GPU workers, we design and build WarmServe, a multi-LLM serving system that (1) mitigates cluster-wide prewarming interference by adopting an evict-aware model placement strategy, (2) prepares universal GPU workers in advance by proactive prewarming, and (3) manages GPU memory with a zero-overhead memory switching mechanism. Evaluation under real-world datasets shows that WarmServe improves TTFT by up to 50.8$\\times$ compared to the state-of-the-art autoscaling-based system, while being capable of serving up to 2.5$\\times$ more requests compared to the GPU-sharing system.",
        "authors": [
            "Chiheng Lou",
            "Sheng Qi",
            "Rui Kang",
            "Yong Zhang",
            "Chen Sun",
            "Pengcheng Wang",
            "Bingyang Liu",
            "Xuanzhe Liu",
            "Xin Jin"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-12-10"
    },
    "http://arxiv.org/abs/2512.09331v1": {
        "id": "http://arxiv.org/abs/2512.09331v1",
        "title": "Passing the Baton: High Throughput Distributed Disk-Based Vector Search with BatANN",
        "link": "http://arxiv.org/abs/2512.09331v1",
        "tags": [
            "RAG",
            "storage",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-12-10",
        "tldr": "Presents BatANN, a distributed disk-based ANN system for scalable vector search using batched query state handoff between servers to maintain locality. Achieves 2.5-6.49x higher throughput over baseline while keeping mean latency below 6ms on billion-point datasets.",
        "abstract": "Vector search underpins modern information-retrieval systems, including retrieval-augmented generation (RAG) pipelines and search engines over unstructured text and images. As datasets scale to billions of vectors, disk-based vector search has emerged as a practical solution. However, looking to the future, we need to anticipate datasets too large for any single server. We present BatANN, a distributed disk-based approximate nearest neighbor (ANN) system that retains the logarithmic search efficiency of a single global graph while achieving near-linear throughput scaling in the number of servers. Our core innovation is that when accessing a neighborhood which is stored on another machine, we send the full state of the query to the other machine to continue executing there for improved locality. On 100M- and 1B-point datasets at 0.95 recall using 10 servers, BatANN achieves 6.21-6.49x and 2.5-5.10x the throughput of the scatter-gather baseline, respectively, while maintaining mean latency below 6 ms. Moreover, we get these results on standard TCP. To our knowledge, BatANN is the first open-source distributed disk-based vector search system to operate over a single global graph.",
        "authors": [
            "Nam Anh Dang",
            "Ben Landrum",
            "Ken Birman"
        ],
        "categories": [
            "cs.DC",
            "cs.IR"
        ],
        "submit_date": "2025-12-10"
    },
    "http://arxiv.org/abs/2512.09309v1": {
        "id": "http://arxiv.org/abs/2512.09309v1",
        "title": "A Distributed Framework for Privacy-Enhanced Vision Transformers on the Edge",
        "link": "http://arxiv.org/abs/2512.09309v1",
        "tags": [
            "edge",
            "offloading",
            "multi-modal"
        ],
        "relevant": true,
        "indexed_date": "2025-12-10",
        "tldr": "Proposes a distributed framework for privacy-enhanced Vision Transformers on edge devices. Uses hierarchical offloading to partition visual data across untrusted clouds and aggregates results locally on a trusted edge device. Reduces reconstruction risk while maintaining near-baseline segmentation performance in SAM case study.",
        "abstract": "Nowadays, visual intelligence tools have become ubiquitous, offering all kinds of convenience and possibilities. However, these tools have high computational requirements that exceed the capabilities of resource-constrained mobile and wearable devices. While offloading visual data to the cloud is a common solution, it introduces significant privacy vulnerabilities during transmission and server-side computation. To address this, we propose a novel distributed, hierarchical offloading framework for Vision Transformers (ViTs) that addresses these privacy challenges by design. Our approach uses a local trusted edge device, such as a mobile phone or an Nvidia Jetson, as the edge orchestrator. This orchestrator partitions the user's visual data into smaller portions and distributes them across multiple independent cloud servers. By design, no single external server possesses the complete image, preventing comprehensive data reconstruction. The final data merging and aggregation computation occurs exclusively on the user's trusted edge device. We apply our framework to the Segment Anything Model (SAM) as a practical case study, which demonstrates that our method substantially enhances content privacy over traditional cloud-based approaches. Evaluations show our framework maintains near-baseline segmentation performance while substantially reducing the risk of content reconstruction and user data exposure. Our framework provides a scalable, privacy-preserving solution for vision tasks in the edge-cloud continuum.",
        "authors": [
            "Zihao Ding",
            "Mufeng Zhu",
            "Zhongze Tang",
            "Sheng Wei",
            "Yao Liu"
        ],
        "categories": [
            "cs.DC",
            "cs.CR",
            "cs.CV"
        ],
        "submit_date": "2025-12-10"
    },
    "http://arxiv.org/abs/2512.15742v1": {
        "id": "http://arxiv.org/abs/2512.15742v1",
        "title": "SHARe-KAN: Holographic Vector Quantization for Memory-Bound Inference",
        "link": "http://arxiv.org/abs/2512.15742v1",
        "tags": [
            "offline",
            "kernel",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Addresses memory constraints in Kolmogorov-Arnold Networks (KANs) during inference. Proposes SHARe-KAN with Gain-Shape-Bias Vector Quantization and LUTHAM compiler for static memory planning. Achieves 88x runtime memory reduction (1.13GB to 12.91MB) while maintaining accuracy.",
        "abstract": "Kolmogorov-Arnold Networks (KANs) face a fundamental memory wall: their learned basis functions create parameter counts that impose extreme bandwidth demands, hindering deployment in memory-constrained environments. We show that Vision KANs exhibit a holographic topology, where information is distributed across the interference of splines rather than localized to specific edges. Consequently, traditional pruning fails (10% sparsity degrades mAP from 85.23% to 45%, a $\\sim$40-point drop). To address this, we present SHARe-KAN, a framework utilizing Gain-Shape-Bias Vector Quantization to exploit functional redundancy while preserving the dense topology. Coupled with LUTHAM, a hardware-aware compiler with static memory planning, we achieve $88\\times$ runtime memory reduction (1.13 GB $\\to$ 12.91 MB) and match uncompressed baseline accuracy on PASCAL VOC. Profiling on NVIDIA Ampere architecture confirms $>90\\%$ L2 cache residency, demonstrating that the workload is decoupled from DRAM bandwidth constraints inherent to spline-based architectures.",
        "authors": [
            "Jeff Smith"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "submit_date": "2025-12-10"
    },
    "http://arxiv.org/abs/2512.09963v2": {
        "id": "http://arxiv.org/abs/2512.09963v2",
        "title": "GoodSpeed: Optimizing Fair Goodput with Adaptive Speculative Decoding in Distributed Edge Inference",
        "link": "http://arxiv.org/abs/2512.09963v2",
        "tags": [
            "serving",
            "offloading",
            "quantization"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes GOODSPEED, an adaptive speculative decoding framework for distributed edge inference. Uses a gradient scheduler to dynamically allocate token verification across heterogeneous draft servers, optimizing fair goodput. Achieves near-optimal goodput with provable fairness and up to 10x lower latency.",
        "abstract": "Large language models (LLMs) have revolutionized natural language processing, yet their high computational demands pose significant challenges for real-time inference, especially in multi-user server speculative decoding and resource-constrained environments. Speculative decoding has emerged as a promising technique to accelerate LLM inference by using lightweight draft models to generate candidate tokens, which are subsequently verified by a larger, more accurate model. However, ensuring both high goodput (the effective rate of accepted tokens) and fairness across multiple draft servers cooperating with a central verification server remains an open challenge. This paper introduces GOODSPEED, a novel distributed inference framework that optimizes goodput through adaptive speculative decoding. GOODSPEED employs a central verification server that coordinates a set of heterogeneous draft servers, each running a small language model to generate speculative tokens. To manage resource allocation effectively, GOODSPEED incorporates a gradient scheduling algorithm that dynamically assigns token verification tasks, maximizing a logarithmic utility function to ensure proportional fairness across servers. By processing speculative outputs from all draft servers in parallel, the framework enables efficient collaboration between the verification server and distributed draft generators, streamlining both latency and throughput. Through rigorous fluid sample path analysis, we show that GOODSPEED converges to the optimal goodput allocation in steady-state conditions and maintains near-optimal performance with provably bounded error under dynamic workloads. These results demonstrate that GOODSPEED provides a scalable, fair and efficient solution for multi-server speculative decoding in distributed LLM inference systems.",
        "authors": [
            "Phuong Tran",
            "Tzu-Hao Liu",
            "Long Tan Le",
            "Tung-Anh Nguyen",
            "Van Quan La",
            "Eason Yu",
            "Han Shu",
            "Choong Seon Hong",
            "Nguyen H. Tran"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-12-10"
    },
    "http://arxiv.org/abs/2512.09277v1": {
        "id": "http://arxiv.org/abs/2512.09277v1",
        "title": "Efficient MoE Serving in the Memory-Bound Regime: Balance Activated Experts, Not Tokens",
        "link": "http://arxiv.org/abs/2512.09277v1",
        "tags": [
            "training",
            "MoE",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-12-10",
        "tldr": "Proposes METRO for efficient MoE serving in memory-bound regimes; balances activated experts per GPU instead of tokens to reduce memory pressure and improve performance. Achieves up to 22% lower decode latency and 4.11x higher throughput.",
        "abstract": "Expert Parallelism (EP) permits Mixture of Experts (MoE) models to scale beyond a single GPU. To address load imbalance across GPUs in EP, existing approaches aim to balance the number of tokens each GPU processes. Surprisingly, we find that this objective degrades performance rather than improving it when processing is memory-bound - a common occurrence in MoE serving, especially in the decode phase. Our analysis reveals that balancing the number of tokens processed per GPU increases the number of activated experts, exacerbating memory pressure in the memory-bound regime.   We propose Minimum Expert Token ROuting, a novel token-routing algorithm for high-performance expert-parallel MoE serving in the memory-bound regime that balances the number of activated experts per GPU rather than token counts. METRO achieves near-optimal routing quality with minimal computational overhead by jointly optimizing algorithmic efficiency and leveraging the GPU's parallel processing power. To guarantee routing quality, METRO also employs a novel allGather scheme to gather global top-k knowledge, which has minimal overhead compared to conventional allToAll. Our evaluation of METRO against EPLB on both real systems (vLLM over 8 A100 GPUs) and a proprietary simulator (8-16 B200 GPUs) shows that METRO reduces decode latency by 11 - 22%, and total token throughput by 3 - 21% for Qwen3 and DeepSeek-V3 serving, where prefill and decode phases are co-deployed. In addition, by trading latency headroom for throughput, METRO improves decode throughput by up to 4.11x over EPLB at a fixed decode SLO.",
        "authors": [
            "Yanpeng Yu",
            "Haiyue Ma",
            "Krish Agarwal",
            "Nicolai Oswald",
            "Qijing Huang",
            "Hugo Linsenmaier",
            "Chunhui Mei",
            "Ritchie Zhao",
            "Ritika Borkar",
            "Bita Darvish Rouhani",
            "David Nellans",
            "Ronny Krashinsky",
            "Anurag Khandelwal"
        ],
        "categories": [
            "cs.DC",
            "cs.AR"
        ],
        "submit_date": "2025-12-10"
    },
    "http://arxiv.org/abs/2512.09961v1": {
        "title": "TDC-Cache: A Trustworthy Decentralized Cooperative Caching Framework for Web3.0",
        "link": "http://arxiv.org/abs/2512.09961v1",
        "abstract": "The rapid growth of Web3.0 is transforming the Internet from a centralized structure to decentralized, which empowers users with unprecedented self-sovereignty over their own data. However, in the context of decentralized data access within Web3.0, it is imperative to cope with efficiency concerns caused by the replication of redundant data, as well as security vulnerabilities caused by data inconsistency. To address these challenges, we develop a Trustworthy Decentralized Cooperative Caching (TDC-Cache) framework for Web3.0 to ensure efficient caching and enhance system resilience against adversarial threats. This framework features a two-layer architecture, wherein the Decentralized Oracle Network (DON) layer serves as a trusted intermediary platform for decentralized caching, bridging the contents from decentralized storage and the content requests from users. In light of the complexity of Web3.0 network topologies and data flows, we propose a Deep Reinforcement Learning-Based Decentralized Caching (DRL-DC) for TDC-Cache to dynamically optimize caching strategies of distributed oracles. Furthermore, we develop a Proof of Cooperative Learning (PoCL) consensus to maintain the consistency of decentralized caching decisions within DON. Experimental results show that, compared with existing approaches, the proposed framework reduces average access latency by 20%, increases the cache hit rate by at most 18%, and improves the average success consensus rate by 10%. Overall, this paper serves as a first foray into the investigation of decentralized caching framework and strategy for Web3.0.",
        "authors": [
            "Jinyu Chen",
            "Long Shi",
            "Taotao Wang",
            "Jiaheng Wang",
            "Wei Zhang"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "id": "http://arxiv.org/abs/2512.09961v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-10"
    },
    "http://arxiv.org/abs/2512.09958v1": {
        "title": "When Quantum Federated Learning Meets Blockchain in 6G Networks",
        "link": "http://arxiv.org/abs/2512.09958v1",
        "abstract": "Quantum federated learning (QFL) is emerging as a key enabler for intelligent, secure, and privacy-preserving model training in next-generation 6G networks. By leveraging the computational advantages of quantum devices, QFL offers significant improvements in learning efficiency and resilience against quantum-era threats. However, future 6G environments are expected to be highly dynamic, decentralized, and data-intensive, which necessitates moving beyond traditional centralized federated learning frameworks. To meet this demand, blockchain technology provides a decentralized, tamper-resistant infrastructure capable of enabling trustless collaboration among distributed quantum edge devices. This paper presents QFLchain, a novel framework that integrates QFL with blockchain to support scalable and secure 6G intelligence. In this work, we investigate four key pillars of \\textit{QFLchain} in the 6G context: (i) communication and consensus overhead, (ii) scalability and storage overhead, (iii) energy inefficiency, and (iv) security vulnerability. A case study is also presented, demonstrating potential advantages of QFLchain, based on simulation, over state-of-the-art approaches in terms of training performance.",
        "authors": [
            "Dinh C. Nguyen",
            "Md Bokhtiar Al Zami",
            "Ratun Rahman",
            "Shaba Shaon",
            "Tuy Tan Nguyen",
            "Fatemeh Afghah"
        ],
        "categories": [
            "cs.CR",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.09958v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-09"
    },
    "http://arxiv.org/abs/2512.09957v1": {
        "title": "CloudFix: Automated Policy Repair for Cloud Access Control Policies Using Large Language Models",
        "link": "http://arxiv.org/abs/2512.09957v1",
        "abstract": "Access control policies are vital for securing modern cloud computing, where organizations must manage access to sensitive data across thousands of users in distributed system settings. Cloud administrators typically write and update policies manually, which can be an error-prone and time-consuming process and can potentially lead to security vulnerabilities. Existing approaches based on symbolic analysis have demonstrated success in automated debugging and repairing access control policies; however, their generalizability is limited in the context of cloud-based access control. Conversely, Large Language Models (LLMs) have been utilized for automated program repair; however, their applicability to repairing cloud access control policies remains unexplored. In this work, we introduce CloudFix, the first automated policy repair framework for cloud access control that combines formal methods with LLMs. Given an access control policy and a specification of allowed and denied access requests, CloudFix employs Formal Methods-based Fault Localization to identify faulty statements in the policy and leverages LLMs to generate potential repairs, which are then verified using SMT solvers. To evaluate CloudFix, we curated a dataset of 282 real-world AWS access control policies extracted from forum posts and augmented them with synthetically generated request sets based on real scenarios. Our experimental results show that CloudFix improves repair accuracy over a Baseline implementation across varying request sizes. Our work is the first to leverage LLMs for policy repair, showcasing the effectiveness of LLMs for access control and enabling efficient and automated repair of cloud access control policies. We make our tool Cloudfix and AWS dataset publicly available.",
        "authors": [
            "Bethel Hall",
            "Owen Ungaro",
            "William Eiers"
        ],
        "categories": [
            "cs.DC",
            "cs.CR",
            "cs.SE"
        ],
        "id": "http://arxiv.org/abs/2512.09957v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-09"
    },
    "http://arxiv.org/abs/2512.08751v1": {
        "title": "Skewness-Guided Pruning of Multimodal Swin Transformers for Federated Skin Lesion Classification on Edge Devices",
        "link": "http://arxiv.org/abs/2512.08751v1",
        "abstract": "In recent years, high-performance computer vision models have achieved remarkable success in medical imaging, with some skin lesion classification systems even surpassing dermatology specialists in diagnostic accuracy. However, such models are computationally intensive and large in size, making them unsuitable for deployment on edge devices. In addition, strict privacy constraints hinder centralized data management, motivating the adoption of Federated Learning (FL). To address these challenges, this study proposes a skewness-guided pruning method that selectively prunes the Multi-Head Self-Attention and Multi-Layer Perceptron layers of a multimodal Swin Transformer based on the statistical skewness of their output distributions. The proposed method was validated in a horizontal FL environment and shown to maintain performance while substantially reducing model complexity. Experiments on the compact Swin Transformer demonstrate approximately 36\\% model size reduction with no loss in accuracy. These findings highlight the feasibility of achieving efficient model compression and privacy-preserving distributed learning for multimodal medical AI on edge devices.",
        "authors": [
            "Kuniko Paxton",
            "Koorosh Aslansefat",
            "Dhavalkumar Thakker",
            "Yiannis Papadopoulos"
        ],
        "categories": [
            "cs.CV",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.08751v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-09"
    },
    "http://arxiv.org/abs/2512.08742v1": {
        "title": "Parallel Batch Dynamic Vertex Coloring in $O(\\log Δ)$ Amortized Update Time",
        "link": "http://arxiv.org/abs/2512.08742v1",
        "abstract": "We present the first parallel batch-dynamic algorithm for maintaining a proper $(Δ+ 1)$-vertex coloring. Our approach builds on a new sequential dynamic algorithm inspired by the work of Bhattacharya et al. (SODA'18). The resulting randomized algorithm achieves $O(\\log Δ)$ expected amortized update time and, for any batch of $b$ updates, has parallel span $O(\\operatorname{polylog} b + \\operatorname{polylog} n)$ with high probability.",
        "authors": [
            "Chase Hutton",
            "Adam Melrod"
        ],
        "categories": [
            "cs.DS",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.08742v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-09"
    },
    "http://arxiv.org/abs/2512.08728v1": {
        "title": "A Task Parallel Orthonormalization Multigrid Method For Multiphase Elliptic Problems",
        "link": "http://arxiv.org/abs/2512.08728v1",
        "abstract": "Multigrid methods have been a popular approach for solving linear systems arising from the discretization of partial differential equations (PDEs) for several decades. They are particularly effective for accelerating convergence rates with optimal complexity in terms of both time and space. K-cycle orthonormalization multigrid is a robust variant of the multigrid method that combines the efficiency of multigrid with the robustness of Krylov-type residual minimalizations for problems with strong anisotropies. However, traditional implementations of K-cycle orthonormalization multigrid often rely on bulk-synchronous parallelism, which can limit scalability on modern high-performance computing (HPC) systems. This paper presents a task-parallel variant of the K-cycle orthonormalization multigrid method that leverages asynchronous execution to improve scalability and performance on large-scale parallel systems.",
        "authors": [
            "Teoman Toprak",
            "Florian Kummer"
        ],
        "categories": [
            "math.NA",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.08728v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-09"
    },
    "http://arxiv.org/abs/2512.08725v1": {
        "title": "Spatio-Temporal Shifting to Reduce Carbon, Water, and Land-Use Footprints of Cloud Workloads",
        "link": "http://arxiv.org/abs/2512.08725v1",
        "abstract": "In this paper, we investigate the potential of spatial and temporal cloud workload shifting to reduce carbon, water, and land-use footprints. Specifically, we perform a simulation study using real-world data from multiple cloud providers (AWS and Azure) and workload traces for different applications (big data analytics and FaaS). Our simulation results indicate that spatial shifting can substantially lower carbon, water, and land use footprints, with observed reductions ranging from 20% to 85%, depending on the scenario and optimization criteria. Temporal shifting also decreases the footprint, though to a lesser extent. When applied together, the two strategies yield the greatest overall reduction, driven mainly by spatial shifting with temporal adjustments providing an additional, incremental benefit. Sensitivity analysis demonstrates that such shifting is robust to prediction errors in grid mix data and to variations across different seasons.",
        "authors": [
            "Giulio Attenni",
            "Youssef Moawad",
            "Novella Bartolini",
            "Lauritz Thamsen"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.08725v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-09"
    },
    "http://arxiv.org/abs/2512.08698v1": {
        "title": "Model-based Testing of Practical Distributed Systems in Actor Model",
        "link": "http://arxiv.org/abs/2512.08698v1",
        "abstract": "Designing and implementing distributed systems correctly can be quite challenging. Although these systems are often accompanied by formal specifications that are verified using model-checking techniques, a gap still exists between the implementation and its formal specification: there is no guarantee that the implementation is free of bugs.   To bridge this gap, we can use model-based testing. Specifically, if the model of the system can be interpreted as a finite-state automaton, we can generate an exhaustive test suite for the implementation that covers all possible states and transitions.   In this paper, we discuss how to efficiently generate such a test suite for distributed systems written in the actor model. Importantly, our approach does not require any modifications to the code or interfering with the distributed system execution environment. As an example, we verified an implementation of a replication algorithm based on Viewstamped Replication, which is used in a real-world system.",
        "authors": [
            "Ilya Kokorin",
            "Evgeny Chernatskiy",
            "Vitaly Aksenov"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.08698v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-09"
    },
    "http://arxiv.org/abs/2512.08563v1": {
        "title": "Basic Lock Algorithms in Lightweight Thread Environments",
        "link": "http://arxiv.org/abs/2512.08563v1",
        "abstract": "Traditionally, multithreaded data structures have been designed for access by the threads of Operating Systems (OS). However, implementations for access by programmable alternatives known as lightweight threads (also referred to as asynchronous calls or coroutines) have not been thoroughly studied. The main advantage of lightweight threads is their significantly lower overhead during launch and context switching. However, this comes at a cost: to achieve proper parallelism, context switches must be manually invoked in the code; without these switches, new lightweight threads will never be executed.   In this paper, we focus on the simplest multithreaded data structure: a mutex (also known as a lock). We demonstrate that original implementations for OS threads cannot be used effectively in this new context due to the potential for deadlocks. Furthermore, correctness is not the only concern. In certain languages, such as C++, there are various lightweight thread libraries, each with different implementations and interfaces, which necessitate distinct lock implementations.   In this work, we present a modification of TTAS and MCS locks for the use from lightweight threads and demonstrate that the two context switch mechanisms of lightweight threads, yielding and sleeping, are crucial. However, the performance of TTAS and MCS may differ significantly depending on the settings. If one wants to have a lock that works well for any library, we suggest using the cohort lock, which strikes a balance between MCS and TTAS by utilizing several MCS queues with a common TTAS.",
        "authors": [
            "Taras Skazhenik",
            "Nikolai Korobenikov",
            "Andrei Churbanov",
            "Anton Malakhov",
            "Vitaly Aksenov"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.08563v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-09"
    },
    "http://arxiv.org/abs/2512.08555v1": {
        "title": "A scalable high-order multigrid-FFT Poisson solver for unbounded domains on adaptive multiresolution grids",
        "link": "http://arxiv.org/abs/2512.08555v1",
        "abstract": "Multigrid solvers are among the most efficient methods for solving the Poisson equation, which is ubiquitous in computational physics. For example, in the context of incompressible flows, it is typically the costliest operation. The present document expounds upon the implementation of a flexible multigrid solver that is capable of handling any type of boundary conditions within murphy, a multiresolution framework for solving partial differential equations (PDEs) on collocated adaptive grids. The utilization of a Fourier-based direct solver facilitates the attainment of flexibility and enhanced performance by accommodating any combination of unbounded and semi-unbounded boundary conditions. The employment of high-order compact stencils contributes to the reduction of communication demands while concurrently enhancing the accuracy of the system. The resulting solver is validated against analytical solutions for periodic and unbounded domains. In conclusion, the solver has been demonstrated to demonstrate scalability to 16,384 cores within the context of leading European high-performance computing infrastructures.",
        "authors": [
            "Gilles Poncelet",
            "Jonathan Lambrechts",
            "Thomas Gillis",
            "Philippe Chatelain"
        ],
        "categories": [
            "math.NA",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.08555v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-09"
    },
    "http://arxiv.org/abs/2512.08365v1": {
        "id": "http://arxiv.org/abs/2512.08365v1",
        "title": "Magneton: Optimizing Energy Efficiency of ML Systems via Differential Energy Debugging",
        "link": "http://arxiv.org/abs/2512.08365v1",
        "tags": [
            "training",
            "serving",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-12-09",
        "tldr": "Proposes differential energy debugging to identify software-caused energy waste in ML systems. Magneton compares energy use at operator level across similar systems to pinpoint inefficient code/configuration. Applied to LLM inference, reduces energy consumption by up to 47% in diagnosed cases.",
        "abstract": "The training and deployment of machine learning (ML) models have become extremely energy-intensive. While existing optimization efforts focus primarily on hardware energy efficiency, a significant but overlooked source of inefficiency is software energy waste caused by poor software design. This often includes redundant or poorly designed operations that consume more energy without improving performance. These inefficiencies arise in widely used ML frameworks and applications, yet developers often lack the visibility and tools to detect and diagnose them.   We propose differential energy debugging, a novel approach that leverages the observation that competing ML systems often implement similar functionality with vastly different energy consumption. Building on this insight, we design and implement Magneton, an energy profiler that compares energy consumption between similar ML systems at the operator level and automatically pinpoints code regions and configuration choices responsible for excessive energy use. Applied to 9 popular ML systems spanning LLM inference, general ML frameworks, and image generation, Magneton detects and diagnoses 16 known cases of software energy inefficiency and further discovers 8 previously unknown cases, 7 of which have been confirmed by developers.",
        "authors": [
            "Yi Pan",
            "Wenbo Qian",
            "Dedong Xie",
            "Ruiyan Hu",
            "Yigong Hu",
            "Baris Kasikci"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-12-09"
    },
    "http://arxiv.org/abs/2512.08321v3": {
        "title": "Emulation of Complex Matrix Multiplication based on the Chinese Remainder Theorem",
        "link": "http://arxiv.org/abs/2512.08321v3",
        "abstract": "Modern computing architectures feature low-precision matrix multiplication units that achieve substantially higher throughput than their high-precision counterparts. Motivated by this architectural trend, the emulation of high-precision matrix multiplication using low-precision hardware has attracted significant interest in the high-performance computing community. Ozaki, Uchino, and Imamura proposed the Ozaki-II scheme as a general framework for emulating matrix multiplication. Building on this framework, Uchino, Ozaki, and Imamura developed high-performance and power-efficient techniques for emulating single- and double-precision real matrix multiplication on INT8 matrix engines. Extending this line of research, the present study proposes high-performance emulation methods for single- and double-precision complex matrix multiplication on INT8 matrix engines, based on the Ozaki-II scheme. On an NVIDIA B200 GPU, the proposed methods achieve 4.4--6.5x and 4.0--5.6x speedups over the native single- and double-precision complex matrix multiplication routines from cuBLAS, respectively, for sufficiently large problem sizes. When lower accuracy than that of the standard routines is acceptable, the proposed methods can operate at even higher speed. Conversely, with only a modest increase in computation time, they can deliver higher accuracy than that of the standard routines. These properties suggest that the proposed approach has the potential to serve as a default algorithm across a wide range of applications.",
        "authors": [
            "Yuki Uchino",
            "Qianxiang Ma",
            "Toshiyuki Imamura",
            "Katsuhisa Ozaki",
            "Patrick Lars Gutsche"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.08321v3",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-09"
    },
    "http://arxiv.org/abs/2512.08288v1": {
        "title": "Synergizing Monetization, Orchestration, and Semantics in Computing Continuum",
        "link": "http://arxiv.org/abs/2512.08288v1",
        "abstract": "Industry demands are growing for hyper-distributed applications that span from the cloud to the edge in domains such as smart manufacturing, transportation, and agriculture. Yet today's solutions struggle to meet these demands due to inherent limitations in scalability, interoperability, and trust. In this article, we introduce HERMES (Heterogeneous Computing Continuum with Resource Monetization, Orchestration, and Semantic) - a novel framework designed to transform connectivity and data utilization across the computing continuum. HERMES establishes an open, seamless, and secure environment where resources, from cloud servers to tiny edge devices, can be orchestrated intelligently, data and services can be monetized in a distributed marketplace, and knowledge is shared through semantic interoperability. By bridging these key facets, HERMES lays a foundation for a new generation of distributed applications that are more efficient, trustworthy, and autonomous.",
        "authors": [
            "Chinmaya Kumar Dehury",
            "Lauri Lovén",
            "Praveen Kumar Donta",
            "Ilir Murturi",
            "Schahram Dustdar"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.08288v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-09"
    },
    "http://arxiv.org/abs/2512.08242v1": {
        "id": "http://arxiv.org/abs/2512.08242v1",
        "title": "Chopper: A Multi-Level GPU Characterization Tool & Derived Insights Into LLM Training Inefficiency",
        "link": "http://arxiv.org/abs/2512.08242v1",
        "tags": [
            "training",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-12-09",
        "tldr": "Introduces Chopper, a multi-level GPU profiling tool for LLM training analysis. Collects and aligns kernel traces and hardware performance counters across granularities to identify bottlenecks in FSDP. Identifies frequency overhead (DVFS) as largest inefficiency source (exceeding MFMA utilization loss etc.) in Llama 3 8B training.",
        "abstract": "Training large language models (LLMs) efficiently requires a deep understanding of how modern GPU systems behave under real-world distributed training workloads. While prior work has focused primarily on kernel-level performance or single-GPU microbenchmarks, the complex interaction between communication, computation, memory behavior, and power management in multi-GPU LLM training remains poorly characterized. In this work, we introduce Chopper, a profiling and analysis framework that collects, aligns, and visualizes GPU kernel traces and hardware performance counters across multiple granularities (i.e., from individual kernels to operations, layers, phases, iterations, and GPUs). Using Chopper, we perform a comprehensive end-to-end characterization of Llama 3 8B training under fully sharded data parallelism (FSDP) on an eight-GPU AMD InstinctTM MI300X node. Our analysis reveals several previously underexplored bottlenecks and behaviors, such as memory determinism enabling higher, more stable GPU and memory frequencies. We identify several sources of inefficiencies, with frequency overhead (DVFS effects) being the single largest contributor to the gap between theoretical and observed performance, exceeding the impact of MFMA utilization loss, communication/computation overlap, and kernel launch overheads. Overall, Chopper provides the first holistic, multi-granularity characterization of LLM training on AMD InstinctTM MI300X GPUs, yielding actionable insights for optimizing training frameworks, improving power-management strategies, and guiding future GPU architecture and system design.",
        "authors": [
            "Marco Kurzynski",
            "Shaizeen Aga",
            "Di Wu"
        ],
        "categories": [
            "cs.DC",
            "cs.AR"
        ],
        "submit_date": "2025-12-09"
    },
    "http://arxiv.org/abs/2512.10990v1": {
        "id": "http://arxiv.org/abs/2512.10990v1",
        "title": "Dora: QoE-Aware Hybrid Parallelism for Distributed Edge AI",
        "link": "http://arxiv.org/abs/2512.10990v1",
        "tags": [
            "serving",
            "training",
            "edge"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes Dora for QoE-aware hybrid parallelism in distributed edge AI training and inference. It uses heterogeneity-aware partitioning, contention-aware scheduling, and runtime adaptation to balance latency and resource efficiency. Achieves 1.1-6.3× faster execution or 21-82% energy reduction while maintaining QoE.",
        "abstract": "With the proliferation of edge AI applications, satisfying user quality of experience (QoE) requirements, such as model inference latency, has become a first class objective, as these models operate in resource constrained settings and directly interact with users. Yet, modern AI models routinely exceed the resource capacity of individual devices, necessitating distributed execution across heterogeneous devices over variable and contention prone networks. Existing planners for hybrid (e.g., data and pipeline) parallelism largely optimize for throughput or device utilization, overlooking QoE, leading to severe resource inefficiency (e.g., unnecessary energy drain) or QoE violations under runtime dynamics.   We present Dora, a framework for QoE aware hybrid parallelism in distributed edge AI training and inference. Dora jointly optimizes heterogeneous computation, contention prone networks, and multi dimensional QoE objectives via three key mechanisms: (i) a heterogeneity aware model partitioner that determines and assigns model partitions across devices, forming a compact set of QoE compliant plans; (ii) a contention aware network scheduler that further refines these candidate plans by maximizing compute communication overlap; and (iii) a runtime adapter that adaptively composes multiple plans to maximize global efficiency while respecting overall QoEs. Across representative edge deployments, including smart homes, traffic analytics, and small edge clusters, Dora achieves 1.1--6.3 times faster execution and, alternatively, reduces energy consumption by 21--82 percent, all while maintaining QoE under runtime dynamics.",
        "authors": [
            "Jianli Jin",
            "Ziyang Lin",
            "Qianli Dong",
            "Yi Chen",
            "Jayanth Srinivasa",
            "Myungjin Lee",
            "Zhaowei Tan",
            "Fan Lai"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-12-09"
    },
    "http://arxiv.org/abs/2512.08067v1": {
        "title": "CapsuleFS A Multi-credential DataCapsule Filesystem",
        "link": "http://arxiv.org/abs/2512.08067v1",
        "abstract": "CapsuleFS (CFS) is the first filesystem to integrate multi-credential functionality within a POSIX-compliant framework, utilizing DataCapsule as the storage provider. This innovative system is established based on the Global Data Plane in the area of edge computing. Our comprehensive design and implementation of CFS successfully fulfill the objective of providing a multi-credential Common Access API. The architecture of CFS is methodically segmented into three integral components: Firstly, the DataCapsule server, tasked with the storage, dissemination, and replication of DataCapsules on the edge. Secondly, the middleware, a crucial element running in a Trusted Execution Environment responsible for the enforcement and management of write permissions and requests. Finally, the client component, which manifests as a POSIX-compliant filesystem, is adaptable and operational across many architectures. Experimental evaluations of CFS reveal that, while its read and write performances are comparatively modest, it upholds a high degree of functional correctness. This attribute distinctly positions CFS as a viable candidate for application in real-world software development scenarios. The paper also delineates potential future enhancements, aimed at augmenting the practicality of CFS in the landscape of software development.",
        "authors": [
            "Qingyang Hu",
            "Yucheng Huang",
            "Manshi Yang"
        ],
        "categories": [
            "cs.DC",
            "cs.CR"
        ],
        "id": "http://arxiv.org/abs/2512.08067v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-08"
    },
    "http://arxiv.org/abs/2512.08005v1": {
        "id": "http://arxiv.org/abs/2512.08005v1",
        "title": "Modeling the Potential of Message-Free Communication via CXL.mem",
        "link": "http://arxiv.org/abs/2512.08005v1",
        "tags": [
            "training",
            "networking",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Models performance benefits of using CXL.mem technology for MPI data exchange. Develops toolchain with memory trace sampling (Mitos) to identify high-potential MPI calls for replacing traditional messages with direct memory pooling. Example: Applied to HPCG benchmark with predicted communication latency reduction.",
        "abstract": "Heterogeneous memory technologies are increasingly important instruments in addressing the memory wall in HPC systems. While most are deployed in single node setups, CXL.mem is a technology that implements memories that can be attached to multiple nodes simultaneously, enabling shared memory pooling. This opens new possibilities, particularly for efficient inter-node communication.   In this paper, we present a novel performance evaluation toolchain combined with an extended performance model for message-based communication, which can be used to predict potential performance benefits from using CXL.mem for data exchange. Our approach analyzes data access patterns of MPI applications: it analyzes on-node accesses to/from MPI buffers, as well as cross-node MPI traffic to gather a full understanding of the impact of memory performance. We combine this data in an extended performance model to predict which data transfers could benefit from direct CXL.mem implementations as compared to traditional MPI messages. Our model works on a per-MPI call granularity, allowing the identification and later optimizations of those MPI invocations in the code with the highest potential for speedup by using CXL.mem.   For our toolchain, we extend the memory trace sampling tool Mitos and use it to extract data access behavior. In the post-processing step, the raw data is automatically analyzed to provide performance models for each individual MPI call. We validate the models on two sample applications -- a 2D heat transfer miniapp and the HPCG benchmark -- and use them to demonstrate their support for targeted optimizations by integrating CXL.mem.",
        "authors": [
            "Stepan Vanecek",
            "Matthew Turner",
            "Manisha Gajbe",
            "Matthew Wolf",
            "Martin Schulz"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-12-08"
    },
    "http://arxiv.org/abs/2512.07827v1": {
        "title": "An Adaptive Multi-Layered Honeynet Architecture for Threat Behavior Analysis via Deep Learning",
        "link": "http://arxiv.org/abs/2512.07827v1",
        "abstract": "The escalating sophistication and variety of cyber threats have rendered static honeypots inadequate, necessitating adaptive, intelligence-driven deception. In this work, ADLAH is introduced: an Adaptive Deep Learning Anomaly Detection Honeynet designed to maximize high-fidelity threat intelligence while minimizing cost through autonomous orchestration of infrastructure. The principal contribution is offered as an end-to-end architectural blueprint and vision for an AI-driven deception platform. Feasibility is evidenced by a functional prototype of the central decision mechanism, in which a reinforcement learning (RL) agent determines, in real time, when sessions should be escalated from low-interaction sensor nodes to dynamically provisioned, high-interaction honeypots. Because sufficient live data were unavailable, field-scale validation is not claimed; instead, design trade-offs and limitations are detailed, and a rigorous roadmap toward empirical evaluation at scale is provided. Beyond selective escalation and anomaly detection, the architecture pursues automated extraction, clustering, and versioning of bot attack chains, a core capability motivated by the empirical observation that exposed services are dominated by automated traffic. Together, these elements delineate a practical path toward cost-efficient capture of high-value adversary behavior, systematic bot versioning, and the production of actionable threat intelligence.",
        "authors": [
            "Lukas Johannes Möller"
        ],
        "categories": [
            "cs.CR",
            "cs.DC",
            "cs.LG"
        ],
        "id": "http://arxiv.org/abs/2512.07827v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-08"
    },
    "http://arxiv.org/abs/2512.07799v1": {
        "id": "http://arxiv.org/abs/2512.07799v1",
        "title": "Quantifying the Carbon Reduction of DAG Workloads: A Job Shop Scheduling Perspective",
        "link": "http://arxiv.org/abs/2512.07799v1",
        "tags": [
            "offline",
            "training",
            "sparse"
        ],
        "relevant": true,
        "indexed_date": "2025-12-08",
        "tldr": "Quantifies carbon reduction for DAG workloads (e.g., video encoding, offline inference) by modeling as job-shop scheduling. Uses offline solver to compute upper bounds, achieving 25% lower carbon emissions without increased makespan; doubling makespan nearly doubles savings.",
        "abstract": "Carbon-aware schedulers aim to reduce the operational carbon footprint of data centers by running flexible workloads during periods of low carbon intensity. Most schedulers treat workloads as single monolithic tasks, ignoring that many jobs, like video encoding or offline inference, consist of smaller tasks with specific dependencies and resource needs; however, knowledge of this structure enables opportunities for greater carbon efficiency.   We quantify the maximum benefit of a dependency-aware approach for batch workloads. We model the problem as a flexible job-shop scheduling variant and use an offline solver to compute upper bounds on carbon and energy savings. Results show up to $25\\%$ lower carbon emissions on average without increasing the optimal makespan (total job completion time) compared to a makespan-only baseline. Although in heterogeneous server setup, these schedules may use more energy than energy-optimal ones. Our results also show that allowing twice the optimal makespan nearly doubles the carbon savings, underscoring the tension between carbon, energy, and makespan. We also highlight key factors such as job structure and server count influence the achievable carbon reductions.",
        "authors": [
            "Roozbeh Bostandoost",
            "Adam Lechowicz",
            "Walid A. Hanafy",
            "Prashant Shenoy",
            "Mohammad Hajiesmaili"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-12-08"
    },
    "http://arxiv.org/abs/2512.07792v1": {
        "id": "http://arxiv.org/abs/2512.07792v1",
        "title": "Designing Co-operation in Systems of Hierarchical, Multi-objective Schedulers for Stream Processing",
        "link": "http://arxiv.org/abs/2512.07792v1",
        "tags": [
            "training",
            "serving",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-12-08",
        "tldr": "Investigates scheduling co-operation to optimize resource allocation in hierarchical stream processing systems at Meta. Proposes integration of new schedulers into existing hierarchies for effective load balancing across compute resources. Enables processing terabytes of data within seconds.",
        "abstract": "Stream processing is a computing paradigm that supports real-time data processing for a wide variety of applications. At Meta, it's used across the company for various tasks such as deriving product insights, providing and improving user services, and enabling AI at scale for our ever-growing user base. Meta's current stream processing framework supports processing TerraBytes(TBs) of data in mere seconds. This is enabled by our efficient schedulers and multi-layered infrastructure, which allocate workloads across various compute resources, working together in hierarchies across various parts of the infrastructure. But with the ever growing complexity of applications, and user needs, areas of the infrastructure that previously required minimal load balancing, now must be made more robust and proactive to application load. In our work we explore how to build and design such a system that focuses on load balancing over key compute resources and properties of these applications. We also showcase how to integrate new schedulers into the hierarchy of the existing ones, allowing multiple schedulers to work together and perform load balancing, at their infrastructure level, effectively.",
        "authors": [
            "Animesh Dangwal",
            "Yufeng Jiang",
            "Charlie Arnold",
            "Jun Fan",
            "Mohamed Bassem",
            "Aish Rajagopal"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-12-08"
    },
    "http://arxiv.org/abs/2512.07750v1": {
        "title": "A Performance Analyzer for a Public Cloud's ML-Augmented VM Allocator",
        "link": "http://arxiv.org/abs/2512.07750v1",
        "abstract": "Many operational cloud systems use one or more machine learning models that help them achieve better efficiency and performance. But operators do not have tools to help them understand how each model and the interaction between them affect the end-to-end system performance. SANJESH is such a tool. SANJESH supports a diverse set of performance-related queries which we answer through a bi-level optimization. We invent novel mechanisms to solve this optimization more quickly. These techniques allow us to solve an optimization which prior work failed to solve even after $24$ hours.   As a proof of concept, we apply SANJESH to an example production system that uses multiple ML models to optimize virtual machine (VM) placement. These models impact how many servers the operators uses to host VMs and the frequency with which it has to live-migrate them because the servers run out of resources. SANJESH finds scenarios where these models cause $~4\\times$ worse performance than what simulation-based approaches detect.",
        "authors": [
            "Roozbeh Bostandoost",
            "Pooria Namyar",
            "Siva Kesava Reddy Kakarla",
            "Ryan Beckett",
            "Santiago Segarra",
            "Eli Cortez",
            "Ankur Mallick",
            "Kevin Hsieh",
            "Rodrigo Fonseca",
            "Mohammad Hajiesmaili",
            "Behnaz Arzani"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.07750v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-08"
    },
    "http://arxiv.org/abs/2512.07536v1": {
        "id": "http://arxiv.org/abs/2512.07536v1",
        "title": "Bandwidth-Aware Network Topology Optimization for Decentralized Learning",
        "link": "http://arxiv.org/abs/2512.07536v1",
        "tags": [
            "training",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-12-08",
        "tldr": "Proposes bandwidth-aware network topology optimization for decentralized learning to maximize consensus speed under edge constraints. Uses Mixed-Integer SDP reformulation and ADMM with conjugate gradient for scalability. Reduces training time by 1.21× for heterogeneous bandwidth settings.",
        "abstract": "Network topology is critical for efficient parameter synchronization in distributed learning over networks. However, most existing studies do not account for bandwidth limitations in network topology design. In this paper, we propose a bandwidth-aware network topology optimization framework to maximize consensus speed under edge cardinality constraints. For heterogeneous bandwidth scenarios, we introduce a maximum bandwidth allocation strategy for the edges to ensure efficient communication among nodes. By reformulating the problem into an equivalent Mixed-Integer SDP problem, we leverage a computationally efficient ADMM-based method to obtain topologies that yield the maximum consensus speed. Within the ADMM substep, we adopt the conjugate gradient method to efficiently solve large-scale linear equations to achieve better scalability. Experimental results demonstrate that the resulting network topologies outperform the benchmark topologies in terms of consensus speed, and reduce the training time required for decentralized learning tasks on real-world datasets to achieve the target test accuracy, exhibiting speedups of more than $1.11\\times$ and $1.21\\times$ for homogeneous and heterogeneous bandwidth settings, respectively.",
        "authors": [
            "Yipeng Shen",
            "Zehan Zhu",
            "Yan Huang",
            "Changzhi Yan",
            "Cheng Zhuo",
            "Jinming Xu"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-12-08"
    },
    "http://arxiv.org/abs/2512.07401v1": {
        "title": "Otus Supercomputer",
        "link": "http://arxiv.org/abs/2512.07401v1",
        "abstract": "Otus is a high-performance computing cluster that was launched in 2025 and is operated by the Paderborn Center for Parallel Computing (PC2) at Paderborn University in Germany. The system is part of the National High Performance Computing (NHR) initiative. Otus complements the previous supercomputer Noctua 2, offering approximately twice the computing power while retaining the three node types that were characteristic of Noctua 2: 1) CPU compute nodes with different memory capacities, 2) high-end GPU nodes, and 3) HPC-grade FPGA nodes. On the Top500 list, which ranks the 500 most powerful supercomputers in the world, Otus is in position 164 with the CPU partition and in position 255 with the GPU partition (June 2025). On the Green500 list, ranking the 500 most energy-efficient supercomputers in the world, Otus is in position 5 with the GPU partition (June 2025).   This article provides a comprehensive overview of the system in terms of its hardware, software, system integration, and its overall integration into the data center building to ensure energy-efficient operation. The article aims to provide unique insights for scientists using the system and for other centers operating HPC clusters. The article will be continuously updated to reflect the latest system setup and measurements.",
        "authors": [
            "Sadaf Ehtesabi",
            "Manoar Hossain",
            "Tobias Kenter",
            "Andreas Krawinkel",
            "Holger Nitsche",
            "Lukas Ostermann",
            "Christian Plessl",
            "Heinrich Riebler",
            "Stefan Rohde",
            "Robert Schade",
            "Michael Schwarz",
            "Jens Simon",
            "Nils Winnwa",
            "Alex Wiens",
            "Xin Wu"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.07401v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-08"
    },
    "http://arxiv.org/abs/2512.07350v1": {
        "id": "http://arxiv.org/abs/2512.07350v1",
        "title": "Communication-Efficient Serving for Video Diffusion Models with Latent Parallelism",
        "link": "http://arxiv.org/abs/2512.07350v1",
        "tags": [
            "video",
            "diffusion",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-12-08",
        "tldr": "Addresses communication bottlenecks in video diffusion model serving. Proposes Latent Parallelism (LP), which dynamically rotates partitioning dimensions in latent space to reduce transfers. Achieves up to 97% communication overhead reduction while maintaining generation quality.",
        "abstract": "Video diffusion models (VDMs) perform attention computation over the 3D spatio-temporal domain. Compared to large language models (LLMs) processing 1D sequences, their memory consumption scales cubically, necessitating parallel serving across multiple GPUs. Traditional parallelism strategies partition the computational graph, requiring frequent high-dimensional activation transfers that create severe communication bottlenecks. To tackle this issue, we exploit the local spatio-temporal dependencies inherent in the diffusion denoising process and propose Latent Parallelism (LP), the first parallelism strategy tailored for VDM serving. \\textcolor{black}{LP decomposes the global denoising problem into parallelizable sub-problems by dynamically rotating the partitioning dimensions (temporal, height, and width) within the compact latent space across diffusion timesteps, substantially reducing the communication overhead compared to prevailing parallelism strategies.} To ensure generation quality, we design a patch-aligned overlapping partition strategy that matches partition boundaries with visual patches and a position-aware latent reconstruction mechanism for smooth stitching. Experiments on three benchmarks demonstrate that LP reduces communication overhead by up to 97\\% over baseline methods while maintaining comparable generation quality. As a non-intrusive plug-in paradigm, LP can be seamlessly integrated with existing parallelism strategies, enabling efficient and scalable video generation services.",
        "authors": [
            "Zhiyuan Wu",
            "Shuai Wang",
            "Li Chen",
            "Kaihui Gao",
            "Dan Li",
            "Yanyu Ren",
            "Qiming Zhang",
            "Yong Wang"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-12-08"
    },
    "http://arxiv.org/abs/2512.07344v1": {
        "id": "http://arxiv.org/abs/2512.07344v1",
        "title": "Venus: An Efficient Edge Memory-and-Retrieval System for VLM-based Online Video Understanding",
        "link": "http://arxiv.org/abs/2512.07344v1",
        "tags": [
            "edge",
            "RAG",
            "video"
        ],
        "relevant": true,
        "indexed_date": "2025-12-08",
        "tldr": "Addresses high deployment overhead for VLM-based online video understanding. Proposes Venus, an edge-cloud disaggregated system with hierarchical memory construction and threshold-based sampling for adaptive cost-accuracy tradeoff. Achieves 15x-131x latency speedup while maintaining accuracy.",
        "abstract": "Vision-language models (VLMs) have demonstrated impressive multimodal comprehension capabilities and are being deployed in an increasing number of online video understanding applications. While recent efforts extensively explore advancing VLMs' reasoning power in these cases, deployment constraints are overlooked, leading to overwhelming system overhead in real-world deployments. To address that, we propose Venus, an on-device memory-and-retrieval system for efficient online video understanding. Venus proposes an edge-cloud disaggregated architecture that sinks memory construction and keyframe retrieval from cloud to edge, operating in two stages. In the ingestion stage, Venus continuously processes streaming edge videos via scene segmentation and clustering, where the selected keyframes are embedded with a multimodal embedding model to build a hierarchical memory for efficient storage and retrieval. In the querying stage, Venus indexes incoming queries from memory, and employs a threshold-based progressive sampling algorithm for keyframe selection that enhances diversity and adaptively balances system cost and reasoning accuracy. Our extensive evaluation shows that Venus achieves a 15x-131x speedup in total response latency compared to state-of-the-art methods, enabling real-time responses within seconds while maintaining comparable or even superior reasoning accuracy.",
        "authors": [
            "Shengyuan Ye",
            "Bei Ouyang",
            "Tianyi Qian",
            "Liekang Zeng",
            "Mu Yuan",
            "Xiaowen Chu",
            "Weijie Hong",
            "Xu Chen"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-12-08"
    },
    "http://arxiv.org/abs/2512.07312v1": {
        "id": "http://arxiv.org/abs/2512.07312v1",
        "title": "DCO: Dynamic Cache Orchestration for LLM Accelerators through Predictive Management",
        "link": "http://arxiv.org/abs/2512.07312v1",
        "tags": [
            "hardware",
            "offloading",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-12-08",
        "tldr": "Proposes DCO, a shared system-level cache with predictive management for LLM accelerators, using dataflow-guided cache replacement and thrashing mitigation. Achieves up to 1.80x speedup over conventional cache architectures and is validated with RTL implementation at 2 GHz.",
        "abstract": "The rapid adoption of large language models (LLMs) is pushing AI accelerators toward increasingly powerful and specialized designs. Instead of further complicating software development with deeply hierarchical scratchpad memories (SPMs) and their asynchronous management, we investigate the opposite point of the design spectrum: a multi-core AI accelerator equipped with a shared system-level cache and application-aware management policies, which keeps the programming effort modest. Our approach exploits dataflow information available in the software stack to guide cache replacement (including dead-block prediction), in concert with bypass decisions and mechanisms that alleviate cache thrashing.   We assess the proposal using a cycle-accurate simulator and observe substantial performance gains (up to 1.80x speedup) compared with conventional cache architectures. In addition, we build and validate an analytical model that takes into account the actual overlapping behaviors to extend the measurement results of our policies to real-world larger-scale workloads. Experiment results show that when functioning together, our bypassing and thrashing mitigation strategies can handle scenarios both with and without inter-core data sharing and achieve remarkable speedups.   Finally, we implement the design in RTL and the area of our design is $\\mathbf{0.064mm^2}$ with 15nm process, which can run at 2 GHz clock frequency. Our findings explore the potential of the shared cache design to assist the development of future AI accelerator systems.",
        "authors": [
            "Zhongchun Zhou",
            "Chengtao Lai",
            "Yuhang Gu",
            "Wei Zhang"
        ],
        "categories": [
            "cs.AR",
            "cs.AI",
            "cs.DC"
        ],
        "submit_date": "2025-12-08"
    },
    "http://arxiv.org/abs/2512.07280v1": {
        "title": "ContinuumConductor : Decentralized Process Mining on the Edge-Cloud Continuum",
        "link": "http://arxiv.org/abs/2512.07280v1",
        "abstract": "Process mining traditionally assumes centralized event data collection and analysis. However, modern Industrial Internet of Things systems increasingly operate over distributed, resource-constrained edge-cloud infrastructures. This paper proposes a structured approach for decentralizing process mining by enabling event data to be mined directly within the IoT systems edge-cloud continuum. We introduce ContinuumConductor a layered decision framework that guides when to perform process mining tasks such as preprocessing, correlation, and discovery centrally or decentrally. Thus, enabling privacy, responsive and resource-efficient process mining. For each step in the process mining pipeline, we analyze the trade-offs of decentralization versus centralization across these layers and propose decision criteria. We demonstrate ContinuumConductor at a real-world use-case of process optimazition in inland ports. Our contributions lay the foundation for computing-aware process mining in cyber-physical and IIoT systems.",
        "authors": [
            "Hendrik Reiter",
            "Janick Edinger",
            "Martin Kabierski",
            "Agnes Koschmider",
            "Olaf Landsiedel",
            "Arvid Lepsien",
            "Xixi Lu",
            "Andrea Marrella",
            "Estefania Serral",
            "Stefan Schulte",
            "Florian Tschorsch",
            "Matthias Weidlich",
            "Wilhelm Hasselbring"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.07280v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-08"
    },
    "http://arxiv.org/abs/2512.07189v1": {
        "title": "PIR-DSN: A Decentralized Storage Network Supporting Private Information Retrieval",
        "link": "http://arxiv.org/abs/2512.07189v1",
        "abstract": "Decentralized Storage Networks (DSNs) are emerging as a foundational infrastructure for Web 3.0, offering global peer-to-peer storage. However, a critical vulnerability persists: user privacy during file retrieval remains largely unaddressed, risking the exposure of sensitive information. To overcome this, we introduce PIR-DSN, the first DSN protocol to integrate Private Information Retrieval (PIR) for both single and multi-server settings. Our key innovations include a novel secure mapping method that transforms sparse file identifiers into compact integer indexes, enabling both public verifiability of file operations and efficient private retrieval. Furthermore, PIR-DSN guarantees Byzantine-robust private retrieval through file replication across multiple miners. We implement and rigorously evaluate PIR-DSN against three prominent industrial DSN systems. Experimental results demonstrate that PIR-DSN achieves comparable overhead for file upload and deletion. While PIR inherently introduces an additional computational cost leading to higher retrieval latency, PIR-DSN maintains comparable throughput. These findings underscore PIR-DSN's practical viability for privacy-sensitive applications within DSN environments.",
        "authors": [
            "Jiahao Zhang",
            "Minghui Xu",
            "Hechuan Guo",
            "Xiuzhen Cheng"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.07189v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-08"
    },
    "http://arxiv.org/abs/2512.07009v1": {
        "id": "http://arxiv.org/abs/2512.07009v1",
        "title": "Optimizing video analytics inference pipelines: a case study",
        "link": "http://arxiv.org/abs/2512.07009v1",
        "tags": [
            "video",
            "serving",
            "offline"
        ],
        "relevant": true,
        "indexed_date": "2025-12-07",
        "tldr": "Optimizes video analytics inference pipelines for livestock monitoring. Introduces multi-level parallelization, GPU acceleration, vectorized clustering, and memory-efficient post-processing. Achieves 2x speedup across pipelines without accuracy loss.",
        "abstract": "Cost-effective and scalable video analytics are essential for precision livestock monitoring, where high-resolution footage and near-real-time monitoring needs from commercial farms generates substantial computational workloads. This paper presents a comprehensive case study on optimizing a poultry welfare monitoring system through system-level improvements across detection, tracking, clustering, and behavioral analysis modules. We introduce a set of optimizations, including multi-level parallelization, Optimizing code with substituting CPU code with GPU-accelerated code, vectorized clustering, and memory-efficient post-processing. Evaluated on real-world farm video footage, these changes deliver up to a 2x speedup across pipelines without compromising model accuracy. Our findings highlight practical strategies for building high-throughput, low-latency video inference systems that reduce infrastructure demands in agricultural and smart sensing deployments as well as other large-scale video analytics applications.",
        "authors": [
            "Saeid Ghafouri",
            "Yuming Ding",
            "Katerine Diaz Chito",
            "Jesús Martinez del Rincón",
            "Niamh O'Connell",
            "Hans Vandierendonck"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.LG"
        ],
        "submit_date": "2025-12-07"
    },
    "http://arxiv.org/abs/2512.09946v1": {
        "id": "http://arxiv.org/abs/2512.09946v1",
        "title": "ELANA: A Simple Energy and Latency Analyzer for LLMs",
        "link": "http://arxiv.org/abs/2512.09946v1",
        "tags": [
            "serving",
            "edge",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Presents ELANA, a lightweight profiler for evaluating latency metrics (TTFT, TPOT, TTLT) and energy consumption of LLMs on multi-GPU and edge platforms. Integrates with Hugging Face models and supports low-precision models. Demonstrates usage for efficient deployment analysis without specific quantitative results mentioned in abstract.",
        "abstract": "The latency and power consumption of large language models (LLMs) are major constraints when serving them across a wide spectrum of hardware platforms, from mobile edge devices to cloud GPU clusters. Benchmarking is crucial for optimizing efficiency in both model deployment and next-generation model development. To address this need, we open-source a simple profiling tool, \\textbf{ELANA}, for evaluating LLMs. ELANA is designed as a lightweight, academic-friendly profiler for analyzing model size, key-value (KV) cache size, prefilling latency (Time-to-first-token, TTFT), generation latency (Time-per-output-token, TPOT), and end-to-end latency (Time-to-last-token, TTLT) of LLMs on both multi-GPU and edge GPU platforms. It supports all publicly available models on Hugging Face and offers a simple command-line interface, along with optional energy consumption logging. Moreover, ELANA is fully compatible with popular Hugging Face APIs and can be easily customized or adapted to compressed or low bit-width models, making it ideal for research on efficient LLMs or for small-scale proof-of-concept studies. We release the ELANA profiling tool at: https://github.com/enyac-group/Elana.",
        "authors": [
            "Hung-Yueh Chiang",
            "Bokun Wang",
            "Diana Marculescu"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-12-07"
    },
    "http://arxiv.org/abs/2512.06852v1": {
        "id": "http://arxiv.org/abs/2512.06852v1",
        "title": "A Chunked-Object Pattern for Multi-Region Large Payload Storage in Managed NoSQL Databases",
        "link": "http://arxiv.org/abs/2512.06852v1",
        "tags": [
            "storage",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-12-07",
        "tldr": "Proposes a 'chunked-object' pattern for storing large payloads exceeding NoSQL item size limits using chunking within the database. Eliminates replication lag risks by avoiding offloading to object storage, reducing p99 cross-region consistency latency for 1 MB payloads by keeping data in a single consistency domain.",
        "abstract": "Many managed key-value and NoSQL databases - such as Amazon DynamoDB, Azure Cosmos DB, and Google Cloud Firestore - enforce strict maximum item sizes (e.g., 400 KB in DynamoDB). This constraint imposes significant architectural challenges for applications requiring low-latency, multi-region access to objects that exceed these limits. The standard industry recommendation is to offload payloads to object storage (e.g., Amazon S3) while retaining a pointer in the database. While cost-efficient, this \"pointer pattern\" introduces network overhead and exposes applications to non-deterministic replication lag between the database and the object store, creating race conditions in active-active architectures.   This paper presents a \"chunked-object\" pattern that persists large logical entities as sets of ordered chunks within the database itself. We precisely define the pattern and provide a reference implementation using Amazon DynamoDB Global Tables. The design generalizes to any key-value store with per-item size limits and multi-region replication. We evaluate the approach using telemetry from a production system processing over 200,000 transactions per hour. Results demonstrate that the chunked-object pattern eliminates cross-system replication lag hazards and reduces p99 cross-region time-to-consistency for 1 MB payloads by keeping data and metadata within a single consistency domain.",
        "authors": [
            "Manideep Reddy Chinthareddy"
        ],
        "categories": [
            "cs.DB",
            "cs.DC"
        ],
        "submit_date": "2025-12-07"
    },
    "http://arxiv.org/abs/2512.06800v1": {
        "title": "Cloud Revolution: Tracing the Origins and Rise of Cloud Computing",
        "link": "http://arxiv.org/abs/2512.06800v1",
        "abstract": "The history behind the development of cloud computing is more than several decades of technological progress in the fields of virtualization, distributed systems, and high-speed networking, but its current application is much broader than the underlying technologies that made it possible. This paper reexamines the historical evolution of the field, including the initial ideas of resource sharing and utility-based computing approaches and the development of hyperscale data centers and modern globally federated cloud ecosystems. We also analyze the technological and economic forces and point to the way cloud platforms altered the organizational computing habits, decreasing the entrance-level to the data-intensive and computation-heavy apps. The study also takes into account the ongoing limitations which have come with the large-scale adoption of clouds which include exposure to security due to the weaknesses in configuration, particular establishment regulations, and structural reliance on the single vendors. Lastly, we address some of the new trends that are transforming the cloud environment, including the convergence of edge and cloud infrastructure, the increased prominence of AI-optimised architectures and the initial adoption of quantum computing services. Collectively, the developments above describe an emerging but quickly changing paradigm with its future direction being determined by a strike of balancing between scalability, openness, and trust.",
        "authors": [
            "Deepa Gurung",
            "S M Zia Ur Rashid",
            "Zain ul Abdeen",
            "Suman Rath"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.06800v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-07"
    },
    "http://arxiv.org/abs/2512.06784v1": {
        "id": "http://arxiv.org/abs/2512.06784v1",
        "title": "Stable-MoE: Lyapunov-based Token Routing for Distributed Mixture-of-Experts Training over Edge Networks",
        "link": "http://arxiv.org/abs/2512.06784v1",
        "tags": [
            "MoE",
            "edge",
            "training"
        ],
        "relevant": true,
        "indexed_date": "2025-12-07",
        "tldr": "Proposes Lyapunov-based token routing for distributed MoE training on edge networks with heterogeneous resources. Formulates stochastic optimization for throughput and gating consistency via online routing/resource allocation. Gains 40% throughput and 5% accuracy on SVHN/CIFAR-100.",
        "abstract": "The sparse activation mechanism of mixture of experts (MoE) model empowers edge intelligence with enhanced training efficiency and reduced computational resource consumption. However, traditional token routing in distributed MoE training faces significant challenges in resource-constrained edge networks characterized by heterogeneous computing capabilities and stochastic token arrivals, which inevitably suffer from workload backlog, resource inefficiency, and performance degradation. To address this issue, we propose a novel Lyapunov-based token routing framework for distributed MoE training over resource-heterogeneous edge networks, termed Stable-MoE. Specifically, we formulate a stochastic optimization problem to maximize both system throughput and gating consistency via optimizing the token routing strategy and computational resource allocation, while ensuring long-term stability of both token and energy queues at the edge devices. Using the Lyapunov optimization, we transform the intractable long-term optimization problem into tractable per-slot subproblems by enabling online decision-making of token routing and computation frequency utilization without the knowledge of future system states. Experimental results on the SVHN and CIFAR-100 datasets demonstrate that Stable-MoE outperforms the baselines with at least 40% and 5% gains in system throughput and test accuracy, respectively.",
        "authors": [
            "Long Shi",
            "Bingyan Ou",
            "Kang Wei",
            "Weihao Zhu",
            "Zhe Wang",
            "Zhiyong Chen"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-12-07"
    },
    "http://arxiv.org/abs/2512.06547v1": {
        "id": "http://arxiv.org/abs/2512.06547v1",
        "title": "A-3PO: Accelerating Asynchronous LLM Training with Staleness-aware Proximal Policy Approximation",
        "link": "http://arxiv.org/abs/2512.06547v1",
        "tags": [
            "training",
            "RL",
            "sparse"
        ],
        "relevant": true,
        "indexed_date": "2025-12-06",
        "tldr": "Addresses computational bottleneck in asynchronous RL training for LLMs caused by proximal policy. Proposes A-3PO, which approximates the proximal policy via interpolation instead of extra forward pass. Reduces training time by 18%.",
        "abstract": "Decoupled loss has been a successful reinforcement learning (RL) algorithm to deal with the high data staleness under the asynchronous RL setting. Decoupled loss improves coupled-loss style of algorithms' (e.g., PPO, GRPO) learning stability by introducing a proximal policy to decouple the off-policy corrections (importance weight) from the controlling policy updates (trust region). However, the proximal policy requires an extra forward pass through the network at each training step, creating a computational bottleneck for large language models. We observe that since the proximal policy only serves as a trust region anchor between the behavior and target policies, we can approximate it through simple interpolation without explicit computation. We call this approach A-3PO (APproximated Proximal Policy Optimization). A-3PO eliminates this overhead, reducing training time by 18% while maintaining comparable performance. Code & off-the-shelf example are available at: https://github.com/inclusionAI/AReaL/blob/main/docs/algorithms/prox_approx.md",
        "authors": [
            "Xiaocan Li",
            "Shiliang Wu",
            "Zheng Shen"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC"
        ],
        "submit_date": "2025-12-06"
    },
    "http://arxiv.org/abs/2512.06443v1": {
        "id": "http://arxiv.org/abs/2512.06443v1",
        "title": "Vec-LUT: Vector Table Lookup for Parallel Ultra-Low-Bit LLM Inference on Edge Devices",
        "link": "http://arxiv.org/abs/2512.06443v1",
        "tags": [
            "offline",
            "quantization",
            "edge"
        ],
        "relevant": true,
        "indexed_date": "2025-12-06",
        "tldr": "Proposes Vec-LUT for efficient parallel ultra-low-bit LLM inference. Replaces scalar LUTs with a vector lookup to reduce bandwidth underutilization via tensor layout and cache-aware techniques. On 5 edge devices, achieves up to 4.2× speedup over baselines.",
        "abstract": "Large language models (LLMs) are increasingly deployed on edge devices. To meet strict resource constraints, real-world deployment has pushed LLM quantization from 8-bit to 4-bit, 2-bit, and now 1.58-bit. Combined with lookup table (LUT)-based inference, CPUs run these ultra-low-bit LLMs even faster than NPUs, opening new opportunities for ubiquitous on-device intelligence.   However, this paper identifies that LUT-based inference underutilizes memory bandwidth during parallel inference, which is required for prefilling, test-time scaling, and other multi-token scenarios. The root cause is the scalar LUT paradigm, which performs repetitive and non-contiguous memory accesses for each token.   To solve the issue, we propose vector LUT, a new lookup paradigm that constructs a unified LUT across parallel tokens, and performs a single $1 \\rightarrow N$ lookup per index. To realize it efficiently, we further introduce (1) Vector LUT-Centric Tensor Layout, and (2) Cache-Aware Streamed Lookup techniques. Evaluations on 5 edge devices across 3 LLMs show that Vec-LUT outperforms state-of-the-art baselines by up to $4.2\\times$. Our implementation is integrated into llama.cpp. The code is available at https://github.com/Cipherxzc/vlut.cpp.",
        "authors": [
            "Xiangyu Li",
            "Chengyu Yin",
            "Weijun Wang",
            "Jianyu Wei",
            "Ting Cao",
            "Yunxin Liu"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-12-06"
    },
    "http://arxiv.org/abs/2512.09942v1": {
        "title": "A study of the spectrum resource leasing method based on ERC4907 extension",
        "link": "http://arxiv.org/abs/2512.09942v1",
        "abstract": "The ERC4907 standard enables rentable Non-Fungible Tokens (NFTs) but is limited to single-user, single-time-slot authorization, which severely limits its applicability and efficiency in decentralized multi-slot scheduling scenarios. To address this limitation, this paper proposes Multi-slot ERC4907 (M-ERC4907) extension method. The M-ERC4907 method introduces novel functionalities to support the batch configuration of multiple time slots and simultaneous authorization of multiple users, thereby effectively eliminating the rigid sequential authorization constraint of ERC4907. The experiment was conducted on the Remix development platform. Experimental results show that the M-ERC4907 method significantly reduces on-chain transactions and overall Gas consumption, leading to enhanced scalability and resource allocation efficiency.",
        "authors": [
            "Zhiming Liang",
            "Bin Chen",
            "Litao Ye",
            "Chen Sun",
            "Shuo Wang",
            "Zhe Peng"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.09942v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-06"
    },
    "http://arxiv.org/abs/2512.10987v1": {
        "title": "Evaluation Framework for Centralized and Decentralized Aggregation Algorithm in Federated Systems",
        "link": "http://arxiv.org/abs/2512.10987v1",
        "abstract": "In recent years, the landscape of federated learning has witnessed significant advancements, particularly in decentralized methodologies. This research paper presents a comprehensive comparison of Centralized Hierarchical Federated Learning (HFL) with Decentralized Aggregated Federated Learning (AFL) and Decentralized Continual Federated Learning (CFL) architectures. While HFL, in its centralized approach, faces challenges such as communication bottlenecks and privacy concerns due to centralized data aggregation, AFL and CFL provide promising alternatives by distributing computation and aggregation processes across devices. Through evaluation of Fashion MNIST and MNIST datasets, this study demonstrates the advantages of decentralized methodologies, showcasing how AFL and CFL outperform HFL in precision, recall, F1 score, and balanced accuracy. The analysis highlights the importance of decentralized aggregation mechanisms in AFL and CFL, which effectively enables collaborative model training across distributed devices. This comparative study contributes valuable insights into the evolving landscape of federated learning, guiding researchers and practitioners towards decentralized methodologies for enhanced performance in collaborative model training scenarios.",
        "authors": [
            "Sumit Chongder"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.10987v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-05"
    },
    "http://arxiv.org/abs/2512.05703v1": {
        "id": "http://arxiv.org/abs/2512.05703v1",
        "title": "Metronome: Differentiated Delay Scheduling for Serverless Functions",
        "link": "http://arxiv.org/abs/2512.05703v1",
        "tags": [
            "serving",
            "offloading",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-12-05",
        "tldr": "Proposes differentiated delay scheduling for serverless FaaS to optimize locality-aware execution. Metronome uses online Random Forest Regression to predict function times and select optimal nodes. Achieves 64.88%-95.83% reduction in mean execution time over baselines.",
        "abstract": "Function-as-a-Service (FaaS) computing is an emerging cloud computing paradigm for its ease-of-management and elasticity. However, optimizing scheduling for serverless functions remains challenging due to their dynamic and event-driven nature. While data locality has been proven effective in traditional cluster computing systems through delay scheduling, its application in serverless platforms remains largely unexplored. In this paper, we systematically evaluate existing delay scheduling methods in serverless environments and identify three key observations: 1) delay scheduling benefits vary significantly based on function input characteristics; 2) serverless computing exhibits more complex locality patterns than cluster computing systems, encompassing both data locality and infrastructure locality; and 3) heterogeneous function execution times make rule-based delay thresholds ineffective. Based on these insights, we propose Metronome, a differentiated delay scheduling framework that employs predictive mechanisms to identify optimal locality-aware nodes for individual functions. Metronome leverages an online Random Forest Regression model to forecast function execution times across various nodes, enabling informed delay decisions while preventing SLA violations. Our implementation on OpenLambda shows that Metronome significantly outperforms baselines, achieving 64.88%-95.83% reduction in mean execution time for functions, while maintaining performance advantages under increased concurrency levels and ensuring SLA compliance.",
        "authors": [
            "Zhuangbin Chen",
            "Juzheng Zheng",
            "Zibin Zheng"
        ],
        "categories": [
            "cs.SE",
            "cs.DC"
        ],
        "submit_date": "2025-12-05"
    },
    "http://arxiv.org/abs/2512.05543v4": {
        "title": "Are Bus-Mounted Edge Servers Feasible?",
        "link": "http://arxiv.org/abs/2512.05543v4",
        "abstract": "Placement of edge servers is the prerequisite of provisioning edge computing services for Internet of Vehicles (IoV). Fixed-site edge servers at Road Side Units (RSUs) or base stations are able to offer basic service coverage for end users, i.e., vehicles on road. However, the server locations and capacity are fixed after deployment, rendering their inefficiency in handling spationtemporal user dynamics. Mobile servers such as buses, on the other hand, have the potential of adding computation elasticity to such system. To this end, this paper studies the feasibility of bus-mounted edge servers based on real traces. First, we investigate the coverage of the buses and base stations using the Shanghai bus/taxi/Telecom datasets, which shows a great potential of bus-based edge servers as they cover a great portion of geographic area and demand points. Next, we build a mathematical model and design a simple greedy heuristic algorithm to select a limited number of buses that maximizes the coverage of demand points, i.e., with a limited purchase budget. We perform trace-driven simulations to verify the performance of the proposed bus selection algorithm. The results show that our approach effectively handles the dynamic user demand under realistic constraints such as server capacity and purchase quantity. Thus, we claim: bus-mounted edge servers for vehicular networks in urban areas are feasible, beneficial, and valuable.",
        "authors": [
            "Xuezhi Li",
            "Jiancong He",
            "Ming Xie",
            "Xuyang Chen",
            "Le Chang",
            "Li Jiang",
            "Gui Gui"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.05543v4",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-05"
    },
    "http://arxiv.org/abs/2512.05516v1": {
        "id": "http://arxiv.org/abs/2512.05516v1",
        "title": "Compiler-supported reduced precision and AoS-SoA transformations for heterogeneous hardware",
        "link": "http://arxiv.org/abs/2512.05516v1",
        "tags": [
            "offloading",
            "quantization"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Explores compiler-supported reduced precision and AoS-SoA layouts for particle simulations on GPUs. Proposes compiler annotations to orchestrate CPU-GPU data offloading. Achieves up to 2.6x speedup on Nvidia GPUs through optimized data transformations.",
        "abstract": "This study evaluates AoS-to-SoA transformations over reduced-precision data layouts for a particle simulation code on several GPU platforms: We hypothesize that SoA fits particularly well to SIMT, while AoS is the preferred storage format for many Lagrangian codes. Reduced-precision (below IEEE accuracy) is an established tool to address bandwidth constraints, although it remains unclear whether AoS and precision conversions should execute on a CPU or be deployed to a GPU if the compute kernel itself must run on an accelerator. On modern superchips where CPUs and GPUs share (logically) one data space, it is also unclear whether it is advantageous to stream data to the accelerator prior to the calculation, or whether we should let the accelerator transform data on demand, i.e.~work in-place logically. We therefore introduce compiler annotations to facilitate such conversions and to give the programmer the option to orchestrate the conversions in combination with GPU offloading. For some of our compute kernels of interest, Nvidia's G200 platforms yield a speedup of around 2.6 while AMD's MI300A exhibits more robust performance yet profits less. We assume that our compiler-based techniques are applicable to a wide variety of Lagrangian codes and beyond.",
        "authors": [
            "Pawel K. Radtke",
            "Tobias Weinzierl"
        ],
        "categories": [
            "cs.PL",
            "cs.DC",
            "cs.MS"
        ],
        "submit_date": "2025-12-05"
    },
    "http://arxiv.org/abs/2512.05462v1": {
        "id": "http://arxiv.org/abs/2512.05462v1",
        "title": "Model Gateway: Model Management Platform for Model-Driven Drug Discovery",
        "link": "http://arxiv.org/abs/2512.05462v1",
        "tags": [
            "serving",
            "RL",
            "RAG"
        ],
        "relevant": true,
        "indexed_date": "2025-12-05",
        "tldr": "Proposes Model Gateway, a management platform for ML models in drug discovery. It integrates LLM Agents for dynamic consensus model, model registration, asynchronous execution, and result retrieval. Achieves 0% failure rate with 10k simultaneous clients.",
        "abstract": "This paper presents the Model Gateway, a management platform for managing machine learning (ML) and scientific computational models in the drug discovery pipeline. The platform supports Large Language Model (LLM) Agents and Generative AI-based tools to perform ML model management tasks in our Machine Learning operations (MLOps) pipelines, such as the dynamic consensus model, a model that aggregates several scientific computational models, registration and management, retrieving model information, asynchronous submission/execution of models, and receiving results once the model complete executions. The platform includes a Model Owner Control Panel, Platform Admin Tools, and Model Gateway API service for interacting with the platform and tracking model execution. The platform achieves a 0% failure rate when testing scaling beyond 10k simultaneous application clients consume models. The Model Gateway is a fundamental part of our model-driven drug discovery pipeline. It has the potential to significantly accelerate the development of new drugs with the maturity of our MLOps infrastructure and the integration of LLM Agents and Generative AI tools.",
        "authors": [
            "Yan-Shiun Wu",
            "Nathan A. Morin"
        ],
        "categories": [
            "cs.SE",
            "cs.DC",
            "cs.LG",
            "q-bio.QM"
        ],
        "submit_date": "2025-12-05"
    },
    "http://arxiv.org/abs/2512.05372v1": {
        "title": "FedGMR: Federated Learning with Gradual Model Restoration under Asynchrony and Model Heterogeneity",
        "link": "http://arxiv.org/abs/2512.05372v1",
        "abstract": "Federated learning (FL) holds strong potential for distributed machine learning, but in heterogeneous environments, Bandwidth-Constrained Clients (BCCs) often struggle to participate effectively due to limited communication capacity. Their small sub-models learn quickly at first but become under-parameterized in later stages, leading to slow convergence and degraded generalization. We propose FedGMR - Federated Learning with Gradual Model Restoration under Asynchrony and Model Heterogeneity. FedGMR progressively increases each client's sub-model density during training, enabling BCCs to remain effective contributors throughout the process. In addition, we develop a mask-aware aggregation rule tailored for asynchronous MHFL and provide convergence guarantees showing that aggregated error scales with the average sub-model density across clients and rounds, while GMR provably shrinks this gap toward full-model FL. Extensive experiments on FEMNIST, CIFAR-10, and ImageNet-100 demonstrate that FedGMR achieves faster convergence and higher accuracy, especially under high heterogeneity and non-IID settings.",
        "authors": [
            "Chengjie Ma",
            "Seungeun Oh",
            "Jihong Park",
            "Seong-Lyun Kim"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.05372v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-05"
    },
    "http://arxiv.org/abs/2512.05224v1": {
        "title": "NVLang: Unified Static Typing for Actor-Based Concurrency on the BEAM",
        "link": "http://arxiv.org/abs/2512.05224v1",
        "abstract": "Actor-based systems like Erlang/OTP power critical infrastructure -- from telecommunications to messaging platforms -- handling millions of concurrent connections with legendary reliability. Yet these systems lack static guarantees about message protocols: processes communicate by sending arbitrary messages that pattern-matched at runtime, deferring protocol violations to production failures.   We present NVLang, a statically typed functional language that brings comprehensive type safety to the BEAM virtual machine while preserving actor model's simplicity and power. NVLang's central contribution that algebraic data types (ADTs) naturally encode actor message protocols: each actor declares the sum type representing its message vocabulary, and the type system enforces protocol conformance at compile time. We introduce typed process identifiers (Pid[T]) that encode the protocol an actor expects, and typed futures (Future[T]) that provide type-safe request-reply patterns.   By extending Hindley-Milner type inference to track message protocols, NVLang eliminates an entire class of message-passing errors while maintaining clean syntax that rivals dynamically typed alternatives. Our implementation compiles to Core Erlang, enabling seamless interoperability with the existing Erlang ecosystem. We formalize the type system and provide proof sketches for type soundness, demonstrating that well-typed NVLang programs cannot send messages that violate actor protocols.",
        "authors": [
            "Miguel de Oliveira Guerreiro"
        ],
        "categories": [
            "cs.PL",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.05224v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-04"
    },
    "http://arxiv.org/abs/2512.04984v1": {
        "title": "Federated Learning for Terahertz Wireless Communication",
        "link": "http://arxiv.org/abs/2512.04984v1",
        "abstract": "The convergence of Terahertz (THz) communications and Federated Learning (FL) promises ultra-fast distributed learning, yet the impact of realistic wideband impairments on optimization dynamics remains theoretically uncharacterized. This paper bridges this gap by developing a multicarrier stochastic framework that explicitly couples local gradient updates with frequency-selective THz effects, including beam squint, molecular absorption, and jitter. Our analysis uncovers a critical diversity trap: under standard unbiased aggregation, the convergence error floor is driven by the harmonic mean of subcarrier SNRs. Consequently, a single spectral hole caused by severe beam squint can render the entire bandwidth useless for reliable model updates. We further identify a fundamental bandwidth limit, revealing that expanding the spectrum beyond a critical point degrades convergence due to the integration of thermal noise and gain collapse at band edges. Finally, we demonstrate that an SNR-weighted aggregation strategy is necessary to suppress the variance singularity at these spectral holes, effectively recovering convergence in high-squint regimes where standard averaging fails. Numerical results validate the expected impact of the discussed physical layer parameters' on performance of THz-FL systems.",
        "authors": [
            "O. Tansel Baydas",
            "Ozgur B. Akan"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.04984v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-04"
    },
    "http://arxiv.org/abs/2512.04527v1": {
        "title": "FLEX: Leveraging FPGA-CPU Synergy for Mixed-Cell-Height Legalization Acceleration",
        "link": "http://arxiv.org/abs/2512.04527v1",
        "abstract": "In this work, we present FLEX, an FPGA-CPU accelerator for mixed-cell-height legalization tasks. We address challenges from the following perspectives. First, we optimize the task assignment strategy and perform an efficient task partition between FPGA and CPU to exploit their complementary strengths. Second, a multi-granularity pipelining technique is employed to accelerate the most time-consuming step, finding optimal placement position (FOP), in legalization. At last, we particularly target the computationally intensive cell shifting process in FOP, optimizing the design to align it seamlessly with the multi-granularity pipelining framework for further speedup. Experimental results show that FLEX achieves up to 18.3x and 5.4x speedups compared to state-of-the-art CPU-GPU and multi-threaded CPU legalizers with better scalability, while improving legalization quality by 4% and 1%.",
        "authors": [
            "Xingyu Liu",
            "Jiawei Liang",
            "Linfeng Du",
            "Yipu Zhang",
            "Chaofang Ma",
            "Hanwei Fan",
            "Jiang Xu",
            "Wei Zhang"
        ],
        "categories": [
            "cs.AR",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.04527v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-04"
    },
    "http://arxiv.org/abs/2512.04449v1": {
        "id": "http://arxiv.org/abs/2512.04449v1",
        "title": "Offloading to CXL-based Computational Memory",
        "link": "http://arxiv.org/abs/2512.04449v1",
        "tags": [
            "offloading",
            "hardware",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-12-04",
        "tldr": "Proposes KAI, a system using Asynchronous Back-Streaming protocol to offload operations to CXL-based Computational Memory. Utilizes layered data/control transfers for async data movement and pipelining, reducing end-to-end runtime by up to 50.4% and idle times by 22.11x/3.85x.",
        "abstract": "CXL-based Computational Memory (CCM) enables near-memory processing within expanded remote memory, presenting opportunities to address data movement costs associated with disaggregated memory systems and to accelerate overall performance. However, existing operation offloading mechanisms are not capable of leveraging the trade-offs of different models based on different CXL protocols. This work first examines these tradeoffs and demonstrates their impact on end-to-end performance and system efficiency for workloads with diverse data and processing requirements. We propose a novel 'Asynchronous Back-Streaming' protocol by carefully layering data and control transfer operations on top of the underlying CXL protocols. We design KAI, a system that realizes the asynchronous back-streaming model that supports asynchronous data movement and lightweight pipelining in host-CCM interactions. Overall, KAI reduces end-to-end runtime by up to 50.4%, and CCM and host idle times by average 22.11x and 3.85x, respectively.",
        "authors": [
            "Suyeon Lee",
            "Kangkyu Park",
            "Kwangsik Shin",
            "Ada Gavrilovska"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-12-04"
    },
    "http://arxiv.org/abs/2512.10980v1": {
        "id": "http://arxiv.org/abs/2512.10980v1",
        "title": "Reducing Fragmentation and Starvation in GPU Clusters through Dynamic Multi-Objective Scheduling",
        "link": "http://arxiv.org/abs/2512.10980v1",
        "tags": [
            "training",
            "serving",
            "offline"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Aims to improve GPU utilization and reduce fragmentation/starvation in multi-tenant clusters running heterogeneous AI workloads. Proposes three dynamic schedulers (HPS, PBS, SBS) that optimize for utilization, fairness, and throughput. Achieves up to 78.2% GPU utilization (vs 45-67% in baselines) and 25.8 jobs/hour throughput.",
        "abstract": "GPU clusters have become essential for training and deploying modern AI systems, yet real deployments continue to report average utilization near 50%. This inefficiency is largely caused by fragmentation, heterogeneous workloads, and the limitations of static scheduling policies. This work presents a systematic evaluation of these issues and introduces three specialized dynamic schedulers: Hybrid Priority (HPS), Predictive Backfill (PBS), and Smart Batch (SBS). These schedulers are designed to improve utilization, fairness, and overall throughput in multi-tenant GPU clusters. We evaluate all schedulers using a controlled simulation of 1,000 AI jobs on a 64-GPU, 8-node cluster that includes a realistic mix of training, inference, and research workloads. Static baselines (FIFO, SJF, Shortest, Shortest-GPU) achieve 45 to 67% GPU utilization and 12.5 to 18.3 jobs per hour and experience severe starvation, with as many as 156 jobs waiting longer than 30 minutes. The dynamic schedulers significantly outperform these policies. HPS achieves the highest utilization (78.2%), highest throughput (25.8 jobs per hour), and the lowest fairness variance among dynamic methods (457), reducing starvation to 12 jobs. PBS improves fragmentation handling and reaches 76.1% utilization, while SBS increases efficiency for structurally similar jobs and reaches 74.6% utilization. Across all key metrics, including throughput, job wait times, fairness variance, and starvation, dynamic multi-objective schedulers consistently outperform single-objective heuristics. These results show that targeted and transparent scheduling strategies can meaningfully increase GPU efficiency in heterogeneous AI clusters and provide a practical foundation for future production scheduling frameworks.",
        "authors": [
            "Akhmadillo Mamirov"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-12-04"
    },
    "http://arxiv.org/abs/2512.04389v1": {
        "title": "A Structure-Aware Irregular Blocking Method for Sparse LU Factorization",
        "link": "http://arxiv.org/abs/2512.04389v1",
        "abstract": "In sparse LU factorization, nonzero elements after symbolic factorization tend to distribute in diagonal and right-bottom region of sparse matrices. However, regular 2D blocking on this non-uniform distribution structure may lead to workload imbalance across blocks. Besides, existing matrix features fail to guide us effectively in blocking. In this paper, we propose a structure-aware irregular blocking method for numerical factorization. A novel diagonal block-based feature is introduced to effectively characterize the local nonzero distribution of sparse matrices. Based on this, we further propose an irregular blocking method that adjusts block sizes according to the local distribution of nonzeros. The strategy utilizes fine-grained blocks in dense regions and coarse-grained blocks in sparse regions, adequately balancing the nonzeros of blocks both within the same level and across levels in the dependency tree. Experiments demonstrate that, on a single NVIDIA A100 GPU, our proposed irregular blocking method achieves average speedups of 1.50x and 3.32x over PanguLU and the latest SuperLU_DIST, respectively. In addition, it achieves speedups of 1.40x and 3.84x over PanguLU and SuperLU_DIST on 4 NVIDIA A100 GPUs.",
        "authors": [
            "Zhen Hu",
            "Dongliang Xiong",
            "Kai Huang",
            "Changjun Wu",
            "Xiaowen Jiang"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.04389v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-04"
    },
    "http://arxiv.org/abs/2512.04355v1": {
        "title": "Counting Without Running: Evaluating LLMs' Reasoning About Code Complexity",
        "link": "http://arxiv.org/abs/2512.04355v1",
        "abstract": "Modern GPU software stacks demand developers who can anticipate performance bottlenecks before ever launching a kernel; misjudging floating-point workloads upstream can derail tuning, scheduling, and even hardware procurement. Yet despite rapid progress in code generation, today's Large Language Models (LLMs) are rarely tested on this kind of forward-looking reasoning. We close that gap with gpuFLOPBench, a benchmark that asks models to \"count without running\" by predicting single and double-precision FLOP counts for 577 CUDA kernels drawn from HeCBench, annotated with ground-truth profiles and eight execution attributes that distinguish trivially analyzable code from kernels whose FLOPs depend on hidden compiler or runtime behavior. Evaluating current closed-source reasoning models shows clear but uneven progress: the newest LLMs achieve perfect classification on straightforward kernels but still incur multiple order-of-magnitude errors whenever implicit FLOPs arise from division, intrinsic math functions, or common subexpressions. These results surface a core limitation of existing code assistants -- the inability to internalize hardware-specific microcode effects -- and position gpuFLOPBench as a focused testbed for developing LLM tooling that can reason about performance with the same rigor as experienced GPU developers. Sources are available at our repository: https://github.com/Scientific-Computing-Lab/gpuFLOPBench",
        "authors": [
            "Gregory Bolet",
            "Giorgis Georgakoudis",
            "Konstantinos Parasyris",
            "Harshitha Menon",
            "Niranjan Hasabnis",
            "Kirk W. Cameron",
            "Gal Oren"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.PF"
        ],
        "id": "http://arxiv.org/abs/2512.04355v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-04"
    },
    "http://arxiv.org/abs/2512.04320v1": {
        "id": "http://arxiv.org/abs/2512.04320v1",
        "title": "VLCs: Managing Parallelism with Virtualized Libraries",
        "link": "http://arxiv.org/abs/2512.04320v1",
        "tags": [
            "training",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Addresses library contention in parallel computing by proposing Virtual Library Contexts (VLCs) that encapsulate and isolate library resources without code changes. Enables resource partitioning and parallel execution. Achieves up to 2.85x speedup in benchmarks with OpenMP, OpenBLAS, and LibTorch.",
        "abstract": "As the complexity and scale of modern parallel machines continue to grow, programmers increasingly rely on composition of software libraries to encapsulate and exploit parallelism. However, many libraries are not designed with composition in mind and assume they have exclusive access to all resources. Using such libraries concurrently can result in contention and degraded performance. Prior solutions involve modifying the libraries or the OS, which is often infeasible.   We propose Virtual Library Contexts (VLCs), which are process subunits that encapsulate sets of libraries and associated resource allocations. VLCs control the resource utilization of these libraries without modifying library code. This enables the user to partition resources between libraries to prevent contention, or load multiple copies of the same library to allow parallel execution of otherwise thread-unsafe code within the same process.   In this paper, we describe and evaluate C++ and Python prototypes of VLCs. Experiments show VLCs enable a speedup up to 2.85x on benchmarks including applications using OpenMP, OpenBLAS, and LibTorch.",
        "authors": [
            "Yineng Yan",
            "William Ruys",
            "Hochan Lee",
            "Ian Henriksen",
            "Arthur Peters",
            "Sean Stephens",
            "Bozhi You",
            "Henrique Fingler",
            "Martin Burtscher",
            "Milos Gligoric",
            "Keshav Pingali",
            "Mattan Erez",
            "George Biros",
            "Christopher J. Rossbach"
        ],
        "categories": [
            "cs.DC",
            "cs.OS"
        ],
        "submit_date": "2025-12-03"
    },
    "http://arxiv.org/abs/2512.04291v1": {
        "title": "Scaling MPI Applications on Aurora",
        "link": "http://arxiv.org/abs/2512.04291v1",
        "abstract": "The Aurora supercomputer, which was deployed at Argonne National Laboratory in 2024, is currently one of three Exascale machines in the world on the Top500 list. The Aurora system is composed of over ten thousand nodes each of which contains six Intel Data Center Max Series GPUs, Intel's first data center-focused discrete GPU, and two Intel Xeon Max Series CPUs, Intel's first Xeon processor to contain HBM memory. To achieve Exascale performance the system utilizes the HPE Slingshot high-performance fabric interconnect to connect the nodes. Aurora is currently the largest deployment of the Slingshot fabric to date with nearly 85,000 Cassini NICs and 5,600 Rosetta switches connected in a dragonfly topology. The combination of the Intel powered nodes and the Slingshot network enabled Aurora to become the second fastest system on the Top500 list in June of 2024 and the fastest system on the HPL MxP benchmark. The system is one of the most powerful systems in the world dedicated to AI and HPC simulations for open science. This paper presents details of the Aurora system design with a particular focus on the network fabric and the approach taken to validating it. The performance of the systems is demonstrated through the presentation of the results of MPI benchmarks as well as performance benchmarks including HPL, HPL-MxP, Graph500, and HPCG run on a large fraction of the system. Additionally results are presented for a diverse set of applications including HACC, AMR-Wind, LAMMPS, and FMM demonstrating that Aurora provides the throughput, latency, and bandwidth across system needed to allow applications to perform and scale to large node counts and providing new levels of capability and enabling breakthrough science.",
        "authors": [
            "Huda Ibeid",
            "Anthony-Trung Nguyen",
            "Aditya Nishtala",
            "Premanand Sakarda",
            "Larry Kaplan",
            "Nilakantan Mahadevan",
            "Michael Woodacre",
            "Victor Anisimov",
            "Kalyan Kumaran",
            "JaeHyuk Kwack",
            "Vitali Morozov",
            "Servesh Muralidharan",
            "Scott Parker"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.04291v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-03"
    },
    "http://arxiv.org/abs/2512.04226v1": {
        "id": "http://arxiv.org/abs/2512.04226v1",
        "title": "tritonBLAS: Triton-based Analytical Approach for GEMM Kernel Parameter Selection",
        "link": "http://arxiv.org/abs/2512.04226v1",
        "tags": [
            "kernel",
            "training",
            "inference"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes tritonBLAS, an analytical model for generating optimized GEMM kernels without autotuning. Uses architectural parameters to predict near-optimal configurations via deterministic modeling. Achieves over 95% of autotuned performance while eliminating tuning overhead.",
        "abstract": "We present tritonBLAS, a fast and deterministic analytical model that uses architectural parameters like the cache hierarchy, and relative code and data placement to generate performant GPU GEMM kernels. tritonBLAS explicitly models the relationship between architectural topology, matrix shapes, and algorithmic blocking behavior to predict near-optimal configurations without runtime autotuning. Based on this model, we developed and implemented a lightweight GEMM framework entirely within Triton. We evaluate the performance of tritonBLAS across a diverse set of GEMM problem sizes on modern GPUs. tritonBLAS achieves over 95% of the performance of autotuning solutions, while reducing autotuning time to zero. This makes tritonBLAS a practical drop-in replacement for empirical tuning in production HPC and ML workloads.",
        "authors": [
            "Ryan Swann",
            "Muhammad Osama",
            "Xiaohu Guo",
            "Bryant Nelson",
            "Lixun Zhang",
            "Alex Brown",
            "Yen Ong",
            "Ali Yazdani",
            "Sean Siddens",
            "Ganesh Dasika",
            "Alex Underwood"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-12-03"
    },
    "http://arxiv.org/abs/2512.04054v1": {
        "title": "A Chronological Analysis of the Evolution of SmartNICs",
        "link": "http://arxiv.org/abs/2512.04054v1",
        "abstract": "Network Interface Cards (NICs) are one of the key enablers of the modern Internet. They serve as gateways for connecting computing devices to networks for the exchange of data with other devices. Recently, the pervasive nature of Internet-enabled devices coupled with the growing demands for faster network access have necessitated the enhancement of NICs to Smart NICs (SNICs), capable of processing enormous volumes of data at near real-time speed. However, despite their popularity, the exact use and applicability of SNICs remains an ongoing debate. These debates are exacerbated by the incorporation of accelerators into SNIC, allowing them to relieve their host's CPUs of various tasks. In this work, we carry out a chronological analysis of SNICs, using 370 articles published in the past 15 years, from 2010 to 2024, to gain some insight into SNICs; and shed some light on their evolution, manufacturers, use cases, and application domains.",
        "authors": [
            "Olasupo Ajayi",
            "Ryan Grant"
        ],
        "categories": [
            "cs.DC",
            "cs.NI"
        ],
        "id": "http://arxiv.org/abs/2512.04054v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-03"
    },
    "http://arxiv.org/abs/2512.03927v1": {
        "id": "http://arxiv.org/abs/2512.03927v1",
        "title": "OD-MoE: On-Demand Expert Loading for Cacheless Edge-Distributed MoE Inference",
        "link": "http://arxiv.org/abs/2512.03927v1",
        "tags": [
            "MoE",
            "edge",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-12-03",
        "tldr": "Proposes OD-MoE, a distributed MoE inference framework for edge devices that eliminates expert caching via on-demand expert loading and an emulative predictor. Achieves 99.94% expert prediction accuracy and 75% decoding speed of fully GPU-cached deployment with only 1/3 GPU memory, enabling sub-1GB deployments.",
        "abstract": "Mixture-of-Experts (MoE), while offering significant advantages as a Large Language Model (LLM) architecture, faces substantial challenges when deployed on low-cost edge devices with tight memory constraints. Expert offloading mitigates this issue by storing expert parameters in CPU memory and caching a subset of popular experts in GPU memory. Although this approach improves GPU memory utilization by caching only the likely-used experts, the GPU memory reserved for expert caching is underutilized compared with dense LLMs. This paper presents OD-MoE, a distributed MoE inference framework that obviates the need for expert caches via fully on-demand expert loading. OD-MoE is built upon two key mechanisms: 1) parallelizing expert loading and expert computation across distributed edge nodes, and 2) an ultra-accurate emulative predictor that forecasts expert activations multiple layers ahead while expert computation is ongoing. With these innovations, OD-MoE dynamically loads each target expert to one of the distributed nodes just-in-time before its activation and promptly evicts it afterward, freeing GPU memory for subsequent experts. We comprehensively benchmark OD-MoE against state-of-the-art MoE offloading systems on a ten-node testbed. Experimental results show that: 1) OD-MoE achieves 99.94% expert activation prediction accuracy, substantially surpassing all existing methods; and 2) OD-MoE delivers approximately 75% of the decoding speed of a fully GPU-cached MoE deployment while using only 1/3 of the GPU memory. More importantly, by eliminating the need for expert caches, OD-MoE enables MoE inference on edge nodes with less-than-1GB GPU memory, paving the way for practical MoE deployment of low-cost IoT devices at the edge in the LLM era.",
        "authors": [
            "Liujianfu Wang",
            "Yuyang Du",
            "Yuchen Pan",
            "Soung Chang Liew",
            "Jiacheng Liu",
            "Kexin Chen"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-12-03"
    },
    "http://arxiv.org/abs/2512.03914v2": {
        "title": "Integrating High Performance In-Memory Data Streaming and In-Situ Visualization in Hybrid MPI+OpenMP PIC MC Simulations Towards Exascale",
        "link": "http://arxiv.org/abs/2512.03914v2",
        "abstract": "Efficient simulation of complex plasma dynamics is crucial for advancing fusion energy research. Particle-in-Cell (PIC) Monte Carlo (MC) simulations provide insights into plasma behavior, including turbulence and confinement, which are essential for optimizing fusion reactor performance. Transitioning to exascale simulations introduces significant challenges, with traditional file input/output (I/O) inefficiencies remaining a key bottleneck. This work advances BIT1, an electrostatic PIC MC code, by improving the particle mover with OpenMP task-based parallelism, integrating the openPMD streaming API, and enabling in-memory data streaming with ADIOS2's Sustainable Staging Transport (SST) engine to enhance I/O performance, computational efficiency, and system storage utilization. We employ profiling tools such as gprof, perf, IPM and Darshan, which provide insights into computation, communication, and I/O operations. We implement time-dependent data checkpointing with the openPMD API enabling seamless data movement and in-situ visualization for real-time analysis without interrupting the simulation. We demonstrate improvements in simulation runtime, data accessibility and real-time insights by comparing traditional file I/O with the ADIOS2 BP4 and SST backends. The proposed hybrid BIT1 openPMD SST enhancement introduces a new paradigm for real-time scientific discovery in plasma simulations, enabling faster insights and more efficient use of exascale computing resources.",
        "authors": [
            "Jeremy J. Williams",
            "Stefan Costea",
            "Daniel Medeiros",
            "Jordy Trilaksono",
            "Pratibha Hegde",
            "David Tskhakaya",
            "Leon Kos",
            "Ales Podolnik",
            "Jakub Hromadka",
            "Kevin A. Huck",
            "Allen D. Malony",
            "Frank Jenko",
            "Erwin Laure",
            "Stefano Markidis"
        ],
        "categories": [
            "physics.plasm-ph",
            "cs.DC",
            "cs.PF"
        ],
        "id": "http://arxiv.org/abs/2512.03914v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-03"
    },
    "http://arxiv.org/abs/2512.10979v1": {
        "title": "Seamless Transitions: A Comprehensive Review of Live Migration Technologies",
        "link": "http://arxiv.org/abs/2512.10979v1",
        "abstract": "Live migration, a technology enabling seamless transition of operational computational entities between various hosts while preserving continuous functionality and client connectivity, has been the subject of extensive research. However, existing reviews often overlook critical technical aspects and practical challenges integral to the usage of live migration techniques in real-world scenarios. This work bridges this gap by integrating the aspects explored in existing reviews together with a comprehensive analysis of live migration technologies across multiple dimensions, with focus on migration techniques, migration units, and infrastructure characteristics. Despite efforts to make live migration widely accessible, its reliance on multiple system factors can create challenges. In certain cases, the complexities and resource demands outweigh the benefits, making its implementation hard to justify. The focus of this work is mainly on container based and virtual machine-based migration technologies, examining the current state of the art and the disparity in adoption between these two approaches. Furthermore, this work explores the impact of migration objectives and operational constraints on the usability and efficacy of existing technologies. By outlining current technical challenges and providing guidelines for future research and development directions, this work serves a dual purpose: first, to equip enthusiasts with a valuable resource on live migration, and second, to contribute to the advancement of live migration technologies and their practical implementation across diverse computing environments.",
        "authors": [
            "Sima Attar-Khorasani",
            "Lincoln Sherpa",
            "Matthias Lieber",
            "Siavash Ghiasvand"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.10979v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-03"
    },
    "http://arxiv.org/abs/2512.03825v1": {
        "title": "Acceleration of Parallel Tempering for Markov Chain Monte Carlo methods",
        "link": "http://arxiv.org/abs/2512.03825v1",
        "abstract": "Markov Chain Monte Carlo methods are algorithms used to sample probability distributions, commonly used to sample the Boltzmann distribution of physical/chemical models (e.g., protein folding, Ising model, etc.). This allows us to study their properties by sampling the most probable states of those systems. However, the sampling capabilities of these methods are not sufficiently accurate when handling complex configuration spaces. This has resulted in the development of new techniques that improve sampling accuracy, usually at the expense of increasing the computational cost. One of such techniques is Parallel Tempering which improves accuracy by running several replicas which periodically exchange their states. Computationally, this imposes a significant slow-down, which can be counteracted by means of parallelization. These schemes enable MCMC/PT techniques to be run more effectively and allow larger models to be studied. In this work, we present a parallel implementation of Metropolis-Hastings with Parallel Tempering, using OpenMP and CUDA for the parallelization in modern CPUs and GPUs, respectively. The results show a maximum speed-up of 52x using OpenMP with 48 cores, and of 986x speed-up with the CUDA version. Furthermore, the results serve as a basic benchmark to compare a future quantum implementation of the same algorithm.",
        "authors": [
            "Aingeru Ramos",
            "Jose A Pascual",
            "Javier Navaridas",
            "Ivan Coluzza"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.03825v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-03"
    },
    "http://arxiv.org/abs/2512.03697v1": {
        "title": "On the Challenges of Energy-Efficiency Analysis in HPC Systems: Evaluating Synthetic Benchmarks and Gromacs",
        "link": "http://arxiv.org/abs/2512.03697v1",
        "abstract": "This paper discusses the challenges encountered when analyzing the energy efficiency of synthetic benchmarks and the Gromacs package on the Fritz and Alex HPC clusters. Experiments were conducted using MPI parallelism on full sockets of Intel Ice Lake and Sapphire Rapids CPUs, as well as Nvidia A40 and A100 GPUs. The metrics and measurements obtained with the Likwid and Nvidia profiling tools are presented, along with the results. The challenges and pitfalls encountered during experimentation and analysis are revealed and discussed. Best practices for future energy efficiency analysis studies are suggested.",
        "authors": [
            "Rafael Ravedutti Lucio Machado",
            "Jan Eitzinger",
            "Georg Hager",
            "Gerhard Wellein"
        ],
        "categories": [
            "cs.DC",
            "cs.MS"
        ],
        "id": "http://arxiv.org/abs/2512.03697v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-03"
    },
    "http://arxiv.org/abs/2512.03685v1": {
        "title": "Distributed Quantum Computing with Fan-Out Operations and Qudits: the Case of Distributed Global Gates (a Preliminary Study)",
        "link": "http://arxiv.org/abs/2512.03685v1",
        "abstract": "Much recent work on distributed quantum computing have focused on the use of entangled pairs and distributed two qubit gates. But there has also been work on efficient schemes for achieving multipartite entanglement between nodes in a single shot, removing the need to generate multipartite entangled states using many entangled pairs. This paper looks at how multipartite entanglement resources (e.g., GHZ states) can be useful for distributed fan-out operations; we also consider the use of qudits of dimension four for distributed quantum circuit compression. In particular, we consider how such fan-out operations and qudits can be used to implement circuits which are challenging for distributed quantum computation, involving pairwise qubit interactions, i.e., what has been called global gates (a.k.a. global Mølmer-Sørensen gates). Such gates have been explored to possibly yield more efficient computations via reduced circuit depth, and can be carried out efficiently in some types of quantum hardware (e.g., trapped-ion quantum computers); we consider this as an exploration of an ``extreme'' case for distribution given the global qubit-qubit interactions. We also conclude with some implications for future work on quantum circuit compilation and quantum data centre design.",
        "authors": [
            "Seng W. Loke"
        ],
        "categories": [
            "quant-ph",
            "cs.DC",
            "cs.ET"
        ],
        "id": "http://arxiv.org/abs/2512.03685v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-03"
    },
    "http://arxiv.org/abs/2512.03644v1": {
        "id": "http://arxiv.org/abs/2512.03644v1",
        "title": "FFTrainer: Fast Failover in Large-Language Model Training with Almost-Free State Management",
        "link": "http://arxiv.org/abs/2512.03644v1",
        "tags": [
            "training",
            "storage",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-12-03",
        "tldr": "Addresses recovery inefficiencies and storage overhead in large-scale LLM training. FFTrainer uses surplus network bandwidth for fast state management and failure rollback reduction. Achieves up to 98% faster recovery time and 68% higher GPU utilization preservation.",
        "abstract": "Recent developments in large language models (LLMs) have introduced new requirements for efficient and robust training. As LLM clusters scale, node failures, lengthy recoveries, and bulky checkpoints erode efficiency. Infrequent asynchronous checkpoints trigger costly rollbacks, yet higher frequencies add prohibitive overhead. To address these challenges, we propose FFTrainer, a system designed for robust LLM training. FFTrainer leverages surplus network capacity to quickly save and load states, thereby preventing rollbacks and accelerating recovery. Compared with prior checkpointing approaches, FFTrainer reduces recovery time by up to 98% and mitigates GPU utilization loss by up to 68% without hindering normal training.",
        "authors": [
            "Bohan Zhao",
            "Yuanhong Wang",
            "Chenglin Liu",
            "Jiagi Pan",
            "Guang Yang",
            "Ruitao Liu",
            "Tingrui Zhang",
            "Kai Luo",
            "Wei Xu"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-12-03"
    },
    "http://arxiv.org/abs/2512.03565v1": {
        "title": "Tuning of Vectorization Parameters for Molecular Dynamics Simulations in AutoPas",
        "link": "http://arxiv.org/abs/2512.03565v1",
        "abstract": "Molecular Dynamics simulations can help scientists to gather valuable insights for physical processes on an atomic scale. This work explores various techniques for SIMD vectorization to improve the pairwise force calculation between molecules in the scope of the particle simulation library AutoPas. The focus lies on the order in which particle values are loaded into vector registers to achieve the most optimal performance regarding execution time or energy consumption.   As previous work indicates that the optimal MD algorithm can change during runtime, this paper investigates simulation-specific parameters like particle density and the impact of the neighbor identification algorithms, which distinguishes this work from related projects. Furthermore, AutoPas' dynamic tuning mechanism is extended to choose the optimal vectorization order during runtime.   The benchmarks show that considering different particle interaction orders during runtime can lead to a considerable performance improvement for the force calculation compared to AutoPas' previous approach.",
        "authors": [
            "Luis Gall",
            "Samuel James Newcome",
            "Fabio Alexander Gratl",
            "Markus Mühlhäußer",
            "Manish Kumar Mishra",
            "Hans-Joachim Bungartz"
        ],
        "categories": [
            "cs.DC",
            "cs.CE",
            "cs.PF"
        ],
        "id": "http://arxiv.org/abs/2512.03565v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-03"
    },
    "http://arxiv.org/abs/2512.03487v1": {
        "title": "Double-Edge-Assisted Computation Offloading and Resource Allocation for Space-Air-Marine Integrated Networks",
        "link": "http://arxiv.org/abs/2512.03487v1",
        "abstract": "In this paper, we propose a double-edge-assisted computation offloading and resource allocation scheme tailored for space-air-marine integrated networks (SAMINs). Specifically, we consider a scenario where both unmanned aerial vehicles (UAVs) and a low earth orbit (LEO) satellite are equipped with edge servers, providing computing services for maritime autonomous surface ships (MASSs). Partial computation workloads of MASSs can be offloaded to both UAVs and the LEO satellite, concurrently, for processing via a multi-access approach. To minimize the energy consumption of SAMINs under latency constraints, we formulate an optimization problem and propose energy efficient algorithms to jointly optimize offloading mode, offloading volume, and computing resource allocation of the LEO satellite and the UAVs, respectively. We further exploit an alternating optimization (AO) method and a layered approach to decompose the original problem to attain the optimal solutions. Finally, we conduct simulations to validate the effectiveness and efficiency of the proposed scheme in comparison with benchmark algorithms.",
        "authors": [
            "Zhen Wang",
            "Bin Lin",
            "Qiang",
            "Ye"
        ],
        "categories": [
            "cs.DC",
            "cs.IT"
        ],
        "id": "http://arxiv.org/abs/2512.03487v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-03"
    },
    "http://arxiv.org/abs/2512.10977v1": {
        "id": "http://arxiv.org/abs/2512.10977v1",
        "title": "Agentic Operator Generation for ML ASICs",
        "link": "http://arxiv.org/abs/2512.10977v1",
        "tags": [
            "kernel",
            "training",
            "inference"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes TritorX, an agentic AI system for generating correct Triton PyTorch ATen kernels for ML accelerators. Uses LLMs with linting, JIT compilation, and PyTorch OpInfo testing for broad coverage. Generates 481 verified operators passing 20,000+ tests.",
        "abstract": "We present TritorX, an agentic AI system designed to generate functionally correct Triton PyTorch ATen kernels at scale for emerging accelerator platforms. TritorX integrates open-source large language models with a custom linter, JIT compilation, and a PyTorch OpInfo-based test harness. This pipeline is compatible with both real Meta Training and Inference Accelerator (MTIA) silicon and in hardware simulation environments for next-generation devices. In contrast to previous kernel-generation approaches that prioritize performance for a limited set of high-usage kernels, TritorX prioritizes coverage. Our system emphasizes correctness and generality across the entire operator set, including diverse data types, shapes, and argument patterns. In our experiments, TritorX successfully generated kernels and wrappers for 481 unique ATen operators that pass all corresponding PyTorch OpInfo tests (over 20,000 in total). TritorX paves the way for overnight generation of complete PyTorch ATen backends for new accelerator platforms.",
        "authors": [
            "Alec M. Hammond",
            "Aram Markosyan",
            "Aman Dontula",
            "Simon Mahns",
            "Zacharias Fisches",
            "Dmitrii Pedchenko",
            "Keyur Muzumdar",
            "Natacha Supper",
            "Mark Saroufim",
            "Joe Isaacson",
            "Laura Wang",
            "Warren Hunt",
            "Kaustubh Gondkar",
            "Roman Levenstein",
            "Gabriel Synnaeve",
            "Richard Li",
            "Jacob Kahn",
            "Ajit Mathews"
        ],
        "categories": [
            "cs.DC",
            "cs.AR",
            "cs.PL"
        ],
        "submit_date": "2025-12-03"
    },
    "http://arxiv.org/abs/2512.03416v1": {
        "id": "http://arxiv.org/abs/2512.03416v1",
        "title": "TokenScale: Timely and Accurate Autoscaling for Disaggregated LLM Serving with Token Velocity",
        "link": "http://arxiv.org/abs/2512.03416v1",
        "tags": [
            "serving",
            "disaggregation"
        ],
        "relevant": true,
        "indexed_date": "2025-12-03",
        "tldr": "Proposes TokenScale for autoscaling disaggregated LLM serving using Token Velocity metric and convertible decoders to handle bursty workloads. Achieves SLO attainment up to 96% and reduces costs by 4-14%.",
        "abstract": "The architectural shift to prefill/decode (PD) disaggregation in LLM serving improves resource utilization but struggles with the bursty nature of modern workloads. Existing autoscaling policies, often retrofitted from monolithic systems like those in AIBrix and DistServe, rely on lagging indicators such as GPU utilization or coarse-grained request counts. This results in slow reactions to load spikes, leading to significant Time-to First-Token (TTFT) and Time-Per-Output-Token (TPOT) SLO violations and costly over-provisioning. We introduce TokenScale, an autoscaling framework that resolves this performance mismatch through two innovations. First, we propose Token Velocity, a novel metric that unifies the prefill, network, and decode stages by quantifying their rate of work. As a leading indicator of system backpressure, it enables proactive scaling. Second, Convertible Decoders allow decoder GPUs to dynamically execute prefill tasks during traffic spikes, creating a rapid-response buffer that absorbs bursts and eliminates the initialization latency of new prefillers. Our evaluation on a GPU cluster with production traces shows TokenScale improves SLO attainment from 50-88% to 80-96% and reduces costs by 4-14% over state-of-the-art systems, including DistServe, BlitzScale, and AIBrix. By uniting a predictive metric with a flexible system design, TokenScale significantly boosts the performance and efficiency of disaggregated LLM serving infrastructure.",
        "authors": [
            "Ruiqi Lai",
            "Hongrui Liu",
            "Chengzhi Lu",
            "Zonghao Liu",
            "Siyu Cao",
            "Siyang Shao",
            "Yixin Zhang",
            "Luo Mai",
            "Dmitrii Ustiugov"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-12-03"
    },
    "http://arxiv.org/abs/2512.03287v1": {
        "title": "Multi-Frequency Federated Learning for Human Activity Recognition Using Head-Worn Sensors",
        "link": "http://arxiv.org/abs/2512.03287v1",
        "abstract": "Human Activity Recognition (HAR) benefits various application domains, including health and elderly care. Traditional HAR involves constructing pipelines reliant on centralized user data, which can pose privacy concerns as they necessitate the uploading of user data to a centralized server. This work proposes multi-frequency Federated Learning (FL) to enable: (1) privacy-aware ML; (2) joint ML model learning across devices with varying sampling frequency. We focus on head-worn devices (e.g., earbuds and smart glasses), a relatively unexplored domain compared to traditional smartwatch- or smartphone-based HAR. Results have shown improvements on two datasets against frequency-specific approaches, indicating a promising future in the multi-frequency FL-HAR task. The proposed network's implementation is publicly available for further research and development.",
        "authors": [
            "Dario Fenoglio",
            "Mohan Li",
            "Davide Casnici",
            "Matias Laporte",
            "Shkurta Gashi",
            "Silvia Santini",
            "Martin Gjoreski",
            "Marc Langheinrich"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.03287v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-02"
    },
    "http://arxiv.org/abs/2512.03024v1": {
        "id": "http://arxiv.org/abs/2512.03024v1",
        "title": "TokenPowerBench: Benchmarking the Power Consumption of LLM Inference",
        "link": "http://arxiv.org/abs/2512.03024v1",
        "tags": [
            "serving",
            "kernel",
            "inference"
        ],
        "relevant": true,
        "indexed_date": "2025-12-02",
        "tldr": "Investigators tackle the lack of power consumption benchmarks for LLM inference by introducing TokenPowerBench, a tool enabling configuration of model, prompt, and engine; capturing multi-level power metrics and attributing energy per request phase. It quantifies joules per token across varying settings, achieving measurable energy efficiency assessment without specialized hardware.",
        "abstract": "Large language model (LLM) services now answer billions of queries per day, and industry reports show that inference, not training, accounts for more than 90% of total power consumption. However, existing benchmarks focus on either training/fine-tuning or performance of inference and provide little support for power consumption measurement and analysis of inference. We introduce TokenPowerBench, the first lightweight and extensible benchmark designed for LLM-inference power consumption studies. The benchmark combines (i) a declarative configuration interface covering model choice, prompt set, and inference engine, (ii) a measurement layer that captures GPU-, node-, and system-level power without specialized power meters, and (iii) a phase-aligned metrics pipeline that attributes energy to the prefill and decode stages of every request. These elements make it straight-forward to explore the power consumed by an LLM inference run; furthermore, by varying batch size, context length, parallelism strategy and quantization, users can quickly assess how each setting affects joules per token and other energy-efficiency metrics. We evaluate TokenPowerBench on four of the most widely used model series (Llama, Falcon, Qwen, and Mistral). Our experiments cover from 1 billion parameters up to the frontier-scale Llama3-405B model. Furthermore, we release TokenPowerBench as open source to help users to measure power consumption, forecast operating expenses, and meet sustainability targets when deploying LLM services.",
        "authors": [
            "Chenxu Niu",
            "Wei Zhang",
            "Jie Li",
            "Yongjian Zhao",
            "Tongyang Wang",
            "Xi Wang",
            "Yong Chen"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CY",
            "cs.DC"
        ],
        "submit_date": "2025-12-02"
    },
    "http://arxiv.org/abs/2512.10974v1": {
        "title": "An Efficient Approach for Energy Conservation in Cloud Computing Environment",
        "link": "http://arxiv.org/abs/2512.10974v1",
        "abstract": "Recent trends of technology have explored a numerous applications of cloud services, which require a significant amount of energy. In the present scenario, most of the energy sources are limited and have a greenhouse effect on the environment. Therefore, it is the need of the hour that the energy consumed by the cloud service providers must be reduced and it is a great challenge to the research community to develop energy-efficient algorithms. To design the same, some researchers tried to maximize the average resource utilization, whereas some researchers tried to minimize the makespan. However, they have not considered different types of resources that are present in the physical machines. In this paper, we propose a task scheduling algorithm, which tries to improve utilization of resources (like CPU, disk, I/O) explicitly, which in turn increases the utilization of active resources. For this, the proposed algorithm uses a fitness value, which is a function of CPU, disk and I/O utilization, and processing time of the task. To demonstrate the performance of the proposed algorithm, extensive simulations are performed on both proposed algorithm and existing algorithm MaxUtil using synthetic datasets. From the simulation results, it can be observed that the proposed algorithm is a better energy-efficient algorithm and consumes less energy than the MaxUtil algorithm.",
        "authors": [
            "Sohan Kumar Pande",
            "Sanjaya Kumar Panda",
            "Preeti Ranjan Sahu"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.10974v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-02"
    },
    "http://arxiv.org/abs/2512.02818v1": {
        "title": "Designing FAIR Workflows at OLCF: Building Scalable and Reusable Ecosystems for HPC Science",
        "link": "http://arxiv.org/abs/2512.02818v1",
        "abstract": "High Performance Computing (HPC) centers provide advanced infrastructure that enables scientific research at extreme scale. These centers operate with hardware configurations, software environments, and security requirements that differ substantially from most users' local systems. As a result, users often develop customized digital artifacts that are tightly coupled to a given HPC center. This practice can lead to significant duplication of effort as multiple users independently create similar solutions to common problems. The FAIR Principles offer a framework to address these challenges. Initially designed to improve data stewardship, the FAIR approach has since been extended to encompass software, workflows, models, and infrastructure. By encouraging the use of rich metadata and community standards, FAIR practices aim to make digital artifacts easier to share and reuse, both within and across scientific domains. Many FAIR initiatives have emerged within individual research communities, often aligned by discipline (e.g. bioinformatics, earth sciences). These communities have made progress in adopting FAIR practices, but their domain-specific nature can lead to silos that limit broader collaboration. Thus, we propose that HPC centers play a more active role in fostering FAIR ecosystems that support research across multiple disciplines. This requires designing infrastructure that enables researchers to discover, share, and reuse computational components more effectively. Here, we build on the architecture of the European Open Science Cloud (EOSC) EOSC-Life FAIR Workflows Collaboratory to propose a model tailored to the needs of HPC. Rather than focusing on entire workflows, we emphasize the importance of making individual workflow components FAIR. This component-based approach better supports the diverse and evolving needs of HPC users while maximizing the long-term value of their work.",
        "authors": [
            "Sean R. Wilkinson",
            "Patrick Widener",
            "Sarp Oral",
            "Rafael Ferreira da Silva"
        ],
        "categories": [
            "cs.DC",
            "cs.DL"
        ],
        "id": "http://arxiv.org/abs/2512.02818v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-02"
    },
    "http://arxiv.org/abs/2512.02683v1": {
        "title": "Distributed and Autonomic Minimum Spanning Trees",
        "link": "http://arxiv.org/abs/2512.02683v1",
        "abstract": "The most common strategy for enabling a process in a distributed system to broadcast a message is one-to-all communication. However, this approach is not scalable, as it places a heavy load on the sender. This work presents an autonomic algorithm that enables the $n$ processes in a distributed system to build and maintain a spanning tree connecting themselves. In this context, processes are the vertices of the spanning tree. By definition, a spanning tree connects all processes without forming cycles. The proposed algorithm ensures that every vertex in the spanning tree has both an in-degree and the tree depth of at most $log_2 n$. When all processes are correct, the degree of each process is exactly $log_2 n$. A spanning tree is dynamically created from any source process and is transparently reconstructed as processes fail or recover. Up to $n-1$ processes can fail, and the correct processes remain connected through a scalable, functioning spanning tree. To build and maintain the tree, processes use the VCube virtual topology, which also serves as a failure detector. Two broadcast algorithms based on the autonomic spanning tree algorithm are presented: one for best-effort broadcast and one for reliable broadcast. Simulation results are provided, including comparisons with other alternatives.",
        "authors": [
            "Luiz A. Rodrigues",
            "Elias P. Duarte",
            "Luciana Arantes"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.02683v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-02"
    },
    "http://arxiv.org/abs/2512.02663v1": {
        "title": "Theoretical analysis of beaconless geocast protocols in 1D",
        "link": "http://arxiv.org/abs/2512.02663v1",
        "abstract": "Beaconless geocast protocols are routing protocols used to send messages in mobile ad-hoc wireless networks, in which the only information available to each node is its own location. Messages get routed in a distributed manner: each node uses local decision rules based on the message source and destination, and its own location. In this paper we analyze six different beaconless geocast protocols, focusing on two relevant 1D scenarios. The selection of protocols reflects the most relevant types of protocols proposed in the literature, including those evaluated in previous computer simulations. We present a formal and structured analysis of the maximum number of messages that a node can receive, for each protocol, in each of the two scenarios. This is a measure of the network load incurred by each protocol. Our analysis, that for some of the protocols requires an involved probabilistic analysis, confirms behaviors that had been observed only through simulations before.",
        "authors": [
            "Joachim Gudmundsson",
            "Irina Kostitsyna",
            "Maarten Löffler",
            "Tobias Müller",
            "Vera Sacristán",
            "Rodrigo I. Silveira"
        ],
        "categories": [
            "cs.CG",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.02663v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-02"
    },
    "http://arxiv.org/abs/2512.02646v1": {
        "id": "http://arxiv.org/abs/2512.02646v1",
        "title": "Offloading Artificial Intelligence Workloads across the Computing Continuum by means of Active Storage Systems",
        "link": "http://arxiv.org/abs/2512.02646v1",
        "tags": [
            "offloading",
            "training",
            "storage"
        ],
        "relevant": true,
        "indexed_date": "2025-12-02",
        "tldr": "Proposes an active storage architecture for offloading AI workloads across heterogeneous devices. Embeds computation in storage to reduce data transfer and uses dataClay platform. Achieves improved memory efficiency and training speeds while maintaining accuracy.",
        "abstract": "The increasing demand for artificial intelligence (AI) workloads across diverse computing environments has driven the need for more efficient data management strategies. Traditional cloud-based architectures struggle to handle the sheer volume and velocity of AI-driven data, leading to inefficiencies in storage, computation, and data movement. This paper explores the integration of active storage systems within the computing continuum to optimize AI workload distribution.   By embedding computation directly into storage architectures, active storage is able to reduce data transfer overhead, enhancing performance and improving resource utilization. Other existing frameworks and architectures offer mechanisms to distribute certain AI processes across distributed environments; however, they lack the flexibility and adaptability that the continuum requires, both regarding the heterogeneity of devices and the rapid-changing algorithms and models being used by domain experts and researchers.   This article proposes a software architecture aimed at seamlessly distributing AI workloads across the computing continuum, and presents its implementation using mainstream Python libraries and dataClay, an active storage platform. The evaluation shows the benefits and trade-offs regarding memory consumption, storage requirements, training times, and execution efficiency across different devices. Experimental results demonstrate that the process of offloading workloads through active storage significantly improves memory efficiency and training speeds while maintaining accuracy. Our findings highlight the potential of active storage to revolutionize AI workload management, making distributed AI deployments more scalable and resource-efficient with a very low entry barrier for domain experts and application developers.",
        "authors": [
            "Alex Barceló",
            "Sebastián A. Cajas Ordoñez",
            "Jaydeep Samanta",
            "Andrés L. Suárez-Cetrulo",
            "Romila Ghosh",
            "Ricardo Simón Carbajo",
            "Anna Queralt"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-12-02"
    },
    "http://arxiv.org/abs/2512.02546v1": {
        "title": "Solutions for Distributed Memory Access Mechanism on HPC Clusters",
        "link": "http://arxiv.org/abs/2512.02546v1",
        "abstract": "Paper presents and evaluates various mechanisms for remote access to memory in distributed systems based on two distinct HPC clusters. We are comparing solutions based on the shared storage and MPI (over Infiniband and Slingshot) to the local memory access. This paper also mentions medical use-cases that would mostly benefit from the described solution. We have found out that results for remote access esp. backed by MPI are similar to local memory access.",
        "authors": [
            "Jan Meizner",
            "Maciej Malawski"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.02546v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-02"
    },
    "http://arxiv.org/abs/2512.02300v1": {
        "title": "DOLMA: A Data Object Level Memory Disaggregation Framework for HPC Applications",
        "link": "http://arxiv.org/abs/2512.02300v1",
        "abstract": "Memory disaggregation is promising to scale memory capacity and improves utilization in HPC systems. However, the performance overhead of accessing remote memory poses a significant challenge, particularly for compute-intensive HPC applications where execution times are highly sensitive to data locality. In this work, we present DOLMA, a Data Object Level M emory dis Aggregation framework designed for HPC applications. DOLMA intelligently identifies and offloads data objects to remote memory, while providing quantitative analysis to decide a suitable local memory size. Furthermore, DOLMA leverages the predictable memory access patterns typical in HPC applications and enables remote memory prefetch via a dual-buffer design. By carefully balancing local and remote memory usage and maintaining multi-thread concurrency, DOLMA provides a flexible and efficient solution for leveraging disaggregated memory in HPC domains while minimally compromising application performance. Evaluating with eight HPC workloads and computational kernels, DOLMA limits performance degradation to less than 16% while reducing local memory usage by up to 63%, on average.",
        "authors": [
            "Haoyu Zheng",
            "Shouwei Gao",
            "Jie Ren",
            "Wenqian Dong"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.02300v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-02"
    },
    "http://arxiv.org/abs/2512.02278v1": {
        "id": "http://arxiv.org/abs/2512.02278v1",
        "title": "Fantasy: Efficient Large-scale Vector Search on GPU Clusters with GPUDirect Async",
        "link": "http://arxiv.org/abs/2512.02278v1",
        "tags": [
            "serving",
            "storage",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Addresses how to efficiently perform large-scale vector similarity search on GPU clusters. Introduces Fantasy, which pipelines search and data transfer with GPUDirect Async for overlapping computation and network communication. Achieves significantly improved throughput for large graphs and large query batches.",
        "abstract": "Vector similarity search has become a critical component in AI-driven applications such as large language models (LLMs). To achieve high recall and low latency, GPUs are utilized to exploit massive parallelism for faster query processing. However, as the number of vectors continues to grow, the graph size quickly exceeds the memory capacity of a single GPU, making it infeasible to store and process the entire index on a single GPU. Recent work uses CPU-GPU architectures to keep vectors in CPU memory or SSDs, but the loading step stalls GPU computation. We present Fantasy, an efficient system that pipelines vector search and data transfer in a GPU cluster with GPUDirect Async. Fantasy overlaps computation and network communication to significantly improve search throughput for large graphs and deliver large query batch sizes.",
        "authors": [
            "Yi Liu",
            "Chen Qian"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-12-01"
    },
    "http://arxiv.org/abs/2512.02175v1": {
        "title": "Sampling on Metric Graphs",
        "link": "http://arxiv.org/abs/2512.02175v1",
        "abstract": "Metric graphs are structures obtained by associating edges in a standard graph with segments of the real line and gluing these segments at the vertices of the graph. The resulting structure has a natural metric that allows for the study of differential operators and stochastic processes on the graph. Brownian motions in these domains have been extensively studied theoretically using their generators. However, less work has been done on practical algorithms for simulating these processes. We introduce the first algorithm for simulating Brownian motions on metric graphs through a timestep splitting Euler-Maruyama-based discretization of their corresponding stochastic differential equation. By applying this scheme to Langevin diffusions on metric graphs, we also obtain the first algorithm for sampling on metric graphs. We provide theoretical guarantees on the number of timestep splittings required for the algorithm to converge to the underlying stochastic process. We also show that the exit probabilities of the simulated particle converge to the vertex-edge jump probabilities of the underlying stochastic differential equation as the timestep goes to zero. Finally, since this method is highly parallelizable, we provide fast, memory-aware implementations of our algorithm in the form of custom CUDA kernels that are up to ~8000x faster than a GPU implementation using PyTorch on simple star metric graphs. Beyond simple star graphs, we benchmark our algorithm on a real cortical vascular network extracted from a DuMuX tissue-perfusion model for tracer transport. Our algorithm is able to run stable simulations with timesteps significantly larger than the stable limit of the finite volume method used in DuMuX while also achieving speedups of up to ~1500x.",
        "authors": [
            "Rajat Vadiraj Dwaraknath",
            "Lexing Ying"
        ],
        "categories": [
            "math.NA",
            "cs.DC",
            "cs.LG",
            "math.PR"
        ],
        "id": "http://arxiv.org/abs/2512.02175v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-01"
    },
    "http://arxiv.org/abs/2512.01983v1": {
        "title": "Feature-Based Semantics-Aware Scheduling for Energy-Harvesting Federated Learning",
        "link": "http://arxiv.org/abs/2512.01983v1",
        "abstract": "Federated Learning (FL) on resource-constrained edge devices faces a critical challenge: The computational energy required for training Deep Neural Networks (DNNs) often dominates communication costs. However, most existing Energy-Harvesting FL (EHFL) strategies fail to account for this reality, resulting in wasted energy due to redundant local computations. For efficient and proactive resource management, algorithms that predict local update contributions must be devised. We propose a lightweight client scheduling framework using the Version Age of Information (VAoI), a semantics-aware metric that quantifies update timeliness and significance. Crucially, we overcome VAoI's typical prohibitive computational cost, which requires statistical distance over the entire parameter space, by introducing a feature-based proxy. This proxy estimates model redundancy using intermediate-layer extraction from a single forward pass, dramatically reducing computational complexity. Experiments conducted under extreme non-IID data distributions and scarce energy availability demonstrate superior learning performance while achieving energy reduction compared to existing baseline selection policies. Our framework establishes semantics-aware scheduling as a practical and vital solution for EHFL in realistic scenarios where training costs dominate transmission costs.",
        "authors": [
            "Eunjeong Jeong",
            "Giovanni Perin",
            "Howard H. Yang",
            "Nikolaos Pappas"
        ],
        "categories": [
            "cs.LG",
            "cs.DC",
            "cs.IT",
            "cs.NI",
            "eess.SP"
        ],
        "id": "http://arxiv.org/abs/2512.01983v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-01"
    },
    "http://arxiv.org/abs/2512.16928v1": {
        "title": "Dion2: A Simple Method to Shrink Matrix in Muon",
        "link": "http://arxiv.org/abs/2512.16928v1",
        "abstract": "The Muon optimizer enjoys strong empirical performance and theoretical grounding. However, the super-linear cost of its orthonormalization step introduces increasing overhead with scale. To alleviate this cost, several works have attempted to reduce the size of the matrix entering the orthonormalization step. We introduce Dion2, a much simpler method for shrinking the matrix involved in Muon's computation compared to prior approaches. At a high level, Dion2 selects a fraction of rows or columns at each iteration and orthonormalizes only those. This sampling procedure makes the update sparse, reducing both computation and communication costs which in turn improves the scalability of Muon.",
        "authors": [
            "Kwangjun Ahn",
            "Noah Amsel",
            "John Langford"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.16928v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-01"
    },
    "http://arxiv.org/abs/2512.17918v1": {
        "title": "QAISim: A Toolkit for Modeling and Simulation of AI in Quantum Cloud Computing Environments",
        "link": "http://arxiv.org/abs/2512.17918v1",
        "abstract": "Quantum computing offers new ways to explore the theory of computation via the laws of quantum mechanics. Due to the rising demand for quantum computing resources, there is growing interest in developing cloud-based quantum resource sharing platforms that enable researchers to test and execute their algorithms on real quantum hardware. These cloud-based systems face a fundamental challenge in efficiently allocating quantum hardware resources to fulfill the growing computational demand of modern Internet of Things (IoT) applications. So far, attempts have been made in order to make efficient resource allocation, ranging from heuristic-based solutions to machine learning. In this work, we employ quantum reinforcement learning based on parameterized quantum circuits to address the resource allocation problem to support large IoT networks. We propose a python-based toolkit called QAISim for the simulation and modeling of Quantum Artificial Intelligence (QAI) models for designing resource management policies in quantum cloud environments. We have simulated policy gradient and Deep Q-Learning algorithms for reinforcement learning. QAISim exhibits a substantial reduction in model complexity compared to its classical counterparts with fewer trainable variables.",
        "authors": [
            "Irwindeep Singh",
            "Sukhpal Singh Gill",
            "Jinzhao Sun",
            "Jan Mol"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "id": "http://arxiv.org/abs/2512.17918v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-01"
    },
    "http://arxiv.org/abs/2512.01764v1": {
        "title": "Trace-based, time-resolved analysis of MPI application performance using standard metrics",
        "link": "http://arxiv.org/abs/2512.01764v1",
        "abstract": "Detailed trace analysis of MPI applications is essential for performance engineering, but growing trace sizes and complex communication behaviour often render comprehensive visual inspection impractical. This work presents a trace-based calculation of time-resolved values of standard MPI performance metrics, load balance, serialisation, and transfer efficiency, by discretising execution traces into fixed or adaptive time segments. The implementation processes Paraver traces postmortem, reconstructing critical execution paths and handling common event anomalies, such as clock inconsistencies and unmatched MPI events, to robustly calculate metrics for each segment. The calculated per-window metric values expose transient performance bottlenecks that the timeaggregated metrics from existing tools may conceal. Evaluations on a synthetic benchmark and real-world applications (LaMEM and ls1-MarDyn) demonstrate how time-resolved metrics reveal localised performance bottlenecks obscured by global aggregates, offering a lightweight and scalable alternative even when trace visualisation is impractical.",
        "authors": [
            "Kingshuk Haldar"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.01764v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-01"
    },
    "http://arxiv.org/abs/2512.01678v3": {
        "title": "Morphling: Fast, Fused, and Flexible GNN Training at Scale",
        "link": "http://arxiv.org/abs/2512.01678v3",
        "abstract": "Graph Neural Networks (GNNs) present a fundamental hardware challenge by fusing irregular, memory-bound graph traversals with regular, compute-intensive dense matrix operations. While frameworks such as PyTorch Geometric (PyG) and Deep Graph Library (DGL) prioritize high-level usability, they fail to address these divergent execution characteristics. As a result, they rely on generic kernels that suffer from poor cache locality, excessive memory movement, and substantial intermediate allocations. To address these limitations, we present Morphling, a domain-specific code synthesizer designed to bridge this gap. Morphling compiles high-level GNN specifications into portable, backend-specialized implementations targeting OpenMP, CUDA, and MPI. It achieves this by instantiating a library of optimized, architecture-aware primitives tailored to each execution environment. Morphling also incorporates a runtime sparsity-aware execution engine that dynamically selects dense or sparse execution paths using input feature statistics, reducing unnecessary computation on zero-valued entries. We evaluate Morphling on eleven real-world datasets spanning diverse graph structures, feature dimensionalities, and sparsity regimes. Morphling improves per-epoch training throughput by an average of 20X on CPUs, 19X on GPUs, and 6X in distributed settings over PyG and DGL, with peak speedups reaching 66X. Morphling's memory-efficient layouts further reduce peak memory consumption by up to 15X, enabling large-scale GNN training on commodity hardware. These findings demonstrate that specialized, architecture-aware code synthesis provides an effective and scalable path toward high-performance GNN execution across diverse parallel and distributed platforms.",
        "authors": [
            "Anubhab",
            "Rupesh Nasre"
        ],
        "categories": [
            "cs.LG",
            "cs.DC",
            "cs.PL"
        ],
        "id": "http://arxiv.org/abs/2512.01678v3",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-01"
    },
    "http://arxiv.org/abs/2512.01646v2": {
        "title": "StarDist: A Code Generator for Distributed Graph Algorithms",
        "link": "http://arxiv.org/abs/2512.01646v2",
        "abstract": "Relational data, occurring in the real world, are often structured as graphs, which provide the logical abstraction required to make analytical derivations simpler. As graphs get larger, the irregular access patterns exhibited in most graph algorithms, hamper performance. This, along with NUMA and physical memory limits, results in scaling complexities with sequential/shared memory frameworks. StarPlat's MPI backend abstracts away the programmatic complexity involved in designing optimal distributed graph algorithms. It provides an instrument for coding graph algorithms that scale over distributed memory. In this work, we provide an analysis-transformation framework that leverages general semantics associated with iterations involving nodes and their neighbors, within StarPlat, to aggregate communication. The framework scans for patterns that warrant re-ordering in neighborhood access patterns, aggregate communication, and avoid communication altogether with opportunistic caching in reduction constructs. We also architect an optimized bulk-reduction substrate using Open MPI's passive Remote Memory Access (RMA) constructs. We applied our optimization logic to StarPlat's distributed backend and outperformed d-Galois by 2.05 and DRONE by 1.44 times in Single Source Shortest Paths across several big data graphs.",
        "authors": [
            "Barenya Kumar Nandy",
            "Rupesh Nasre"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.01646v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-01"
    },
    "http://arxiv.org/abs/2512.01549v1": {
        "title": "Delta Sum Learning: an approach for fast and global convergence in Gossip Learning",
        "link": "http://arxiv.org/abs/2512.01549v1",
        "abstract": "Federated Learning is a popular approach for distributed learning due to its security and computational benefits. With the advent of powerful devices in the network edge, Gossip Learning further decentralizes Federated Learning by removing centralized integration and relying fully on peer to peer updates. However, the averaging methods generally used in both Federated and Gossip Learning are not ideal for model accuracy and global convergence. Additionally, there are few options to deploy Learning workloads in the edge as part of a larger application using a declarative approach such as Kubernetes manifests. This paper proposes Delta Sum Learning as a method to improve the basic aggregation operation in Gossip Learning, and implements it in a decentralized orchestration framework based on Open Application Model, which allows for dynamic node discovery and intent-driven deployment of multi-workload applications. Evaluation results show that Delta Sum performance is on par with alternative integration methods for 10 node topologies, but results in a 58% lower global accuracy drop when scaling to 50 nodes. Overall, it shows strong global convergence and a logarithmic loss of accuracy with increasing topology size compared to a linear loss for alternatives under limited connectivity.",
        "authors": [
            "Tom Goethals",
            "Merlijn Sebrechts",
            "Stijn De Schrijver",
            "Filip De Turck",
            "Bruno Volckaert"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "id": "http://arxiv.org/abs/2512.01549v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-01"
    },
    "http://arxiv.org/abs/2512.01357v1": {
        "id": "http://arxiv.org/abs/2512.01357v1",
        "title": "Tangram: Accelerating Serverless LLM Loading through GPU Memory Reuse and Affinity",
        "link": "http://arxiv.org/abs/2512.01357v1",
        "tags": [
            "serving",
            "offloading",
            "storage"
        ],
        "relevant": true,
        "indexed_date": "2025-12-01",
        "tldr": "Tackles high cold-start latency in serverless LLM serving via GPU memory reuse. Proposes Tangram with unified GPU memory pool, on-demand KV cache allocation, and affinity-aware scheduling. Achieves 6.2x faster model loading and 23-55% lower Time-To-First-Token.",
        "abstract": "Serverless Large Language Models (LLMs) have emerged as a cost-effective solution for deploying AI services by enabling a 'pay-as-you-go' pricing model through GPU resource sharing. However, cold-start latency, especially the model loading phase, has become a critical performance bottleneck, as it scales linearly with model size and severely limits the practical deployment of large-scale LLM services. This paper presents Tangram, a novel system that accelerates Serverless LLM loading through efficient GPU memory reuse. By leveraging the unused GPU memory to retain model parameters, Tangram significantly reduces model transfer time and cold-start latency. Its design includes three key components: unified GPU memory pool for tensor-level parameter sharing across models, on-demand KV cache allocation for dynamic memory management, and GPU-affinity-aware scheduling for maximizing resource utilization. These techniques collectively address the critical challenges of inefficient memory usage and the cold-start problem in Serverless LLM platforms. We have implemented a fully functional prototype, and experiments show that Tangram achieves up to 6.2 times faster loading and reduces Time-To-First-Token (TTFT) during cold-start by 23--55% over state-of-the-art methods.",
        "authors": [
            "Wenbin Zhu",
            "Zhaoyan Shen",
            "Zili Shao",
            "Hongjun Dai",
            "Feng Chen"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.AR"
        ],
        "submit_date": "2025-12-01"
    },
    "http://arxiv.org/abs/2512.01039v1": {
        "id": "http://arxiv.org/abs/2512.01039v1",
        "title": "Joint Partitioning and Placement of Foundation Models for Real-Time Edge AI",
        "link": "http://arxiv.org/abs/2512.01039v1",
        "tags": [
            "edge",
            "serving",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes dynamic partitioning and placement of foundation models for real-time edge inference. Introduces a runtime optimization framework for layer-wise assignments under latency and resource constraints. Achieves adaptive orchestration responsive to infrastructure volatility in 6G edge networks.",
        "abstract": "Inference over large-scale foundation models within heterogeneous edge environments necessitates a fundamentally reconfigurable orchestration substrate. Static partitioning of model layers presumes temporal stability across compute and network resources, which is misaligned with the volatility of real-world deployments. We introduce a framework in which both the spatial placement and internal segmentation of foundation models are elevated to runtime-resolved constructs. The orchestration problem is formalized as a constrained optimization over layer-wise assignments, subject to evolving latency, utilization, and privacy gradients. The framework implements reactive inference composition responsive to infrastructural fluctuations by integrating model-aware capacity profiling with dynamic graph re-partitioning and reallocation. We introduce architectural and algorithmic components, along with a representative use case in 6G multi-access edge computing.",
        "authors": [
            "Aladin Djuhera",
            "Fernando Koch",
            "Alecio Binotto"
        ],
        "categories": [
            "cs.DC",
            "cs.LG",
            "cs.NI"
        ],
        "submit_date": "2025-11-30"
    },
    "http://arxiv.org/abs/2512.00902v1": {
        "id": "http://arxiv.org/abs/2512.00902v1",
        "title": "Elastic Mixture of Rank-Wise Experts for Knowledge Reuse in Federated Fine-Tuning",
        "link": "http://arxiv.org/abs/2512.00902v1",
        "tags": [
            "training",
            "RL",
            "MoE"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes SmartFed for efficient federated fine-tuning of LLMs via Mixture of Rank-Wise Experts (MoRE) and Elastic Expert Quota Allocation (EEQA). Leverages knowledge reuse in LoRA modules to reduce computation. Achieves higher accuracy with 2.3× faster training than baselines.",
        "abstract": "Federated fine-tuning offers a promising solution for adapting Large Language Models (LLMs) to downstream tasks while safeguarding data privacy. However, its high computational and communication demands hinder its deployment on resource-constrained devices. In this paper, we propose SmartFed, a resource-efficient federated fine-tuning framework. SmartFed intelligently reuses knowledge embedded in existing LoRA modules, eliminating the need for expensive training from scratch when adapting LLMs to new tasks. To effectively exploit this knowledge and ensure scalability, we introduce the Mixture of Rank-Wise Experts (MoRE). MoRE decomposes LoRA modules into fine-grained rank-level experts. These experts are selectively activated and combined based on input semantics and resource budgets. Moreover, to optimize resource utilization, we present the Elastic Expert Quota Allocation (EEQA). EEQA adaptively allocates expert capacity across parameter matrices based on their contribution to model performance, focusing computing resources on the critical experts. Extensive evaluations across multiple benchmarks demonstrate that SmartFed significantly outperforms existing methods in model performance and training efficiency.",
        "authors": [
            "Yebo Wu",
            "Jingguang Li",
            "Zhijiang Guo",
            "Li Li"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-11-30"
    },
    "http://arxiv.org/abs/2512.00844v1": {
        "title": "FC-ADL: Efficient Microservice Anomaly Detection and Localisation Through Functional Connectivity",
        "link": "http://arxiv.org/abs/2512.00844v1",
        "abstract": "Microservices have transformed software architecture through the creation of modular and independent services. However, they introduce operational complexities in service integration and system management that makes swift and accurate anomaly detection and localisation challenging. Despite the complex, dynamic, and interconnected nature of microservice architectures, prior works that investigate metrics for anomaly detection rarely include explicit information about time-varying interdependencies. And whilst prior works on fault localisation typically do incorporate information about dependencies between microservices, they scale poorly to real world large-scale deployments due to their reliance on computationally expensive causal inference. To address these challenges we propose FC-ADL, an end-to-end scalable approach for detecting and localising anomalous changes from microservice metrics based on the neuroscientific concept of functional connectivity. We show that by efficiently characterising time-varying changes in dependencies between microservice metrics we can both detect anomalies and provide root cause candidates without incurring the significant overheads of causal and multivariate approaches. We demonstrate that our approach can achieve top detection and localisation performance across a wide degree of different fault scenarios when compared to state-of-the-art approaches. Furthermore, we illustrate the scalability of our approach by applying it to Alibaba's extremely large real-world microservice deployment.",
        "authors": [
            "Giles Winchester",
            "George Parisis",
            "Luc Berthouze"
        ],
        "categories": [
            "cs.SE",
            "cs.DC",
            "cs.LG"
        ],
        "id": "http://arxiv.org/abs/2512.00844v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-30"
    },
    "http://arxiv.org/abs/2512.00719v1": {
        "id": "http://arxiv.org/abs/2512.00719v1",
        "title": "SIMPLE: Disaggregating Sampling from GPU Inference into a Decision Plane for Faster Distributed LLM Serving",
        "link": "http://arxiv.org/abs/2512.00719v1",
        "tags": [
            "serving",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Addresses sampling bottleneck in distributed LLM serving by disaggregating to CPU with sequence-parallel sampling and speculative hot-vocab sampling. Achieves up to 96% throughput increase and 20-65% latency reduction.",
        "abstract": "As large language models (LLMs) scale out with tensor parallelism (TP) and pipeline parallelism (PP) and production stacks have aggressively optimized the data plane (attention/GEMM and KV cache), sampling, the decision plane that turns logits into tokens, becomes a new bottleneck. This creates a structural holdout: sampling neither expands with TP nor balances across PP stages, so its share of iteration time grows as GPUs get faster and it caps pipeline frequency at the last stage. We present SIMPLE, a stage-agnostic, sequence-parallel, overlappable decision plane that disaggregates sampling into a CPU-side service and shrinks its runtime footprint back to a minor, hidden role. SIMPLE combines: (1) sequence-parallel sampling, which shards work along the batch dimension and removes vocabulary-axis collectives; (2) a CPU-based algorithm with column-wise penalties and truncation-first filtering to realize single-pass, linear-time kernels; and (3) speculative hot-vocab sampling (SHVS), which samples on a small hot set with rejection-correctness and uses a simple sizing model to choose the hot-vocab size that maximizes throughput. In evaluation, SIMPLE improves end-to-end throughput by up to 96% and reduces P95 latency by 20-65%. Crucially, SIMPLE requires no user-side code changes and composes with existing data-plane optimizations, unlocking scaling benefits that compound with future GPU generations.",
        "authors": [
            "Bohan Zhao",
            "Zane Cao",
            "Yongchao He"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-11-30"
    },
    "http://arxiv.org/abs/2512.00711v1": {
        "title": "Cross-Domain Federated Semantic Communication with Global Representation Alignment and Domain-Aware Aggregation",
        "link": "http://arxiv.org/abs/2512.00711v1",
        "abstract": "Semantic communication can significantly improve bandwidth utilization in wireless systems by exploiting the meaning behind raw data. However, the advancements achieved through semantic communication are closely dependent on the development of deep learning (DL) models for joint source-channel coding (JSCC) encoder/decoder techniques, which require a large amount of data for training. To address this data-intensive nature of DL models, federated learning (FL) has been proposed to train a model in a distributed manner, where the server broadcasts the DL model to clients in the network for training with their local data. However, the conventional FL approaches suffer from catastrophic degradation when client data are from different domains. In contrast, in this paper, a novel FL framework is proposed to address this domain shift by constructing the global representation, which aligns with the local features of the clients to preserve the semantics of different data domains. In addition, the dominance problem of client domains with a large number of samples is identified and, then, addressed with a domain-aware aggregation approach. This work is the first to consider the domain shift in training the semantic communication system for the image reconstruction task. Finally, simulation results demonstrate that the proposed approach outperforms the model-contrastive FL (MOON) framework by 0.5 for PSNR values under three domains at an SNR of 1 dB, and this gap continues to widen as the channel quality improves.",
        "authors": [
            "Loc X. Nguyen",
            "Ji Su Yoon",
            "Huy Q. Le",
            "Yu Qiao",
            "Avi Deb Raha",
            "Eui-Nam Huh",
            "Walid Saad",
            "Dusit Niyato",
            "Zhu Han",
            "Choong Seon Hong"
        ],
        "categories": [
            "cs.IT",
            "cs.DC",
            "cs.ET",
            "cs.IR"
        ],
        "id": "http://arxiv.org/abs/2512.00711v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-30"
    },
    "http://arxiv.org/abs/2512.00705v1": {
        "title": "FlexiWalker: Extensible GPU Framework for Efficient Dynamic Random Walks with Runtime Adaptation",
        "link": "http://arxiv.org/abs/2512.00705v1",
        "abstract": "Dynamic random walks are fundamental to various graph analysis applications, offering advantages by adapting to evolving graph properties. Their runtime-dependent transition probabilities break down the pre-computation strategy that underpins most existing CPU and GPU static random walk optimizations. This leaves practitioners suffering from suboptimal frameworks and having to write hand-tuned kernels that do not adapt to workload diversity. To handle this issue, we present FlexiWalker, the first GPU framework that delivers efficient, workload-generic support for dynamic random walks. Our design-space study shows that rejection sampling and reservoir sampling are more suitable than other sampling techniques under massive parallelism. Thus, we devise (i) new high-performance kernels for them that eliminate global reductions, redundant memory accesses, and random-number generation. Given the necessity of choosing the best-fitting sampling strategy at runtime, we adopt (ii) a lightweight first-order cost model that selects the faster kernel per node at runtime. To enhance usability, we introduce (iii) a compile-time component that automatically specializes user-supplied walk logic into optimized building blocks. On various dynamic random walk workloads with real-world graphs, FlexiWalker outperforms the best published CPU/GPU baselines by geometric means of 73.44x and 5.91x, respectively, while successfully executing workloads that prior systems cannot support. We open-source FlexiWalker in https://github.com/AIS-SNU/FlexiWalker.",
        "authors": [
            "Seongyeon Park",
            "Jaeyong Song",
            "Changmin Shin",
            "Sukjin Kim",
            "Junguk Hong",
            "Jinho Lee"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.00705v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-30"
    },
    "http://arxiv.org/abs/2512.00623v1": {
        "title": "Steady and Energy-Efficient Multi-Hop Clustering for Flying Ad-Hoc Networks (FANETs)",
        "link": "http://arxiv.org/abs/2512.00623v1",
        "abstract": "Flying Ad-hoc Networks (FANETs), formed by Unmanned Aerial Vehicles (UAVs), represent an emerging and promising communication paradigm. These networks face unique challenges due to UAVs high mobility, limited energy resources, and dynamic topology. In this work, we propose a novel multi-hop clustering algorithm aimed at creating stable, energy-efficient clusters in FANET environments. The proposed solution enhances cluster longevity and communication efficiency through mobility-aware clustering, energy-centric cluster head (CH) selection, and a ground station(GS)-assisted cluster maintenance management mechanism. First, steady multi-hop clusters are constructed, having CHs with not only high stability and high energy but also with steady and high-energy neighboring areas, and then a proper GS-assisted cluster maintenance mechanism is applied. Experimental results, based on extended simulations, demonstrate that our approach outperforms existing schemes significantly, in terms of cluster stability, communication overhead, and security resilience.",
        "authors": [
            "Basilis Mamalis",
            "Marios Perlitis"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.00623v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-29"
    },
    "http://arxiv.org/abs/2512.00595v1": {
        "id": "http://arxiv.org/abs/2512.00595v1",
        "title": "IslandRun: Privacy-Aware Multi-Objective Orchestration for Distributed AI Inference",
        "link": "http://arxiv.org/abs/2512.00595v1",
        "tags": [
            "serving",
            "edge",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-11-29",
        "tldr": "Addresses the challenge of orchestrating LLM inference across heterogeneous devices (edge, cloud) with conflicting goals of performance, privacy, and cost. Proposes IslandRun, a system using agent-based routing and reversible anonymization to route compute to data. Achieves improved privacy and multi-objective optimization.",
        "abstract": "Modern AI inference faces an irreducible tension: no single computational resource simultaneously maximizes performance, preserves privacy, minimizes cost, and maintains trust. Existing orchestration frameworks optimize single dimensions (Kubernetes prioritizes latency, federated learning preserves privacy, edge computing reduces network distance), creating solutions that struggle under real-world heterogeneity. We present IslandRun, a multi-objective orchestration system that treats computational resources as autonomous \"islands\" spanning personal devices, private edge servers, and public cloud. Our key insights: (1) request-level heterogeneity demands policy-constrained multi-objective optimization, (2) data locality enables routing compute to data rather than data to compute, and (3) typed placeholder sanitization preserves context semantics across trust boundaries. IslandRun introduces agent-based routing, tiered island groups with differential trust, and reversible anonymization. This establishes a new paradigm for privacy-aware, decentralized inference orchestration across heterogeneous personal computing ecosystems.",
        "authors": [
            "Bala Siva Sai Akhil Malepati"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.CR"
        ],
        "submit_date": "2025-11-29"
    },
    "http://arxiv.org/abs/2512.00401v1": {
        "title": "UNIQ: Communication-Efficient Distributed Quantum Computing via Unified Nonlinear Integer Programming",
        "link": "http://arxiv.org/abs/2512.00401v1",
        "abstract": "Distributed quantum computing (DQC) is widely regarded as a promising approach to overcome quantum hardware limitations. A major challenge in DQC lies in reducing the communication cost introduced by remote CNOT gates, which are significantly slower and more resource-consuming than local operations. Existing DQC approaches treat the three essential components (qubit allocation, entanglement management, and network scheduling) as independent stages, optimizing each in isolation. However, we observe that these components are inherently interdependent, and therefore adopting a unified optimization strategy can be more efficient to achieve the global optimal solutions. Consequently, we propose UNIQ, a novel DQC optimization framework that integrates all three components into a non-linear integer programming (NIP) model. UNIQ aims to reduce the circuit runtime by maximizing parallel Einstein-Podolsky-Rosen (EPR) pair generation through the use of idle communication qubits, while simultaneously minimizing the communication cost of remote gates. To solve this NP-hard formulated problem, we adopt two key strategies: a greedy algorithm for efficiently mapping logical qubits to different QPUs, and a JIT (Just-In-Time) approach that builds EPR pairs in parallel within each time slot. Extensive simulation results demonstrate that our approach is widely applicable to diverse quantum circuits and QPU topologies, while substantially reducing communication cost and runtime over existing methods.",
        "authors": [
            "Hui Zhong",
            "Jiachen Shen",
            "Lei Fan",
            "Xinyue Zhang",
            "Hao Wang",
            "Miao Pan",
            "Zhu Han"
        ],
        "categories": [
            "quant-ph",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.00401v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-29"
    },
    "http://arxiv.org/abs/2512.00398v1": {
        "id": "http://arxiv.org/abs/2512.00398v1",
        "title": "Heimdall++: Optimizing GPU Utilization and Pipeline Parallelism for Efficient Single-Pulse Detection",
        "link": "http://arxiv.org/abs/2512.00398v1",
        "tags": [
            "training",
            "offline",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes Heimdall++, an optimized GPU parallelization framework for radio astronomy single-pulse detection pipelines. Introduces fine-grained GPU parallelization, enhanced memory management, and multi-threading to decouple CPU/GPU stages, reducing GPU stalls. Achieves up to 2.66x speedup in single-file processing.",
        "abstract": "With the increasing time and frequency resolution of modern radio telescopes and the exponential growth in observational data volumes, real-time single-pulse detection has become a critical requirement for time-domain radio astronomy. Heimdall, as a representative GPU-accelerated single-pulse search tool, offers substantial performance advantages over CPU-based approaches. However, its sequential execution model and resource contention in intermediate processing stages limit GPU utilization, leading to suboptimal throughput and increased computational latency. To address these limitations, we present Heimdall++, an optimized successor to Heimdall that incorporates fine-grained GPU parallelization, enhanced memory management, and a multi-threaded framework to decouple CPU-bound and GPU-bound processing stages. This design mitigates the GPU stall problem and improves end-to-end efficiency. We evaluated Heimdall++ on a system equipped with NVIDIA RTX 3080 Ti GPUs using both a single large-scale observational file and multiple files. Experimental results demonstrate that Heimdall++ achieves up to 2.66x speedup in single-file processing and 2.05x speedup in multi-file batch processing, while maintaining full consistency with the original Heimdall's search results.",
        "authors": [
            "Bingzheng Xia",
            "Zujie Ren",
            "Kuang Ma",
            "Xiaoqian Li",
            "Wenda Li",
            "Shuibing He"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-11-29"
    },
    "http://arxiv.org/abs/2512.00298v1": {
        "title": "Challenges of Heterogeneity in Big Data: A Comparative Study of Classification in Large-Scale Structured and Unstructured Domains",
        "link": "http://arxiv.org/abs/2512.00298v1",
        "abstract": "This study analyzes the impact of heterogeneity (\"Variety\") in Big Data by comparing classification strategies across structured (Epsilon) and unstructured (Rest-Mex, IMDB) domains. A dual methodology was implemented: evolutionary and Bayesian hyperparameter optimization (Genetic Algorithms, Optuna) in Python for numerical data, and distributed processing in Apache Spark for massive textual corpora. The results reveal a \"complexity paradox\": in high-dimensional spaces, optimized linear models (SVM, Logistic Regression) outperformed deep architectures and Gradient Boosting. Conversely, in text-based domains, the constraints of distributed fine-tuning led to overfitting in complex models, whereas robust feature engineering -- specifically Transformer-based embeddings (ROBERTa) and Bayesian Target Encoding -- enabled simpler models to generalize effectively. This work provides a unified framework for algorithm selection based on data nature and infrastructure constraints.",
        "authors": [
            "González Trigueros Jesús Eduardo",
            "Alonso Sánchez Alejandro",
            "Muñoz Rivera Emilio",
            "Peñarán Prieto Mariana Jaqueline",
            "Mendoza González Camila Natalia"
        ],
        "categories": [
            "cs.LG",
            "cs.CL",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.00298v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-29"
    },
    "http://arxiv.org/abs/2512.05134v1": {
        "id": "http://arxiv.org/abs/2512.05134v1",
        "title": "InvarDiff: Cross-Scale Invariance Caching for Accelerated Diffusion Models",
        "link": "http://arxiv.org/abs/2512.05134v1",
        "tags": [
            "diffusion",
            "serving",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes Invariance Caching (InvarDiff), a technique to accelerate diffusion model inference by exploiting temporal and layer-wise feature invariance. Implements per-module binary cache plans with re-sampling correction. Achieves 2-3× end-to-end speedups with minimal fidelity impact on DiT and FLUX models.",
        "abstract": "Diffusion models deliver high-fidelity synthesis but remain slow due to iterative sampling. We empirically observe there exists feature invariance in deterministic sampling, and present InvarDiff, a training-free acceleration method that exploits the relative temporal invariance across timestep-scale and layer-scale. From a few deterministic runs, we compute a per-timestep, per-layer, per-module binary cache plan matrix and use a re-sampling correction to avoid drift when consecutive caches occur. Using quantile-based change metrics, this matrix specifies which module at which step is reused rather than recomputed. The same invariance criterion is applied at the step scale to enable cross-timestep caching, deciding whether an entire step can reuse cached results. During inference, InvarDiff performs step-first and layer-wise caching guided by this matrix. When applied to DiT and FLUX, our approach reduces redundant compute while preserving fidelity. Experiments show that InvarDiff achieves $2$-$3\\times$ end-to-end speed-ups with minimal impact on standard quality metrics. Qualitatively, we observe almost no degradation in visual quality compared with full computations.",
        "authors": [
            "Zihao Wu"
        ],
        "categories": [
            "cs.CV",
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-11-29"
    },
    "http://arxiv.org/abs/2512.00233v1": {
        "title": "A Parallel and Distributed Rust Library for Core Decomposition on Large Graphs",
        "link": "http://arxiv.org/abs/2512.00233v1",
        "abstract": "In this paper, we investigate the parallelization of $k$-core decomposition, a method used in graph analysis to identify cohesive substructures and assess node centrality. Although efficient sequential algorithms exist for this task, the scale of modern networks requires faster, multicore-ready approaches. To this end, we adapt a distributed $k$-core algorithm originally proposed by Montresor et al. to shared-memory systems and implement it in Rust, leveraging the language's strengths in concurrency and memory safety. We developed three progressively optimized versions: SequentialK as a baseline, ParallelK introducing multi-threaded message passing, and FastK further reducing synchronization overhead. Extensive experiments on real-world datasets, including road networks, web graphs, and social networks, show that FastK consistently outperforms both SequentialK and ParallelK, as well as a reference Python implementation available in the NetworkX library. Results indicate up to an 11x speedup on 16 threads and execution times up to two orders of magnitude faster than the Python implementation.",
        "authors": [
            "Davide Rucci",
            "Sebastian Parfeniuc",
            "Matteo Mordacchini",
            "Emanuele Carlini",
            "Alfredo Cuzzocrea",
            "Patrizio Dazzi"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.00233v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-28"
    },
    "http://arxiv.org/abs/2511.23440v1": {
        "title": "Accelerated Execution of Bayesian Neural Networks using a Single Probabilistic Forward Pass and Code Generation",
        "link": "http://arxiv.org/abs/2511.23440v1",
        "abstract": "Machine learning models perform well across domains such as diagnostics, weather forecasting, NLP, and autonomous driving, but their limited uncertainty handling restricts use in safety-critical settings. Traditional neural networks often fail to detect out-of-domain (OOD) data and may output confident yet incorrect predictions. Bayesian neural networks (BNNs) address this by providing probabilistic estimates, but incur high computational cost because predictions require sampling weight distributions and multiple forward passes. The Probabilistic Forward Pass (PFP) offers a highly efficient approximation to Stochastic Variational Inference (SVI) by assuming Gaussian-distributed weights and activations, enabling fully analytic uncertainty propagation and replacing sampling with a single deterministic forward pass. We present an end-to-end pipeline for training, compiling, optimizing, and deploying PFP-based BNNs on embedded ARM CPUs. Using the TVM deep learning compiler, we implement a dedicated library of Gaussian-propagating operators for multilayer perceptrons and convolutional neural networks, combined with manual and automated tuning strategies. Ablation studies show that PFP consistently outperforms SVI in computational efficiency, achieving speedups of up to 4200x for small mini-batches. PFP-BNNs match SVI-BNNs on Dirty-MNIST in accuracy, uncertainty estimation, and OOD detection while greatly reducing compute cost. These results highlight the potential of combining Bayesian approximations with code generation to enable efficient BNN deployment on resource-constrained systems.",
        "authors": [
            "Bernhard Klein",
            "Falk Selker",
            "Hendrik Borras",
            "Sophie Steger",
            "Franz Pernkopf",
            "Holger Fröning"
        ],
        "categories": [
            "cs.LG",
            "cs.AR",
            "cs.DC",
            "stat.ML"
        ],
        "id": "http://arxiv.org/abs/2511.23440v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-28"
    },
    "http://arxiv.org/abs/2511.23297v1": {
        "title": "Beyond 2-Edge-Connectivity: Algorithms and Impossibility for Content-Oblivious Leader Election",
        "link": "http://arxiv.org/abs/2511.23297v1",
        "abstract": "The content-oblivious model, introduced by Censor-Hillel, Cohen, Gelles, and Sel (PODC 2022; Distributed Computing 2023), captures an extremely weak form of communication where nodes can only send asynchronous, content-less pulses. Censor-Hillel, Cohen, Gelles, and Sel showed that no non-constant function $f(x,y)$ can be computed correctly by two parties using content-oblivious communication over a single edge, where one party holds $x$ and the other holds $y$. This seemingly ruled out many natural graph problems on non-2-edge-connected graphs.   In this work, we show that, with the knowledge of network topology $G$, leader election is possible in a wide range of graphs.   Impossibility: Graphs symmetric about an edge admit no randomized terminating leader election algorithm, even when nodes have unique identifiers and full knowledge of $G$.   Leader election algorithms: Trees that are not symmetric about any edge admit a quiescently terminating leader election algorithm with topology knowledge, even in anonymous networks, using $O(n^2)$ messages, where $n$ is the number of nodes. Moreover, even-diameter trees admit a terminating leader election given only the knowledge of the network diameter $D = 2r$, with message complexity $O(nr)$.   Necessity of topology knowledge: In the family of graphs $\\mathcal{G} = \\{P_3, P_5\\}$, both the 3-path $P_3$ and the 5-path $P_5$ admit a quiescently terminating leader election if nodes know the topology exactly. However, if nodes only know that the underlying topology belongs to $\\mathcal{G}$, then terminating leader election is impossible.",
        "authors": [
            "Yi-Jun Chang",
            "Lyuting Chen",
            "Haoran Zhou"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.23297v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-28"
    },
    "http://arxiv.org/abs/2511.23282v1": {
        "title": "Closing the Generalization Gap in Parameter-efficient Federated Edge Learning",
        "link": "http://arxiv.org/abs/2511.23282v1",
        "abstract": "Federated edge learning (FEEL) provides a promising foundation for edge artificial intelligence (AI) by enabling collaborative model training while preserving data privacy. However, limited and heterogeneous local datasets, as well as resource-constrained deployment, severely degrade both model generalization and resource utilization, leading to a compromised learning performance. Therefore, we propose a parameter-efficient FEEL framework that jointly leverages model pruning and client selection to tackle such challenges. First, we derive an information-theoretic generalization statement that characterizes the discrepancy between training and testing function losses and embed it into the convergence analysis. It reveals that a larger local generalization statement can undermine the global convergence. Then, we formulate a generalization-aware average squared gradient norm bound minimization problem, by jointly optimizing the pruning ratios, client selection, and communication-computation resources under energy and delay constraints. Despite its non-convexity, the resulting mixed-integer problem is efficiently solved via an alternating optimization algorithm. Extensive experiments demonstrate that the proposed design achieves superior learning performance than state-of-the-art baselines, validating the effectiveness of coupling generalization-aware analysis with system-level optimization for efficient FEEL.",
        "authors": [
            "Xinnong Du",
            "Zhonghao Lyu",
            "Xiaowen Cao",
            "Chunyang Wen",
            "Shuguang Cui",
            "Jie Xu"
        ],
        "categories": [
            "cs.LG",
            "cs.DC",
            "cs.IT"
        ],
        "id": "http://arxiv.org/abs/2511.23282v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-28"
    },
    "http://arxiv.org/abs/2511.23278v1": {
        "title": "RetryGuard: Preventing Self-Inflicted Retry Storms in Cloud Microservices Applications",
        "link": "http://arxiv.org/abs/2511.23278v1",
        "abstract": "Modern cloud applications are built on independent, diverse microservices, offering scalability, flexibility, and usage-based billing. However, the structural design of these varied services, along with their reliance on auto-scalers for dynamic internet traffic, introduces significant coordination challenges. As we demonstrate in this paper, common default retry patterns used between misaligned services can turn into retry storms which drive up resource usage and costs, leading to self-inflicted Denial-of-Wallet (DoW) scenarios. To overcome these problems we introduce RetryGuard, a distributed framework for productive control of retry patterns across interdependent microservices. By managing retry policy on a per-service basis and making parallel decisions, RetryGuard prevents retry storms, curbs resource contention, and mitigates escalating operational costs. RetryGuard makes its decisions based on an analytic model that captures the relationships among retries, throughput (rejections), delays, and costs. Experimental results show that RetryGuard significantly reduces resource usage and costs compared to AWS standard and advanced retry policies. We further demonstrate its scalability and superior performance in a more complex Kubernetes deployment with the Istio service mesh, where it achieves substantial improvements.",
        "authors": [
            "Jhonatan Tavori",
            "Anat Bremler-Barr",
            "Hanoch Levy",
            "Ofek Lavi"
        ],
        "categories": [
            "cs.NI",
            "cs.CR",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.23278v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-28"
    },
    "http://arxiv.org/abs/2512.16926v1": {
        "title": "Fixed-Priority and EDF Schedules for ROS2 Graphs on Uniprocessor",
        "link": "http://arxiv.org/abs/2512.16926v1",
        "abstract": "This paper addresses limitations of current scheduling methods in the Robot Operating System (ROS)2, focusing on scheduling tasks beyond simple chains and analyzing arbitrary Directed Acyclic Graphs (DAGs). While previous research has focused mostly on chain-based scheduling with ad-hoc response time analyses, we propose a novel approach using the events executor to implement fixed-job-level-priority schedulers for arbitrary ROS2 graphs on uniprocessor systems. We demonstrate that ROS 2 applications can be abstracted as forests of trees, enabling the mapping of ROS 2 applications to traditional real-time DAG task models. Our usage of the events executor requires a special implementation of the events queue and a communication middleware that supports LIFO-ordered message delivery, features not yet standard in ROS2. We show that our implementation generates the same schedules as a conventional fixed-priority DAG task scheduler, in spite of lacking access to the precedence information that usually is required. This further closes the gap between established real-time systems theory and ROS2 scheduling analyses.",
        "authors": [
            "Oren Bell",
            "Harun Teper",
            "Mario Günzel",
            "Chris Gill",
            "Jian-Jia Chen"
        ],
        "categories": [
            "cs.DC",
            "cs.OS",
            "cs.RO",
            "cs.SE"
        ],
        "id": "http://arxiv.org/abs/2512.16926v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-28"
    },
    "http://arxiv.org/abs/2511.23167v1": {
        "id": "http://arxiv.org/abs/2511.23167v1",
        "title": "Communication-Computation Pipeline Parallel Split Learning over Wireless Edge Networks",
        "link": "http://arxiv.org/abs/2511.23167v1",
        "tags": [
            "training",
            "offloading",
            "edge"
        ],
        "relevant": true,
        "indexed_date": "2025-11-28",
        "tldr": "Proposes a pipeline-parallel split learning method to overlap communication and computation in distributed training over wireless edge networks. Uses micro-batching and jointly optimizes task split and resource allocation. Reduces total training time by over 38% while maintaining accuracy.",
        "abstract": "Split learning (SL) offloads main computing tasks from multiple resource-constrained user equippments (UEs) to the base station (BS), while preserving local data privacy. However, its computation and communication processes remain sequential, resulting in limited system efficiency. To overcome this limitation, this paper applies pipeline parallelism (PP) of distributed training to SL in wireless networks, proposing the so-called communication-computation pipeline parallel split learning (C$^2$P$^2$SL). By considering the communicating and computing processes of UEs and BS as an overall pipeline, C$^2$P$^2$SL achieves pipeline parallelization among different micro-batches which are split from each batch of data samples. The overlap of communication and computation in this way significantly reduces the total training time. Given that training efficiency is affected by position of cutting layer and heterogeneity of the UEs, we formulate a joint optimization problem of task split and resource allocation, and design a solution based on alternating optimization. Experimental results demonstrate that C$^2$P$^2$SL significantly reduces system training time by over 38\\% while maintaining convergence accuracy under different communication conditions.",
        "authors": [
            "Chenyu Liu",
            "Zhaoyang Zhang",
            "Zirui Chen",
            "Zhaohui Yang"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-11-28"
    },
    "http://arxiv.org/abs/2511.23025v1": {
        "title": "Areon: Latency-Friendly and Resilient Multi-Proposer Consensus",
        "link": "http://arxiv.org/abs/2511.23025v1",
        "abstract": "We present Areon, a family of latency-friendly, stake-weighted, multi-proposer proof-of-stake consensus protocols. By allowing multiple proposers per slot and organizing blocks into a directed acyclic graph (DAG), Areon achieves robustness under partial synchrony. Blocks reference each other within a sliding window, forming maximal antichains that represent parallel ``votes'' on history. Conflicting subDAGs are resolved by a closest common ancestor (CCA)-local, window-filtered fork choice that compares the weight of each subDAG -- the number of recent short references -- and prefers the heavier one. Combined with a structural invariant we call Tip-Boundedness (TB), this yields a bounded-width frontier and allows honest work to aggregate quickly.   We formalize an idealized protocol (Areon-Ideal) that abstracts away network delay and reference bounds, and a practical protocol (Areon-Base) that adds VRF-based eligibility, bounded short and long references, and application-level validity and conflict checks at the block level. On top of DAG analogues of the classical common-prefix, chain-growth, and chain-quality properties, we prove a backbone-style $(k,\\varepsilon)$-finality theorem that calibrates confirmation depth as a function of the window length and target tail probability. We focus on consensus at the level of blocks; extending the framework to richer transaction selection, sampling, and redundancy policies is left to future work.   Finally, we build a discrete-event simulator and compare Areon-Base against a chain-based baseline (Ouroboros Praos) under matched block-arrival rates. Across a wide range of adversarial stakes and network delays, Areon-Base achieves bounded-latency finality with consistently lower reorganization frequency and depth.",
        "authors": [
            "Álvaro Castro-Castilla",
            "Marcin Pawlowski",
            "Hong-Sheng Zhou"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.23025v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-28"
    },
    "http://arxiv.org/abs/2511.22880v1": {
        "id": "http://arxiv.org/abs/2511.22880v1",
        "title": "Serving Heterogeneous LoRA Adapters in Distributed LLM Inference Systems",
        "link": "http://arxiv.org/abs/2511.22880v1",
        "tags": [
            "serving",
            "RL",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-11-28",
        "tldr": "Addresses performance skew from serving heterogeneous LoRA adapters. Proposes LoRAServe, a dynamic adapter placement and routing framework that rebalances adapters across GPUs using RDMA for remote access. Achieves up to 2× higher throughput and 50% fewer GPUs under SLOs compared to SOTA.",
        "abstract": "Low-Rank Adaptation (LoRA) has become the de facto method for parameter-efficient fine-tuning of large language models (LLMs), enabling rapid adaptation to diverse domains. In production, LoRA-based models are served at scale, creating multi-tenant environments with hundreds of adapters sharing a base model. However, state-of-the-art serving systems co-batch heterogeneous adapters without accounting for rank (size) variability, leading to severe performance skew, which ultimately requires adding more GPUs to satisfy service-level objectives (SLOs). Existing optimizations, focused on loading, caching, and kernel execution, ignore this heterogeneity, leaving GPU resources underutilized. We present LoRAServe, a workload-aware dynamic adapter placement and routing framework designed to tame rank diversity in LoRA serving. By dynamically rebalancing adapters across GPUs and leveraging GPU Direct RDMA for remote access, LoRAServe maximizes throughput and minimizes tail latency under real-world workload drift. Evaluations on production traces from Company X show that LoRAServe elicits up to 2$\\times$ higher throughput, up to 9$\\times$ lower TTFT, while using up to 50% fewer GPUs under SLO constraints compared to state-of-the-art systems.",
        "authors": [
            "Shashwat Jaiswal",
            "Shrikara Arun",
            "Anjaly Parayil",
            "Ankur Mallick",
            "Spyros Mastorakis",
            "Alind Khare",
            "Chloi Alverti",
            "Renee St Amant",
            "Chetan Bansal",
            "Victor Rühle",
            "Josep Torrellas"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.LG"
        ],
        "submit_date": "2025-11-28"
    },
    "http://arxiv.org/abs/2511.22779v1": {
        "title": "Accelerating mesh-based Monte Carlo simulations using contemporary graphics ray-tracing hardware",
        "link": "http://arxiv.org/abs/2511.22779v1",
        "abstract": "Significance: Monte Carlo (MC) methods are the gold-standard for modeling light-tissue interactions due to their accuracy. Mesh-based MC (MMC) offers enhanced precision for complex tissue structures using tetrahedral mesh models. Despite significant speedups achieved on graphics processing units (GPUs), MMC performance remains hindered by the computational cost of frequent ray-boundary intersection tests.   Aim: We propose a highly accelerated MMC algorithm, RT-MMC, that leverages the hardware-accelerated ray traversal and intersection capabilities of ray-tracing cores (RT-cores) on modern GPUs.   Approach: Implemented using NVIDIA's OptiX platform, RT-MMC extends graphics ray-tracing pipelines towards volumetric ray-tracing in turbid media, eliminating the need for challenging tetrahedral mesh generation while delivering significant speed improvements through hardware acceleration. It also intrinsically supports wide-field sources without complex mesh retesselation.   Results: RT-MMC demonstrates excellent agreement with traditional software-ray-tracing MMC algorithms while achieving 1.5x to 4.5x speedups across multiple GPU architectures. These performance gains significantly enhance the practicality of MMC for routine simulations.   Conclusion: Migration from software- to hardware-based ray-tracing not only greatly simplifies MMC simulation workflows, but also results in significant speedups that are expected to increase further as ray-tracing hardware rapidly gains adoption. Adoption of graphics ray-tracing pipelines in quantitative MMC simulations enables leveraging of emerging hardware resources and benefits a wide range of biophotonics applications.",
        "authors": [
            "Shijie Yan",
            "Douglas Dwyer",
            "David R. Kaeli",
            "Qianqian Fang"
        ],
        "categories": [
            "cs.DC",
            "physics.med-ph",
            "physics.optics"
        ],
        "id": "http://arxiv.org/abs/2511.22779v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-27"
    },
    "http://arxiv.org/abs/2511.22745v1": {
        "title": "A lasso-alternative to Dijkstra's algorithm for identifying short paths in networks",
        "link": "http://arxiv.org/abs/2511.22745v1",
        "abstract": "We revisit the problem of finding the shortest path between two selected vertices of a graph and formulate this as an $\\ell_1$-regularized regression -- Least Absolute Shrinkage and Selection Operator (lasso). We draw connections between a numerical implementation of this lasso-formulation, using the so-called LARS algorithm, and a more established algorithm known as the bi-directional Dijkstra. Appealing features of our formulation include the applicability of the Alternating Direction of Multiplier Method (ADMM) to the problem to identify short paths, and a relatively efficient update to topological changes.",
        "authors": [
            "Anqi Dong",
            "Amirhossein Taghvaei",
            "Tryphon T. Georgiou"
        ],
        "categories": [
            "math.OC",
            "cs.DC",
            "cs.SI",
            "eess.IV"
        ],
        "id": "http://arxiv.org/abs/2511.22745v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-27"
    },
    "http://arxiv.org/abs/2511.22616v1": {
        "title": "Federated Learning Survey: A Multi-Level Taxonomy of Aggregation Techniques, Experimental Insights, and Future Frontiers",
        "link": "http://arxiv.org/abs/2511.22616v1",
        "abstract": "The integration of IoT and AI has unlocked innovation across industries, but growing privacy concerns and data isolation hinder progress. Traditional centralized ML struggles to overcome these challenges, which has led to the rise of Federated Learning (FL), a decentralized paradigm that enables collaborative model training without sharing local raw data. FL ensures data privacy, reduces communication overhead, and supports scalability, yet its heterogeneity adds complexity compared to centralized approaches. This survey focuses on three main FL research directions: personalization, optimization, and robustness, offering a structured classification through a hybrid methodology that combines bibliometric analysis with systematic review to identify the most influential works. We examine challenges and techniques related to heterogeneity, efficiency, security, and privacy, and provide a comprehensive overview of aggregation strategies, including architectures, synchronization methods, and diverse federation objectives. To complement this, we discuss practical evaluation approaches and present experiments comparing aggregation methods under IID and non-IID data distributions. Finally, we outline promising research directions to advance FL, aiming to guide future innovation in this rapidly evolving field.",
        "authors": [
            "Meriem Arbaoui",
            "Mohamed-el-Amine Brahmia",
            "Abdellatif Rahmoun",
            "Mourad Zghal"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.22616v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-27"
    },
    "http://arxiv.org/abs/2511.22599v1": {
        "id": "http://arxiv.org/abs/2511.22599v1",
        "title": "DisCEdge: Distributed Context Management for Large Language Models at the Edge",
        "link": "http://arxiv.org/abs/2511.22599v1",
        "tags": [
            "serving",
            "edge",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-11-27",
        "tldr": "Proposes DisCEdge, a distributed context management system for edge LLM serving. It stores/replicates user context as token sequences instead of raw text to reduce synchronization overhead. Improves median response time by 14.46% and reduces client request size by 90%.",
        "abstract": "Deploying Large Language Model (LLM) services at the edge benefits latency-sensitive and privacy-aware applications. However, the stateless nature of LLMs makes managing user context (e.g., sessions, preferences) across geo-distributed edge nodes challenging. Existing solutions, such as client-side context storage, often introduce network latency and bandwidth overhead, undermining the advantages of edge deployment.   We propose DisCEdge, a distributed context management system that stores and replicates user context in tokenized form across edge nodes. By maintaining context as token sequences rather than raw text, our system avoids redundant computation and enables efficient data replication. We implement and evaluate an open-source prototype in a realistic edge environment with commodity hardware. We show DisCEdge improves median response times by up to 14.46% and lowers median inter-node synchronization overhead by up to 15% compared to a raw-text-based system. It also reduces client request sizes by a median of 90% compared to client-side context management, while guaranteeing data consistency.",
        "authors": [
            "Mohammadreza Malekabbasi",
            "Minghe Wang",
            "David Bermbach"
        ],
        "categories": [
            "cs.DC",
            "cs.DB",
            "cs.LG"
        ],
        "submit_date": "2025-11-27"
    },
    "http://arxiv.org/abs/2511.22481v1": {
        "id": "http://arxiv.org/abs/2511.22481v1",
        "title": "OmniInfer: System-Wide Acceleration Techniques for Optimizing LLM Serving Throughput and Latency",
        "link": "http://arxiv.org/abs/2511.22481v1",
        "tags": [
            "serving",
            "MoE",
            "sparse"
        ],
        "relevant": true,
        "indexed_date": "2025-11-27",
        "tldr": "Proposes OmniInfer, a unified LLM serving system that optimizes throughput and latency via Mixture-of-Experts scheduling and sparse attention acceleration. Built on vLLM, it reduces TPOT by 36% and TTFT by 38% in cluster evaluations.",
        "abstract": "Large Language Models drive a wide range of modern AI applications but impose substantial challenges on large-scale serving systems due to intensive computation, strict latency constraints, and throughput bottlenecks. We introduce OmniInfer, a unified system-level acceleration framework designed to maximize end-to-end serving efficiency through fine-grained optimization of expert placement, cache compression, and scheduling. OmniInfer integrates three complementary components: OmniPlacement for load-aware Mixture-of-Experts scheduling, OmniAttn for sparse attention acceleration, and OmniProxy for disaggregation-aware request scheduling. Built atop vLLM, OmniInfer delivers system-wide performance gains through adaptive resource disaggregation, efficient sparsity exploitation, and global coordination across prefill and decode phases. Evaluated on DeepSeek-R1 within a 10-node Ascend 910C cluster, OmniInfer achieves 616 QPM, where the unified framework reduces TPOT by 36\\%, and the superimposition of OmniProxy further slashes TTFT by 38\\%. The project is open-sourced at [this https URL](https://gitee.com/omniai/omniinfer).",
        "authors": [
            "Jun Wang",
            "Yunxiang Yao",
            "Wenwei Kuang",
            "Runze Mao",
            "Zhenhao Sun",
            "Zhuang Tao",
            "Ziyang Zhang",
            "Dengyu Li",
            "Jiajun Chen",
            "Zhili Wang",
            "Kai Cui",
            "Congzhi Cai",
            "Longwen Lan",
            "Ken Zhang"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-11-27"
    },
    "http://arxiv.org/abs/2512.00110v1": {
        "title": "Quantum-Adversary-Resilient Evidence Structures and Migration Strategies for Regulated AI Audit Trails",
        "link": "http://arxiv.org/abs/2512.00110v1",
        "abstract": "Constant-size cryptographic evidence records are increasingly used to build audit trails for regulated AI workloads in clinical, pharmaceutical, and financial settings, where each execution is summarized by a compact, verifiable record of code identity, model version, data digests, and platform measurements. Existing instantiations, however, typically rely on classical signature schemes whose long-term security is threatened by quantum-capable adversaries. In this paper we formalize security notions for evidence structures in the presence of quantum adversaries and study post-quantum (PQ) instantiations and migration strategies for deployed audit logs. We recall an abstraction of constant-size evidence structures and introduce game-based definitions of Q-Audit Integrity, Q-Non-Equivocation, and Q-Binding, capturing the inability of a quantum adversary to forge, equivocate, or rebind evidence items. We then analyze a hash-and-sign instantiation in the quantum random-oracle model (QROM), assuming an existentially unforgeable PQ signature scheme against quantum adversaries, and show that the resulting evidence structure satisfies these notions under standard assumptions. Building on this, we present three migration patterns for existing evidence logs: hybrid signatures, re-signing of legacy evidence, and Merkle-root anchoring, and analyze their security, storage, and computational trade-offs. A case study based on an industrial constant-size evidence platform for regulated AI at Codebat Technologies Inc. suggests that quantum-safe audit trails are achievable with moderate overhead and that systematic migration can significantly extend the evidentiary lifetime of existing deployments.",
        "authors": [
            "Leo Kao"
        ],
        "categories": [
            "cs.CR",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.00110v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-27"
    },
    "http://arxiv.org/abs/2511.22380v1": {
        "title": "Optimality of Simultaneous Consensus with Limited Information Exchange (Extended Abstract)",
        "link": "http://arxiv.org/abs/2511.22380v1",
        "abstract": "Work on the development of optimal fault-tolerant Agreement protocols using the logic of knowledge has concentrated on the \"full information\" approach to information exchange, which is costly with respect to message size. Alpturer, Halpern, and van der Meyden (PODC 2023) introduced the notion of optimality with respect to a limited information exchange, and studied the Eventual Agreement problem in the sending omissions failure model. The present paper studies the Simultaneous Agreement problem for the crash failures model, and a number of limited information exchanges from the literature. In particular, the paper considers information exchanges from a FloodSet protocol (Lynch, Distributed Algorithms 1996), a variant of this in which agents also count the number of failures (Castañeda et al, NETYS 2017), and a variant in which agents associate each agent with a value (Raynal, PRDC 2002). A new information exchange is also introduced that enables decisions to be made at worst one round later than the optimal protocol of Dwork and Moses (I&C 88), but with lower computation cost and space requirements. By determining implementations of a knowledge based program, protocols are derived that are optimal amongst protocols for each of these information exchanges.",
        "authors": [
            "Kaya Alpturer",
            "Ron van der Meyden",
            "Sushmita Ruj",
            "Godfrey Wong"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.22380v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-27"
    },
    "http://arxiv.org/abs/2511.22333v2": {
        "id": "http://arxiv.org/abs/2511.22333v2",
        "title": "PAT: Accelerating LLM Decoding via Prefix-Aware Attention with Resource Efficient Multi-Tile Kernel",
        "link": "http://arxiv.org/abs/2511.22333v2",
        "tags": [
            "kernel",
            "serving",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-11-27",
        "tldr": "Addresses memory-bound decode attention in LLM serving by exploiting shared prefixes. Proposes PAT: a prefix-aware attention kernel using pack-forward-merge execution with multi-tile design. Reduces attention latency by 53.5% on average and TPOT by up to 93.1% versus SOTA.",
        "abstract": "LLM serving is increasingly dominated by decode attention, which is a memory-bound operation due to massive KV cache loading from global memory. Meanwhile, real-world workloads exhibit substantial, hierarchical shared prefixes across requests (e.g., system prompts, tools/templates, RAG). Existing attention implementations fail to fully exploit prefix sharing: one-query-per-CTA execution repeatedly loads shared prefix KV cache, while one-size-fits-all tiling leaves on-chip resources idle and exacerbates bubbles for uneven KV lengths. These choices amplify memory bandwidth pressure and stall memory-bound decode attention.   This paper introduces PAT, a prefix-aware attention kernel implementation for LLM decoding that organizes execution with a pack-forward-merge paradigm. PAT packs queries by shared prefix to reduce repeated memory accesses, runs a customized multi-tile kernel to achieve high resource efficiency. It further applies practical multi-stream forwarding and KV splitting to reduce resource bubbles. The final merge performs online softmax with negligible overhead. We implement PAT as an off-the-shelf plugin for vLLM. Evaluation on both real-world and synthetic workloads shows that PAT reduces attention latency by 53.5% on average and TPOT by 17.0-93.1% under the same configurations against state-of-the-art attention kernels. PAT's source code is publicly available at https://github.com/flashserve/PAT.",
        "authors": [
            "Jinjun Yi",
            "Zhixin Zhao",
            "Yitao Hu",
            "Ke Yan",
            "Weiwei Sun",
            "Hao Wang",
            "Laiping Zhao",
            "Yuhao Zhang",
            "Wenxin Li",
            "Keqiu Li"
        ],
        "categories": [
            "cs.DC",
            "cs.CL"
        ],
        "submit_date": "2025-11-27"
    },
    "http://arxiv.org/abs/2511.22302v1": {
        "title": "When AI Bends Metal: AI-Assisted Optimization of Design Parameters in Sheet Metal Forming",
        "link": "http://arxiv.org/abs/2511.22302v1",
        "abstract": "Numerical simulations have revolutionized the industrial design process by reducing prototyping costs, design iterations, and enabling product engineers to explore the design space more efficiently. However, the growing scale of simulations demands substantial expert knowledge, computational resources, and time. A key challenge is identifying input parameters that yield optimal results, as iterative simulations are costly and can have a large environmental impact. This paper presents an AI-assisted workflow that reduces expert involvement in parameter optimization through the use of Bayesian optimization. Furthermore, we present an active learning variant of the approach, assisting the expert if desired. A deep learning model provides an initial parameter estimate, from which the optimization cycle iteratively refines the design until a termination condition (e.g., energy budget or iteration limit) is met. We demonstrate our approach, based on a sheet metal forming process, and show how it enables us to accelerate the exploration of the design space while reducing the need for expert involvement.",
        "authors": [
            "Ahmad Tarraf",
            "Koutaiba Kassem-Manthey",
            "Seyed Ali Mohammadi",
            "Philipp Martin",
            "Lukas Moj",
            "Semih Burak",
            "Enju Park",
            "Christian Terboven",
            "Felix Wolf"
        ],
        "categories": [
            "cs.AI",
            "cs.DC",
            "cs.PF"
        ],
        "id": "http://arxiv.org/abs/2511.22302v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-27"
    },
    "http://arxiv.org/abs/2511.22259v1": {
        "title": "Silence Speaks Volumes: A New Paradigm for Covert Communication via History Timing Patterns",
        "link": "http://arxiv.org/abs/2511.22259v1",
        "abstract": "A Covert Channel (CC) exploits legitimate communication mechanisms to stealthily transmit information, often bypassing traditional security controls. Among these, a novel paradigm called History Covert Channels (HCC) leverages past network events as reference points to embed covert messages. Unlike traditional timing- or storage-based CCs, which directly manipulate traffic patterns or packet contents, HCCs minimize detectability by encoding information through small pointers to historical data. This approach enables them to amplify the size of transmitted covert data by referring to more bits than are actually embedded. Recent research has explored the feasibility of such methods, demonstrating their potential to evade detection by repurposing naturally occurring network behaviors as a covert transmission medium.   This paper introduces a novel method for establishing and maintaining covert communication links using relative pointers to network timing patterns, which minimizes the reliance of the HCC on centralized timekeeping and reduces the likelihood of being detected by standard network monitoring tools. We also explore the tailoring of HCCs to optimize their robustness and undetectability characteristics. Our experiments reveal a better bitrate compared to previous work.",
        "authors": [
            "Christoph Weissenborn",
            "Steffen Wendzel"
        ],
        "categories": [
            "cs.CR",
            "cs.DC",
            "cs.NI"
        ],
        "id": "http://arxiv.org/abs/2511.22259v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-27"
    },
    "http://arxiv.org/abs/2511.22080v1": {
        "title": "A Fast and Flat Federated Learning Method via Weighted Momentum and Sharpness-Aware Minimization",
        "link": "http://arxiv.org/abs/2511.22080v1",
        "abstract": "In federated learning (FL), models must \\emph{converge quickly} under tight communication budgets while \\emph{generalizing} across non-IID client distributions. These twin requirements have naturally led to two widely used techniques: client/server \\emph{momentum} to accelerate progress, and \\emph{sharpness-aware minimization} (SAM) to prefer flat solutions. However, simply combining momentum and SAM leaves two structural issues unresolved in non-IID FL. We identify and formalize two failure modes: \\emph{local-global curvature misalignment} (local SAM directions need not reflect the global loss geometry) and \\emph{momentum-echo oscillation} (late-stage instability caused by accumulated momentum). To our knowledge, these failure modes have not been jointly articulated and addressed in the FL literature. We propose \\textbf{FedWMSAM} to address both failure modes. First, we construct a momentum-guided global perturbation from server-aggregated momentum to align clients' SAM directions with the global descent geometry, enabling a \\emph{single-backprop} SAM approximation that preserves efficiency. Second, we couple momentum and SAM via a cosine-similarity adaptive rule, yielding an early-momentum, late-SAM two-phase training schedule. We provide a non-IID convergence bound that \\emph{explicitly models the perturbation-induced variance} $σ_ρ^2=σ^2+(Lρ)^2$ and its dependence on $(S, K, R, N)$ on the theory side. We conduct extensive experiments on multiple datasets and model architectures, and the results validate the effectiveness, adaptability, and robustness of our method, demonstrating its superiority in addressing the optimization challenges of Federated Learning. Our code is available at https://github.com/Huang-Yongzhi/NeurlPS_FedWMSAM.",
        "authors": [
            "Tianle Li",
            "Yongzhi Huang",
            "Linshan Jiang",
            "Chang Liu",
            "Qipeng Xie",
            "Wenfeng Du",
            "Lu Wang",
            "Kaishun Wu"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.22080v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-27"
    },
    "http://arxiv.org/abs/2512.17913v1": {
        "title": "Byzantine Fault-Tolerant Multi-Agent System for Healthcare: A Gossip Protocol Approach to Secure Medical Message Propagation",
        "link": "http://arxiv.org/abs/2512.17913v1",
        "abstract": "Recent advances in generative AI have enabled sophisticated multi-agent architectures for healthcare, where large language models power collaborative clinical decision-making. However, these distributed systems face critical challenges in ensuring message integrity and fault tolerance when operating in adversarial or untrusted environments.This paper presents a novel Byzantine fault-tolerant multi-agent system specifically designed for healthcare applications, integrating gossip-based message propagation with cryptographic validation mechanisms. Our system employs specialized AI agents for diagnosis, treatment planning, emergency response, and data analysis, coordinated through a Byzantine consensus protocol that tolerates up to f faulty nodes among n = 3f + 1 total nodes. We implement a gossip protocol for decentralized message dissemination, achieving consensus with 2f + 1 votes while maintaining system operation even under Byzantine failures. Experimental results demonstrate that our approach successfully validates medical messages with cryptographic signatures, prevents replay attacks through timestamp validation, and maintains consensus accuracy of 100% with up to 33% Byzantine nodes. The system provides real-time visualization of consensus rounds, vote tallies, and network topology, enabling transparent monitoring of fault-tolerant operations. This work contributes a practical framework for building secure, resilient healthcare multi-agent systems capable of collaborative medical decision-making in untrusted environments.",
        "authors": [
            "Nihir Chadderwala"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "id": "http://arxiv.org/abs/2512.17913v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-27"
    },
    "http://arxiv.org/abs/2511.22010v2": {
        "title": "An Empirical Study of Cross-Language Interoperability in Replicated Data Systems",
        "link": "http://arxiv.org/abs/2511.22010v2",
        "abstract": "BACKGROUND: Modern distributed systems replicate data across multiple execution sites. Business requirements and resource constraints often necessitate mixing different languages across replica sites. To facilitate the management of replicated data, modern software engineering practices integrate special-purpose replicated data libraries (RDLs) that provide read-write access to the data and ensure its synchronization. Irrespective of the implementation languages, an RDL typically uses a single language or offers bindings to a designated one. Hence, integrating existing RDLs in multilingual environments requires special-purpose code, whose software quality and performance characteristics are poorly understood.   AIMS: We aim to bridge this knowledge gap to understand the software quality and performance characteristics of RDL integration in multilingual environments.   METHOD: We conduct an empirical study of two key strategies for integrating RDLs in the context of multilingual replicated data systems: foreign-function interface (FFI) and a common data format (CDF); we measure and compare their respective software metrics and performance to understand their suitability for the task at hand.   RESULTS: Our results reveal that adopting CDF for cross-language interaction offers software quality, latency, memory consumption, and throughput advantages. We further validate our findings by (1) creating a CDF-based RDL for mixing compiled, interpreted, and managed languages; and (2) enhancing our RDL with plug-in extensibility that enables adding functionality in a single language while maintaining integration within a multilingual environment.   CONCLUSIONS: With modern distributed systems utilizing multiple languages, our findings provide novel insights for designing RDLs in multilingual replicated data systems.",
        "authors": [
            "Provakar Mondal",
            "Eli Tilevich"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.22010v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-27"
    },
    "http://arxiv.org/abs/2511.21969v1": {
        "title": "ZipperChain: Transmuting Trusted Third-Party Services Into Trustless Atomic Broadcast",
        "link": "http://arxiv.org/abs/2511.21969v1",
        "abstract": "Distributed ledger technologies (DLTs) rely on distributed consensus mechanisms to reach agreement over the order of transactions and to provide immutability and availability of transaction data. Distributed consensus suffers from performance limitations of network communication between participating nodes. BLOCKY ZipperChain guarantees immutability, agreement, and availability of transaction data, but without relying on distributed consensus. Instead, its construction process transfers trust from widely-used, third-party services onto ZipperChains's correctness guarantees. ZipperChain blocks are built by a pipeline of specialized services deployed on a small number of nodes connected by a fast data center network. As a result, ZipperChain transaction throughput approaches network line speeds and block finality is on the order of 500 ms. Finally, ZipperChain infrastructure creates blocks centrally and so does not need a native token to incentivize a community of verifiers.",
        "authors": [
            "Matteo Bjornsson",
            "Taylor Hardin",
            "Taylor Heinecke",
            "Marcin Furtak",
            "David L. Millman",
            "Mike P. Wittie"
        ],
        "categories": [
            "cs.DC",
            "cs.NI"
        ],
        "id": "http://arxiv.org/abs/2511.21969v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-26"
    },
    "http://arxiv.org/abs/2511.21958v2": {
        "title": "Clock2Q+: A Simple and Efficient Replacement Algorithm for Metadata Cache in VMware vSAN",
        "link": "http://arxiv.org/abs/2511.21958v2",
        "abstract": "Cache replacement algorithms are critical building blocks of storage systems. This paper examines the characteristics of metadata caches and argues that they inherently exhibit correlated references, even when the corresponding data accesses do not contain correlated references. The presence of correlated references reduces the effectiveness of cache replacement algorithms because these references are often mistakenly categorized as hot blocks. Clock2Q+ is specifically designed for metadata caches and has been implemented in vSAN and VDFS, two flagship storage products of VMware by Broadcom. Similar to S3-FIFO, Clock2Q+ uses three queues; however, Clock2Q+ introduces a correlation window in the Small FIFO queue, where blocks in this window do not set the reference bit. This simple enhancement allows Clock2Q+ to outperform state-of-the-art replacement algorithms. Compared to S3-FIFO, the second-best performing algorithm, Clock2Q+ achieves up to a 28.5% lower miss ratio on metadata traces. Clock2Q+ possesses the essential properties required for large-scale storage systems: it has low CPU overhead on cache hits, low memory overhead, scales efficiently to multiple CPUs, and is both easy to tune and implement. Additionally, Clock2Q+ outperforms state-of-the-art cache replacement algorithms on data traces as well.",
        "authors": [
            "Yiyan Zhai",
            "Bintang Dwi Marthen",
            "Sarath Balivada",
            "Vamsi Sudhakar Bojji",
            "Eric Knauft",
            "Jitender Rohilla",
            "Jiaqi Zuo",
            "Quanxing Liu",
            "Maxime Austruy",
            "Wenguang Wang",
            "Juncheng Yang"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.21958v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-26"
    },
    "http://arxiv.org/abs/2511.21862v1": {
        "id": "http://arxiv.org/abs/2511.21862v1",
        "title": "OOCO: Latency-disaggregated Architecture for Online-Offline Co-locate LLM Serving",
        "link": "http://arxiv.org/abs/2511.21862v1",
        "tags": [
            "serving",
            "offline",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Addresses load imbalance in co-located online-offline LLM serving. Proposes latency-disaggregated architecture with separate pools, bottleneck-based scheduler, and preemption mechanism. Achieves 3x higher offline throughput while maintaining online SLOs.",
        "abstract": "Large Language Models (LLMs) are increasingly deployed in both latency-sensitive online services and cost-sensitive offline workloads. Co-locating these workloads on shared serving instances can improve resource utilization, but directly applying this approach to Prefill/Decode (P/D) disaggregated systems introduces severe load imbalance, as fluctuating request mixes alter the intrinsic P/D ratio. Existing dynamic adjustment techniques cannot keep up with the bursty traffic patterns of online services.   We propose a latency-constraint disaggregated architecture, which separates cluster resources into latency-strict and latency-relaxed pools based on task latency requirements. This design enables flexible placement of offline decode tasks, mitigating P/D imbalance while preserving online performance. To fully exploit this flexibility, we propose (1) a bottleneck-based scheduler guided by a Roofline-based performance model for performance bottleneck based scheduling, and (2) a fast preemption mechanism that strictly enforces Service Level Objectives (SLOs) for online requests.   Experiments on real-world traces show that compared to existing offline system approaches, our method improves offline throughput by up to 3x, while maintaining online request SLOs.",
        "authors": [
            "Siyu Wu",
            "Zihan Tang",
            "Yuting Zeng",
            "Hui Chen",
            "Guiguang Ding",
            "Tongxuan Liu",
            "Ke Zhang",
            "Hailong Yang"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-11-26"
    },
    "http://arxiv.org/abs/2511.21859v1": {
        "title": "Equivalence and Separation between Heard-Of and Asynchronous Message-Passing Models",
        "link": "http://arxiv.org/abs/2511.21859v1",
        "abstract": "We revisit the relationship between two fundamental models of distributed computation: the asynchronous message-passing model with up to $f$ crash failures ($\\operatorname{AMP}_f$) and the Heard-Of model with up to $f$ message omissions ($\\operatorname{HO}_f$). We show that for $n > 2f$, the two models are equivalent with respect to the solvability of colorless tasks, and that for colored tasks the equivalence holds only when $f = 1$ (and $n > 2$). The separation for larger $f$ arises from the presence of silenced processes in $\\operatorname{HO}_f$, which may lead to incompatible decisions. The proofs proceed through bidirectional simulations between $\\operatorname{AMP}_f$ and $\\operatorname{HO}_f$ via an intermediate model that captures this notion of silencing. The results extend to randomized protocols against a non-adaptive adversary, indicating that the expressive limits of canonical rounds are structural rather than probabilistic. Together, these results delineate precisely where round-based abstractions capture asynchronous computation, and where they do not.",
        "authors": [
            "Hagit Attiya",
            "Armando Castañeda",
            "Dhrubajyoti Ghosh",
            "Thomas Nowak"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.21859v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-26"
    },
    "http://arxiv.org/abs/2511.21844v1": {
        "id": "http://arxiv.org/abs/2511.21844v1",
        "title": "A Sustainable and Reward Incentivized High-Performance Cluster Computing for Artificial Intelligence: A Novel Bayesian-Time-Decay Trust Mechanism in Blockchain",
        "link": "http://arxiv.org/abs/2511.21844v1",
        "tags": [
            "offloading",
            "hardware",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes a blockchain-based incentive mechanism for distributed AI cluster computing. Integrates dynamic trust ratings and a statistical draw system to efficiently utilize diverse hardware. Achieves up to 30% improvement in resource utilization while enabling broader participation.",
        "abstract": "In an age where sustainability is of paramount importance, the significance of both high-performance computing and intelligent algorithms cannot be understated. Yet, these domains often demand hefty computational power, translating to substantial energy usage and potentially sidelining less robust computing systems. It's evident that we need an approach that is more encompassing, scalable, and eco-friendly for intelligent algorithm development and implementation. The strategy we present in this paper offers a compelling answer to these issues. We unveil a fresh framework that seamlessly melds high-performance cluster computing with intelligent algorithms, all within a blockchain infrastructure. This promotes both efficiency and a broad-based participation. At its core, our design integrates an evolved proof-of-work consensus process, which links computational efforts directly to rewards for producing blocks. This ensures both optimal resource use and participation from a wide spectrum of computational capacities. Additionally, our approach incorporates a dynamic 'trust rating' that evolves based on a track record of accurate block validations. This rating determines the likelihood of a node being chosen for block generation, creating a merit-based system that recognizes and rewards genuine and precise contributions. To level the playing field further, we suggest a statistical 'draw' system, allowing even less powerful nodes a chance to be part of the block creation process.",
        "authors": [
            "Murat Yaslioglu"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-11-26"
    },
    "http://arxiv.org/abs/2511.21669v2": {
        "id": "http://arxiv.org/abs/2511.21669v2",
        "title": "DSD: A Distributed Speculative Decoding Solution for Edge-Cloud Agile Large Model Serving",
        "link": "http://arxiv.org/abs/2511.21669v2",
        "tags": [
            "serving",
            "offloading",
            "edge"
        ],
        "relevant": true,
        "indexed_date": "2025-11-26",
        "tldr": "Proposes DSD, a distributed speculative decoding framework for LLM serving across edge-cloud environments, featuring a simulator (DSD-Sim) and adaptive window control. Achieves 1.1x speedup and 9.7% higher throughput over baselines.",
        "abstract": "Large language model (LLM) inference often suffers from high decoding latency and limited scalability across heterogeneous edge-cloud environments. Existing speculative decoding (SD) techniques accelerate token generation but remain confined to single-node execution. We propose DSD, a distributed speculative decoding framework that extends SD to multi-device deployments through coordinated draft-target execution. Given the lack of prior work on simulating this paradigm, we first introduce DSD-Sim, a discrete-event simulator that captures network, batching, and scheduling dynamics. Building on insights from DSD-Sim, we further design an Adaptive Window Control (AWC) policy that dynamically adjusts speculation window size to optimize throughput. Experiments across diverse workloads show that DSD achieves up to 1.1x speedup and 9.7% higher throughput over existing SD baselines, enabling agile and scalable LLM serving across edge and cloud.",
        "authors": [
            "Fengze Yu",
            "Leshu Li",
            "Brad McDanel",
            "Sai Qian Zhang"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "submit_date": "2025-11-26"
    },
    "http://arxiv.org/abs/2511.21661v1": {
        "title": "AI/ML Model Cards in Edge AI Cyberinfrastructure: towards Agentic AI",
        "link": "http://arxiv.org/abs/2511.21661v1",
        "abstract": "AI/ML model cards can contain a benchmarked evaluation of an AI/ML model against intended use but a one time assessment during model training does not get at how and where a model is actually used over its lifetime. Through Patra Model Cards embedded in the ICICLE AI Institute software ecosystem we study model cards as dynamic objects. The study reported here assesses the benefits and tradeoffs of adopting the Model Context Protocol (MCP) as an interface to the Patra Model Card server. Quantitative assessment shows the overhead of MCP as compared to a REST interface. The core question however is of active sessions enabled by MCP; this is a qualitative question of fit and use in the context of dynamic model cards that we address as well.",
        "authors": [
            "Beth Plale",
            "Neelesh Karthikeyan",
            "Isuru Gamage",
            "Joe Stubbs",
            "Sachith Withana"
        ],
        "categories": [
            "cs.DC",
            "cs.DB"
        ],
        "id": "http://arxiv.org/abs/2511.21661v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-26"
    },
    "http://arxiv.org/abs/2511.21612v1": {
        "title": "Diagonal Scaling: A Multi-Dimensional Resource Model and Optimization Framework for Distributed Databases",
        "link": "http://arxiv.org/abs/2511.21612v1",
        "abstract": "Modern cloud databases present scaling as a binary decision: scale-out by adding nodes or scale-up by increasing per-node resources. This one-dimensional view is limiting because database performance, cost, and coordination overhead emerge from the joint interaction of horizontal elasticity and per-node CPU, memory, network bandwidth, and storage IOPS. As a result, systems often overreact to load spikes, underreact to memory pressure, or oscillate between suboptimal states. We introduce the Scaling Plane, a two-dimensional model in which each distributed database configuration is represented as a point (H, V), with H denoting node count and V a vector of resources. Over this plane, we define smooth approximations of latency, throughput, coordination overhead, and monetary cost, providing a unified view of performance trade-offs. We show analytically and empirically that optimal scaling trajectories frequently lie along diagonal paths: sequences of joint horizontal and vertical adjustments that simultaneously exploit cluster parallelism and per-node improvements. To compute such actions, we propose DIAGONALSCALE, a discrete local-search algorithm that evaluates horizontal, vertical, and diagonal moves in the Scaling Plane and selects the configuration minimizing a multi-objective function subject to SLA constraints. Using synthetic surfaces, microbenchmarks, and experiments on distributed SQL and KV systems, we demonstrate that diagonal scaling reduces p95 latency by up to 40 percent, lowers cost-per-query by up to 37 percent, and reduces rebalancing by 2 to 5 times compared to horizontal-only and vertical-only autoscaling. Our results highlight the need for multi-dimensional scaling models and provide a foundation for next-generation autoscaling in cloud database systems.",
        "authors": [
            "Shahir Abdullah",
            "Syed Rohit Zaman"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.21612v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-26"
    },
    "http://arxiv.org/abs/2511.21552v1": {
        "title": "MAD-DAG: Protecting Blockchain Consensus from MEV",
        "link": "http://arxiv.org/abs/2511.21552v1",
        "abstract": "Blockchain security is threatened by selfish mining, where a miner (operator) deviates from the protocol to increase their revenue. Selfish mining is exacerbated by adverse conditions: rushing (network propagation advantage for the selfish miner), varying block rewards due to block contents, called miner extractable value (MEV), and petty-compliant miners who accept bribes from the selfish miner.   The state-of-the-art selfish-mining-resistant blockchain protocol, Colordag, does not treat these adverse conditions and was proven secure only when its latency is impractically high.   We present MAD-DAG, Mutually-Assured-Destruction Directed-Acyclic-Graph, the first practical protocol to counter selfish mining under adverse conditions. MAD-DAG achieves this thanks to its novel ledger function, which discards the contents of equal-length chains competing to be the longest.   We analyze selfish mining in both Colordag and MAD-DAG by modeling a rational miner using a Markov Decision Process (MDP). We obtain a tractable model for both by developing conservative reward rules that favor the selfish miner to yield an upper bound on selfish mining revenue. To the best of our knowledge, this is the first tractable model of selfish mining in a practical DAG-based blockchain. This enables us to obtain a lower bound on the security threshold, the minimum fraction of computational power a miner needs in order to profit from selfish mining.   MAD-DAG withstands adverse conditions under which Colordag and Bitcoin fail, while otherwise maintaining comparable security. For example, with petty-compliant miners and high levels of block reward variability, MAD-DAG's security threshold ranges from 11% to 31%, whereas both Colordag and Bitcoin achieve 0% for all levels.",
        "authors": [
            "Roi Bar-Zur",
            "Aviv Tamar",
            "Ittay Eyal"
        ],
        "categories": [
            "cs.CR",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.21552v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-26"
    },
    "http://arxiv.org/abs/2511.21535v1": {
        "title": "Modeling the Effect of Data Redundancy on Speedup in MLFMA Near-Field Computation",
        "link": "http://arxiv.org/abs/2511.21535v1",
        "abstract": "The near-field (P2P) operator in the Multilevel Fast Multipole Algorithm (MLFMA) is a performance bottleneck on GPUs due to poor memory locality. This work introduces data redundancy to improve spatial locality by reducing memory access dispersion. For validation of results, we propose an analytical model based on a Locality metric that combines data volume and access dispersion to predict speedup trends without hardware-specific profiling. The approach is validated on two MLFMA-based applications: an electromagnetic solver (DBIM-MLFMA) with regular structure, and a stellar dynamics code (PhotoNs-2.0) with irregular particle distribution. Results show up to 7X kernel speedup due to improved cache behavior. However, increased data volume raises overheads in data restructuring, limiting end-to-end application speedup to 1.04X. While the model cannot precisely predict absolute speedups, it reliably captures performance trends across different problem sizes and densities. The technique is injectable into existing implementations with minimal code changes. This work demonstrates that data redundancy can enhance GPU performance for P2P operator, provided locality gains outweigh data movement costs.",
        "authors": [
            "Morteza Sadeghi"
        ],
        "categories": [
            "cs.DC",
            "cs.PF"
        ],
        "id": "http://arxiv.org/abs/2511.21535v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-26"
    },
    "http://arxiv.org/abs/2511.21431v1": {
        "id": "http://arxiv.org/abs/2511.21431v1",
        "title": "MemFine: Memory-Aware Fine-Grained Scheduling for MoE Training",
        "link": "http://arxiv.org/abs/2511.21431v1",
        "tags": [
            "training",
            "MoE",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-11-26",
        "tldr": "Proposes MemFine, a memory-aware scheduling framework to solve the memory bottleneck in MoE training caused by dynamic token routing. It uses chunk-level decomposition and optimized recomputation guided by a memory model. Reduces activation memory by 48.03% and increases throughput by 4.42% versus full recomputation.",
        "abstract": "The training of large-scale Mixture of Experts (MoE) models faces a critical memory bottleneck due to severe load imbalance caused by dynamic token routing. This imbalance leads to memory overflow on GPUs with limited capacity, constraining model scalability. Existing load balancing methods, which cap expert capacity, compromise model accuracy and fail on memory-constrained hardware. To address this, we propose MemFine, a memory-aware fine-grained scheduling framework for MoE training. MemFine decomposes the token distribution and expert computation into manageable chunks and employs a chunked recomputation strategy, dynamically optimized through a theoretical memory model to balance memory efficiency and throughput. Experiments demonstrate that MemFine reduces activation memory by 48.03% and improves throughput by 4.42% compared to full recomputation-based baselines, enabling stable large-scale MoE training on memory-limited GPUs.",
        "authors": [
            "Lu Zhao",
            "Rong Shi",
            "Shaoqing Zhang",
            "Yueqiang Chen",
            "Baoguo He",
            "Hongfeng Sun",
            "Ziqing Yin",
            "Shangchao Su",
            "Zhiyan Cui",
            "Liang Dong",
            "Xiyuan Li",
            "Lingbin Wang",
            "Jianwei He",
            "Jiesong Ma",
            "Weikang Huang",
            "Jianglei Tong",
            "Dongdong Gao",
            "Jian Zhang",
            "Hong Tian"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-11-26"
    },
    "http://arxiv.org/abs/2511.21413v1": {
        "id": "http://arxiv.org/abs/2511.21413v1",
        "title": "Automated Dynamic AI Inference Scaling on HPC-Infrastructure: Integrating Kubernetes, Slurm and vLLM",
        "link": "http://arxiv.org/abs/2511.21413v1",
        "tags": [
            "serving",
            "edge"
        ],
        "relevant": true,
        "indexed_date": "2025-11-26",
        "tldr": "Proposes a system to dynamically scale LLM inference on HPC infrastructure by integrating vLLM, Slurm, and Kubernetes to handle synchronous user-facing workloads. Achieves efficient scaling for up to 1000 concurrent requests with only ~500 ms end-to-end latency overhead.",
        "abstract": "Due to rising demands for Artificial Inteligence (AI) inference, especially in higher education, novel solutions utilising existing infrastructure are emerging. The utilisation of High-Performance Computing (HPC) has become a prevalent approach for the implementation of such solutions. However, the classical operating model of HPC does not adapt well to the requirements of synchronous, user-facing dynamic AI application workloads. In this paper, we propose our solution that serves LLMs by integrating vLLM, Slurm and Kubernetes on the supercomputer \\textit{RAMSES}. The initial benchmark indicates that the proposed architecture scales efficiently for 100, 500 and 1000 concurrent requests, incurring only an overhead of approximately 500 ms in terms of end-to-end latency.",
        "authors": [
            "Tim Trappen",
            "Robert Keßler",
            "Roland Pabel",
            "Viktor Achter",
            "Stefan Wesner"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.DB",
            "cs.PF"
        ],
        "submit_date": "2025-11-26"
    },
    "http://arxiv.org/abs/2511.21181v1": {
        "title": "Privacy in Federated Learning with Spiking Neural Networks",
        "link": "http://arxiv.org/abs/2511.21181v1",
        "abstract": "Spiking neural networks (SNNs) have emerged as prominent candidates for embedded and edge AI. Their inherent low power consumption makes them far more efficient than conventional ANNs in scenarios where energy budgets are tightly constrained. In parallel, federated learning (FL) has become the prevailing training paradigm in such settings, enabling on-device learning while limiting the exposure of raw data. However, gradient inversion attacks represent a critical privacy threat in FL, where sensitive training data can be reconstructed directly from shared gradients. While this vulnerability has been widely investigated in conventional ANNs, its implications for SNNs remain largely unexplored. In this work, we present the first comprehensive empirical study of gradient leakage in SNNs across diverse data domains. SNNs are inherently non-differentiable and are typically trained using surrogate gradients, which we hypothesized would be less correlated with the original input and thus less informative from a privacy perspective. To investigate this, we adapt different gradient leakage attacks to the spike domain. Our experiments reveal a striking contrast with conventional ANNs: whereas ANN gradients reliably expose salient input content, SNN gradients yield noisy, temporally inconsistent reconstructions that fail to recover meaningful spatial or temporal structure. These results indicate that the combination of event-driven dynamics and surrogate-gradient training substantially reduces gradient informativeness. To the best of our knowledge, this work provides the first systematic benchmark of gradient inversion attacks for spiking architectures, highlighting the inherent privacy-preserving potential of neuromorphic computation.",
        "authors": [
            "Dogukan Aksu",
            "Jesus Martinez del Rincon",
            "Ihsen Alouani"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.21181v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-26"
    },
    "http://arxiv.org/abs/2512.07853v1": {
        "id": "http://arxiv.org/abs/2512.07853v1",
        "title": "GPU Memory Prediction for Multimodal Model Training",
        "link": "http://arxiv.org/abs/2512.07853v1",
        "tags": [
            "training"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Introduces a framework for predicting peak GPU memory usage during multimodal model training to prevent OOM errors. It decomposes models into layers and applies factorization for memory estimation. Achieves ~8.7% MAPE accuracy.",
        "abstract": "As deep learning models in agentic AI systems grow in scale and complexity, GPU memory requirements increase and often exceed the available GPU memory capacity, so that out-of-memory (OoM) errors occur. It is well known that OoM interrupts the whole training itself and wastes substantial computational resources. Therefore, to prevent OoM, accurate prediction of GPU memory usage is essential. However, previous studies focus only on unimodal architectures and fail to generalize to multimodal models, even though the multimodal models are a common choice in agentic AI systems. To address this limitation, we propose a framework that predicts the peak GPU memory usage by analyzing the model architecture and training behavior of multimodal models. Specifically, the framework decomposes the multimodal model into its constituent layers and applies factorization to estimate the memory usage of each layer. Our evaluation shows that our framework achieves high prediction accuracy of ~8.7% average MAPE.",
        "authors": [
            "Jinwoo Jeong",
            "Minchul Kang",
            "Younghun Go",
            "Changyong Shin",
            "Hyunho Lee",
            "Junho Yoon",
            "Gyeongsik Yang",
            "Chuck Yoo"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC"
        ],
        "submit_date": "2025-11-26"
    },
    "http://arxiv.org/abs/2512.00083v1": {
        "id": "http://arxiv.org/abs/2512.00083v1",
        "title": "LLaMCAT: Optimizing Large Language Model Inference with Cache Arbitration and Throttling",
        "link": "http://arxiv.org/abs/2512.00083v1",
        "tags": [
            "serving",
            "offloading",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes LLaMCAT for reducing KV cache stalls in LLM inference via LLC optimization. Combines MSHR-aware cache arbitration, load balancing, and thread throttling. Achieves up to 1.58x speedup over unoptimized baseline under limited cache scenarios.",
        "abstract": "Large Language Models (LLMs) have achieved unprecedented success across various applications, but their substantial memory requirements pose significant challenges to current memory system designs, especially during inference. Our work targets last-level cache (LLC) based architectures, including GPUs (e.g., NVIDIA GPUs) and AI accelerators. We introduce LLaMCAT, a novel approach to optimize the LLC for LLM inference. LLaMCAT combines Miss Status Holding Register (MSHR)- and load balance-aware cache arbitration with thread throttling to address stringent bandwidth demands and minimize cache stalls in KV Cache access. We also propose a hybrid simulation framework integrating analytical models with cycle-level simulators via memory traces, balancing architecture detail and efficiency.   Experiments demonstrate that LLaMCAT achieves an average speedup of 1.26x when the system is mainly bottlenecked by miss handling throughput, while baselines mostly show negative improvements since they are not optimized for this scenario. When the cache size is also limited, our policy achieves a speedup of 1.58x over the unoptimized version, and a 1.26x improvement over the best baseline (dyncta). Overall, LLaMCAT is the first to target LLM decoding-specific MSHR contention, a gap in previous work. It presents a practical solution for accelerating LLM inference on future hardware platforms.",
        "authors": [
            "Zhongchun Zhou",
            "Chengtao Lai",
            "Wei Zhang"
        ],
        "categories": [
            "cs.AR",
            "cs.DC"
        ],
        "submit_date": "2025-11-26"
    },
    "http://arxiv.org/abs/2511.21018v1": {
        "id": "http://arxiv.org/abs/2511.21018v1",
        "title": "Handling of Memory Page Faults during Virtual-Address RDMA",
        "link": "http://arxiv.org/abs/2511.21018v1",
        "tags": [
            "networking",
            "hardware"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes a hardware-software mechanism for handling memory page faults during RDMA communication. Integrates fault detection via ARM SMMU and resolution through retransmission, modifying Linux drivers and DMA engine. Reduces overhead and improves efficiency compared to pinning.",
        "abstract": "Nowadays, avoiding system calls during cluster communication (e.g., in Data Centers and High Performance Computing) in modern high-speed interconnection networks has become a necessity, due to the high overhead of multiple data copies between kernel and user space. User-level zero-copy Remote Direct Memory Access (RDMA) technologies address this problem by improving performance and reducing system energy consumption. However, traditional RDMA engines cannot tolerate page faults and therefore use various techniques to avoid them.   State-of-the-art RDMA approaches typically rely on pinning address spaces or multiple pages per application. This method introduces long-term disadvantages due to increased programming complexity (pinning and unpinning buffers), limits on how much memory can be pinned, and inefficient memory utilization. In addition, pinning does not fully prevent page faults because modern operating systems apply internal optimization mechanisms, such as Transparent Huge Pages (THP), which are enabled by default in Linux.   This thesis implements a page-fault handling mechanism integrated with the DMA engine of the ExaNeSt project. Faults are detected by the ARM System Memory Management Unit (SMMU) and resolved through a hardware-software solution that can request retransmission when needed. This mechanism required modifications to the Linux SMMU driver, the development of a new software library, changes to the DMA engine hardware, and adjustments to the DMA scheduling logic. Experiments were conducted on the Quad-FPGA Daughter Board (QFDB) of ExaNeSt, which uses Xilinx Zynq UltraScale+ MPSoCs.   Finally, we evaluate our mechanism and compare it against alternatives such as pinning and pre-faulting, and discuss the advantages of our approach.",
        "authors": [
            "Antonis Psistakis"
        ],
        "categories": [
            "cs.DC",
            "cs.AR"
        ],
        "submit_date": "2025-11-26"
    },
    "http://arxiv.org/abs/2512.17910v1": {
        "id": "http://arxiv.org/abs/2512.17910v1",
        "title": "Efficient Multi-Adapter LLM Serving via Cross-Model KV-Cache Reuse with Activated LoRA",
        "link": "http://arxiv.org/abs/2512.17910v1",
        "tags": [
            "serving",
            "RAG",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Addresses inefficiency in multi-adapter LLM serving pipelines via Activated LoRA for cross-model KV-cache reuse. Proposes base-aligned block hashing and activation-aware masking in vLLM. Achieves up to 58x latency reduction and 100x TTFT improvement over standard LoRA baselines.",
        "abstract": "Modern large language model (LLM) systems increasingly rely on multi-turn pipelines that are composed of multiple task-specific adapters, yet existing serving frameworks remain inefficient, incurring substantial recomputation overhead when switching between adapters. We present the first LLM serving engine that supports cross-model prefix cache reuse between base and adapted models via Activated LoRA (aLoRA), enabling efficient and fine-grained adapter switching during inference. Our design extends the vLLM framework by introducing base-aligned block hashing and activation-aware masking within the model execution path, permitting cache reuse across models while preserving compatibility with existing serving engine optimizations. Integrated into a production-grade inference stack, this approach supports dynamic adapter activation without excessive key-value tensor recomputation. Evaluation across representative multi-turn, multi-adapter pipelines demonstrates up to 58x end-to-end latency reduction and over 100x time-to-first-token improvement relative to standard LoRA baselines, with benefits that scale with model size and sequence length and manifest across all stages of the request lifecycle. This work bridges parameter-efficient model adaptation with high-performance serving, providing the first complete realization of cross-model KV-cache reuse in modern LLM inference engines.",
        "authors": [
            "Allison Li",
            "Kristjan Greenewald",
            "Thomas Parnell",
            "Navid Azizan"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.LG"
        ],
        "submit_date": "2025-11-26"
    },
    "http://arxiv.org/abs/2511.20982v2": {
        "id": "http://arxiv.org/abs/2511.20982v2",
        "title": "DOPO: A Dynamic PD-Disaggregation Architecture for Maximizing Goodput in LLM Inference Serving",
        "link": "http://arxiv.org/abs/2511.20982v2",
        "tags": [
            "serving",
            "disaggregation",
            "autoscaling"
        ],
        "relevant": true,
        "indexed_date": "2025-11-26",
        "tldr": "Proposes DOPD, a dynamic LLM inference system that adjusts prefill/decoding instance allocation based on real-time load for optimal P/D ratio. Mitigates workload imbalance and improves goodput by up to 1.5× vs vLLM/DistServe, while reducing TTFT and TPOT significantly.",
        "abstract": "To meet strict Service-Level Objectives (SLOs),contemporary Large Language Models (LLMs) decouple the prefill and decoding stages and place them on separate GPUs to mitigate the distinct bottlenecks inherent to each phase. However, the heterogeneity of LLM workloads causes producerconsumer imbalance between the two instance types in such disaggregated architecture. To address this problem, we propose DOPD (Dynamic Optimal Prefill/Decoding), a dynamic LLM inference system that adjusts instance allocations to achieve an optimal prefill-to-decoding (P/D) ratio based on real-time load monitoring. Combined with an appropriate request-scheduling policy, DOPD effectively resolves imbalances between prefill and decoding instances and mitigates resource allocation mismatches due to mixed-length requests under high concurrency. Experimental evaluations show that, compared with vLLM and DistServe (representative aggregation-based and disaggregationbased approaches), DOPD improves overall system goodput by up to 1.5X, decreases P90 time-to-first-token (TTFT) by up to 67.5%, and decreases P90 time-per-output-token (TPOT) by up to 22.8%. Furthermore, our dynamic P/D adjustment technique performs proactive reconfiguration based on historical load, achieving over 99% SLOs attainment while using less additional resources.",
        "authors": [
            "Junhan Liao",
            "Minxian Xu",
            "Wanyi Zheng",
            "Yan Wang",
            "Kejiang Ye",
            "Rajkumar Buyya",
            "Chengzhong Xu"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-11-26"
    },
    "http://arxiv.org/abs/2511.20975v2": {
        "id": "http://arxiv.org/abs/2511.20975v2",
        "title": "Aragog: Just-in-Time Model Routing for Scalable Serving of Agentic Workflows",
        "link": "http://arxiv.org/abs/2511.20975v2",
        "tags": [
            "agentic",
            "serving",
            "autoscaling"
        ],
        "relevant": true,
        "indexed_date": "2025-11-26",
        "tldr": "Addresses runtime inefficiency in serving agentic workflows via dynamic model routing. Proposes a two-stage approach with offline configuration pruning and a just-in-time scheduler using live system metrics. Achieves 50-217% higher throughput and 32.5-78.9% lower latency at peak loads.",
        "abstract": "Agentic workflows have emerged as a powerful paradigm for solving complex, multi-stage tasks, but serving them at scale is computationally expensive given the many LLM inferences that each request must pass through. Configuration selection, or the cost-aware assignment of workflow agents to specific LLMs, can reduce these costs, but existing approaches bind configuration decisions before request execution, making them ill-suited for the heterogeneous and lengthy execution of workflows. Specifically, system loads can fluctuate rapidly and substantially during a request's lifetime, causing fixed configurations to quickly become suboptimal. We present Aragog, a system that progressively adapts a request's configuration throughout its execution to match runtime dynamics. To make this practical despite the massive space of workflow configurations, Aragog decouples the problem into two core elements -- a one-time routing step that identifies all accuracy-preserving configurations, and a cheap per-stage scheduler that selects among them using up-to-date system observations -- and introduces novel strategies to accelerate each. Across diverse workflows and model families, Aragog increases maximum serving throughput by 50.0--217.0\\% and reduces median latency by 32.5--78.9\\% at peak request rates, while maintaining accuracy comparable to the most expensive configurations.",
        "authors": [
            "Yinwei Dai",
            "Zhuofu Chen",
            "Anand Iyer",
            "Ravi Netravali"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-11-26"
    },
    "http://arxiv.org/abs/2511.20922v2": {
        "title": "Readout-Side Bypass for Residual Hybrid Quantum-Classical Models",
        "link": "http://arxiv.org/abs/2511.20922v2",
        "abstract": "Quantum machine learning (QML) promises compact and expressive representations, but suffers from the measurement bottleneck - a narrow quantum-to-classical readout that limits performance and amplifies privacy risk. We propose a lightweight residual hybrid architecture that concatenates quantum features with raw inputs before classification, bypassing the bottleneck without increasing quantum complexity. Experiments show our model outperforms pure quantum and prior hybrid models in both centralized and federated settings. It achieves up to +55% accuracy improvement over quantum baselines, while retaining low communication cost and enhanced privacy robustness. Ablation studies confirm the effectiveness of the residual connection at the quantum-classical interface. Our method offers a practical, near-term pathway for integrating quantum models into privacy-sensitive, resource-constrained settings like federated edge learning.",
        "authors": [
            "Guilin Zhang",
            "Wulan Guo",
            "Ziqi Tan",
            "Hongyang He",
            "Qiang Guan",
            "Hailong Jiang"
        ],
        "categories": [
            "cs.CR",
            "cs.DC",
            "cs.LG"
        ],
        "id": "http://arxiv.org/abs/2511.20922v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-25"
    },
    "http://arxiv.org/abs/2511.20834v1": {
        "title": "Accelerating Sparse Convolutions in Voxel-Based Point Cloud Networks",
        "link": "http://arxiv.org/abs/2511.20834v1",
        "abstract": "Sparse Convolution (SpC) powers 3D point cloud networks widely used in autonomous driving and AR/VR. SpC builds a kernel map that stores mappings between input voxel coordinates, output coordinates, and weight offsets, then uses this map to compute feature vectors for output coordinates. Our work identifies three key properties of voxel coordinates: they are integer-valued, bounded within a limited spatial range, and geometrically continuous-neighboring voxels on the same object surface are highly likely to exist at small spatial offsets from each other. Prior SpC engines do not fully exploit these properties and suffer from high pre-processing and post-processing overheads during kernel map construction. To address this, we design Spira, the first voxel-property-aware SpC engine for GPUs. Spira proposes: (i) a high-performance one-shot search algorithm that builds the kernel map with no preprocessing and high memory locality, (ii) an effective packed-native processing scheme that accesses packed voxel coordinates at low cost, (iii) a flexible dual-dataflow execution mechanism that efficiently computes output feature vectors by adapting to layer characteristics, and (iv) a network-wide parallelization strategy that builds kernel maps for all SpC layers concurrently at network start. Our evaluation shows that Spira significantly outperforms prior SpC engines by 1.71x on average and up to 2.31x for end-to-end inference, and by 2.13x on average and up to 3.32x for layer-wise execution across diverse layer configurations.",
        "authors": [
            "Dionysios Adamopoulos",
            "Anastasia Poulopoulou",
            "Georgios Goumas",
            "Christina Giannoula"
        ],
        "categories": [
            "cs.DC",
            "cs.AR",
            "cs.LG",
            "cs.PF"
        ],
        "id": "http://arxiv.org/abs/2511.20834v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-25"
    },
    "http://arxiv.org/abs/2511.20780v1": {
        "title": "Assessing Redundancy Strategies to Improve Availability in Virtualized System Architectures",
        "link": "http://arxiv.org/abs/2511.20780v1",
        "abstract": "Cloud-based storage platforms are becoming more common in both academic and business settings due to their flexible access to data and support for collaborative functionalities. As reliability becomes a vital requirement, particularly for organizations looking for alternatives to public cloud services, assessing the dependability of these systems is crucial. This paper presents a methodology for analyzing the availability of a file server (Nextcloud) hosted in a private cloud environment using Apache CloudStack. The analysis is based on a modeling approach through Stochastic Petri Nets (SPNs) that allows the evaluation of different redundancy strategies to enhance the availability of such systems. Four architectural configurations were modeled, including the baseline, host-level redundancy, virtual machine (VM) redundancy, and a combination of both. The results show that redundancy at both the host and VM levels significantly improves availability and reduces expected downtime. The proposed approach provides a method to evaluate the availability of a private cloud and support infrastructure design decisions.",
        "authors": [
            "Alison Silva",
            "Gustavo Callou"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.20780v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-25"
    },
    "http://arxiv.org/abs/2511.20511v2": {
        "title": "Efficient Parallel Implementation of the Pilot Assignment Problem in Massive MIMO Systems",
        "link": "http://arxiv.org/abs/2511.20511v2",
        "abstract": "The assignment of the pilot sequence is a critical challenge in massive MIMO systems, as sharing the same pilot sequence among multiple users causes interference, which degrades the accuracy of the channel estimation. This problem, equivalent to the NP-hard graph coloring problem, directly impacts real-time applications such as autonomous driving and industrial IoT, where minimizing channel estimation time is crucial. This paper proposes an optimized hybrid K-means clustering and Genetic Algorithm (SK-means GA) to improve the pilot assignment efficiency, achieving a 29.3% reduction in convergence time (82s vs. 116s for conventional GA). A parallel implementation (PK-means GA) is developed on an FPGA using Vivado High-Level Synthesis Tools (HLST) to further enhance the run-time performance, accelerating convergence to 3.5 milliseconds. Within Vivado implementation, different optimization techniques such as loop unrolling, pipelining, and function inlining are applied to realize the reported speedup. This significant improvement of PK-means GA in execution speed makes it highly suitable for low-latency real-time wireless networks (6G)",
        "authors": [
            "Eman Alqudah",
            "Ashfaq Khokhar"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.20511v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-25"
    },
    "http://arxiv.org/abs/2511.20391v1": {
        "title": "Interactive Visualization of Proof-of-Work Consensus Protocol on Raspberry Pi",
        "link": "http://arxiv.org/abs/2511.20391v1",
        "abstract": "We describe a prototype of a fully capable Ethereum Proof-of-Work (PoW) blockchain network running on multiple Raspberry Pi (RPi) computers. The prototype is easy to set up and is intended to function as a completely standalone system, using a local WiFi router for connectivity. It features LCD screens for visualization of the local state of blockchain ledgers on each RPi, making it ideal for educational purposes and to demonstrate fundamental blockchain concepts to a wide audience. For example, a functioning PoW consensus is easily visible from the LCD screens, as well as consensus degradation which might arise from various factors, including peer-to-peer topology and communication latency - all parameters which can be configured from the central web-based interface.",
        "authors": [
            "Anton Ivashkevich",
            "Matija Piškorec",
            "Claudio J. Tessone"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.20391v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-25"
    },
    "http://arxiv.org/abs/2512.03054v1": {
        "title": "Energy-Efficient Federated Learning via Adaptive Encoder Freezing for MRI-to-CT Conversion: A Green AI-Guided Research",
        "link": "http://arxiv.org/abs/2512.03054v1",
        "abstract": "Federated Learning (FL) holds the potential to advance equality in health by enabling diverse institutions to collaboratively train deep learning (DL) models, even with limited data. However, the significant resource requirements of FL often exclude centres with limited computational infrastructure, further widening existing healthcare disparities. To address this issue, we propose a Green AI-oriented adaptive layer-freezing strategy designed to reduce energy consumption and computational load while maintaining model performance. We tested our approach using different federated architectures for Magnetic Resonance Imaging (MRI)-to-Computed Tomography (CT) conversion. The proposed adaptive strategy optimises the federated training by selectively freezing the encoder weights based on the monitored relative difference of the encoder weights from round to round. A patience-based mechanism ensures that freezing only occurs when updates remain consistently minimal. The energy consumption and CO2eq emissions of the federation were tracked using the CodeCarbon library. Compared to equivalent non-frozen counterparts, our approach reduced training time, total energy consumption and CO2eq emissions by up to 23%. At the same time, the MRI-to-CT conversion performance was maintained, with only small variations in the Mean Absolute Error (MAE). Notably, for three out of the five evaluated architectures, no statistically significant differences were observed, while two architectures exhibited statistically significant improvements. Our work aligns with a research paradigm that promotes DL-based frameworks meeting clinical requirements while ensuring climatic, social, and economic sustainability. It lays the groundwork for novel FL evaluation frameworks, advancing privacy, equity and, more broadly, justice in AI-driven healthcare.",
        "authors": [
            "Ciro Benito Raggio",
            "Lucia Migliorelli",
            "Nils Skupien",
            "Mathias Krohmer Zabaleta",
            "Oliver Blanck",
            "Francesco Cicone",
            "Giuseppe Lucio Cascini",
            "Paolo Zaffino",
            "Maria Francesca Spadea"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "cs.DC",
            "physics.med-ph"
        ],
        "id": "http://arxiv.org/abs/2512.03054v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-25"
    },
    "http://arxiv.org/abs/2511.20172v2": {
        "id": "http://arxiv.org/abs/2511.20172v2",
        "title": "Beluga: A CXL-Based Memory Architecture for Scalable and Efficient LLM KVCache Management",
        "link": "http://arxiv.org/abs/2511.20172v2",
        "tags": [
            "offloading",
            "kernel",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-11-25",
        "tldr": "Proposes Beluga-KVCache, a CXL-based architecture enabling GPU-native access to shared memory for efficient KV cache management in LLM inference. Achieves 89.6% TTFT reduction and 7.35× throughput improvement over RDMA solutions in vLLM.",
        "abstract": "The rapid increase in LLM model sizes and the growing demand for long-context inference have made memory a critical bottleneck in GPU-accelerated serving systems. Although high-bandwidth memory (HBM) on GPUs offers fast access, its limited capacity necessitates reliance on host memory (CPU DRAM) to support larger working sets such as the KVCache. However, the maximum DRAM capacity is constrained by the limited number of memory channels per CPU socket. To overcome this limitation, current systems often adopt RDMA-based disaggregated memory pools, which introduce significant challenges including high access latency, complex communication protocols, and synchronization overhead. Fortunately, the emerging CXL technology introduces new opportunities in KVCache design. In this paper, we propose Beluga, a novel memory architecture that enables GPUs and CPUs to access a shared, large-scale memory pool through CXL switches. By supporting native load/store access semantics over the CXL fabric, our design delivers near-local memory latency, while reducing programming complexity and minimizing synchronization overhead. We conduct a systematic characterization of a commercial CXL switch-based memory pool and propose a set of design guidelines. Based on Beluga, we design and implement Beluga-KVCache, a system tailored for managing the large-scale KVCache in LLM inference. Beluga-KVCache achieves an 89.6% reduction in Time-To-First-Token (TTFT) and 7.35x throughput improvement in the vLLM inference engine compared to RDMA-based solutions. To the best of our knowledge, Beluga is the first system that enables GPUs to directly access large-scale memory pools through CXL switches, marking a significant step toward low-latency, shared access to vast memory resources by GPUs.",
        "authors": [
            "Xinjun Yang",
            "Qingda Hu",
            "Junru Li",
            "Feifei Li",
            "Yicong Zhu",
            "Yuqi Zhou",
            "Qiuru Lin",
            "Jian Dai",
            "Yang Kong",
            "Jiayu Zhang",
            "Guoqiang Xu",
            "Qiang Liu"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-11-25"
    },
    "http://arxiv.org/abs/2511.20142v1": {
        "title": "Parallel simulation and adaptive mesh refinement for 3D elastostatic contact mechanics problems between deformable bodies",
        "link": "http://arxiv.org/abs/2511.20142v1",
        "abstract": "Parallel implementation of numerical adaptive mesh refinement (AMR)strategies for solving 3D elastostatic contact mechanics problems is an essential step toward complex simulations that exceed current performance levels. This paper introduces a scalable, robust, and efficient algorithm to deal with 2D and 3D elastostatics contact problems between deformable bodies in a finite element framework. The proposed solution combines a treatment of the contact problem by a node-to-node pairing algorithm with a penalization technique and a non-conforming h-adaptive refinement of quadrilateral/hexahedral meshes based on an estimate-mark-refine approach in a parallel framework. One of the special features of our parallel strategy is that contact paired nodes are hosted by the same MPI tasks, which reduces the number of exchanges between processes for building the contact operator. The mesh partitioning introduced in this paper respects this rule and is based on an equidistribution of elements over processes, without any other constraints. In order to preserve the domain curvature while hierarchical mesh refinement, super-parametric elements are used. This functionality enables the contact zone to be well detected during the AMR process, even for an initial coarse mesh and low-order discretization schemes. The efficiency of our contact-AMR-HPC strategy is assessed on 2D and 3D Hertzian contact problems. Different AMR detection criteria are considered. Various convergence analyses are conducted. Parallel performances up to 1024 cores are illustrated. Furthermore, memory footprint and preconditionners performance are analyzed.",
        "authors": [
            "Alexandre Epalle",
            "Isabelle Ramière",
            "Guillaume Latu",
            "Frédéric Lebon"
        ],
        "categories": [
            "math.NA",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.20142v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-25"
    },
    "http://arxiv.org/abs/2511.20100v1": {
        "id": "http://arxiv.org/abs/2511.20100v1",
        "title": "QiMeng-Kernel: Macro-Thinking Micro-Coding Paradigm for LLM-Based High-Performance GPU Kernel Generation",
        "link": "http://arxiv.org/abs/2511.20100v1",
        "tags": [
            "training",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-11-25",
        "tldr": "Proposes Macro Thinking Micro Coding (MTMC), a hierarchical LLM framework for generating high-performance GPU kernels. Decouples optimization strategy (reinforcement learning) from implementation (LLM coding). Achieves up to 7.3x speedup over LLMs and 2.2x over expert-optimized PyTorch kernels.",
        "abstract": "Developing high-performance GPU kernels is critical for AI and scientific computing, but remains challenging due to its reliance on expert crafting and poor portability. While LLMs offer promise for automation, both general-purpose and finetuned LLMs suffer from two fundamental and conflicting limitations: correctness and efficiency. The key reason is that existing LLM-based approaches directly generate the entire optimized low-level programs, requiring exploration of an extremely vast space encompassing both optimization policies and implementation codes. To address the challenge of exploring an intractable space, we propose Macro Thinking Micro Coding (MTMC), a hierarchical framework inspired by the staged optimization strategy of human experts. It decouples optimization strategy from implementation details, ensuring efficiency through high-level strategy and correctness through low-level implementation. Specifically, Macro Thinking employs reinforcement learning to guide lightweight LLMs in efficiently exploring and learning semantic optimization strategies that maximize hardware utilization. Micro Coding leverages general-purpose LLMs to incrementally implement the stepwise optimization proposals from Macro Thinking, avoiding full-kernel generation errors. Together, they effectively navigate the vast optimization space and intricate implementation details, enabling LLMs for high-performance GPU kernel generation. Comprehensive results on widely adopted benchmarks demonstrate the superior performance of MTMC on GPU kernel generation in both accuracy and running time. On KernelBench, MTMC achieves near 100% and 70% accuracy at Levels 1-2 and 3, over 50% than SOTA general-purpose and domain-finetuned LLMs, with up to 7.3x speedup over LLMs, and 2.2x over expert-optimized PyTorch Eager kernels. On the more challenging TritonBench, MTMC attains up to 59.64% accuracy and 34x speedup.",
        "authors": [
            "Xinguo Zhu",
            "Shaohui Peng",
            "Jiaming Guo",
            "Yunji Chen",
            "Qi Guo",
            "Yuanbo Wen",
            "Hang Qin",
            "Ruizhi Chen",
            "Qirui Zhou",
            "Ke Gao",
            "Yanjun Wu",
            "Chen Zhao",
            "Ling Li"
        ],
        "categories": [
            "cs.DC",
            "cs.CL"
        ],
        "submit_date": "2025-11-25"
    },
    "http://arxiv.org/abs/2511.19978v1": {
        "title": "SwitchDelta: Asynchronous Metadata Updating for Distributed Storage with In-Network Data Visibility",
        "link": "http://arxiv.org/abs/2511.19978v1",
        "abstract": "Distributed storage systems typically maintain strong consistency between data nodes and metadata nodes by adopting ordered writes: 1) first installing data; 2) then updating metadata to make data visible.We propose SwitchDelta to accelerate ordered writes by moving metadata updates out of the critical path. It buffers in-flight metadata updates in programmable switches to enable data visibility in the network and retain strong consistency. SwitchDelta uses a best-effort data plane design to overcome the resource limitation of switches and designs a novel metadata update protocol to exploit the benefits of in-network data visibility. We evaluate SwitchDelta in three distributed in-memory storage systems: log-structured key-value stores, file systems, and secondary indexes. The evaluation shows that SwitchDelta reduces the latency of write operations by up to 52.4% and boosts the throughput by up to 126.9% under write-heavy workloads.",
        "authors": [
            "Junru Li",
            "Qing Wang",
            "Zhe Yang",
            "Shuo Liu",
            "Jiwu Shu",
            "Youyou Lu"
        ],
        "categories": [
            "cs.DC",
            "cs.DB"
        ],
        "id": "http://arxiv.org/abs/2511.19978v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-25"
    },
    "http://arxiv.org/abs/2511.19966v1": {
        "title": "Stragglers Can Contribute More: Uncertainty-Aware Distillation for Asynchronous Federated Learning",
        "link": "http://arxiv.org/abs/2511.19966v1",
        "abstract": "Asynchronous federated learning (FL) has recently gained attention for its enhanced efficiency and scalability, enabling local clients to send model updates to the server at their own pace without waiting for slower participants. However, such a design encounters significant challenges, such as the risk of outdated updates from straggler clients degrading the overall model performance and the potential bias introduced by faster clients dominating the learning process, especially under heterogeneous data distributions. Existing methods typically address only one of these issues, creating a conflict where mitigating the impact of outdated updates can exacerbate the bias created by faster clients, and vice versa. To address these challenges, we propose FedEcho, a novel framework that incorporates uncertainty-aware distillation to enhance the asynchronous FL performances under large asynchronous delays and data heterogeneity. Specifically, uncertainty-aware distillation enables the server to assess the reliability of predictions made by straggler clients, dynamically adjusting the influence of these predictions based on their estimated uncertainty. By prioritizing more certain predictions while still leveraging the diverse information from all clients, FedEcho effectively mitigates the negative impacts of outdated updates and data heterogeneity. Through extensive experiments, we demonstrate that FedEcho consistently outperforms existing asynchronous federated learning baselines, achieving robust performance without requiring access to private client data.",
        "authors": [
            "Yujia Wang",
            "Fenglong Ma",
            "Jinghui Chen"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.19966v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-25"
    },
    "http://arxiv.org/abs/2511.19959v1": {
        "id": "http://arxiv.org/abs/2511.19959v1",
        "title": "ParaBlock: Communication-Computation Parallel Block Coordinate Federated Learning for Large Language Models",
        "link": "http://arxiv.org/abs/2511.19959v1",
        "tags": [
            "training",
            "networking",
            "sparse"
        ],
        "relevant": true,
        "indexed_date": "2025-11-25",
        "tldr": "Addresses high communication latency in federated fine-tuning of large LLM blocks. Proposes ParaBlock, a scheme with parallel communication and computation threads to overlap these phases. Reduces communication costs by up to 42% while maintaining model convergence and performance.",
        "abstract": "Federated learning (FL) has been extensively studied as a privacy-preserving training paradigm. Recently, federated block coordinate descent scheme has become a popular option in training large-scale models, as it allows clients to train only a subset of the model locally instead of the entire model. However, in the era of large language models (LLMs), even a single block can contain a significant number of parameters, posing substantial communication latency, particularly for resource-constrained clients. To address this challenge in federated training/fine-tuning LLMs, we propose ParaBlock, a novel approach that establishes two parallel threads for communication and computation to enhance communication efficiency. We theoretically prove that the proposed ParaBlock achieves the same convergence rate as the standard federated block coordinate descent methods. Empirical evaluations on fine-tuning LLMs on general instruction following and mathematical reasoning confirm that ParaBlock not only maintains strong performance but also significantly improves communication efficiency.",
        "authors": [
            "Yujia Wang",
            "Yuanpu Cao",
            "Jinghui Chen"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "submit_date": "2025-11-25"
    },
    "http://arxiv.org/abs/2511.19949v1": {
        "title": "PolarStore: High-Performance Data Compression for Large-Scale Cloud-Native Databases",
        "link": "http://arxiv.org/abs/2511.19949v1",
        "abstract": "In recent years, resource elasticity and cost optimization have become essential for RDBMSs. While cloud-native RDBMSs provide elastic computing resources via disaggregated computing and storage, storage costs remain a critical user concern. Consequently, data compression emerges as an effective strategy to reduce storage costs. However, existing compression approaches in RDBMSs present a stark trade-off: software-based approaches incur significant performance overheads, while hardware-based alternatives lack the flexibility required for diverse database workloads. In this paper, we present PolarStore, a compressed shared storage system for cloud-native RDBMSs. PolarStore employs a dual-layer compression mechanism that combines in-storage compression in PolarCSD hardware with lightweight compression in software. This design leverages the strengths of both approaches. PolarStore also incorporates database-oriented optimizations to maintain high performance on critical I/O paths. Drawing from large-scale deployment experiences, we also introduce hardware improvements for PolarCSD to ensure host-level stability and propose a compression-aware scheduling scheme to improve cluster-level space efficiency. PolarStore is currently deployed on thousands of storage servers within PolarDB, managing over 100 PB of data. It achieves a compression ratio of 3.55 and reduces storage costs by approximately 60%. Remarkably, these savings are achieved while maintaining performance comparable to uncompressed clusters.",
        "authors": [
            "Qingda Hu",
            "Xinjun Yang",
            "Feifei Li",
            "Junru Li",
            "Ya Lin",
            "Yuqi Zhou",
            "Yicong Zhu",
            "Junwei Zhang",
            "Rongbiao Xie",
            "Ling Zhou",
            "Bin Wu",
            "Wenchao Zhou"
        ],
        "categories": [
            "cs.DC",
            "cs.DB"
        ],
        "id": "http://arxiv.org/abs/2511.19949v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-25"
    },
    "http://arxiv.org/abs/2511.19880v1": {
        "title": "Improved Linear-Time Construction of Minimal Dominating Set via Mobile Agents",
        "link": "http://arxiv.org/abs/2511.19880v1",
        "abstract": "Mobile agents have emerged as a powerful framework for solving fundamental graph problems in distributed settings in recent times. These agents, modelled as autonomous physical or software entities, possess local computation power, finite memory and have the ability to traverse a graph, offering efficient solutions to a range of classical problems. In this work, we focus on the problem of computing a \\emph{minimal dominating set} (mDS) in anonymous graphs using mobile agents. Building on the recently proposed optimal dispersion algorithm on the synchronous mobile agent model, we design two new algorithms that achieve a \\emph{linear-time} solution for this problem in the synchronous setting. Specifically, given a connected $n$-node graph with $n$ agents initially placed in either rooted or arbitrary configurations, we show that an mDS can be computed in $O(n)$ rounds using only $O(\\log n)$ bits of memory per agent, without using any prior knowledge of any global parameters. This improves upon the best-known complexity results in the literature over the same model. In addition, as natural by-products of our methodology, our algorithms also construct a spanning tree and elect a unique leader in $O(n)$ rounds, which are also important results of independent interest in the mobile-agent framework.",
        "authors": [
            "Prabhat Kumar Chand",
            "Anisur Rahaman Molla"
        ],
        "categories": [
            "cs.DC",
            "cs.DS",
            "cs.MA",
            "cs.RO"
        ],
        "id": "http://arxiv.org/abs/2511.19880v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-25"
    },
    "http://arxiv.org/abs/2511.19851v1": {
        "title": "Accelerating Wireless Distributed Learning via Hybrid Split and Federated Learning Optimization",
        "link": "http://arxiv.org/abs/2511.19851v1",
        "abstract": "Federated learning (FL) and split learning (SL) are two effective distributed learning paradigms in wireless networks, enabling collaborative model training across mobile devices without sharing raw data. While FL supports low-latency parallel training, it may converge to less accurate model. In contrast, SL achieves higher accuracy through sequential training but suffers from increased delay. To leverage the advantages of both, hybrid split and federated learning (HSFL) allows some devices to operate in FL mode and others in SL mode. This paper aims to accelerate HSFL by addressing three key questions: 1) How does learning mode selection affect overall learning performance? 2) How does it interact with batch size? 3) How can these hyperparameters be jointly optimized alongside communication and computational resources to reduce overall learning delay? We first analyze convergence, revealing the interplay between learning mode and batch size. Next, we formulate a delay minimization problem and propose a two-stage solution: a block coordinate descent method for a relaxed problem to obtain a locally optimal solution, followed by a rounding algorithm to recover integer batch sizes with near-optimal performance. Experimental results demonstrate that our approach significantly accelerates convergence to the target accuracy compared to existing methods.",
        "authors": [
            "Kun Guo",
            "Xuefei Li",
            "Xijun Wang",
            "Howard H. Yang",
            "Wei Feng",
            "Tony Q. S. Quek"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.19851v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-25"
    },
    "http://arxiv.org/abs/2511.19847v1": {
        "id": "http://arxiv.org/abs/2511.19847v1",
        "title": "Batch Denoising for AIGC Service Provisioning in Wireless Edge Networks",
        "link": "http://arxiv.org/abs/2511.19847v1",
        "tags": [
            "diffusion",
            "edge"
        ],
        "relevant": true,
        "indexed_date": "2025-11-25",
        "tldr": "Optimizes image generation service quality in wireless edge networks under latency constraints. Proposes STACKING, an algorithm for batch denoising that exploits step importance for parallelism. Achieves lower per-step delay and higher service quality compared to baseline methods.",
        "abstract": "Artificial intelligence-generated content (AIGC) service provisioning in wireless edge networks involves two phases: content generation on edge servers and content transmission to mobile devices. In this paper, we take image generation as a representative application and propose a batch denoising framework, followed by a joint optimization of content generation and transmission, with the objective of maximizing the average AIGC service quality under an end-to-end service delay constraint. Motivated by the empirical observations that (i) batch denoising effectively reduces per-step denoising delay by enhancing parallelism and (ii) early denoising steps have a greater impact on generation quality than later steps, we develop the STACKING algorithm to optimize batch denoising. The STACKING operates independently of any specific form of the content quality function and achieves lower computational complexity. Building on the batch solution, we further optimize bandwidth allocation across AIGC services. Simulation results demonstrate the superior performance of our algorithm in delivering high-quality, lower-latency AIGC services.",
        "authors": [
            "Jinghang Xu",
            "Kun Guo",
            "Wei Teng",
            "Chenxi Liu",
            "Wei Feng"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-11-25"
    },
    "http://arxiv.org/abs/2511.19832v1": {
        "title": "Enabling Scientific Workflow Scheduling Research in Non-Uniform Memory Access Architectures",
        "link": "http://arxiv.org/abs/2511.19832v1",
        "abstract": "Data-intensive scientific workflows increasingly rely on high-performance computing (HPC) systems, complementing traditional Grid and Cloud platforms. However, workflow scheduling on HPC infrastructures remains challenging due to the prevalence of non-uniform memory access (NUMA) architectures. These systems require schedulers to account for data locality not only across distributed environments but also within each node. Modern HPC nodes integrate multiple NUMA domains and heterogeneous memory regions, such as high-bandwidth memory (HBM) and DRAM, and frequently attach accelerators (GPUs or FPGAs) and network interface cards (NICs) to specific NUMA nodes. This design increases the variability of data-access latency and complicates the placement of both tasks and data. Despite these constraints, most workflow scheduling strategies were originally developed for Grid or Cloud environments and rarely incorporate NUMA-aware considerations. To address this gap, this work introduces nFlows, a NUMA-aware Workflow Execution Runtime System that enables the modeling, bare-metal execution, simulation, and validation of scheduling algorithms for data-intensive workflows on NUMA-based HPC systems. The system's design, implementation, and validation methodology are presented. nFlows supports the construction of simulation models and their direct execution on physical systems, enabling studies of NUMA effects on scheduling, the design of NUMA-aware algorithms, the analysis of data-movement behavior, the identification of performance bottlenecks, and the exploration of in-memory workflow execution.",
        "authors": [
            "Aurelio Vivas",
            "Harold Castro"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.19832v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-25"
    },
    "http://arxiv.org/abs/2511.19258v2": {
        "title": "IOMMU Support for Virtual-Address Remote DMA in an ARMv8 environment",
        "link": "http://arxiv.org/abs/2511.19258v2",
        "abstract": "In complex systems with many compute nodes containing multiple CPUs that are coherent within each node, a key challenge is maintaining efficient and correct coherence between nodes. The Unimem system addresses this by proposing a virtualized global address space that enables such coherence, relying on the I/O Memory Management Unit (IOMMU) in each node. The goal of this thesis is to support this approach by successfully testing and using the IOMMU of a single node. For this purpose, we used ARM's IOMMU, known as the System Memory Management Unit (SMMU), which translates virtual addresses to physical addresses. Because Linux documentation for the SMMU is limited and unclear, we implemented custom kernel modules to test and use its functionality.   First, we tested the SMMU in the Processing System (PS) of the Xilinx Zynq UltraScale+ MPSoC by developing a module that inserted virtual-to-physical address mappings into the SMMU. We then triggered a DMA transfer to a virtual address and observed that the request passed through the SMMU for address translation. We repeated this experiment by initiating DMA transactions from the Programmable Logic (PL) and similarly confirmed that the transactions were translated by the SMMU. Finally, we developed a module that enables transactions from the PL without requiring explicit pre-mapping of virtual and physical address pairs. This was achieved by configuring the SMMU with the page table pointer of a user process, allowing it to translate all relevant virtual addresses dynamically.   Overall, we successfully demonstrated the correct operation of the SMMU across all tested scenarios. Due to time constraints, further exploration of advanced SMMU features is left for future work.",
        "authors": [
            "Antonis Psistakis"
        ],
        "categories": [
            "cs.DC",
            "cs.AR"
        ],
        "id": "http://arxiv.org/abs/2511.19258v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-24"
    },
    "http://arxiv.org/abs/2511.19208v1": {
        "title": "Constant-Size Certificates for Leader Election in Chordal Graphs and Related Classes",
        "link": "http://arxiv.org/abs/2511.19208v1",
        "abstract": "In distributed computing a certification scheme consists of a set of states and conditions over those states that enable each node of a graph to efficiently verify the correctness of a solution to a given problem. This work focuses on two fundamental problems: leader election and spanning tree construction. For each problem, we present a constant-size (per edge), local certification scheme, where the conditions available to each node can only refer to the graph induced by its one-hop neighborhood. In particular, we provide certification schemes for leader election in chordal and $K_4$-free dismantlable graphs and for spanning tree construction in dismantlable graphs, assuming a root is given. For chordal graphs, our leader election certification scheme additionally ensures an acyclic orientation, a property that is not generally verifiable using constant-size certificates in arbitrary graphs. To the best of our knowledge, these are the first local certification results tailored to these graph classes, potentially highlighting structural properties useful for verifying additional problems. Finally, we propose an algorithm that automatically transforms any certification scheme into a silent self-stabilizing algorithm (i.e., an algorithm that automatically recovers from faults) by adding only one extra state to the set of states of the certification scheme, assuming a Gouda fair scheduler. This transformation may be of independent interest.",
        "authors": [
            "Jérémie Chalopin",
            "Maria Kokkou"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.19208v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-24"
    },
    "http://arxiv.org/abs/2511.19192v1": {
        "id": "http://arxiv.org/abs/2511.19192v1",
        "title": "AME: An Efficient Heterogeneous Agentic Memory Engine for Smartphones",
        "link": "http://arxiv.org/abs/2511.19192v1",
        "tags": [
            "edge",
            "storage",
            "RAG"
        ],
        "relevant": true,
        "indexed_date": "2025-11-24",
        "tldr": "Proposes AME, a hardware-aware vector database engine for on-device agents on smartphones. It co-designs an efficient matrix pipeline and workload scheduler with mobile SoC constraints. Achieves 1.4x higher query throughput and 7x faster index construction.",
        "abstract": "On-device agents on smartphones increasingly require continuously evolving memory to support personalized, context-aware, and long-term behaviors. To meet both privacy and responsiveness demands, user data is embedded as vectors and stored in a vector database for fast similarity search. However, most existing vector databases target server-class environments. When ported directly to smartphones, two gaps emerge: (G1) a mismatch between mobile SoC constraints and vector-database assumptions, including tight bandwidth budgets, limited on-chip memory, and stricter data type and layout constraints; and (G2) a workload mismatch, because on-device usage resembles a continuously learning memory, in which queries must coexist with frequent inserts, deletions, and ongoing index maintenance. To address these challenges, we propose AME, an on-device Agentic Memory Engine co-designed with modern smartphone SoCs. AME introduces two key techniques: (1) a hardware-aware, high-efficiency matrix pipeline that maximizes compute-unit utilization and exploits multi-level on-chip storage to sustain high throughput; and (2) a hardware- and workload-aware scheduling scheme that coordinates querying, insertion, and index rebuilding to minimize latency. We implement AME on Snapdragon 8-series SoCs and evaluate it on HotpotQA. In our experiments, AME improves query throughput by up to 1.4x at matched recall, achieves up to 7x faster index construction, and delivers up to 6x higher insertion throughput under concurrent query workloads.",
        "authors": [
            "Xinkui Zhao",
            "Qingyu Ma",
            "Yifan Zhang",
            "Hengxuan Lou",
            "Guanjie Cheng",
            "Shuiguang Deng",
            "Jianwei Yin"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-11-24"
    },
    "http://arxiv.org/abs/2511.18906v1": {
        "id": "http://arxiv.org/abs/2511.18906v1",
        "title": "An Online Fragmentation-Aware GPU Scheduler for Multi-Tenant MIG-based Clouds",
        "link": "http://arxiv.org/abs/2511.18906v1",
        "tags": [
            "training",
            "serving",
            "hardware"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Addresses GPU fragmentation in multi-tenant MIG clouds for AI workloads. Proposes an online fragmentation-aware scheduler using a fragmentation metric and greedy algorithm. Achieves 10% higher workload acceptance rate under heavy load.",
        "abstract": "The explosive growth of AI applications has created unprecedented demand for GPU resources. Cloud providers meet this demand through GPU-as-a-Service platforms that offer rentable GPU resources for running AI workloads. In this context, the sharing of GPU resources between different tenants is essential to maximize the number of scheduled workloads. Among the various GPU sharing technologies, NVIDIA's Multi-Instance GPU (MIG) stands out by partitioning GPUs at hardware level into isolated slices with dedicated compute and memory, ensuring strong tenant isolation, preventing resource contention, and enhancing security. Despite these advantages, MIG's fixed partitioning introduces scheduling rigidity, leading to severe GPU fragmentation in multi-tenant environments, where workloads are continuously deployed and terminated. Fragmentation leaves GPUs underutilized, limiting the number of workloads that can be accommodated. To overcome this challenge, we propose a novel scheduling framework for MIG-based clouds that maximizes workload acceptance while mitigating fragmentation in an online, workload-agnostic setting. We introduce a fragmentation metric to quantify resource inefficiency and guide allocation decisions. Building on this metric, our greedy scheduling algorithm selects GPUs and MIG slices that minimize fragmentation growth for each incoming workload. We evaluate our approach against multiple baseline strategies under diverse workload distributions. Results demonstrate that our method consistently achieves higher workload acceptance rates, leading to an average 10% increase in the number of scheduled workloads in heavy load conditions, while using approximately the same number of GPUs as the benchmark methods.",
        "authors": [
            "Marco Zambianco",
            "Lorenzo Fasol",
            "Roberto Doriguzzi-Corin"
        ],
        "categories": [
            "cs.DC",
            "cs.NI"
        ],
        "submit_date": "2025-11-24"
    },
    "http://arxiv.org/abs/2511.18841v1": {
        "title": "Federated style aware transformer aggregation of representations",
        "link": "http://arxiv.org/abs/2511.18841v1",
        "abstract": "Personalized Federated Learning (PFL) faces persistent challenges, including domain heterogeneity from diverse client data, data imbalance due to skewed participation, and strict communication constraints. Traditional federated learning often lacks personalization, as a single global model cannot capture client-specific characteristics, leading to biased predictions and poor generalization, especially for clients with highly divergent data distributions.   To address these issues, we propose FedSTAR, a style-aware federated learning framework that disentangles client-specific style factors from shared content representations. FedSTAR aggregates class-wise prototypes using a Transformer-based attention mechanism, allowing the server to adaptively weight client contributions while preserving personalization.   Furthermore, by exchanging compact prototypes and style vectors instead of full model parameters, FedSTAR significantly reduces communication overhead. Experimental results demonstrate that combining content-style disentanglement with attention-driven prototype aggregation improves personalization and robustness in heterogeneous environments without increasing communication cost.",
        "authors": [
            "Mincheol Jeon",
            "Euinam Huh"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.18841v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-24"
    },
    "http://arxiv.org/abs/2511.18723v4": {
        "title": "N2N: A Parallel Framework for Large-Scale MILP under Distributed Memory",
        "link": "http://arxiv.org/abs/2511.18723v4",
        "abstract": "Parallelization has emerged as a promising approach for accelerating MILP solving. However, the complexity of the branch-and-bound (B&B) framework and the numerous effective algorithm components in MILP solvers make it difficult to parallelize. In this study, a scalable parallel framework, N2N (a node-to-node framework that maps the B&B nodes to distributed computing nodes), was proposed to solve large-scale problems in a distributed memory computing environment. Both deterministic and nondeterministic modes are supported, and the framework is designed to be easily integrated with existing solvers. Regarding the deterministic mode, a novel sliding-window-based algorithm was designed and implemented to ensure that tasks are generated and solved in a deterministic order. Moreover, several advanced techniques, such as the utilization of CP search and general primal heuristics, have been developed to fully utilize distributed computing resources and capabilities of base solvers. Adaptive solving and data communication optimization were also investigated. A popular open-source MILP solver, SCIP, was integrated into N2N as the base solver, yielding N2N-SCIP. Extensive computational experiments were conducted to evaluate the performance of N2N-SCIP compared to ParaSCIP, which is a state-of-the-art distributed parallel MILP solver under the UG framework. The nondeterministic N2N-SCIP achieves speedups of 22.52 and 12.71 with 1,000 MPI processes on the Kunpeng and x86 computing clusters, which is 1.98 and 2.08 times faster than ParaSCIP, respectively. In the deterministic mode, N2N-SCIP also shows significant performance improvements over ParaSCIP across different process numbers and computing clusters. To validate the generality of N2N, HiGHS, another open-source solver, was integrated into N2N. The related results are analyzed, and the requirements of N2N on base solvers are also concluded.",
        "authors": [
            "Longfei Wang",
            "Junyan Liu",
            "Fan Zhang",
            "Jiangwen Wei",
            "Yuanhua Tang",
            "Jie Sun",
            "Xiaodong Luo"
        ],
        "categories": [
            "cs.AI",
            "cs.DC",
            "math.OC"
        ],
        "id": "http://arxiv.org/abs/2511.18723v4",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-24"
    },
    "http://arxiv.org/abs/2511.18674v1": {
        "id": "http://arxiv.org/abs/2511.18674v1",
        "title": "Low-Rank GEMM: Efficient Matrix Multiplication via Low-Rank Approximation with FP8 Acceleration",
        "link": "http://arxiv.org/abs/2511.18674v1",
        "tags": [
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-11-24",
        "tldr": "Proposes Low-Rank GEMM, a matrix multiplication method using low-rank approximations and FP8 precision to reduce complexity and improve hardware efficiency. Achieves a 7.8x speedup and 75% memory savings over PyTorch FP32 on large matrices.",
        "abstract": "Large matrix multiplication is a cornerstone of modern machine learning workloads, yet traditional approaches suffer from cubic computational complexity (e.g., $\\mathcal{O}(n^3)$ for a matrix of size $n\\times n$). We present Low-Rank GEMM, a novel approach that leverages low-rank matrix approximations to achieve sub-quadratic complexity while maintaining hardware-accelerated performance through FP8 precision and intelligent kernel selection. On a NVIDIA RTX 4090, our implementation achieves up to 378 TFLOPS on matrices up to $N=20480$, providing 75\\% memory savings and $7.8\\times$ speedup over PyTorch FP32 for large matrices. The system automatically adapts to hardware capabilities, selecting optimal decomposition methods (SVD, randomized SVD) and precision levels based on matrix characteristics and available accelerators. Comprehensive benchmarking on NVIDIA RTX 4090 demonstrates that Low-Rank GEMM becomes the fastest approach for matrices $N\\geq10240$, surpassing traditional cuBLAS implementations through memory bandwidth optimization rather than computational shortcuts.",
        "authors": [
            "Alfredo Metere"
        ],
        "categories": [
            "cs.PF",
            "cs.AI",
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-11-24"
    },
    "http://arxiv.org/abs/2511.18611v1": {
        "title": "CycleSL: Server-Client Cyclical Update Driven Scalable Split Learning",
        "link": "http://arxiv.org/abs/2511.18611v1",
        "abstract": "Split learning emerges as a promising paradigm for collaborative distributed model training, akin to federated learning, by partitioning neural networks between clients and a server without raw data exchange. However, sequential split learning suffers from poor scalability, while parallel variants like parallel split learning and split federated learning often incur high server resource overhead due to model duplication and aggregation, and generally exhibit reduced model performance and convergence owing to factors like client drift and lag. To address these limitations, we introduce CycleSL, a novel aggregation-free split learning framework that enhances scalability and performance and can be seamlessly integrated with existing methods. Inspired by alternating block coordinate descent, CycleSL treats server-side training as an independent higher-level machine learning task, resampling client-extracted features (smashed data) to mitigate heterogeneity and drift. It then performs cyclical updates, namely optimizing the server model first, followed by client updates using the updated server for gradient computation. We integrate CycleSL into previous algorithms and benchmark them on five publicly available datasets with non-iid data distribution and partial client attendance. Our empirical findings highlight the effectiveness of CycleSL in enhancing model performance. Our source code is available at https://gitlab.lrz.de/hctl/CycleSL.",
        "authors": [
            "Mengdi Wang",
            "Efe Bozkir",
            "Enkelejda Kasneci"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.18611v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-23"
    },
    "http://arxiv.org/abs/2511.18315v1": {
        "title": "Monotone Decontamination of Arbitrary Dynamic Graphs with Mobile Agents",
        "link": "http://arxiv.org/abs/2511.18315v1",
        "abstract": "Network decontamination is a well-known problem, in which the aim of the mobile agents should be to decontaminate the network (i.e., both nodes and edges). This problem comes with an added constraint, i.e., of \\emph{monotonicity}, in which whenever a node or an edge is decontaminated, it must not get recontaminated. Hence, the name comes \\emph{monotone decontamination}. This problem has been relatively explored in static graphs, but nothing is known yet in dynamic graphs. We, in this paper, study the \\emph{monotone decontamination} problem in arbitrary dynamic graphs. We designed two models of dynamicity, based on the time within which a disappeared edge must reappear. In each of these two models, we proposed lower bounds as well as upper bounds on the number of agents, required to fully decontaminate the underlying dynamic graph, monotonically. Our results also highlight the difficulties faced due to the sudden disappearance or reappearance of edges. Our aim in this paper has been to primarily optimize the number of agents required to solve monotone decontamination in these dynamic networks.",
        "authors": [
            "Rajashree Bar",
            "Daibik Barik",
            "Adri Bhattacharya",
            "Partha Sarathi Mandal"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.18315v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-23"
    },
    "http://arxiv.org/abs/2511.18291v1": {
        "id": "http://arxiv.org/abs/2511.18291v1",
        "title": "ADF-LoRA: Alternating Low-Rank Aggregation for Decentralized Federated Fine-Tuning",
        "link": "http://arxiv.org/abs/2511.18291v1",
        "tags": [
            "RL",
            "training"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes ADF-LoRA for decentralized federated fine-tuning, alternating updates of one low-rank matrix per round while mixing both to stabilize decentralized propagation. Achieves faster convergence and higher average accuracy on GLUE tasks vs. existing LoRA variants in decentralized FL.",
        "abstract": "This paper revisits alternating low-rank updates for federated fine-tuning and examines their behavior in decentralized federated learning (DFL). While alternating the LoRA matrices has been shown to stabilize aggregation in centralized FL, extending this mechanism to decentralized, peer-to-peer communication introduces new challenges due to phase-state mismatch and block-wise divergence across clients. We introduce ADF-LoRA, which synchronizes the update of only one low-rank matrix per round and mixes both matrices to maintain more consistent parameter states under decentralized propagation. This design preserves the cross-term suppression effect of alternating updates while improving stability in serverless topologies. We provide a convergence analysis under standard smoothness assumptions and evaluate ADF-LoRA on multiple GLUE tasks. Experiments show that ADF-LoRA achieves faster and smoother convergence and delivers the highest average accuracy across tasks, outperforming existing LoRA variants in decentralized FL by a consistent margin.",
        "authors": [
            "Xiaoyu Wang",
            "Xiaotian Li",
            "Zhixiang Zhou",
            "Chen Li",
            "Yong Liu"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "submit_date": "2025-11-23"
    },
    "http://arxiv.org/abs/2511.18151v1": {
        "id": "http://arxiv.org/abs/2511.18151v1",
        "title": "AVERY: Adaptive VLM Split Computing through Embodied Self-Awareness for Efficient Disaster Response Systems",
        "link": "http://arxiv.org/abs/2511.18151v1",
        "tags": [
            "edge",
            "multi-modal",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-11-22",
        "tldr": "Presents AVERY, an adaptive split-computing framework for Vision-Language Models on UAVs. It uses a dual-stream split (context/insight) and a self-aware controller to dynamically offload processing. Outperforms baselines with 93.98% lower energy consumption versus full-edge execution.",
        "abstract": "Unmanned Aerial Vehicles (UAVs) in disaster response require complex, queryable intelligence that on-board CNNs cannot provide. While Vision-Language Models (VLMs) offer this semantic reasoning, their high resource demands make on-device deployment infeasible, and naive cloud offloading fails under the low-bandwidth networks common in disaster zones. We present AVERY, a framework that enables VLM deployment through adaptive split computing. We advance the split computing paradigm beyond traditional depth-wise partitioning by introducing a functional, cognitive-inspired dual-stream split that separates the VLM into a high-frequency, low-resolution \"context stream\" for real-time awareness and a low-frequency, high-fidelity \"insight stream\" for deep analysis. A lightweight, self-aware on-board controller manages this architecture, monitoring network conditions and operator intent to dynamically select from pre-trained compression models, navigating the fundamental accuracy-throughput trade-off. Evaluated using the VLM LISA-7B across an edge-cloud scenario under fluctuating network conditions, AVERY consistently outperforms static configurations, achieving 11.2% higher accuracy than raw image compression and 93.98% lower energy consumption compared to full-edge execution, thereby enhancing mission efficiency and enabling real-time, queryable intelligence on resource-constrained platforms in dynamic environments.",
        "authors": [
            "Rajat Bhattacharjya",
            "Sing-Yao Wu",
            "Hyunwoo Oh",
            "Chaewon Nam",
            "Suyeon Koo",
            "Mohsen Imani",
            "Elaheh Bozorgzadeh",
            "Nikil Dutt"
        ],
        "categories": [
            "cs.DC",
            "cs.AR",
            "cs.CV",
            "cs.LG",
            "cs.NI"
        ],
        "submit_date": "2025-11-22"
    },
    "http://arxiv.org/abs/2511.19479v1": {
        "title": "Federated Learning Framework for Scalable AI in Heterogeneous HPC and Cloud Environments",
        "link": "http://arxiv.org/abs/2511.19479v1",
        "abstract": "As the demand grows for scalable and privacy-aware AI systems, Federated Learning (FL) has emerged as a promising solution, allowing decentralized model training without moving raw data. At the same time, the combination of high-performance computing (HPC) and cloud infrastructure offers vast computing power but introduces new complexities, especially when dealing with heterogeneous hardware, communication limits, and non-uniform data. In this work, we present a federated learning framework built to run efficiently across mixed HPC and cloud environments. Our system addresses key challenges such as system heterogeneity, communication overhead, and resource scheduling, while maintaining model accuracy and data privacy. Through experiments on a hybrid testbed, we demonstrate strong performance in terms of scalability, fault tolerance, and convergence, even under non-Independent and Identically Distributed (non-IID) data distributions and varied hardware. These results highlight the potential of federated learning as a practical approach to building scalable Artificial Intelligence (AI) systems in modern, distributed computing settings.",
        "authors": [
            "Sangam Ghimire",
            "Paribartan Timalsina",
            "Nirjal Bhurtel",
            "Bishal Neupane",
            "Bigyan Byanju Shrestha",
            "Subarna Bhattarai",
            "Prajwal Gaire",
            "Jessica Thapa",
            "Sudan Jha"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "id": "http://arxiv.org/abs/2511.19479v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-22"
    },
    "http://arxiv.org/abs/2511.18137v1": {
        "id": "http://arxiv.org/abs/2511.18137v1",
        "title": "Simulating Dynamic Cloud Marketspaces: Modeling Spot Instance Behavior and Scheduling with CloudSim Plus",
        "link": "http://arxiv.org/abs/2511.18137v1",
        "tags": [
            "scheduling",
            "training",
            "storage"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Extends CloudSim Plus to model spot instance lifecycle for cost-effective workload scheduling in cloud environments. Proposes HLEM-VMP algorithm to reduce interruptions under volatility. Achieves fewer interruptions and shorter max interruption duration in simulations using Google Cluster Trace.",
        "abstract": "The increasing reliance on dynamic pricing models, such as spot instances, in public cloud environments presents new challenges for workload scheduling and reliability. While these models offer cost advantages, they introduce volatility and uncertainty that are not fully addressed by current allocation algorithms or simulation tools. This work contributes to the modeling and evaluation of such environments by extending the CloudSim Plus simulation framework to support realistic spot instance lifecycle management, including interruption, termination, hibernation, and reallocation. The enhanced simulator is validated using synthetic scenarios and large-scale simulations based on the Google Cluster Trace dataset. Building on this foundation, the HLEM-VMP allocation algorithm, originally proposed in earlier research, was adapted to operate under dynamic spot market conditions. Its performance was evaluated against baseline allocation strategies to assess its efficiency and resilience in volatile workload environments. The comparison demonstrated a reduction in the number of spot instance interruptions as well as a decrease in the maximum interruption duration. Overall, this work provides both a simulation framework for simulating dynamic cloud behavior and analytical insights into virtual machine allocation performance and market risk, contributing to more robust and cost-effective resource management in cloud computing.",
        "authors": [
            "Christoph Goldgruber",
            "Benedikt Pittl",
            "Erich Schikuta"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-11-22"
    },
    "http://arxiv.org/abs/2511.18124v1": {
        "title": "MIDAS: Adaptive Proxy Middleware for Mitigating Metadata Hotspots in HPC I/O at Scale",
        "link": "http://arxiv.org/abs/2511.18124v1",
        "abstract": "Metadata hotspots remain one of the key obstacles to scalable Input/Output (I/O) in both High-Performance Computing (HPC) and cloud-scale storage environments. Situations such as job start-ups, checkpoint storms, or heavily skewed namespace access can trigger thousands of concurrent metadata requests against a small subset of servers. The result is long queues, inflated tail latencies, and reduced system throughput. Prior efforts including static namespace partitioning, backend-specific extensions, and kernel-level modifications address parts of the problem, but they often prove too rigid, intrusive to deploy, or unstable under shifting workloads. We present MIDAS, an adaptive middleware layer that operates transparently between clients and metadata servers, requiring no changes to kernels or storage backends. The design brings together three mechanisms: (i) a namespace-aware load balancer that enhances consistent hashing with power-of-d sampling informed by live telemetry, (ii) a cooperative caching layer that preserves backend semantics through leases, invalidations, or adaptive timeouts, and (iii) a self-stabilizing control loop that dynamically adjusts routing aggressiveness and cache lifetimes while avoiding oscillations under bursty workloads. Analysis of the model and controlled experiments show that MIDAS reduces average queue lengths by roughly 23% and mitigates worst-case hotspots by up to 80% when compared to round-robin scheduling. These findings highlight that a stability-aware, middleware-based strategy can provide backend-agnostic improvements to metadata management, enabling better scalability in bursty scenarios, more predictable tail latencies, and stronger overall system performance.",
        "authors": [
            "Sangam Ghimire",
            "Nigam Niraula",
            "Nirjal Bhurtel",
            "Paribartan Timalsina",
            "Bishal Neupane",
            "James Bhattarai",
            "Sudan Jha"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.18124v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-22"
    },
    "http://arxiv.org/abs/2511.17882v1": {
        "title": "SAGkit: A Python SAG Toolkit for Response Time Analysis of Hybrid-Triggered Jobs",
        "link": "http://arxiv.org/abs/2511.17882v1",
        "abstract": "For distributed control systems, modern latency-critical applications are increasingly demanding real-time guarantees and robustness. Response-time analysis (RTA) is useful for this purpose, as it helps analyze and guarantee timing bounds. However, conventional RTA methods struggle with the state-space explosion problem, especially in non-preemptive systems with release jitter and execution time variations. In this paper, we introduce SAGkit, a Python toolkit that implements the schedule-abstraction graph (SAG) framework. SAGkit novelly enables exact and sustainable RTA of hybrid-triggered jobs by allowing job absence on the SAG basis. Our experiments demonstrate that SAGkit achieves exactness with acceptable runtime and memory overhead. This lightweight toolkit empowers researchers to analyze complex distributed control systems and is open-access for further development.",
        "authors": [
            "Ruide Cao",
            "Zhuyun Qi",
            "Qinyang He",
            "Chenxi Ling",
            "Yi Wang",
            "Guoming Tang"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.17882v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-22"
    },
    "http://arxiv.org/abs/2511.17849v2": {
        "id": "http://arxiv.org/abs/2511.17849v2",
        "title": "Pier: Efficient Large Language Model pretraining with Relaxed Global Communication",
        "link": "http://arxiv.org/abs/2511.17849v2",
        "tags": [
            "training",
            "RL"
        ],
        "relevant": true,
        "indexed_date": "2025-11-22",
        "tldr": "Addresses global communication bottleneck in LLM pretraining. Proposes Pier optimizer with relaxed communication via momentum warmup/decay and efficient system architecture. Speeds up GPT-2 XL training by up to 3.7x on 256 A100 GPUs without loss degradation.",
        "abstract": "Global communication, such as all-reduce and allgather, is the prominent performance bottleneck in large language model (LLM) pretraining. To address this issue, we present Pier, an efficient and scalable optimizer with relaxed global communication. Pier is built upon DiLoCo, which leverages an inner optimizer within groups of processors and an outer optimizer that requires global communication. To preserve the convergence and model performance, Pier incorporates two key techniques for the outer optimizer: momentum warmup and momentum decay. Pier employs an efficient and scalable system architecture to enable complex parallelization strategies in LLM pretraining. We examine the model performance and runtime reduction of Pier using the GPT model family (e.g., small, medium, XL, and 7B) and the OpenWebText dataset with a suite of thirteen downstream tasks. With data parallel strategy, Pier speeds up GPT-2 XL training by up to 2.7x-3.7x on 256 NVIDIA A100 GPUs and 1.2x-1.9x on 64 GH200 Superchips, respectively, without degradation of validation loss or downstream task performance. With data parallel and tensor parallel, Pier reduces the time cost GPT-2 7B model training by 54.5% on 128 A100s.",
        "authors": [
            "Shuyuan Fan",
            "Zhao Zhang"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-11-22"
    },
    "http://arxiv.org/abs/2511.19468v1": {
        "id": "http://arxiv.org/abs/2511.19468v1",
        "title": "Towards a future space-based, highly scalable AI infrastructure system design",
        "link": "http://arxiv.org/abs/2511.19468v1",
        "tags": [
            "training",
            "hardware",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Investigates space-based AI infrastructure for scalable ML compute using solar power. Proposes satellite fleets with TPUs, optical interlinks, and formation flight for low-latency communication. Radiation tests show Trillium TPUs withstand 5-year mission doses; projected launch costs drop below $200/kg by 2030s.",
        "abstract": "If AI is a foundational general-purpose technology, we should anticipate that demand for AI compute -- and energy -- will continue to grow. The Sun is by far the largest energy source in our solar system, and thus it warrants consideration how future AI infrastructure could most efficiently tap into that power. This work explores a scalable compute system for machine learning in space, using fleets of satellites equipped with solar arrays, inter-satellite links using free-space optics, and Google tensor processing unit (TPU) accelerator chips. To facilitate high-bandwidth, low-latency inter-satellite communication, the satellites would be flown in close proximity. We illustrate the basic approach to formation flight via a 81-satellite cluster of 1 km radius, and describe an approach for using high-precision ML-based models to control large-scale constellations. Trillium TPUs are radiation tested. They survive a total ionizing dose equivalent to a 5 year mission life without permanent failures, and are characterized for bit-flip errors. Launch costs are a critical part of overall system cost; a learning curve analysis suggests launch to low-Earth orbit (LEO) may reach $\\lesssim$\\$200/kg by the mid-2030s.",
        "authors": [
            "Blaise Agüera y Arcas",
            "Travis Beals",
            "Maria Biggs",
            "Jessica V. Bloom",
            "Thomas Fischbacher",
            "Konstantin Gromov",
            "Urs Köster",
            "Rishiraj Pravahan",
            "James Manyika"
        ],
        "categories": [
            "cs.DC",
            "cs.ET",
            "cs.LG",
            "physics.space-ph"
        ],
        "submit_date": "2025-11-22"
    },
    "http://arxiv.org/abs/2511.19464v1": {
        "title": "Temperature in SLMs: Impact on Incident Categorization in On-Premises Environments",
        "link": "http://arxiv.org/abs/2511.19464v1",
        "abstract": "SOCs and CSIRTs face increasing pressure to automate incident categorization, yet the use of cloud-based LLMs introduces costs, latency, and confidentiality risks. We investigate whether locally executed SLMs can meet this challenge. We evaluated 21 models ranging from 1B to 20B parameters, varying the temperature hyperparameter and measuring execution time and precision across two distinct architectures. The results indicate that temperature has little influence on performance, whereas the number of parameters and GPU capacity are decisive factors.",
        "authors": [
            "Marcio Pohlmann",
            "Alex Severo",
            "Gefté Almeida",
            "Diego Kreutz",
            "Tiago Heinrich",
            "Lourenço Pereira"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.CR",
            "cs.LG",
            "cs.PF"
        ],
        "id": "http://arxiv.org/abs/2511.19464v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-21"
    },
    "http://arxiv.org/abs/2511.19463v1": {
        "title": "Urban Buildings Energy Consumption Estimation Using HPC: A Case Study of Bologna",
        "link": "http://arxiv.org/abs/2511.19463v1",
        "abstract": "Urban Building Energy Modeling (UBEM) plays a central role in understanding and forecasting energy consumption at the city scale. In this work, we present a UBEM pipeline that integrates EnergyPlus simulations, high-performance computing (HPC), and open geospatial datasets to estimate the energy demand of buildings in Bologna, Italy. Geometric information including building footprints and heights was obtained from the Bologna Open Data portal and enhanced with aerial LiDAR measurements. Non-geometric attributes such as construction materials, insulation characteristics, and window performance were derived from regional building regulations and the European TABULA database. The computation was carried out on Leonardo, the Cineca-hosted supercomputer, enabling the simulation of approximately 25,000 buildings in under 30 minutes.",
        "authors": [
            "Aldo Canfora",
            "Eleonora Bergamaschi",
            "Riccardo Mioli",
            "Federico Battini",
            "Mirko Degli Esposti",
            "Giorgio Pedrazzi",
            "Chiara Dellacasa"
        ],
        "categories": [
            "cs.DC",
            "physics.app-ph"
        ],
        "id": "http://arxiv.org/abs/2511.19463v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-21"
    },
    "http://arxiv.org/abs/2512.04096v1": {
        "title": "Formal Specification for Fast ACS: Low-Latency File-Based Ordered Message Delivery at Scale",
        "link": "http://arxiv.org/abs/2512.04096v1",
        "abstract": "Low-latency message delivery is crucial for real-time systems. Data originating from a producer must be delivered to consumers, potentially distributed in clusters across metropolitan and continental boundaries. With the growing scale of computing, there can be several thousand consumers of the data. Such systems require a robust messaging system capable of transmitting messages containing data across clusters and efficiently delivering them to consumers. The system must offer guarantees like ordering and at-least-once delivery while avoiding overload on consumers, allowing them to consume messages at their own pace.   This paper presents the design of Fast ACS (an abbreviation for Ads Copy Service), a file-based ordered message delivery system that leverages a combination of two-sided (inter-cluster) and one-sided (intra-cluster) communication primitives - namely, Remote Procedure Call and Remote Memory Access, respectively - to deliver messages. The system has been successfully deployed to dozens of production clusters and scales to accommodate several thousand consumers within each cluster, which amounts to Tbps-scale intra-cluster consumer traffic at peak. Notably, Fast ACS delivers messages to consumers across the globe within a few seconds or even sub-seconds (p99) based on the message volume and consumer scale, at a low resource cost.",
        "authors": [
            "Sushant Kumar Gupta",
            "Anil Raghunath Iyer",
            "Chang Yu",
            "Neel Bagora",
            "Olivier Pomerleau",
            "Vivek Kumar",
            "Prunthaban Kanthakumar"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.04096v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-21"
    },
    "http://arxiv.org/abs/2511.19460v1": {
        "title": "Systemic approach for modeling a generic smart grid",
        "link": "http://arxiv.org/abs/2511.19460v1",
        "abstract": "Smart grid technological advances present a recent class of complex interdisciplinary modeling and increasingly difficult simulation problems to solve using traditional computational methods. To simulate a smart grid requires a systemic approach to integrated modeling of power systems, energy markets, demand-side management, and much other resources and assets that are becoming part of the current paradigm of the power grid. This paper presents a backbone model of a smart grid to test alternative scenarios for the grid. This tool simulates disparate systems to validate assumptions before the human scale model. Thanks to a distributed optimization of subsystems, the production and consumption scheduling is achieved while maintaining flexibility and scalability.",
        "authors": [
            "Sofiane Ben Amor",
            "Guillaume Guerard",
            "Loup-Noé Levy"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "eess.SY"
        ],
        "id": "http://arxiv.org/abs/2511.19460v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-21"
    },
    "http://arxiv.org/abs/2511.17127v2": {
        "id": "http://arxiv.org/abs/2511.17127v2",
        "title": "Training Foundation Models on a Full-Stack AMD Platform: Compute, Networking, and System Design",
        "link": "http://arxiv.org/abs/2511.17127v2",
        "tags": [
            "training",
            "MoE",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-11-21",
        "tldr": "Investigates efficient large-scale MoE model training on AMD hardware. Proposes cluster characterization, networking microbenchmarks for Pollara, and kernel sizing rules to optimize throughput. Achieves competitive model performance with ZAYA1-base (760M active params) while demonstrating AMD stack maturity.",
        "abstract": "We report on the first large-scale mixture-of-experts (MoE) pretraining study on pure AMD hardware, utilizing both MI300X GPUs and Pollara networking. We distill practical guidance for both systems and model design. On the systems side, we deliver a comprehensive cluster and networking characterization: microbenchmarks for all core collectives (all-reduce, reduce-scatter, all-gather, broadcast) across message sizes and GPU counts over Pollara. To our knowledge, this is the first at this scale. We further provide MI300X microbenchmarks on kernel sizing and memory bandwidth to inform model design. On the modeling side, we introduce and apply MI300X-aware transformer sizing rules for attention and MLP blocks and justify MoE widths that jointly optimize training throughput and inference latency. We describe our training stack in depth, including often-ignored utilities such as fault-tolerance and checkpoint-reshaping, as well as detailed information on our training recipe. We also provide a preview of our model architecture and base model - ZAYA1 (760M active, 8.3B total parameters MoE, available at https://huggingface.co/Zyphra/ZAYA1-base) - which will be further improved upon in forthcoming papers. ZAYA1-base achieves performance comparable to leading base models such as Qwen3-4B and Gemma3-12B at its scale and larger, and outperforms models including Llama-3-8B and OLMoE across reasoning, mathematics, and coding benchmarks. Together, these results demonstrate that the AMD hardware, network, and software stack are mature and optimized enough for competitive large-scale pretraining.",
        "authors": [
            "Quentin Anthony",
            "Yury Tokpanov",
            "Skyler Szot",
            "Srivatsan Rajagopal",
            "Praneeth Medepalli",
            "Anna Golubeva",
            "Vasu Shyam",
            "Robert Washbourne",
            "Rishi Iyer",
            "Ansh Chaurasia",
            "Tomas Figliolia",
            "Xiao Yang",
            "Abhinav Sarje",
            "Drew Thorstensen",
            "Amartey Pearson",
            "Zack Grossbart",
            "Jason van Patten",
            "Emad Barsoum",
            "Zhenyu Gu",
            "Yao Fu",
            "Beren Millidge"
        ],
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.DC"
        ],
        "submit_date": "2025-11-21"
    },
    "http://arxiv.org/abs/2511.17119v1": {
        "title": "Modeling Anomaly Detection in Cloud Services: Analysis of the Properties that Impact Latency and Resource Consumption",
        "link": "http://arxiv.org/abs/2511.17119v1",
        "abstract": "Detecting and resolving performance anomalies in Cloud services is crucial for maintaining desired performance objectives. Scaling actions triggered by an anomaly detector help achieve target latency at the cost of extra resource consumption. However, performance anomaly detectors make mistakes. This paper studies which characteristics of performance anomaly detection are important to optimize the trade-off between performance and cost. Using Stochastic Reward Nets, we model a Cloud service monitored by a performance anomaly detector. Using our model, we study the impact of detector characteristics, namely precision, recall and inspection frequency, on the average latency and resource consumption of the monitored service. Our results show that achieving a high precision and a high recall is not always necessary. If detection can be run frequently, a high precision is enough to obtain a good performance-to-cost trade-off, but if the detector is run infrequently, recall becomes the most important.",
        "authors": [
            "Gabriel Job Antunes Grabher",
            "Fumio Machida",
            "Thomas Ropars"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.17119v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-21"
    },
    "http://arxiv.org/abs/2511.19457v1": {
        "id": "http://arxiv.org/abs/2511.19457v1",
        "title": "SparOA: Sparse and Operator-aware Hybrid Scheduling for Edge DNN Inference",
        "link": "http://arxiv.org/abs/2511.19457v1",
        "tags": [
            "edge",
            "sparse",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes SparOA, a CPU-GPU hybrid DNN inference framework for edge devices using sparsity and operator-aware scheduling. It includes a RL-based scheduler and asynchronous execution with batch optimization. Achieves 1.22-1.31x speedup and 7-16% lower energy consumption than baselines.",
        "abstract": "The resource demands of deep neural network (DNN) models introduce significant performance challenges, especially when deployed on resource-constrained edge devices. Existing solutions like model compression often sacrifice accuracy, while specialized hardware remains costly and inflexible. Hybrid inference methods, however, typically overlook how operator characteristics impact performance. In this work, we present SparOA, a CPU-GPU hybrid inference framework, which leverages both sparsity and computational intensity to optimize operator scheduling. SparOA embraces aforementioned challenges through three key components: (1) a threshold predictor that accurately determines optimal sparsity and computational intensity thresholds; (2) a reinforcement learning-based scheduler that dynamically optimizes resource allocation based on real-time hardware states; and (3) a hybrid inference engine that enhances efficiency through asynchronous execution and batch size optimization.Extensive results show that SparOA achieves an average speedup of 1.22-1.31x compared to all baselines, and outperforms the CPU-Only by up to 50.7x. Also, SparOA achieves optimal energy-per-inference, consuming 7\\%-16\\% less energy than the SOTA co-execution baseline.",
        "authors": [
            "Ziyang Zhang",
            "Jie Liu",
            "Luca Mottola"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-11-21"
    },
    "http://arxiv.org/abs/2511.16964v1": {
        "id": "http://arxiv.org/abs/2511.16964v1",
        "title": "Optimizing PyTorch Inference with LLM-Based Multi-Agent Systems",
        "link": "http://arxiv.org/abs/2511.16964v1",
        "tags": [
            "kernel",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-11-21",
        "tldr": "Investigates multi-agent LLM systems for optimizing PyTorch inference. Proposes a framework to compare different agent strategies, finding exploit-heavy strategies with error-fixing agents work best. Achieves an average 2.88x speedup on an H100 GPU across the KernelBench suite.",
        "abstract": "Maximizing performance on available GPU hardware is an ongoing challenge for modern AI inference systems. Traditional approaches include writing custom GPU kernels and using specialized model compilers to tune high-level code for specific GPU targets. Recent work shows that LLM-based multi-agent systems can effectively perform such tuning, often outperforming existing compilers and eliminating the need for manual kernel development. However, the dynamics of multi-agent systems for this task remain unexplored. In this work, we present a logical framework for comparing multi-agent PyTorch optimization systems. Our evaluation shows that exploit-heavy strategies perform best when paired with error-fixing agents, and that performance correlates with the granularity of optimization steps. The best implementation achieves an average 2.88x speedup on an H100 GPU across diverse tasks in KernelBench, a benchmark suite covering a range of machine learning architectures in PyTorch.",
        "authors": [
            "Kirill Nagaitsev",
            "Luka Grbcic",
            "Samuel Williams",
            "Costin Iancu"
        ],
        "categories": [
            "cs.MA",
            "cs.AI",
            "cs.DC"
        ],
        "submit_date": "2025-11-21"
    },
    "http://arxiv.org/abs/2511.16947v1": {
        "id": "http://arxiv.org/abs/2511.16947v1",
        "title": "MicroMoE: Fine-Grained Load Balancing for Mixture-of-Experts with Token Scheduling",
        "link": "http://arxiv.org/abs/2511.16947v1",
        "tags": [
            "training",
            "MoE"
        ],
        "relevant": true,
        "indexed_date": "2025-11-21",
        "tldr": "Addresses load imbalance in MoE model training. Proposes MicroMoE, a distributed system with MicroEP, a fine-grained parallelization strategy using token scheduling for load balancing. Improves end-to-end training throughput by up to 47.6%.",
        "abstract": "Mixture-of-Experts (MoE) has emerged as a promising approach to scale up deep learning models due to its significant reduction in computational resources. However, the dynamic nature of MoE leads to load imbalance among experts, severely impacting training efficiency. While previous research has attempted to address the load balancing challenge, existing solutions either compromise model accuracy or introduce additional system overhead. As a result, they fail to achieve fine-grained load balancing, which is crucial to optimizing training efficiency.   We propose MicroEP, a novel parallelization strategy to achieve fine-grained load balancing in MoE systems. MicroEP is capable of achieving optimal load balancing in every micro-batch through efficient token scheduling across GPUs. Furthermore, we propose MicroMoE, an efficient distributed MoE training system with MicroEP's load balancing capabilities. Our experimental results demonstrate that MicroMoE improves the end-to-end training throughput by up to 47.6% compared with the state-of-the-art system, and almost consistently achieves optimal load balance among GPUs.",
        "authors": [
            "Chenqi Zhao",
            "Wenfei Wu",
            "Linhai Song",
            "Yuchen Xu"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-11-21"
    },
    "http://arxiv.org/abs/2511.16665v1": {
        "id": "http://arxiv.org/abs/2511.16665v1",
        "title": "Taming the Long-Tail: Efficient Reasoning RL Training with Adaptive Drafter",
        "link": "http://arxiv.org/abs/2511.16665v1",
        "tags": [
            "training",
            "RL",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-11-20",
        "tldr": "Proposes TLT, a system to accelerate reasoning RL training by using adaptive speculative decoding to overcome the bottleneck of long-tail response generation. Achieves over 1.7x end-to-end training speedup while preserving model accuracy.",
        "abstract": "The emergence of Large Language Models (LLMs) with strong reasoning capabilities marks a significant milestone, unlocking new frontiers in complex problem-solving. However, training these reasoning models, typically using Reinforcement Learning (RL), encounters critical efficiency bottlenecks: response generation during RL training exhibits a persistent long-tail distribution, where a few very long responses dominate execution time, wasting resources and inflating costs. To address this, we propose TLT, a system that accelerates reasoning RL training losslessly by integrating adaptive speculative decoding. Applying speculative decoding in RL is challenging due to the dynamic workloads, evolving target model, and draft model training overhead. TLT overcomes these obstacles with two synergistic components: (1) Adaptive Drafter, a lightweight draft model trained continuously on idle GPUs during long-tail generation to maintain alignment with the target model at no extra cost; and (2) Adaptive Rollout Engine, which maintains a memory-efficient pool of pre-captured CUDAGraphs and adaptively select suitable SD strategies for each input batch. Evaluations demonstrate that TLT achieves over 1.7x end-to-end RL training speedup over state-of-the-art systems, preserves the model accuracy, and yields a high-quality draft model as a free byproduct suitable for efficient deployment. Code is released at https://github.com/mit-han-lab/fastrl.",
        "authors": [
            "Qinghao Hu",
            "Shang Yang",
            "Junxian Guo",
            "Xiaozhe Yao",
            "Yujun Lin",
            "Yuxian Gu",
            "Han Cai",
            "Chuang Gan",
            "Ana Klimovic",
            "Song Han"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC"
        ],
        "submit_date": "2025-11-20"
    },
    "http://arxiv.org/abs/2511.16533v1": {
        "title": "Distributed MIS Algorithms for Rational Agents using Games",
        "link": "http://arxiv.org/abs/2511.16533v1",
        "abstract": "We study the problem of computing a Maximal Independent Set (MIS) in distributed networks where each node is a rational agent whose payoff depends on whether it joins the MIS. Classical distributed algorithms assume that nodes follow the prescribed protocol, but this assumption fails when nodes are strategic and may deviate if doing so increases their expected utility.   Standard MIS algorithms rely on honest randomness or unique identifiers to break symmetry. In rational settings, however, agents may manipulate randomness, and relying solely on identifiers can create unfairness, giving some nodes zero probability of joining the MIS and thus no incentive to participate. To address these issues, we propose two algorithms based on a utility model in which agents seek locally correct solutions while also having preferences over which solution is chosen. Randomness in our algorithms is generated through pairwise interactions between neighboring nodes, viewed as simple games in which no single node can unilaterally affect the outcome. This allows symmetry breaking while remaining compatible with rational behavior.   For both algorithms, we prove that at every stage of the execution, given any history, no agent can increase its expected utility through a unilateral deviation, assuming others follow the algorithm. This gives a stronger guarantee than Trembling-Hand Perfect Equilibrium. When all nodes follow the protocol, every node has a positive probability of joining the MIS, and the final output is a correct MIS. Under mild additional assumptions, both algorithms terminate in $O(\\log n)$ rounds with high probability.",
        "authors": [
            "Nithin Salevemula",
            "Shreyas Pai"
        ],
        "categories": [
            "cs.DC",
            "cs.GT"
        ],
        "id": "http://arxiv.org/abs/2511.16533v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-20"
    },
    "http://arxiv.org/abs/2511.16450v1": {
        "title": "Optimizing Federated Learning in the Era of LLMs: Message Quantization and Streaming",
        "link": "http://arxiv.org/abs/2511.16450v1",
        "abstract": "Federated Learning (FL) offers a promising solution for training machine learning models across distributed data sources while preserving data privacy. However, FL faces critical challenges related to communication overhead and local resource constraints, especially in the era of Large Language Models (LLMs) with billions of parameters. The sheer size of these models exacerbates both memory and communication constraints, making efficient transmission and processing essential for practical deployment. NVIDIA FLARE, an open-source SDK for federated learning, addresses these challenges by introducing advanced communication capabilities. Building upon existing solutions for large object streaming, we enhance FL workflows for LLMs through two key techniques: message quantization and container/file streaming. Quantization reduces message size, while streaming enables efficient memory management, improving scalability and integration with existing workflows. These advancements significantly enhance the robustness and efficiency of FL with LLMs, ensuring better performance in real-world federated learning scenarios.",
        "authors": [
            "Ziyue Xu",
            "Zhihong Zhang",
            "Holger R. Roth",
            "Chester Chen",
            "Yan Cheng",
            "Andrew Feng"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.16450v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-20"
    },
    "http://arxiv.org/abs/2511.16420v2": {
        "title": "A Fast Relax-and-Round Approach to Unit Commitment for Data Center Own Generation",
        "link": "http://arxiv.org/abs/2511.16420v2",
        "abstract": "The rapid growth of data centers increasingly requires data center operators to \"bring own generation\" to complement the available utility power plants to supply all or part of data center load. This practice sharply increases the number of generators on the bulk power system and shifts operational focus toward fuel costs rather than traditional startup and runtime constraints. Conventional mixed-integer unit commitment formulations are not well suited for systems with thousands of flexible, fast-cycling units. We propose a unit commitment formulation that relaxes binary commitment decisions by allowing generators to be fractionally on, enabling the use of algorithms for continuous solvers. We then use a rounding approach to get a feasible unit commitment. For a 276-unit system, solution time decreases from 10 hours to less than a second, with no accuracy degradation. Our approach scales with no issues to tens of thousands of generators, which allows solving problems on the scale of the major North America interconnections. The bulk of computation is parallel and GPU compatible, enabling further acceleration in future work.",
        "authors": [
            "Shaked Regev",
            "Eve Tsybina",
            "Slaven Peles"
        ],
        "categories": [
            "math.OC",
            "cs.DC",
            "math.NA"
        ],
        "id": "http://arxiv.org/abs/2511.16420v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-20"
    },
    "http://arxiv.org/abs/2511.19456v1": {
        "title": "Optimizations on Graph-Level for Domain Specific Computations in Julia and Application to QED",
        "link": "http://arxiv.org/abs/2511.19456v1",
        "abstract": "Complex computational problems in science often consist of smaller parts that can have largely distinct compute requirements from one another. For optimal efficiency, analyzing each subtask and scheduling it on the best-suited hardware would be necessary. Other considerations must be taken into account, too, such as parallelism, dependencies between different subtasks, and data transfer speeds between devices. To achieve this, directed acyclic graphs are often employed to represent these problems and enable utilizing as much hardware as possible on a given machine. In this paper, we present a software framework written in Julia capable of automatically and dynamically producing statically scheduled and compiled code. We lay theoretical foundations and add domain-specific information about the computation to the existing concepts of DAG scheduling, enabling optimizations that would otherwise be impossible. To illustrate the theory we implement an example application: the computation of matrix elements for scattering processes with many external particles in quantum electrodynamics.",
        "authors": [
            "Anton Reinhard",
            "Simeon Ehrig",
            "René Widera",
            "Michael Bussmann",
            "Uwe Hernandez Acosta"
        ],
        "categories": [
            "cs.DC",
            "cs.PF"
        ],
        "id": "http://arxiv.org/abs/2511.19456v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-20"
    },
    "http://arxiv.org/abs/2511.16193v2": {
        "id": "http://arxiv.org/abs/2511.16193v2",
        "title": "Fast LLM Post-training via Decoupled and Best-of-N Speculation",
        "link": "http://arxiv.org/abs/2511.16193v2",
        "tags": [
            "training",
            "RL",
            "sparse"
        ],
        "relevant": true,
        "indexed_date": "2025-11-20",
        "tldr": "Accelerates RLHF-like post-training rollout via decoupled speculation and best-of-N drafting. Designs dynamic methods to maximize GPU efficiency and improve speculation accuracy for large batches. Achieves 1.3–1.5× faster speculative decoding than baselines.",
        "abstract": "Rollout dominates the training time in large language model (LLM) post-training, where the trained model is used to generate tokens given a batch of prompts. SpecActor achieves fast rollout with speculative decoding that deploys a fast path (e.g., a smaller model) to accelerate the unparallelizable generation, while the correctness is guaranteed by fast parallel verification of the outputs with the original model. SpecActor addresses two foundational challenges in speculative rollout by (1) a \\emph{dynamic decoupled speculation} execution method that maximizes the GPU computational efficiency to realize speedup for large-batch execution -- a configuration common in training but unfriendly to speculative execution and (2) a \\emph{dynamic Best-of-N speculation} method that selects and combines different drafting methods according to the rollout progress. It substantially improves the speculation accuracy even when the best drafting method is unknown a priori, meanwhile without requiring adding extra computation resources. {\\sys} is {1.7}\\,$\\times$ faster than veRL in end-to-end training, and is {1.3--1.5}\\,$\\times$ faster compared to baselines with speculative decoding.",
        "authors": [
            "Rongxin Cheng",
            "Kai Zhou",
            "Xingda Wei",
            "Siyuan Liu",
            "Mingcong Han",
            "Mingjing Ai",
            "Yeju Zhou",
            "Baoquan Zhong",
            "Wencong Xiao",
            "Rong Chen",
            "Haibo Chen"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-11-20"
    },
    "http://arxiv.org/abs/2511.16177v1": {
        "id": "http://arxiv.org/abs/2511.16177v1",
        "title": "Mitigating Shared Storage Congestion Using Control Theory",
        "link": "http://arxiv.org/abs/2511.16177v1",
        "tags": [
            "storage",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes a control-theoretic approach to regulate client I/O rates dynamically in HPC shared storage systems. Uses runtime load metrics to mitigate congestion, reducing total runtime by up to 20% and tail latency while ensuring stability.",
        "abstract": "Efficient data access in High-Performance Computing (HPC) systems is essential to the performance of intensive computing tasks. Traditional optimizations of the I/O stack aim to improve peak performance but are often workload specific and require deep expertise, making them difficult to generalize or re-use. In shared HPC environments, resource congestion can lead to unpredictable performance, causing slowdowns and timeouts. To address these challenges, we propose a self-adaptive approach based on Control Theory to dynamically regulate client-side I/O rates. Our approach leverages a small set of runtime system load metrics to reduce congestion and enhance performance stability. We implement a controller in a multi-node cluster and evaluate it on a real testbed under a representative workload. Experimental results demonstrate that our method effectively mitigates I/O congestion, reducing total runtime by up to 20% and lowering tail latency, while maintaining stable performance.",
        "authors": [
            "Thomas Collignon",
            "Kouds Halitim",
            "Raphaël Bleuse",
            "Sophie Cerf",
            "Bogdan Robu",
            "Éric Rutten",
            "Lionel Seinturier",
            "Alexandre van Kempen"
        ],
        "categories": [
            "cs.DC",
            "cs.AR"
        ],
        "submit_date": "2025-11-20"
    },
    "http://arxiv.org/abs/2511.16174v1": {
        "title": "Pipelined Dense Symmetric Eigenvalue Decomposition on Multi-GPU Architectures",
        "link": "http://arxiv.org/abs/2511.16174v1",
        "abstract": "Large symmetric eigenvalue problems are commonly observed in many disciplines such as Chemistry and Physics, and several libraries including cuSOLVERMp, MAGMA and ELPA support computing large eigenvalue decomposition on multi-GPU or multi-CPU-GPU hybrid architectures. However, these libraries do not provide satisfied performance that all of the libraries only utilize around 1.5\\% of the peak multi-GPU performance. In this paper, we propose a pipelined two-stage eigenvalue decomposition algorithm instead of conventional subsequent algorithm with substantial optimizations. On an 8$\\times$A100 platform, our implementation surpasses state-of-the-art cuSOLVERMp and MAGMA baselines, delivering mean speedups of 5.74$\\times$ and 6.59$\\times$, with better strong and weak scalability.",
        "authors": [
            "Hansheng Wang",
            "Ruiyi Zhan",
            "Dajun Huang",
            "Xingchen Liu",
            "Qiao Li",
            "Hancong Duan",
            "Dingwen Tao",
            "Guangming Tan",
            "Shaoshuai Zhang"
        ],
        "categories": [
            "cs.MS",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.16174v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-20"
    },
    "http://arxiv.org/abs/2511.16041v1": {
        "id": "http://arxiv.org/abs/2511.16041v1",
        "title": "Can Asymmetric Tile Buffering Be Beneficial?",
        "link": "http://arxiv.org/abs/2511.16041v1",
        "tags": [
            "kernel",
            "hardware"
        ],
        "relevant": true,
        "indexed_date": "2025-11-20",
        "tldr": "Proposes Asymmetric Tile Buffering (ATB) to decouple input and output operand tile dimensions in GEMM, increasing arithmetic intensity. A performance model balances ATB benefits against kernel switching overhead. On AMD XDNA2 AI Engine, achieves 4.54x speedup (4.8 to 24.6 TFLOPS) for BFP16-BF16 GEMM.",
        "abstract": "General matrix multiplication (GEMM) is the computational backbone of modern AI workloads, and its efficiency is critically dependent on effective tiling strategies. Conventional approaches employ symmetric tile buffering, where the buffered tile size of the input $A$ along the dimension $M$ matches the output tile size of $C$.   In this paper, we introduce asymmetric tile buffering (ATB), a simple but powerful technique that decouples the buffered tile dimensions of the input and output operands. We show, for the first time, that ATB is both practical and highly beneficial. To explain this effect, we develop a performance model that incorporates both the benefits of ATB (higher arithmetic intensity) and its overheads (higher kernel switching costs), providing insight into how to select effective ATB tiling factors. As a case study, we apply ATB to AMD's latest XDNA2 AI Engine (AIE), achieving up to a 4.54x speedup, from 4.8 to 24.6 TFLOPS on mixed-precision BFP16--BF16 GEMM, establishing a new performance record for XDNA2 AIE.",
        "authors": [
            "Chengyue Wang",
            "Wesley Pang",
            "Xinrui Wu",
            "Gregory Jun",
            "Luis Romero",
            "Endri Taka",
            "Diana Marculescu",
            "Tony Nowatzki",
            "Pranathi Vasireddy",
            "Joseph Melber",
            "Deming Chen",
            "Jason Cong"
        ],
        "categories": [
            "cs.DC",
            "cs.AR",
            "cs.PF"
        ],
        "submit_date": "2025-11-20"
    },
    "http://arxiv.org/abs/2511.15990v1": {
        "title": "Digital Agriculture Sandbox for Collaborative Research",
        "link": "http://arxiv.org/abs/2511.15990v1",
        "abstract": "Digital agriculture is transforming the way we grow food by utilizing technology to make farming more efficient, sustainable, and productive. This modern approach to agriculture generates a wealth of valuable data that could help address global food challenges, but farmers are hesitant to share it due to privacy concerns. This limits the extent to which researchers can learn from this data to inform improvements in farming. This paper presents the Digital Agriculture Sandbox, a secure online platform that solves this problem. The platform enables farmers (with limited technical resources) and researchers to collaborate on analyzing farm data without exposing private information. We employ specialized techniques such as federated learning, differential privacy, and data analysis methods to safeguard the data while maintaining its utility for research purposes. The system enables farmers to identify similar farmers in a simplified manner without needing extensive technical knowledge or access to computational resources. Similarly, it enables researchers to learn from the data and build helpful tools without the sensitive information ever leaving the farmer's system. This creates a safe space where farmers feel comfortable sharing data, allowing researchers to make important discoveries. Our platform helps bridge the gap between maintaining farm data privacy and utilizing that data to address critical food and farming challenges worldwide.",
        "authors": [
            "Osama Zafar",
            "Rosemarie Santa González",
            "Alfonso Morales",
            "Erman Ayday"
        ],
        "categories": [
            "cs.CR",
            "cs.CY",
            "cs.DC",
            "cs.LG"
        ],
        "id": "http://arxiv.org/abs/2511.15990v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-20"
    },
    "http://arxiv.org/abs/2511.15977v1": {
        "title": "Efficient Chromosome Parallelization for Precision Medicine Genomic Workflows",
        "link": "http://arxiv.org/abs/2511.15977v1",
        "abstract": "Large-scale genomic workflows used in precision medicine can process datasets spanning tens to hundreds of gigabytes per sample, leading to high memory spikes, intensive disk I/O, and task failures due to out-of-memory errors. Simple static resource allocation methods struggle to handle the variability in per-chromosome RAM demands, resulting in poor resource utilization and long runtimes. In this work, we propose multiple mechanisms for adaptive, RAM-efficient parallelization of chromosome-level bioinformatics workflows. First, we develop a symbolic regression model that estimates per-chromosome memory consumption for a given task and introduces an interpolating bias to conservatively minimize over-allocation. Second, we present a dynamic scheduler that adaptively predicts RAM usage with a polynomial regression model, treating task packing as a Knapsack problem to optimally batch jobs based on predicted memory requirements. Additionally, we present a static scheduler that optimizes chromosome processing order to minimize peak memory while preserving throughput. Our proposed methods, evaluated on simulations and real-world genomic pipelines, provide new mechanisms to reduce memory overruns and balance load across threads. We thereby achieve faster end-to-end execution, showcasing the potential to optimize large-scale genomic workflows.",
        "authors": [
            "Daniel Mas Montserrat",
            "Ray Verma",
            "Míriam Barrabés",
            "Francisco M. de la Vega",
            "Carlos D. Bustamante",
            "Alexander G. Ioannidis"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.LG",
            "cs.PF",
            "q-bio.GN"
        ],
        "id": "http://arxiv.org/abs/2511.15977v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-20"
    },
    "http://arxiv.org/abs/2511.15957v1": {
        "title": "Optimizing Communication in Byzantine Agreement Protocols with Slim-HBBFT",
        "link": "http://arxiv.org/abs/2511.15957v1",
        "abstract": "Byzantine agreement protocols in asynchronous networks have received renewed interest because they do not rely on network behavior to achieve termination. Conventional asynchronous Byzantine agreement protocols require every party to broadcast its requests (e.g., transactions), and at the end of the protocol, parties agree on one party's request. If parties agree on one party's requests while exchanging every party's request, the protocol becomes expensive. These protocols are used to design an atomic broadcast (ABC) protocol where parties agree on $\\langle n-f \\rangle$ parties' requests (assuming $n=3f+1$, where $n$ is the total number of parties, and $f$ is the number of Byzantine parties). Although the parties agree on a subset of requests in the ABC protocol, if the requests do not vary (are duplicated), investing in a costly protocol is not justified. We propose Slim-HBBFT, an atomic broadcast protocol that considers requests from a fraction of $n$ parties and improves communication complexity by a factor of $O(n)$. At the core of our design is a prioritized provable-broadcast (P-PB) protocol that generates proof of broadcast only for selected parties. We use the P-PB protocol to design the Slim-HBBFT atomic broadcast protocol. Additionally, we conduct a comprehensive security analysis to demonstrate that Slim-HBBFT satisfies the properties of the Asynchronous Common Subset protocol, ensuring robust security and reliability.",
        "authors": [
            "Nasit S Sony",
            "Xianzhong Ding"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.15957v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-20"
    },
    "http://arxiv.org/abs/2511.15950v1": {
        "id": "http://arxiv.org/abs/2511.15950v1",
        "title": "A Scalable NorthPole System with End-to-End Vertical Integration for Low-Latency and Energy-Efficient LLM Inference",
        "link": "http://arxiv.org/abs/2511.15950v1",
        "tags": [
            "hardware",
            "serving",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-11-20",
        "tldr": "Presents a vertically integrated system with NorthPole accelerators for scalable, low-latency LLM inference. Combines custom hardware, a runtime stack, and containerized pipeline. Delivers 2.8 ms per-user inter-token latency for an 8B model while consuming only 30 kW.",
        "abstract": "A vertically integrated, end-to-end, research prototype system combines 288 NorthPole neural inference accelerator cards, offline training algorithms, a high-performance runtime stack, and a containerized inference pipeline to deliver a scalable and efficient cloud inference service. The system delivers 115 peta-ops at 4-bit integer precision and 3.7 PB/s of memory bandwidth across 18 2U servers, while consuming only 30 kW of power and weighing 730 kg in a 0.67 m^2 42U rack footprint. The system can run 3 simultaneous instances of the 8-billion-parameter open-source IBM Granite-3.3-8b-instruct model at 2,048 context length with 28 simultaneous users and a per-user inter-token latency of 2.8 ms. The system is scalable, modular, and reconfigurable, supporting various model sizes and context lengths, and is ideal for deploying agentic workflows for enterprise AI applications in existing data center (cloud, on-prem) environments. For example, the system can support 18 instances of a 3-billion-parameter model or a single instance of a 70-billion-parameter model.",
        "authors": [
            "Michael V. DeBole",
            "Rathinakumar Appuswamy",
            "Neil McGlohon",
            "Brian Taba",
            "Steven K. Esser",
            "Filipp Akopyan",
            "John V. Arthur",
            "Arnon Amir",
            "Alexander Andreopoulos",
            "Peter J. Carlson",
            "Andrew S. Cassidy",
            "Pallab Datta",
            "Myron D. Flickner",
            "Rajamohan Gandhasri",
            "Guillaume J. Garreau",
            "Megumi Ito",
            "Jennifer L. Klamo",
            "Jeffrey A. Kusnitz",
            "Nathaniel J. McClatchey",
            "Jeffrey L. McKinstry",
            "Tapan K. Nayak",
            "Carlos Ortega Otero",
            "Hartmut Penner",
            "William P. Risk",
            "Jun Sawada",
            "Jay Sivagnaname",
            "Daniel F. Smith",
            "Rafael Sousa",
            "Ignacio Terrizzano",
            "Takanori Ueda",
            "Trent Gray-Donald",
            "David Cox",
            "Dharmendra S. Modha"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.AR"
        ],
        "submit_date": "2025-11-20"
    },
    "http://arxiv.org/abs/2511.19453v1": {
        "title": "AVS: A Computational and Hierarchical Storage System for Autonomous Vehicles",
        "link": "http://arxiv.org/abs/2511.19453v1",
        "abstract": "Autonomous vehicles (AVs) are evolving into mobile computing platforms, equipped with powerful processors and diverse sensors that generate massive heterogeneous data, for example 14 TB per day. Supporting emerging third-party applications calls for a general-purpose, queryable onboard storage system. Yet today's data loggers and storage stacks in vehicles fail to deliver efficient data storage and retrieval. This paper presents AVS, an Autonomous Vehicle Storage system that co-designs computation with a hierarchical layout: modality-aware reduction and compression, hot-cold tiering with daily archival, and a lightweight metadata layer for indexing. The design is grounded with system-level benchmarks on AV data that cover SSD and HDD filesystems and embedded indexing, and is validated on embedded hardware with real L4 autonomous driving traces. The prototype delivers predictable real-time ingest, fast selective retrieval, and substantial footprint reduction under modest resource budgets. The work also outlines observations and next steps toward more scalable and longer deployments to motivate storage as a first-class component in AV stacks.",
        "authors": [
            "Yuxin Wang",
            "Yuankai He",
            "Weisong Shi"
        ],
        "categories": [
            "cs.DC",
            "cs.DB",
            "cs.OS",
            "cs.RO"
        ],
        "id": "http://arxiv.org/abs/2511.19453v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-19"
    },
    "http://arxiv.org/abs/2511.15517v1": {
        "title": "Beluga: Block Synchronization for BFT Consensus Protocols",
        "link": "http://arxiv.org/abs/2511.15517v1",
        "abstract": "Modern high-throughput BFT consensus protocols use streamlined push-pull mechanisms to disseminate blocks and keep happy-path performance optimal. Yet state-of-the-art designs lack a principled and efficient way to exchange blocks, which leaves them open to targeted attacks and performance collapse under network asynchrony. This work introduces the concept of a block synchronizer, a simple abstraction that drives incremental block retrieval and enforces resource-aware exchange. Its interface and role fit cleanly inside a modern BFT consensus stack. We also uncover a new attack, where an adversary steers honest validators into redundant, uncoordinated pulls that exhaust bandwidth and stall progress. Beluga is a modular and scarcity-aware instantiation of the block synchronizer. It achieves optimal common-case latency while bounding the cost of recovery under faults and adversarial behavior. We integrate Beluga into Mysticeti, the consensus core of the Sui blockchain, and show on a geo-distributed AWS deployment that Beluga sustains optimal performance in the optimistic path and, under attack, delivers up to 3x higher throughput and 25x lower latency than prior designs. The Sui blockchain adopted Beluga in production.",
        "authors": [
            "Tasos Kichidis",
            "Lefteris Kokoris-Kogias",
            "Arun Koshy",
            "Ilya Sergey",
            "Alberto Sonnino",
            "Mingwei Tian",
            "Jianting Zhang"
        ],
        "categories": [
            "cs.CR",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.15517v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-19"
    },
    "http://arxiv.org/abs/2511.15503v1": {
        "id": "http://arxiv.org/abs/2511.15503v1",
        "title": "A Tensor Compiler for Processing-In-Memory Architectures",
        "link": "http://arxiv.org/abs/2511.15503v1",
        "tags": [
            "kernel",
            "hardware",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-11-19",
        "tldr": "Proposes DCC, a data-centric compiler optimizing data layouts and compute code jointly for ML kernels on PIM architectures. It uses a unified tuning process with performance prediction. Achieves up to 7.71x speedup for LLM inference over GPU-only execution.",
        "abstract": "Processing-In-Memory (PIM) devices integrated with high-performance Host processors (e.g., GPUs) can accelerate memory-intensive kernels in Machine Learning (ML) models, including Large Language Models (LLMs), by leveraging high memory bandwidth at PIM cores. However, Host processors and PIM cores require different data layouts: Hosts need consecutive elements distributed across DRAM banks, while PIM cores need them within local banks. This necessitates data rearrangements in ML kernel execution that pose significant performance and programmability challenges, further exacerbated by the need to support diverse PIM backends. Current compilation approaches lack systematic optimization for diverse ML kernels across multiple PIM backends and may largely ignore data rearrangements during compute code optimization. We demonstrate that data rearrangements and compute code optimization are interdependent, and need to be jointly optimized during the tuning process. To address this, we design DCC, the first data-centric ML compiler for PIM systems that jointly co-optimizes data rearrangements and compute code in a unified tuning process. DCC integrates a multi-layer PIM abstraction that enables various data distribution and processing strategies on different PIM backends. DCC enables effective co-optimization by mapping data partitioning strategies to compute loop partitions, applying PIM-specific code optimizations and leveraging a fast and accurate performance prediction model to select optimal configurations. Our evaluations in various individual ML kernels demonstrate that DCC achieves up to 7.68x speedup (2.7x average) on HBM-PIM and up to 13.17x speedup (5.75x average) on AttAcc PIM backend over GPU-only execution. In end-to-end LLM inference, DCC on AttAcc accelerates GPT-3 and LLaMA-2 by up to 7.71x (4.88x average) over GPU.",
        "authors": [
            "Peiming Yang",
            "Sankeerth Durvasula",
            "Ivan Fernandez",
            "Mohammad Sadrosadati",
            "Onur Mutlu",
            "Gennady Pekhimenko",
            "Christina Giannoula"
        ],
        "categories": [
            "cs.AR",
            "cs.DC",
            "cs.LG",
            "cs.PF"
        ],
        "submit_date": "2025-11-19"
    },
    "http://arxiv.org/abs/2511.15491v1": {
        "title": "Proving there is a leader without naming it",
        "link": "http://arxiv.org/abs/2511.15491v1",
        "abstract": "Local certification is a mechanism for certifying to the nodes of a network that a certain property holds. In this framework, nodes are assigned labels, called certificates, which are supposed to prove that the property holds. The nodes then communicate with their neighbors to verify the correctness of these certificates.   Certifying that there is a unique leader in a network is one of the most classical problems in this setting. It is well-known that this can be done using certificates that encode node identifiers and distances in the graph. These require $O(\\log n)$ and $O(\\log D)$ bits respectively, where $n$ is the number of nodes and $D$ is the diameter. A matching lower bound is known in cycle graphs (where $n$ and $D$ are equal up to multiplicative constants).   A recent line of work has shown that network structure greatly influences local certification. For example, certifying that a network does not contain triangles takes $Θ(n)$ bits in general graphs, but only $O(\\log n)$ bits in graphs of bounded treewidth. This observation raises the question: Is it possible to achieve sublogarithmic leader certification in graph classes that do not contain cycle graphs? And since in that case we cannot write identifiers in a certificate, do we actually need identifiers at all in such topologies? [We answer these questions with results on small diameter graphs, chordal graphs, grids, and dense graphs. See full abstract in the paper.]",
        "authors": [
            "Laurent Feuilloley",
            "Josef Erik Sedláček",
            "Martin Slávik"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.15491v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-19"
    },
    "http://arxiv.org/abs/2511.15479v1": {
        "title": "Towards a Formal Verification of Secure Vehicle Software Updates",
        "link": "http://arxiv.org/abs/2511.15479v1",
        "abstract": "With the rise of software-defined vehicles (SDVs), where software governs most vehicle functions alongside enhanced connectivity, the need for secure software updates has become increasingly critical. Software vulnerabilities can severely impact safety, the economy, and society. In response to this challenge, Strandberg et al. [escar Europe, 2021] introduced the Unified Software Update Framework (UniSUF), designed to provide a secure update framework that integrates seamlessly with existing vehicular infrastructures.   Although UniSUF has previously been evaluated regarding cybersecurity, these assessments have not employed formal verification methods. To bridge this gap, we perform a formal security analysis of UniSUF. We model UniSUF's architecture and assumptions to reflect real-world automotive systems and develop a ProVerif-based framework that formally verifies UniSUF's compliance with essential security requirements - confidentiality, integrity, authenticity, freshness, order, and liveness - demonstrating their satisfiability through symbolic execution. Our results demonstrate that UniSUF adheres to the specified security guarantees, ensuring the correctness and reliability of its security framework.",
        "authors": [
            "Martin Slind Hagen",
            "Emil Lundqvist",
            "Alex Phu",
            "Yenan Wang",
            "Kim Strandberg",
            "Elad Michael Schiller"
        ],
        "categories": [
            "cs.CR",
            "cs.DC",
            "cs.LO"
        ],
        "id": "http://arxiv.org/abs/2511.15479v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-19"
    },
    "http://arxiv.org/abs/2511.15421v1": {
        "title": "When Can You Trust Bitcoin? Value-Dependent Block Confirmation to Determine Transaction Finalit",
        "link": "http://arxiv.org/abs/2511.15421v1",
        "abstract": "We study financial transaction confirmation finality in Bitcoin as a function of transaction amount and user risk tolerance. A transaction is recorded in a block on a blockchain. However, a transaction may be revoked due to a fork in the blockchain, the odds of which decrease over time but never reach zero. Therefore, a transaction is considered confirmed if its block is sufficiently deep in the blockchain. This depth is usually set empirically at some fixed number such as six blocks. We analyze forks under varying network delays in simulation and actual Bitcoin data. Based on this analysis, we establish a relationship between block depth and the probability of confirmation revocation due to a fork. We use prospect theory to relate transaction confirmation probability to transaction amount and user risk tolerance.",
        "authors": [
            "Ethan Hicks",
            "Joseph Oglio",
            "Mikhail Nesterenko",
            "Gokarna Sharma"
        ],
        "categories": [
            "cs.DC",
            "cs.CR"
        ],
        "id": "http://arxiv.org/abs/2511.15421v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-19"
    },
    "http://arxiv.org/abs/2511.15388v1": {
        "title": "Multiple Sides of 36 Coins: Measuring Peer-to-Peer Infrastructure Across Cryptocurrencies",
        "link": "http://arxiv.org/abs/2511.15388v1",
        "abstract": "Blockchain technologies underpin an expanding ecosystem of decentralized applications, financial systems, and infrastructure. However, the fundamental networking layer that sustains these systems, the peer-to-peer layer, of all but the top few ecosystems remains largely opaque. In this paper, we present the first longitudinal, cross-network measurement study of 36 public blockchain networks. Over 9 months, we deployed 15 active crawlers, sourced data from two additional community crawlers, and conducted hourly connectivity probes to observe the evolving state of these networks. Furthermore, by leveraging Ethereum's discovery protocols, we inferred metadata for an additional 19 auxiliary networks that utilize the Ethereum peer discovery protocol. We also explored Internet-wide scans, which only require probing each protocol's default ports with a simple, network-specific payload. This approach allows us to rapidly identify responsive peers across the entire address space without having to implement custom discovery and handshake logic for every blockchain. We validated this method on Bitcoin and similar networks with known ground truth, then applied it to Cardano, which we could not crawl directly.   Our study uncovers dramatic variation in network size from under 10 to more than 10,000 active nodes. We quantify trends in IPv4 versus IPv6 usage, analyze autonomous systems and geographic concentration, and characterize churn, diurnal behavior, and the coverage and redundancy of discovery protocols. These findings expose critical differences in network resilience, decentralization, and observability. Beyond characterizing each network, our methodology demonstrates a general framework for measuring decentralized networks at scale. This opens the door for continued monitoring, benchmarking, and more transparent assessments of blockchain infrastructure across diverse ecosystems.",
        "authors": [
            "Lucianna Kiffer",
            "Lioba Heimbach",
            "Dennis Trautwein",
            "Yann Vonlanthen",
            "Oliver Gasser"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.15388v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-19"
    },
    "http://arxiv.org/abs/2511.15361v1": {
        "title": "BlueBottle: Fast and Robust Blockchains through Subsystem Specialization",
        "link": "http://arxiv.org/abs/2511.15361v1",
        "abstract": "Blockchain consensus faces a trilemma of security, latency, and decentralization. High-throughput systems often require a reduction in decentralization or robustness against strong adversaries, while highly decentralized and secure systems tend to have lower performance. We present BlueBottle, a two-layer consensus architecture. The core layer, BB-Core, is an n=5f+1 protocol that trades some fault tolerance for a much lower finality latency with a medium-sized core validator set. Our experiments show that BB-Core reduces latency by 20-25% in comparison to Mysticeti. The guard layer, BB-Guard, provides decentralized timestamping, proactive misbehavior detection in BB-Core, and a synchronous recovery path. When it observes equivocations or liveness failures in the core -- while tolerating up to f<3n/5 faulty nodes in the primary layer -- guard validators disseminate evidence, agree on misbehaving parties for exclusion or slashing, and either restart the core protocol (for liveness violations) or select a canonical fork (for safety violations). Together, these layers enable optimistic sub-second finality at high throughput while maintaining strong safety and liveness under a mild synchrony assumption.",
        "authors": [
            "Preston Vander Vos",
            "Alberto Sonnino",
            "Giorgos Tsimos",
            "Philipp Jovanovic",
            "Lefteris Kokoris-Kogias"
        ],
        "categories": [
            "cs.DC",
            "cs.CR"
        ],
        "id": "http://arxiv.org/abs/2511.15361v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-19"
    },
    "http://arxiv.org/abs/2511.15278v1": {
        "title": "Privacy-Preserving IoT in Connected Aircraft Cabin",
        "link": "http://arxiv.org/abs/2511.15278v1",
        "abstract": "The proliferation of IoT devices in shared, multi-vendor environments like the modern aircraft cabin creates a fundamental conflict between the promise of data collaboration and the risks to passenger privacy, vendor intellectual property (IP), and regulatory compliance. While emerging standards like the Cabin Secure Media-Independent Messaging (CSMIM) protocol provide a secure communication backbone, they do not resolve data governance challenges at the application layer, leaving a privacy gap that impedes trust. This paper proposes and evaluates a framework that closes this gap by integrating a configurable layer of Privacy-Enhancing Technologies (PETs) atop a CSMIM-like architecture. We conduct a rigorous, empirical analysis of two pragmatic PETs: Differential Privacy (DP) for statistical sharing, and an additive secret sharing scheme (ASS) for data obfuscation. Using a high-fidelity testbed with resource-constrained hardware, we quantify the trade-offs between data privacy, utility, and computing performance. Our results demonstrate that the computational overhead of PETs is often negligible compared to inherent network and protocol latencies. We prove that architectural choices, such as on-device versus virtualized processing, have a far greater impact on end-to-end latency and computational performance than the PETs themselves. The findings provide a practical roadmap for system architects to select and configure appropriate PETs, enabling the design of trustworthy collaborative IoT ecosystems in avionics and other critical domains.",
        "authors": [
            "Nilesh Vyas",
            "Benjamin Zhao",
            "Aygün Baltaci",
            "Gustavo de Carvalho Bertoli",
            "Hassan Asghar",
            "Markus Klügel",
            "Gerrit Schramm",
            "Martin Kubisch",
            "Dali Kaafar"
        ],
        "categories": [
            "cs.CR",
            "cs.DC",
            "cs.NI"
        ],
        "id": "http://arxiv.org/abs/2511.15278v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-19"
    },
    "http://arxiv.org/abs/2511.15076v2": {
        "id": "http://arxiv.org/abs/2511.15076v2",
        "title": "GPU-Initiated Networking for NCCL",
        "link": "http://arxiv.org/abs/2511.15076v2",
        "tags": [
            "MoE",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-11-19",
        "tldr": "Proposes GPU-Initiated Networking (GIN) in NCCL to enable device-initiated communication for reducing CPU coordination overhead in GPU-to-GPU networking. Integrates with MoE workloads via a three-layer architecture including direct GPU-to-NIC paths. Achieves up to 10× lower latency in MoE communication benchmarks.",
        "abstract": "Modern AI workloads, especially Mixture-of-Experts (MoE) architectures, increasingly demand low-latency, fine-grained GPU-to-GPU communication with device-side control. Traditional GPU communication follows a host-initiated model, where the CPU orchestrates all communication operations - a characteristic of the CUDA runtime. Although robust for collective operations, applications requiring tight integration of computation and communication can benefit from device-initiated communication that eliminates CPU coordination overhead.   NCCL 2.28 introduces the Device API with three operation modes: Load/Store Accessible (LSA) for NVLink/PCIe, Multimem for NVLink SHARP, and GPU-Initiated Networking (GIN) for network RDMA. This paper presents the GIN architecture, design, semantics, and highlights its impact on MoE communication. GIN builds on a three-layer architecture: i) NCCL Core host-side APIs for device communicator setup and collective memory window registration; ii) Device-side APIs for remote memory operations callable from CUDA kernels; and iii) A network plugin architecture with dual semantics (GPUDirect Async Kernel-Initiated and Proxy) for broad hardware support. The GPUDirect Async Kernel-Initiated backend leverages DOCA GPUNetIO for direct GPU-to-NIC communication, while the Proxy backend provides equivalent functionality via lock-free GPU-to-CPU queues over standard RDMA networks. We demonstrate GIN's practicality through integration with DeepEP, an MoE communication library. Comprehensive benchmarking shows that GIN provides device-initiated communication within NCCL's unified runtime, combining low-latency operations with NCCL's collective algorithms and production infrastructure.",
        "authors": [
            "Khaled Hamidouche",
            "John Bachan",
            "Pak Markthub",
            "Peter-Jan Gootzen",
            "Elena Agostini",
            "Sylvain Jeaugey",
            "Aamir Shafi",
            "Georgios Theodorakis",
            "Manjunath Gorentla Venkata"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.AR",
            "cs.LG"
        ],
        "submit_date": "2025-11-19"
    },
    "http://arxiv.org/abs/2511.15031v1": {
        "title": "GeoShield: Byzantine Fault Detection and Recovery for Geo-Distributed Real-Time Cyber-Physical Systems",
        "link": "http://arxiv.org/abs/2511.15031v1",
        "abstract": "Large-scale cyber-physical systems (CPS), such as railway control systems and smart grids, consist of geographically distributed subsystems that are connected via unreliable, asynchronous inter-region networks. Their scale and distribution make them especially vulnerable to faults and attacks. Unfortunately, existing fault-tolerant methods either consume excessive resources or provide only eventual guarantees, making them unsuitable for real-time resource-constrained CPS.   We present GeoShield, a resource-efficient solution for defending geo-distributed CPS against Byzantine faults. GeoShield leverages the property that CPS are designed to tolerate brief disruptions and maintain safety, as long as they recover (i.e., resume normal operations or transition to a safe mode) within a bounded amount of time following a fault. Instead of masking faults, it detects them and recovers the system within bounded time, thus guaranteeing safety with much fewer resources. GeoShield introduces protocols for Byzantine fault-resilient network measurement and inter-region omission fault detection that proactively detect malicious message delays, along with recovery mechanisms that guarantee timely recovery while maximizing operational robustness. It is the first bounded-time recovery solution that operates effectively under unreliable networks without relying on trusted hardware. Evaluations using real-world case studies show that it significantly outperforms existing methods in both effectiveness and resource efficiency.",
        "authors": [
            "Yifan Cai",
            "Linh Thi Xuan Phan"
        ],
        "categories": [
            "cs.CR",
            "cs.DC",
            "eess.SY"
        ],
        "id": "http://arxiv.org/abs/2511.15031v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-19"
    },
    "http://arxiv.org/abs/2511.14966v1": {
        "title": "A Graph-Based, Distributed Memory, Modeling Abstraction for Optimization",
        "link": "http://arxiv.org/abs/2511.14966v1",
        "abstract": "We present a general, flexible modeling abstraction for building and working with distributed optimization problems called a RemoteOptiGraph. This abstraction extends the OptiGraph model in Plasmo$.$jl, where optimization problems are represented as hypergraphs with nodes that define modular subproblems (variables, constraints, and objectives) and edges that encode algebraic linking constraints between nodes. The RemoteOptiGraph allows OptiGraphs to be utilized in distributed memory environments through InterWorkerEdges, which manage linking constraints that span workers. This abstraction offers a unified approach for modeling optimization problems on distributed memory systems (avoiding bespoke modeling approaches), and provides a basis for developing general-purpose meta-algorithms that can exploit distributed memory structure such as Benders or Lagrangian decompositions. We implement this abstraction in the open-source package, Plasmo$.$jl and we illustrate how it can be used by solving a mixed integer capacity expansion model for the western United States containing over 12 million variables and constraints. The RemoteOptiGraph abstraction together with Benders decomposition performs 7.5 times faster than solving the same problem without decomposition.",
        "authors": [
            "David L. Cole",
            "Jordan Jalving",
            "Jonah Langlieb",
            "Jesse D. Jenkins"
        ],
        "categories": [
            "cs.DC",
            "cs.MS",
            "math.OC"
        ],
        "id": "http://arxiv.org/abs/2511.14966v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-18"
    },
    "http://arxiv.org/abs/2511.19450v1": {
        "title": "AI-driven Predictive Shard Allocation for Scalable Next Generation Blockchains",
        "link": "http://arxiv.org/abs/2511.19450v1",
        "abstract": "Sharding has emerged as a key technique to address blockchain scalability by partitioning the ledger into multiple shards that process transactions in parallel. Although this approach improves throughput, static or heuristic shard allocation often leads to workload skew, congestion, and excessive cross-shard communication diminishing the scalability benefits of sharding. To overcome these challenges, we propose the Predictive Shard Allocation Protocol (PSAP), a dynamic and intelligent allocation framework that proactively assigns accounts and transactions to shards based on workload forecasts. PSAP integrates a Temporal Workload Forecasting (TWF) model with a safety-constrained reinforcement learning (Safe-PPO) controller, jointly enabling multi-block-ahead prediction and adaptive shard reconfiguration. The protocol enforces deterministic inference across validators through a synchronized quantized runtime and a safety gate that limits stake concentration, migration gas, and utilization thresholds. By anticipating hotspot formation and executing bounded, atomic migrations, PSAP achieves stable load balance while preserving Byzantine safety. Experimental evaluation on heterogeneous datasets, including Ethereum, NEAR, and Hyperledger Fabric mapped via address-clustering heuristics, demonstrates up to 2x throughput improvement, 35\\% lower latency, and 20\\% reduced cross-shard overhead compared to existing dynamic sharding baselines. These results confirm that predictive, deterministic, and security-aware shard allocation is a promising direction for next-generation scalable blockchain systems.",
        "authors": [
            "M. Zeeshan Haider",
            "Tayyaba Noreen",
            "M. D. Assuncao",
            "Kaiwen Zhang"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.19450v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-18"
    },
    "http://arxiv.org/abs/2511.14852v1": {
        "id": "http://arxiv.org/abs/2511.14852v1",
        "title": "PolyKAN: Efficient Fused GPU Operators for Polynomial Kolmogorov-Arnold Network Variants",
        "link": "http://arxiv.org/abs/2511.14852v1",
        "tags": [
            "kernel",
            "training",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes PolyKAN, an optimized CUDA kernel library for polynomial KAN variants to overcome low GPU utilization. Fuses forward/backward passes using lookup tables, 2D tiling, two-stage reduction, and coefficient reordering. Achieves up to 10× faster inference and 12× faster training over Triton+cuBLAS baseline.",
        "abstract": "Kolmogorov-Arnold Networks (KANs) promise higher expressive capability and stronger interpretability than Multi-Layer Perceptron, particularly in the domain of AI for Science. However, practical adoption has been hindered by low GPU utilization of existing parallel implementations. To address this challenge, we present a GPU-accelerated operator library, named PolyKAN which is the first general open-source implementation of KAN and its variants. PolyKAN fuses the forward and backward passes of polynomial KAN layers into a concise set of optimized CUDA kernels. Four orthogonal techniques underpin the design: (i) \\emph{lookup-table} with linear interpolation that replaces runtime expensive math-library functions; (ii) \\emph{2D tiling} to expose thread-level parallelism with preserving memory locality; (iii) a \\emph{two-stage reduction} scheme converting scattered atomic updates into a single controllable merge step; and (iv) \\emph{coefficient-layout reordering} yielding unit-stride reads under the tiled schedule. Using a KAN variant, Chebyshev KAN, as a case-study, PolyKAN delivers $1.2$--$10\\times$ faster inference and $1.4$--$12\\times$ faster training than a Triton + cuBLAS baseline, with identical accuracy on speech, audio-enhancement, and tabular-regression workloads on both highend GPU and consumer-grade GPU.",
        "authors": [
            "Mingkun Yu",
            "Heming Zhong",
            "Dan Huang",
            "Yutong Lu",
            "Jiazhi Jiang"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-11-18"
    },
    "http://arxiv.org/abs/2511.14715v2": {
        "title": "FLARE: Adaptive Multi-Dimensional Reputation for Robust Client Reliability in Federated Learning",
        "link": "http://arxiv.org/abs/2511.14715v2",
        "abstract": "Federated learning (FL) enables collaborative model training while preserving data privacy. However, it remains vulnerable to malicious clients who compromise model integrity through Byzantine attacks, data poisoning, or adaptive adversarial behaviors. Existing defense mechanisms rely on static thresholds and binary classification, failing to adapt to evolving client behaviors in real-world deployments. We propose FLARE, an adaptive reputation-based framework that transforms client reliability assessment from binary decisions to a continuous, multi-dimensional trust evaluation. FLARE integrates: (i) a multi-dimensional reputation score capturing performance consistency, statistical anomaly indicators, and temporal behavior, (ii) a self-calibrating adaptive threshold mechanism that adjusts security strictness based on model convergence and recent attack intensity, (iii) reputation-weighted aggregation with soft exclusion to proportionally limit suspicious contributions rather than eliminating clients outright, and (iv) a Local Differential Privacy (LDP) mechanism enabling reputation scoring on privatized client updates. We further introduce a highly evasive Statistical Mimicry (SM) attack, a benchmark adversary that blends honest gradients with synthetic perturbations and persistent drift to remain undetected by traditional filters. Extensive experiments with 100 clients on MNIST, CIFAR-10, and SVHN demonstrate that FLARE maintains high model accuracy and converges faster than state-of-the-art Byzantine-robust methods under diverse attack types, including label flipping, gradient scaling, adaptive attacks, ALIE, and SM. FLARE improves robustness by up to 16% and preserves model convergence within 30% of the non-attacked baseline, while achieving strong malicious-client detection performance with minimal computational overhead. https://github.com/Anonymous0-0paper/FLARE",
        "authors": [
            "Abolfazl Younesi",
            "Leon Kiss",
            "Zahra Najafabadi Samani",
            "Juan Aznar Poveda",
            "Thomas Fahringer"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CR",
            "cs.DC",
            "cs.MA"
        ],
        "id": "http://arxiv.org/abs/2511.14715v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-18"
    },
    "http://arxiv.org/abs/2512.04093v1": {
        "title": "Energy-Efficient Resource Management in Microservices-based Fog and Edge Computing: State-of-the-Art and Future Directions",
        "link": "http://arxiv.org/abs/2512.04093v1",
        "abstract": "The exponential growth of Internet of Things (IoT) devices has intensified the demand for efficient and responsive services. To address this demand, fog and edge computing have emerged as distributed paradigms that bring computational resources closer to end users, reducing latency, bandwidth limitations, and energy consumption. However, these paradigms present challenges in resource management due to resource constraints, computational heterogeneity, dynamic workloads, and diverse Quality of Service (QoS) requirements. This paper presents a comprehensive survey of state-of-the-art resource management strategies in microservices-based fog and edge computing, focusing on energy-efficient solutions. We systematically review and classify more than 136 studies (2020-2024) into five key subdomains: service placement, resource provisioning, task scheduling and offloading, resource allocation, and instance selection. Our categorization is based on optimization techniques, targeted objectives, and the strengths and limitations of each approach. In addition, we examine existing surveys and identify unresolved challenges and gaps in the literature. By highlighting the lack of synergy among fundamental resource management components, we outline promising research directions leveraging AI-driven optimization, quantum computing, and serverless computing. This survey serves as a comprehensive reference for researchers and practitioners by providing a unified and energy-aware perspective on resource management in microservices-based fog and edge computing, paving the way for more integrated, efficient, and sustainable future solutions.",
        "authors": [
            "Ali Akbar Vali",
            "Sadoon Azizi",
            "Mohammad Shojafar",
            "Rajkumar Buyya"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.04093v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-18"
    },
    "http://arxiv.org/abs/2511.14664v1": {
        "title": "Multi-GPU Quantum Circuit Simulation and the Impact of Network Performance",
        "link": "http://arxiv.org/abs/2511.14664v1",
        "abstract": "As is intrinsic to the fundamental goal of quantum computing, classical simulation of quantum algorithms is notoriously demanding in resource requirements. Nonetheless, simulation is critical to the success of the field and a requirement for algorithm development and validation, as well as hardware design. GPU-acceleration has become standard practice for simulation, and due to the exponential scaling inherent in classical methods, multi-GPU simulation can be required to achieve representative system sizes. In this case, inter-GPU communications can bottleneck performance. In this work, we present the introduction of MPI into the QED-C Application-Oriented Benchmarks to facilitate benchmarking on HPC systems. We review the advances in interconnect technology and the APIs for multi-GPU communication. We benchmark using a variety of interconnect paths, including the recent NVIDIA Grace Blackwell NVL72 architecture that represents the first product to expand high-bandwidth GPU-specialized interconnects across multiple nodes. We show that while improvements to GPU architecture have led to speedups of over 4.5X across the last few generations of GPUs, advances in interconnect performance have had a larger impact with over 16X performance improvements in time to solution for multi-GPU simulations.",
        "authors": [
            "W. Michael Brown",
            "Anurag Ramesh",
            "Thomas Lubinski",
            "Thien Nguyen",
            "David E. Bernal Neira"
        ],
        "categories": [
            "cs.DC",
            "quant-ph"
        ],
        "id": "http://arxiv.org/abs/2511.14664v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-18"
    },
    "http://arxiv.org/abs/2511.14617v2": {
        "id": "http://arxiv.org/abs/2511.14617v2",
        "title": "Seer: Online Context Learning for Fast Synchronous LLM Reinforcement Learning",
        "link": "http://arxiv.org/abs/2511.14617v2",
        "tags": [
            "RL",
            "training",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-11-18",
        "tldr": "Addresses long-tail latency and poor resource utilization in synchronous RL for LLMs. Proposes Seer with divided rollout, context-aware scheduling, and adaptive grouped speculative decoding. Achieves 97% higher rollout throughput and 93% lower long-tail latency versus state-of-the-art.",
        "abstract": "Reinforcement Learning (RL) has become critical for advancing modern Large Language Models (LLMs), yet existing synchronous RL systems face severe performance bottlenecks. The rollout phase, which dominates end-to-end iteration time, suffers from substantial long-tail latency and poor resource utilization due to inherent workload imbalance. We present Seer, a novel online context learning system that addresses these challenges by exploiting previously overlooked similarities in output lengths and generation patterns among requests sharing the same prompt. Seer introduces three key techniques: divided rollout for dynamic load balancing, context-aware scheduling, and adaptive grouped speculative decoding. Together, these mechanisms substantially reduce long-tail latency and improve resource efficiency during rollout. Evaluations on production-grade RL workloads demonstrate that Seer improves end-to-end rollout throughput by 74% to 97% and reduces long-tail latency by 75% to 93% compared to state-of-the-art synchronous RL systems, significantly accelerating RL training iterations.",
        "authors": [
            "Ruoyu Qin",
            "Weiran He",
            "Weixiao Huang",
            "Yangkun Zhang",
            "Yikai Zhao",
            "Bo Pang",
            "Xinran Xu",
            "Yingdi Shan",
            "Yongwei Wu",
            "Mingxing Zhang"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-11-18"
    },
    "http://arxiv.org/abs/2511.14608v1": {
        "title": "Hapax Locks : Value-Based Mutual Exclusion",
        "link": "http://arxiv.org/abs/2511.14608v1",
        "abstract": "We present Hapax Locks, a novel locking algorithm that is simple, enjoys constant-time arrival and unlock paths, provides FIFO admission order, and which is also space efficient and generates relatively little coherence traffic under contention in the common case. Hapax Locks offer performance (both latency and scalability) that is comparable with the best state of the art locks, while at the same time Hapax Locks impose fewer constraints and dependencies on the ambient runtime environment, making them particularly easy to integrate or retrofit into existing systems or under existing application programming interfaces Of particular note, no pointers shift or escape ownership between threads in our algorithm.",
        "authors": [
            "Dave Dice",
            "Alex Kogan"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.14608v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-18"
    },
    "http://arxiv.org/abs/2511.14502v1": {
        "title": "Overview and Prospects of Using Integer Surrogate Keys for Data Warehouse Performance Optimization",
        "link": "http://arxiv.org/abs/2511.14502v1",
        "abstract": "The aim of this paper is to examine and demonstrate how integer-based datetime labels (integer surrogate keys for time) can optimize data-warehouse and time-series performance, proposing practical formats and algorithms and validating their efficiency on real-world workloads. It is shown that replacing standard DATE and TIMESTAMP types with 32- and 64-bit integer formats reduces storage requirements by 30-60 percent and speeds up query execution by 25-40 percent. The paper presents indexing, aggregation, compression, and batching algorithms demonstrating up to an eightfold increase in throughput. Practical examples from finance, telecommunications, IoT, and scientific research confirm the efficiency and versatility of the proposed approach.",
        "authors": [
            "Sviatoslav Stumpf",
            "Vladislav Povyshev"
        ],
        "categories": [
            "cs.DB",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.14502v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-18"
    },
    "http://arxiv.org/abs/2511.14456v1": {
        "title": "Analyzing the Impact of Participant Failures in Cross-Silo Federated Learning",
        "link": "http://arxiv.org/abs/2511.14456v1",
        "abstract": "Federated learning (FL) is a new paradigm for training machine learning (ML) models without sharing data. While applying FL in cross-silo scenarios, where organizations collaborate, it is necessary that the FL system is reliable; however, participants can fail due to various reasons (e.g., communication issues or misconfigurations). In order to provide a reliable system, it is necessary to analyze the impact of participant failures. While this problem received attention in cross-device FL where mobile devices with limited resources participate, there is comparatively little research in cross-silo FL.   Therefore, we conduct an extensive study for analyzing the impact of participant failures on the model quality in the context of inter-organizational cross-silo FL with few participants. In our study, we focus on analyzing generally influential factors such as the impact of the timing and the data as well as the impact on the evaluation, which is important for deciding, if the model should be deployed. We show that under high skews the evaluation is optimistic and hides the real impact. Furthermore, we demonstrate that the timing impacts the quality of the trained model. Our results offer insights for researchers and software architects aiming to build robust FL systems.",
        "authors": [
            "Fabian Stricker",
            "David Bermbach",
            "Christian Zirpins"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "id": "http://arxiv.org/abs/2511.14456v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-18"
    },
    "http://arxiv.org/abs/2511.14450v2": {
        "id": "http://arxiv.org/abs/2511.14450v2",
        "title": "Hyperion: Hierarchical Scheduling for Parallel LLM Acceleration in Multi-tier Networks",
        "link": "http://arxiv.org/abs/2511.14450v2",
        "tags": [
            "serving",
            "offloading",
            "edge"
        ],
        "relevant": true,
        "indexed_date": "2025-11-18",
        "tldr": "Proposes Hyperion, a hierarchical framework for latency-minimized LLM inference in multi-tier edge networks. Combines offline inter-tier partitioning (HypSplit-DP) and online intra-tier scheduling (HypSched-RT) for joint optimization. Reduces latency by up to 52.1% vs GPipe while improving GPU utilization.",
        "abstract": "LLMs are increasingly executed in edge where limited GPU memory and heterogeneous computation jointly constrain deployment which motivates model partitioning and request scheduling. In this setting, minimizing latency requires addressing the tight coupling between model placement and request scheduling across heterogeneous nodes, as suboptimal decisions in one domain can negate benefits in the other. In this paper, we propose Hyperion, a hierarchical two-stage framework that jointly optimizes partitioning and scheduling for pipelined LLM inference. Hyperion minimizes latency by balancing resources across tiers without requiring model retraining or incurring significant runtime overhead. Leveraging the timescale difference between partitioning and request arrivals, Stage 1 performs offline, inter-tier partitioning via a Hyperion Split with Dynamic Programming (HypSplit-DP) procedure to produce balanced stage times under tier capacity and memory constraints; to adapt to time-varying load, Stage 2 performs online, intra-tier scheduling with a lightweight Hyperion Scheduling for Real-Time (HypSched-RT) that maps each request to the best available node using real-time estimates of queue length and effective capacity. Experiments with Phi-3-medium demonstrate that Hyperion reduces latency by up to 52.1% (vs. GPipe) and 31.2% (vs. HEFT). Furthermore, Hyperion exhibits superior scalability for long-sequence generation, maintaining 44.5% lower latency and higher GPU utilization.",
        "authors": [
            "Mulei Ma",
            "Xinyi Xu",
            "Minrui Xu",
            "Zihan Chen",
            "Yang Yang",
            "Tony Q. S. Quek"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-11-18"
    },
    "http://arxiv.org/abs/2511.14124v1": {
        "id": "http://arxiv.org/abs/2511.14124v1",
        "title": "10Cache: Heterogeneous Resource-Aware Tensor Caching and Migration for LLM Training",
        "link": "http://arxiv.org/abs/2511.14124v1",
        "tags": [
            "training",
            "offloading",
            "storage"
        ],
        "relevant": true,
        "indexed_date": "2025-11-18",
        "tldr": "Addresses memory bottlenecks and tensor migration latency in LLM training. Proposes 10Cache, a system for intelligent tensor caching/migration across GPU, CPU, and NVMe tiers using prefetching and buffer reuse. Achieves up to 2x training speedup and an 86.6x higher GPU cache hit rate.",
        "abstract": "Training large language models (LLMs) in the cloud faces growing memory bottlenecks due to the limited capacity and high cost of GPUs. While GPU memory offloading to CPU and NVMe has made large-scale training more feasible, existing approaches suffer from high tensor migration latency and suboptimal device memory utilization, ultimately increasing training time and cloud costs. To address these challenges, we present 10Cache, a resource-aware tensor caching and migration system that accelerates LLM training by intelligently coordinating memory usage across GPU, CPU, and NVMe tiers. 10Cache profiles tensor execution order to construct prefetch policies, allocates memory buffers in pinned memory based on tensor size distributions, and reuses memory buffers to minimize allocation overhead.   Designed for cloud-scale deployments, 10Cache improves memory efficiency and reduces reliance on high-end GPUs. Across diverse LLM workloads, it achieves up to 2x speedup in training time, improves GPU cache hit rate by up to 86.6x, and increases CPU/GPU memory utilization by up to 2.15x and 1.33x, respectively, compared to state-of-the-art offloading methods. These results demonstrate that 10Cache is a practical and scalable solution for optimizing LLM training throughput and resource efficiency in cloud environments.",
        "authors": [
            "Sabiha Afroz",
            "Redwan Ibne Seraj Khan",
            "Hadeel Albahar",
            "Jingoo Han",
            "Ali R. Butt"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-11-18"
    },
    "http://arxiv.org/abs/2511.14116v1": {
        "id": "http://arxiv.org/abs/2511.14116v1",
        "title": "FailSafe: High-performance Resilient Serving",
        "link": "http://arxiv.org/abs/2511.14116v1",
        "tags": [
            "serving",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-11-18",
        "tldr": "Addresses how to sustain LLM serving performance under GPU failures in tensor-parallel settings. Proposes FailSafe with cyclic KVCache placement, hybrid attention, and dynamic routing, plus proactive backup and recovery. Achieves 2x higher throughput and 100x lower recovery latency vs. standard fault handling.",
        "abstract": "Tensor parallelism (TP) enables large language models (LLMs) to scale inference efficiently across multiple GPUs, but its tight coupling makes systems fragile: a single GPU failure can halt execution, trigger costly KVCache recomputation, and introduce long-term compute and memory imbalance. We present FailSafe, a fault-tolerant TP serving system that sustains high performance under irregular GPU availability. FailSafe introduces three techniques to balance computation and memory across GPUs: (1) Cyclic KVCache Placement for uniform memory utilization, (2) Hybrid Attention combining tensor- and data-parallel attention to eliminate stragglers, and (3) Fine-Grained Load-Aware Routing to dynamically balance requests. It further employs proactive KVCache backup and on-demand weight recovery to avoid expensive recomputation and redundant data transfers. We implement these techniques in a lightweight serving engine compatible with existing LLM infrastructures. Evaluated on an 8xH100 DGX system with real-world fault traces and representative workloads, FailSafe achieves up to 2x higher throughput and two orders of magnitude lower recovery latency compared to standard fault handling approaches. Even with up to three GPU failures, FailSafe sustains high throughput and balanced utilization, demonstrating robust and efficient LLM serving under dynamic and unreliable hardware conditions.",
        "authors": [
            "Ziyi Xu",
            "Zhiqiang Xie",
            "Swapnil Gandhi",
            "Christos Kozyrakis"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-11-18"
    },
    "http://arxiv.org/abs/2511.14102v1": {
        "id": "http://arxiv.org/abs/2511.14102v1",
        "title": "MoE-SpeQ: Speculative Quantized Decoding with Proactive Expert Prefetching and Offloading for Mixture-of-Experts",
        "link": "http://arxiv.org/abs/2511.14102v1",
        "tags": [
            "MoE",
            "offloading",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-11-18",
        "tldr": "Addresses the I/O bottleneck of expert offloading in MoE model inference. Proposes MoE-SpeQ, a system that uses a draft model for speculative expert prefetching and an adaptive governor to overlap I/O with computation. Achieves a 2.34x speedup over the state-of-the-art offloading framework.",
        "abstract": "The immense memory requirements of state-of-the-art Mixture-of-Experts (MoE) models present a significant challenge for inference, often exceeding the capacity of a single accelerator. While offloading experts to host memory is a common solution, it introduces a severe I/O bottleneck over the PCIe bus, as the data-dependent nature of expert selection places these synchronous transfers directly on the critical path of execution, crippling performance.   This paper argues that the I/O bottleneck can be overcome by trading a small amount of cheap, on-device computation to hide the immense cost of data movement. We present MoE-SpeQ, a new inference system built on a novel co-design of speculative execution and expert offloading. MoE-SpeQ employs a small, on-device draft model to predict the sequence of required experts for future tokens. This foresight enables a runtime orchestrator to prefetch these experts from host memory, effectively overlapping the expensive I/O with useful computation and hiding the latency from the critical path. To maximize performance, an adaptive governor, guided by an Amortization Roofline Model, dynamically tunes the speculation strategy to the underlying hardware. Our evaluation on memory-constrained devices shows that for the Phi-MoE model, MoE-SpeQ achieves at most 2.34x speedup over the state-of-the-art offloading framework. Our work establishes a new, principled approach for managing data-dependent memory access in resource-limited environments, making MoE inference more accessible on commodity hardware.",
        "authors": [
            "Wenfeng Wang",
            "Jiacheng Liu",
            "Xiaofeng Hou",
            "Xinfeng Xia",
            "Peng Tang",
            "Mingxuan Zhang",
            "Chao Li",
            "Minyi Guo"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "submit_date": "2025-11-18"
    },
    "http://arxiv.org/abs/2511.13940v1": {
        "id": "http://arxiv.org/abs/2511.13940v1",
        "title": "ParallelKittens: Systematic and Practical Simplification of Multi-GPU AI Kernels",
        "link": "http://arxiv.org/abs/2511.13940v1",
        "tags": [
            "training",
            "kernel",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes ParallelKittens, a minimal CUDA framework simplifying multi-GPU kernel design for communication-efficient AI workloads. Embodies principles via reusable primitives and unified template across architectures. Achieves up to 4.08× speedup for sequence-parallel workloads on Hopper/Blackwell GPUs.",
        "abstract": "Inter-GPU communication has become a major bottleneck for modern AI workloads as models scale and improvements in hardware compute throughput outpace improvements in interconnect bandwidth. Existing systems mitigate this through compute-communication overlap but often fail to meet theoretical peak performance across heterogeneous workloads and new accelerators. Instead of operator-specific techniques, we ask whether a small set of simple, reusable principles can systematically guide the design of optimal multi-GPU kernels. We present ParallelKittens (PK), a minimal CUDA framework that drastically simplifies the development of overlapped multi-GPU kernels. PK extends the ThunderKittens framework and embodies the principles of multi-GPU kernel design through eight core primitives and a unified programming template, derived from a comprehensive analysis of the factors that govern multi-GPU performance$\\unicode{x2014}$data-transfer mechanisms, resource scheduling, and design overheads. We validate PK on both Hopper and Blackwell architectures. With fewer than 50 lines of device code, PK achieves up to $2.33 \\times$ speedup for data- and tensor-parallel workloads, $4.08 \\times$ for sequence-parallel workloads, and $1.22 \\times$ for expert-parallel workloads.",
        "authors": [
            "Stuart H. Sul",
            "Simran Arora",
            "Benjamin F. Spector",
            "Christopher Ré"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-11-17"
    },
    "http://arxiv.org/abs/2511.17593v1": {
        "id": "http://arxiv.org/abs/2511.17593v1",
        "title": "Comparative Analysis of Large Language Model Inference Serving Systems: A Performance Study of vLLM and HuggingFace TGI",
        "link": "http://arxiv.org/abs/2511.17593v1",
        "tags": [
            "serving",
            "offloading",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Empirically evaluates LLM serving systems vLLM and TGI. Compares throughput, latency, memory, and scalability using LLaMA-2 models. vLLM achieves 24× higher throughput than TGI under high concurrency via PagedAttention, while TGI offers lower tail latency for interactive use.",
        "abstract": "The deployment of Large Language Models (LLMs) in production environments requires efficient inference serving systems that balance throughput, latency, and resource utilization. This paper presents a comprehensive empirical evaluation of two prominent open-source LLM serving frameworks: vLLM and HuggingFace Text Generation Inference (TGI). We benchmark these systems across multiple dimensions including throughput performance, end-to-end latency, GPU memory utilization, and scalability characteristics using LLaMA-2 models ranging from 7B to 70B parameters. Our experiments reveal that vLLM achieves up to 24x higher throughput than TGI under high-concurrency workloads through its novel PagedAttention mechanism, while TGI demonstrates lower tail latencies for interactive single-user scenarios. We provide detailed performance profiles for different deployment scenarios and offer practical recommendations for system selection based on workload characteristics. Our findings indicate that the choice between these frameworks should be guided by specific use-case requirements: vLLM excels in high-throughput batch processing scenarios, while TGI is better suited for latency-sensitive interactive applications with moderate concurrency.",
        "authors": [
            "Saicharan Kolluru"
        ],
        "categories": [
            "cs.LG",
            "cs.DC",
            "cs.PF"
        ],
        "submit_date": "2025-11-17"
    },
    "http://arxiv.org/abs/2511.13492v1": {
        "title": "Asymptotic analysis of cooperative censoring policies in sensor networks",
        "link": "http://arxiv.org/abs/2511.13492v1",
        "abstract": "The problem of cooperative data censoring in battery-powered multihop sensor networks is analyzed in this paper. We are interested in scenarios where nodes generate messages (which are related to the sensor measurements) that can be graded with some importance value. Less important messages can be censored in order to save energy for later communications. The problem is modeled using a joint Markov Decision Process of the whole network dynamics, and a theoretically optimal censoring policy, which maximizes a long-term reward, is found. Though the optimal censoring rules are computationally prohibitive, our analysis suggests that, under some conditions, they can be approximated by a finite collection of constant-threshold rules. A centralized algorithm for the computation of these thresholds is proposed. The experimental simulations show that cooperative censoring policies are energy-efficient, and outperform other non-cooperative schemes.",
        "authors": [
            "Jesus Fernandez-Bes",
            "Rocío Arroyo-Valles",
            "Jesús Cid-Sueiro"
        ],
        "categories": [
            "cs.MA",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.13492v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-17"
    },
    "http://arxiv.org/abs/2511.13804v1": {
        "title": "Do MPI Derived Datatypes Actually Help? A Single-Node Cross-Implementation Study on Shared-Memory Communication",
        "link": "http://arxiv.org/abs/2511.13804v1",
        "abstract": "MPI's derived datatypes (DDTs) promise easier, copy-free communication of non-contiguous data, yet their practical performance remains debated and is often reported only for a single MPI stack. We present a cross-implementation assessment using three 2D applications: a Jacobi CFD solver, Conway's Game of Life, and a lattice-based image reconstruction. Each application is written in two ways: (i) a BASIC version with manual packing and unpacking of non-contiguous regions and (ii) a DDT version using MPI_Type_vector and MPI_Type_create_subarray with correct true extent via MPI_Type_create_resized. For API parity, we benchmark identical communication semantics: non-blocking point-to-point (Irecv/Isend + Waitall), neighborhood collectives (MPI_Neighbor_alltoallw), and MPI-4 persistent operations (*_init). We run strong and weak scaling on 1-4 ranks, validate bitwise-identical halos, and evaluate four widely used MPI implementations: MPICH, Open MPI, Intel MPI, and MVAPICH2 on a single ARCHER2 node. Results are mixed. DDTs can be fastest, for example for the image reconstruction code on Intel MPI and MPICH, but can also be among the slowest on other stacks, such as Open MPI and MVAPICH2 for the same code. For the CFD solver, BASIC variants generally outperform DDTs across semantics, whereas for Game of Life the ranking flips depending on the MPI library. We also observe stack-specific anomalies, for example MPICH slowdowns with DDT neighborhood and persistent modes. Overall, no strategy dominates across programs, semantics, and MPI stacks; performance portability for DDTs is not guaranteed. We therefore recommend profiling both DDT-based and manual-packing designs under the intended MPI implementation and communication mode. Our study is limited to a single node and does not analyze memory overhead; multi-node and GPU-aware paths are left for future work.",
        "authors": [
            "Temitayo Adefemi"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.13804v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-17"
    },
    "http://arxiv.org/abs/2511.13365v1": {
        "title": "InfoDecom: Decomposing Information for Defending against Privacy Leakage in Split Inference",
        "link": "http://arxiv.org/abs/2511.13365v1",
        "abstract": "Split inference (SI) enables users to access deep learning (DL) services without directly transmitting raw data. However, recent studies reveal that data reconstruction attacks (DRAs) can recover the original inputs from the smashed data sent from the client to the server, leading to significant privacy leakage. While various defenses have been proposed, they often result in substantial utility degradation, particularly when the client-side model is shallow. We identify a key cause of this trade-off: existing defenses apply excessive perturbation to redundant information in the smashed data. To address this issue in computer vision tasks, we propose InfoDecom, a defense framework that first decomposes and removes redundant information and then injects noise calibrated to provide theoretically guaranteed privacy. Experiments demonstrate that InfoDecom achieves a superior utility-privacy trade-off compared to existing baselines. The code and the appendix are available at https://github.com/SASA-cloud/InfoDecom.",
        "authors": [
            "Ruijun Deng",
            "Zhihui Lu",
            "Qiang Duan"
        ],
        "categories": [
            "cs.CR",
            "cs.AI",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.13365v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-17"
    },
    "http://arxiv.org/abs/2511.13313v2": {
        "id": "http://arxiv.org/abs/2511.13313v2",
        "title": "Distributed Hierarchical Machine Learning for Joint Resource Allocation and Slice Selection in In-Network Edge Systems",
        "link": "http://arxiv.org/abs/2511.13313v2",
        "tags": [
            "edge",
            "training",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes a distributed hierarchical model (DeepSets-S) for joint resource allocation and slice selection in edge systems. Uses a DeepSets architecture with slack-aware normalization and task-specific decoders for efficient online inference. Reduces execution time by 86.1% while maintaining within 6.1% of optimal cost.",
        "abstract": "The Metaverse promises immersive, real-time experiences; however, meeting its stringent latency and resource demands remains a major challenge. Conventional optimization techniques struggle to respond effectively under dynamic edge conditions and high user loads. In this study, we explore a slice-enabled in-network edge architecture that combines computing-in-the-network (COIN) with multi-access edge computing (MEC). In addition, we formulate the joint problem of wireless and computing resource management with optimal slice selection as a mixed-integer nonlinear program (MINLP). Because solving this model online is computationally intensive, we decompose it into three sub-problems (SP1) intra-slice allocation, (SP2) inter-slice allocation, and (SP3) offloading decision and train a distributed hierarchical DeepSets-based model (DeepSets-S) on optimal solutions obtained offline. In the proposed model, we design a slack-aware normalization mechanism for a shared encoder and task-specific decoders, ensuring permutation equivariance over variable-size wireless device (WD) sets. The learned system produces near-optimal allocations with low inference time and maintains permutation equivariance over variable-size device sets. Our experimental results show that DeepSets-S attains high tolerance-based accuracies on SP1/SP2 (Acc1 = 95.26% and 95.67%) and improves multiclass offloading accuracy on SP3 (Acc = 0.7486; binary local/offload Acc = 0.8824). Compared to exact solvers, the proposed approach reduces the execution time by 86.1%, while closely tracking the optimal system cost (within 6.1% in representative regimes). Compared with baseline models, DeepSets-S consistently achieves higher cost ratios and better utilization across COIN/MEC resources.",
        "authors": [
            "Sulaiman Muhammad Rashid",
            "Ibrahim Aliyu",
            "Jaehyung Park",
            "Jinsul Kim"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-11-17"
    },
    "http://arxiv.org/abs/2511.13253v1": {
        "id": "http://arxiv.org/abs/2511.13253v1",
        "title": "Pico-Cloud: Cloud Infrastructure for Tiny Edge Devices",
        "link": "http://arxiv.org/abs/2511.13253v1",
        "tags": [
            "edge",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes Pico-Cloud, a micro-edge cloud architecture for ultra-minimal hardware like Raspberry Pi Zero. Features container virtualization and lightweight orchestration for local low-latency workloads, including edge AI inference. Achieves cost-effective and decentralized operation for lightweight distributed applications.",
        "abstract": "This paper introduces the Pico-Cloud, a micro-edge cloud architecture built on ultra-minimal hardware platforms such as the Raspberry Pi Zero and comparable single-board computers. The Pico-Cloud delivers container-based virtualization, service discovery, and lightweight orchestration directly at the device layer, enabling local operation with low latency and low power consumption without reliance on centralized data centers. We present its architectural model, outline representative use cases including rural connectivity, educational clusters, and edge AI inference, and analyze design challenges in computation, networking, storage, and power management. The results highlight Pico-Clouds as a cost-effective, decentralized, and sustainable platform for lightweight distributed workloads at the network edge.",
        "authors": [
            "Mordechai Guri"
        ],
        "categories": [
            "cs.DC",
            "cs.SE"
        ],
        "submit_date": "2025-11-17"
    },
    "http://arxiv.org/abs/2511.13155v1": {
        "id": "http://arxiv.org/abs/2511.13155v1",
        "title": "Learning Process Energy Profiles from Node-Level Power Data",
        "link": "http://arxiv.org/abs/2511.13155v1",
        "tags": [
            "training",
            "storage",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes a method to model per-process energy consumption using synchronized node-level power data and process-level resource metrics via regression. Enables fine-grained energy prediction, improving efficiency monitoring for AI workloads over coarse tools like RAPL.",
        "abstract": "The growing demand for data center capacity, driven by the growth of high-performance computing, cloud computing, and especially artificial intelligence, has led to a sharp increase in data center energy consumption. To improve energy efficiency, gaining process-level insights into energy consumption is essential. While node-level energy consumption data can be directly measured with hardware such as power meters, existing mechanisms for estimating per-process energy usage, such as Intel RAPL, are limited to specific hardware and provide only coarse-grained, domain-level measurements. Our proposed approach models per-process energy profiles by leveraging fine-grained process-level resource metrics collected via eBPF and perf, which are synchronized with node-level energy measurements obtained from an attached power distribution unit. By statistically learning the relationship between process-level resource usage and node-level energy consumption through a regression-based model, our approach enables more fine-grained per-process energy predictions.",
        "authors": [
            "Jonathan Bader",
            "Julius Irion",
            "Jannis Kappel",
            "Joel Witzke",
            "Niklas Fomin",
            "Diellza Sherifi",
            "Odej Kao"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-11-17"
    },
    "http://arxiv.org/abs/2511.13061v1": {
        "id": "http://arxiv.org/abs/2511.13061v1",
        "title": "MACKO: Sparse Matrix-Vector Multiplication for Low Sparsity",
        "link": "http://arxiv.org/abs/2511.13061v1",
        "tags": [
            "kernel",
            "inference",
            "sparse"
        ],
        "relevant": true,
        "indexed_date": "2025-11-17",
        "tldr": "Proposes MACKO-SpMV, a GPU-optimized kernel and storage format for efficient sparse matrix-vector multiplication at low (30-90%) sparsity levels common in pruned LLMs. Applied to Llama2-7B, it achieves 1.5x memory reduction and 1.5x faster inference at 50% sparsity compared to a dense baseline.",
        "abstract": "Sparse Matrix-Vector Multiplication (SpMV) is a fundamental operation in the inference of sparse Large Language Models (LLMs). Because existing SpMV methods perform poorly under the low and unstructured sparsity (30-90%) commonly observed in pruned LLMs, unstructured pruning provided only limited memory reduction and speedup. We propose MACKO-SpMV, a GPU-optimized format and kernel co-designed to reduce storage overhead while preserving compatibility with the GPU's execution model. This enables efficient SpMV for unstructured sparsity without specialized hardware units (e.g., tensor cores) or format-specific precomputation. Empirical results show that at sparsity 50%, MACKO is the first approach with significant 1.5x memory reduction and 1.2-1.5x speedup over dense representation. Speedups over other SpMV baselines: 2.8-13.0x over cuSPARSE, 1.9-2.6x over Sputnik, and 2.2-2.5x over DASP. Applied to Llama2-7B pruned with Wanda to sparsity 50%, it delivers 1.5x memory reduction and 1.5x faster inference at fp16 precision. Thanks to MACKO, unstructured pruning at 50% sparsity is now justified in real-world LLM workloads.",
        "authors": [
            "Vladimír Macko",
            "Vladimír Boža"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC",
            "cs.DS"
        ],
        "submit_date": "2025-11-17"
    },
    "http://arxiv.org/abs/2511.12869v1": {
        "title": "On the Fundamental Limits of LLMs at Scale",
        "link": "http://arxiv.org/abs/2511.12869v1",
        "abstract": "Large Language Models (LLMs) have benefited enormously from scaling, yet these gains are bounded by five fundamental limitations: (1) hallucination, (2) context compression, (3) reasoning degradation, (4) retrieval fragility, and (5) multimodal misalignment. While existing surveys describe these phenomena empirically, they lack a rigorous theoretical synthesis connecting them to the foundational limits of computation, information, and learning. This work closes that gap by presenting a unified, proof-informed framework that formalizes the innate theoretical ceilings of LLM scaling. First, computability and uncomputability imply an irreducible residue of error: for any computably enumerable model family, diagonalization guarantees inputs on which some model must fail, and undecidable queries (e.g., halting-style tasks) induce infinite failure sets for all computable predictors. Second, information-theoretic and statistical constraints bound attainable accuracy even on decidable tasks, finite description length enforces compression error, and long-tail factual knowledge requires prohibitive sample complexity. Third, geometric and computational effects compress long contexts far below their nominal size due to positional under-training, encoding attenuation, and softmax crowding. We further show how likelihood-based training favors pattern completion over inference, how retrieval under token limits suffers from semantic drift and coupling noise, and how multimodal scaling inherits shallow cross-modal alignment. Across sections, we pair theorems and empirical evidence to outline where scaling helps, where it saturates, and where it cannot progress, providing both theoretical foundations and practical mitigation paths like bounded-oracle retrieval, positional curricula, and sparse or hierarchical attention.",
        "authors": [
            "Muhammad Ahmed Mohsin",
            "Muhammad Umer",
            "Ahsan Bilal",
            "Zeeshan Memon",
            "Muhammad Ibtsaam Qadir",
            "Sagnik Bhattacharya",
            "Hassan Rizwan",
            "Abhiram R. Gorle",
            "Maahe Zehra Kazmi",
            "Ayesha Mohsin",
            "Muhammad Usman Rafique",
            "Zihao He",
            "Pulkit Mehta",
            "Muhammad Ali Jamshed",
            "John M. Cioffi"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC",
            "cs.IT",
            "cs.MA"
        ],
        "id": "http://arxiv.org/abs/2511.12869v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-17"
    },
    "http://arxiv.org/abs/2511.12695v1": {
        "title": "A Closer Look at Personalized Fine-Tuning in Heterogeneous Federated Learning",
        "link": "http://arxiv.org/abs/2511.12695v1",
        "abstract": "Federated Learning (FL) enables decentralized, privacy-preserving model training but struggles to balance global generalization and local personalization due to non-identical data distributions across clients. Personalized Fine-Tuning (PFT), a popular post-hoc solution, fine-tunes the final global model locally but often overfits to skewed client distributions or fails under domain shifts. We propose adapting Linear Probing followed by full Fine-Tuning (LP-FT), a principled centralized strategy for alleviating feature distortion (Kumar et al., 2022), to the FL setting. Through systematic evaluation across seven datasets and six PFT variants, we demonstrate LP-FT's superiority in balancing personalization and generalization. Our analysis uncovers federated feature distortion, a phenomenon where local fine-tuning destabilizes globally learned features, and theoretically characterizes how LP-FT mitigates this via phased parameter updates. We further establish conditions (e.g., partial feature overlap, covariate-concept shift) under which LP-FT outperforms standard fine-tuning, offering actionable guidelines for deploying robust personalization in FL.",
        "authors": [
            "Minghui Chen",
            "Hrad Ghoukasian",
            "Ruinan Jin",
            "Zehua Wang",
            "Sai Praneeth Karimireddy",
            "Xiaoxiao Li"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC",
            "stat.ML"
        ],
        "id": "http://arxiv.org/abs/2511.12695v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-16"
    },
    "http://arxiv.org/abs/2511.12687v1": {
        "title": "The Time to Consensus in a Blockchain: Insights into Bitcoin's \"6 Blocks Rule''",
        "link": "http://arxiv.org/abs/2511.12687v1",
        "abstract": "We investigate the time to consensus in Nakamoto blockchains. Specifically, we consider two competing growth processes, labeled \\emph{honest} and \\emph{adversarial}, and determine the time after which the honest process permananetly exceeds the adversarial process. This is done via queueing techniques. The predominant difficulty is that the honest growth process is subject to \\emph{random delays}. In a stylized Bitcoin model, we compute the Laplace transform for the time to consensus and verify it via simulation.",
        "authors": [
            "Partha S. Dey",
            "Aditya S. Gopalan",
            "Vijay G. Subramanian"
        ],
        "categories": [
            "cs.DC",
            "math.PR"
        ],
        "id": "http://arxiv.org/abs/2511.12687v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-16"
    },
    "http://arxiv.org/abs/2511.12667v1": {
        "title": "Artifact for A Non-Intrusive Framework for Deferred Integration of Cloud Patterns in Energy-Efficient Data-Sharing Pipelines",
        "link": "http://arxiv.org/abs/2511.12667v1",
        "abstract": "As data mesh architectures grow, organizations increasingly build consumer-specific data-sharing pipelines from modular, cloud-based transformation services. While reusable transformation services can improve cost and energy efficiency, applying traditional cloud design patterns can reduce reusability of services in different pipelines. We present a Kubernetes-based tool that enables non-intrusive, deferred application of design patterns without modifying services code. The tool automates pattern injection and collects energy metrics, supporting energy-aware decisions while preserving reusability of transformation services in various pipeline structures.",
        "authors": [
            "Sepideh Masoudi",
            "Mark Edward Michael Daly",
            "Jannis Kiesel"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.12667v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-16"
    },
    "http://arxiv.org/abs/2511.12617v1": {
        "title": "QPU Micro-Kernels for Stencil Computation",
        "link": "http://arxiv.org/abs/2511.12617v1",
        "abstract": "We introduce QPU micro-kernels: shallow quantum circuits that perform a stencil node update and return a Monte Carlo estimate from repeated measurements. We show how to use them to solve Partial Differential Equations (PDEs) explicitly discretized on a computational stencil. From this point of view, the QPU serves as a sampling accelerator. Each micro-kernel consumes only stencil inputs (neighbor values and coefficients), runs a shallow parameterized circuit, and reports the sample mean of a readout rule. The resource footprint in qubits and depth is fixed and independent of the global grid. This makes micro-kernels easy to orchestrate from a classical host and to parallelize across grid points. We present two realizations. The Bernoulli micro-kernel targets convex-sum stencils by encoding values as single-qubit probabilities with shot allocation proportional to stencil weights. The branching micro-kernel prepares a selector over stencil branches and applies addressed rotations to a single readout qubit. In contrast to monolithic quantum PDE solvers that encode the full space-time problem in one deep circuit, our approach keeps the classical time loop and offloads only local updates. Batching and in-circuit fusion amortize submission and readout overheads. We test and validate the QPU micro-kernel method on two PDEs commonly arising in scientific computing: the Heat and viscous Burgers' equations. On noiseless quantum circuit simulators, accuracy improves as the number of samples increases. On the IBM Brisbane quantum computer, single-step diffusion tests show lower errors for the Bernoulli realization than for branching at equal shot budgets, with QPU micro-kernel execution dominating the wall time.",
        "authors": [
            "Stefano Markidis",
            "Luca Pennati",
            "Marco Pasquale",
            "Gilbert Netzer",
            "Ivy Peng"
        ],
        "categories": [
            "cs.ET",
            "cs.DC",
            "quant-ph"
        ],
        "id": "http://arxiv.org/abs/2511.12617v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-16"
    },
    "http://arxiv.org/abs/2511.19445v1": {
        "title": "Asynchronous Cooperative Optimization of a Capacitated Vehicle Routing Problem Solution",
        "link": "http://arxiv.org/abs/2511.19445v1",
        "abstract": "We propose a parallel shared-memory schema to cooperatively optimize the solution of a Capacitated Vehicle Routing Problem instance with minimal synchronization effort and without the need for an explicit decomposition. To this end, we design FILO2$^x$ as a single-trajectory parallel adaptation of the FILO2 algorithm originally proposed for extremely large-scale instances and described in Accorsi and Vigo (2024). Using the locality of the FILO2 optimization applications, in FILO2$^x$ several possibly unrelated solution areas are concurrently asynchronously optimized. The overall search trajectory emerges as an iteration-based parallelism obtained by the simultaneous optimization of the same underlying solution performed by several solvers. Despite the high efficiency exhibited by the single-threaded FILO2 algorithm, the computational results show that, by better exploiting the available computing resources, FILO2$^x$ can greatly enhance the resolution time compared to the original approach, still maintaining a similar final solution quality for instances ranging from hundreds to hundreds of thousands customers.",
        "authors": [
            "Luca Accorsi",
            "Demetrio Laganà",
            "Federico Michelotto",
            "Roberto Musmanno",
            "Daniele Vigo"
        ],
        "categories": [
            "cs.DC",
            "cs.DM"
        ],
        "id": "http://arxiv.org/abs/2511.19445v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-16"
    },
    "http://arxiv.org/abs/2511.12500v1": {
        "id": "http://arxiv.org/abs/2511.12500v1",
        "title": "Iris: First-Class Multi-GPU Programming Experience in Triton",
        "link": "http://arxiv.org/abs/2511.12500v1",
        "tags": [
            "kernel",
            "training"
        ],
        "relevant": true,
        "indexed_date": "2025-11-16",
        "tldr": "Presents Iris, a multi-GPU communication library implemented in Python/Triton to simplify and optimize compute-communication overlap. Introduces tile-based symmetric memory abstractions for single-source kernels. Achieves up to 1.79x speedup over PyTorch and RCCL in GEMM+All-Scatter workloads.",
        "abstract": "Multi-GPU programming traditionally requires developers to navigate complex trade-offs between performance and programmability. High-performance implementations typically rely on low-level HIP/CUDA communication libraries that demand substantial engineering effort for even basic overlap patterns, while simpler abstractions often sacrifice performance. We present Iris, a multi-GPU communication library implemented entirely in Python and Triton that eliminates this trade-off. Iris provides tile-based symmetric memory abstractions that naturally align with Triton's programming model, enabling developers to write single-source kernels that seamlessly interleave computation and communication. We demonstrate a taxonomy of compute-communication overlap patterns--from bulk-synchronous to fine-grained workgroup specialization--that can be implemented with minimal code changes in Iris, often requiring just a few additional lines within the same Triton kernel. Our evaluation shows that Iris achieves near-optimal bandwidth utilization in microbenchmarks and delivers up to 1.79x speedup over PyTorch and RCCL for GEMM+All-Scatter workloads, demonstrating that high-level implementations can match or exceed heavily-optimized libraries while dramatically simplifying multi-GPU programming.",
        "authors": [
            "Muhammad Awad",
            "Muhammad Osama",
            "Brandon Potter"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-11-16"
    },
    "http://arxiv.org/abs/2511.12486v1": {
        "title": "A Decentralized Root Cause Localization Approach for Edge Computing Environments",
        "link": "http://arxiv.org/abs/2511.12486v1",
        "abstract": "Edge computing environments host increasingly complex microservice-based IoT applications, which are prone to performance anomalies that can propagate across dependent services. Identifying the true source of such anomalies, known as Root Cause Localization (RCL), is essential for timely mitigation. However, existing RCL approaches are designed for cloud environments and rely on centralized analysis, which increases latency and communication overhead when applied at the edge. This paper proposes a decentralized RCL approach that executes localization directly at the edge device level using the Personalized PageRank (PPR) algorithm. The proposed method first groups microservices into communication- and colocation-aware clusters, thereby confining most anomaly propagation within cluster boundaries. Within each cluster, PPR is executed locally to identify the root cause, significantly reducing localization time. For the rare cases where anomalies propagate across clusters, we introduce an inter-cluster peer-to-peer approximation process, enabling lightweight coordination among clusters with minimal communication overhead. To enhance the accuracy of localization in heterogeneous edge environments, we also propose a novel anomaly scoring mechanism tailored to the diverse anomaly triggers that arise across microservice, device, and network layers. Evaluation results on the publicly available edge dataset, MicroCERCL, demonstrate that the proposed decentralized approach achieves comparable or higher localization accuracy than its centralized counterpart while reducing localization time by up to 34%. These findings highlight that decentralized graph-based RCL can provide a practical and efficient solution for anomaly diagnosis in resource-constrained edge environments.",
        "authors": [
            "Duneesha Fernando",
            "Maria A. Rodriguez",
            "Rajkumar Buyya"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.12486v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-16"
    },
    "http://arxiv.org/abs/2511.12461v3": {
        "title": "Design of A Low-Latency and Parallelizable SVD Dataflow Architecture on FPGA",
        "link": "http://arxiv.org/abs/2511.12461v3",
        "abstract": "Singular value decomposition (SVD) is widely used for dimensionality reduction and noise suppression, and it plays a pivotal role in numerous scientific and engineering applications. As the dimensions of the matrix grow rapidly, the computational cost increases significantly, posing a serious challenge to the efficiency of data analysis and signal processing systems, especially in time-sensitive scenarios involving large-scale datasets. Although various dedicated hardware architectures have been proposed to accelerate the computation of intensive SVD, many of these designs suffer from limited scalability and high consumption of on-chip memory resources. Moreover, they typically overlook the computational and data transfer challenges associated with SVD, making them unsuitable for real-time processing of large-scale data stream matrices in embedded systems. In this paper, we propose a Data Stream-Based SVD processing algorithm (DSB Jacobi), which significantly reduces on-chip BRAM usage while improving computational speed, offering a practical solution for real-time SVD computation of large-scale data streams. Compared to previous works, our experimental results indicate that the proposed method reduces on-chip RAM consumption by 41.5 percent and improves computational efficiency by a factor of 23.",
        "authors": [
            "Fangqiang Du",
            "Sixuan Chong",
            "Zixuan Huang",
            "Rui Qin",
            "Fengnan Mi",
            "Caibao Hu",
            "Jiangang Chen"
        ],
        "categories": [
            "cs.DC",
            "eess.SP"
        ],
        "id": "http://arxiv.org/abs/2511.12461v3",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-16"
    },
    "http://arxiv.org/abs/2511.12457v1": {
        "title": "SEE++: Evolving Snowpark Execution Environment for Modern Workloads",
        "link": "http://arxiv.org/abs/2511.12457v1",
        "abstract": "Snowpark enables Data Engineering and AI/ML workloads to run directly within Snowflake by deploying a secure sandbox on virtual warehouse nodes. This Snowpark Execution Environment (SEE) allows users to execute arbitrary workloads in Python and other languages in a secure and performant manner. As adoption has grown, the diversity of workloads has introduced increasingly sophisticated needs for sandboxing. To address these evolving requirements, Snowpark transitioned its in-house sandboxing solution to gVisor, augmented with targeted optimizations. This paper describes both the functional and performance objectives that guided the upgrade, outlines the new sandbox architecture, and details the challenges encountered during the journey, along with the solutions developed to resolve them. Finally, we present case studies that highlight new features enabled by the upgraded architecture, demonstrating SEE's extensibility and flexibility in supporting the next generation of Snowpark workloads.",
        "authors": [
            "Gaurav Jain",
            "Brandon Baker",
            "Joe Yin",
            "Chenwei Xie",
            "Zihao Ye",
            "Sidh Kulkarni",
            "Sara Abdelrahman",
            "Nova Qi",
            "Urjeet Shrestha",
            "Mike Halcrow",
            "Dave Bailey",
            "Yuxiong He"
        ],
        "categories": [
            "cs.DB",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.12457v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-16"
    },
    "http://arxiv.org/abs/2511.13779v1": {
        "id": "http://arxiv.org/abs/2511.13779v1",
        "title": "Semantic Multiplexing",
        "link": "http://arxiv.org/abs/2511.13779v1",
        "tags": [
            "edge",
            "networking",
            "multi-modal"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes Semantic Multiplexing to offload parallel tasks by merging compressed task representations for edge devices. Shifts multiplexing from bits to tasks, enabling more concurrent tasks than physical channels. Reduces latency by 8× and energy consumption by 25× while maintaining accuracy within 4% drop for 8 tasks over 4×4 channel.",
        "abstract": "Mobile devices increasingly require the parallel execution of several computing tasks offloaded at the wireless edge. Existing communication systems only support parallel transmissions at the bit level, which fundamentally limits the number of tasks that can be concurrently processed. To address this bottleneck, this paper introduces the new concept of Semantic Multiplexing. Our approach shifts stream multiplexing from bits to tasks by merging multiple task-related compressed representations into a single semantic representation. As such, Semantic Multiplexing can multiplex more tasks than the number of physical channels without adding antennas or widening bandwidth by extending the effective degrees of freedom at the semantic layer, without contradicting Shannon capacity rules. We have prototyped Semantic Multiplexing on an experimental testbed with Jetson Orin Nano and millimeter-wave software-defined radios and tested its performance on image classification and sentiment analysis while comparing to several existing baselines in semantic communications. Our experiments demonstrate that Semantic Multiplexing allows jointly processing multiple tasks at the semantic level while maintaining sufficient task accuracy. For example, image classification accuracy drops by less than 4% when increasing from 2 to 8 the number of tasks multiplexed over a 4$\\times$4 channel. Semantic Multiplexing reduces latency, energy consumption, and communication load respectively by up to 8$\\times$, 25$\\times$, and 54$\\times$ compared to the baselines while keeping comparable performance. We pledge to publicly share the complete software codebase and the collected datasets for reproducibility.",
        "authors": [
            "Mohammad Abdi",
            "Francesca Meneghello",
            "Francesco Restuccia"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.LG",
            "cs.NI",
            "eess.IV"
        ],
        "submit_date": "2025-11-16"
    },
    "http://arxiv.org/abs/2511.13778v1": {
        "id": "http://arxiv.org/abs/2511.13778v1",
        "title": "Guaranteed DGEMM Accuracy While Using Reduced Precision Tensor Cores Through Extensions of the Ozaki Scheme",
        "link": "http://arxiv.org/abs/2511.13778v1",
        "tags": [
            "kernel",
            "quantization"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes Automatic Dynamic Precision (ADP) for emulating FP64 accuracy using low-precision Tensor Cores. ESC estimator determines decomposition parameters, with exception handling and fallback to FP64. Achieves up to 13.2x speedup over native FP64 GEMM while preserving accuracy with <10% overhead.",
        "abstract": "The rapid growth of artificial intelligence (AI) has made low-precision formats such as FP16, FP8, and, most recently, block-scaled FP4 the primary focus of modern GPUs, where Tensor Cores now deliver orders-of-magnitude higher throughput than traditional FP64 pipelines. This hardware shift has sparked a new line of algorithm research: using low-precision units to emulate double-precision accuracy through schemes such as Ozaki decompositions. We advance this direction with Automatic Dynamic Precision (ADP), a fully GPU-resident framework that makes emulated FP64 matrix multiplication both efficient and reliable. At its core is the Exponent Span Capacity (ESC), a hardware-agnostic estimator that conservatively determines the decomposition parameter (also known as slices) required to achieve FP64-level accuracy. Built on ESC, ADP integrates exception handling, run time heuristics, and seamless fallback to native FP64, ensuring correctness without host-device synchronization or user intervention. Additionally, we further improve Ozaki-style decompositions with an unsigned integer slicing scheme, which increases representational efficiency and reduces computational waste. Validated against recently proposed BLAS grading tests, ADP consistently preserves FP64 fidelity on challenging inputs while incurring less than 10% run time overhead. In a 55-bit mantissa setting, our approach achieves up to 2.3x and 13.2x speedups over native FP64 GEMM on NVIDIA Blackwell GB200 and the RTX Pro 6000 Blackwell Server Edition, respectively. Our results demonstrate that low-precision accelerators can serve as a practical, production-ready foundation for high-fidelity and high-performance scientific computing workloads.",
        "authors": [
            "Angelika Schwarz",
            "Anton Anders",
            "Cole Brower",
            "Harun Bayraktar",
            "John Gunnels",
            "Kate Clark",
            "RuQing G. Xu",
            "Samuel Rodriguez",
            "Sebastien Cayrols",
            "Paweł Tabaszewski",
            "Victor Podlozhnyuk"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-11-16"
    },
    "http://arxiv.org/abs/2511.17580v1": {
        "title": "A novel strategy for multi-resource load balancing in agent-based systems",
        "link": "http://arxiv.org/abs/2511.17580v1",
        "abstract": "The paper presents a multi-resource load balancing strategy which can be utilised within an agent-based system. This approach can assist system designers in their attempts to optimise the structure for complex enterprise architectures. In this system, the social behaviour of the agent and its adaptation abilities are applied to determine an optimal setup for a given configuration. All the methods have been developed to allow the agent's self-assessment. The proposed agent system has been implemented and the experiment results are presented here.",
        "authors": [
            "Leszek Sliwko",
            "Aleksander Zgrzywa"
        ],
        "categories": [
            "cs.MA",
            "cs.AI",
            "cs.DC",
            "cs.SE"
        ],
        "id": "http://arxiv.org/abs/2511.17580v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-15"
    },
    "http://arxiv.org/abs/2511.12216v1": {
        "title": "Distributed Seasonal Temporal Pattern Mining",
        "link": "http://arxiv.org/abs/2511.12216v1",
        "abstract": "The explosive growth of IoT-enabled sensors is producing enormous amounts of time series data across many domains, offering valuable opportunities to extract insights through temporal pattern mining. Among these patterns, an important class exhibits periodic occurrences, referred to as \\textit{seasonal temporal patterns} (STPs). However, mining STPs poses challenges, as traditional measures such as support and confidence cannot capture seasonality, and the lack of the anti-monotonicity property results in an exponentially large search space. Existing STP mining methods operate sequentially and therefore do not scale to large datasets. In this paper, we propose the Distributed Seasonal Temporal Pattern Mining (DSTPM), the first distributed framework for mining seasonal temporal patterns from time series. DSTPM leverages efficient data structures, specifically distributed hierarchical lookup hash structures, to enable efficient computation. Extensive experimental evaluations demonstrate that DSTPM significantly outperforms sequential baselines in runtime and memory usage, while scaling effectively to very large datasets.",
        "authors": [
            "Van Ho-Long",
            "Nguyen Ho",
            "Anh-Vu Dinh-Duc",
            "Ha Manh Tran",
            "Ky Trung Nguyen",
            "Tran Dung Pham",
            "Quoc Viet Hung Nguyen"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.12216v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-15"
    },
    "http://arxiv.org/abs/2511.12185v2": {
        "id": "http://arxiv.org/abs/2511.12185v2",
        "title": "Combining Serverless and High-Performance Computing Paradigms to support ML Data-Intensive Applications",
        "link": "http://arxiv.org/abs/2511.12185v2",
        "tags": [
            "serverless",
            "storage",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes Cylon, a high-performance distributed data frame solution using TCP hole punching for serverless environments. Enables direct communication between functions to speed up machine learning data pipelines. Achieves performance close to EC2/HPC clusters.",
        "abstract": "Data is found everywhere, from health and human infrastructure to the surge of sensors and the proliferation of internet-connected devices. To meet this challenge, the data engineering field has expanded significantly in recent years in both research and industry. Traditionally, data engineering, Machine Learning, and AI workloads have been run on large clusters within data center environments, requiring substantial investment in hardware and maintenance. With the rise of the public cloud, it is now possible to run large applications across nodes without owning or maintaining hardware. Serverless functions such as AWS Lambda provide horizontal scaling and precise billing without the hassle of managing traditional cloud infrastructure. However, when processing large datasets, users often rely on external storage options that are significantly slower than direct communication typical of HPC clusters. We introduce Cylon, a high-performance distributed data frame solution that has shown promising results for data processing using Python. We describe how we took inspiration from the FMI library and designed a serverless communicator to tackle communication and performance issues associated with serverless functions. With our design, we demonstrate that the performance of AWS Lambda falls below one percent of strong scaling experiments compared to serverful AWS (EC2) and HPCs based on implementing direct communication via NAT Traversal TCP Hole Punching.",
        "authors": [
            "Mills Staylor",
            "Arup Kumar Sarker",
            "Gregor von Laszewski",
            "Geoffrey Fox",
            "Yue Cheng",
            "Judy Fox"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-11-15"
    },
    "http://arxiv.org/abs/2511.12056v1": {
        "id": "http://arxiv.org/abs/2511.12056v1",
        "title": "PipeDiT: Accelerating Diffusion Transformers in Video Generation with Task Pipelining and Model Decoupling",
        "link": "http://arxiv.org/abs/2511.12056v1",
        "tags": [
            "video",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-11-15",
        "tldr": "Proposes PipeDiT, a pipelining framework to reduce inference latency and memory consumption for DiT-based video generation. Key innovations include pipeline sequence parallelism, decoupling diffusion/VAE modules, and attention co-processing. Achieves 1.06x-4.02x speedups over baseline frameworks.",
        "abstract": "Video generation has been advancing rapidly, and diffusion transformer (DiT) based models have demonstrated remark- able capabilities. However, their practical deployment is of- ten hindered by slow inference speeds and high memory con- sumption. In this paper, we propose a novel pipelining frame- work named PipeDiT to accelerate video generation, which is equipped with three main innovations. First, we design a pipelining algorithm (PipeSP) for sequence parallelism (SP) to enable the computation of latent generation and commu- nication among multiple GPUs to be pipelined, thus reduc- ing inference latency. Second, we propose DeDiVAE to de- couple the diffusion module and the variational autoencoder (VAE) module into two GPU groups, whose executions can also be pipelined to reduce memory consumption and infer- ence latency. Third, to better utilize the GPU resources in the VAE group, we propose an attention co-processing (Aco) method to further reduce the overall video generation latency. We integrate our PipeDiT into both OpenSoraPlan and Hun- yuanVideo, two state-of-the-art open-source video generation frameworks, and conduct extensive experiments on two 8- GPU systems. Experimental results show that, under many common resolution and timestep configurations, our PipeDiT achieves 1.06x to 4.02x speedups over OpenSoraPlan and HunyuanVideo.",
        "authors": [
            "Sijie Wang",
            "Qiang Wang",
            "Shaohuai Shi"
        ],
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.DC"
        ],
        "submit_date": "2025-11-15"
    },
    "http://arxiv.org/abs/2511.12031v1": {
        "id": "http://arxiv.org/abs/2511.12031v1",
        "title": "Striking the Right Balance between Compute and Copy: Improving LLM Inferencing Under Speculative Decoding",
        "link": "http://arxiv.org/abs/2511.12031v1",
        "tags": [
            "serving",
            "kernel",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-11-15",
        "tldr": "Proposes BMC, a KV cache allocation method balancing memory copies and compute redundancy for LLM inference. It pre-allocates tensors with extra rows for in-place updates, repurposing redundancy for speculative decoding. Achieves up to 3.2x throughput acceleration over baseline HuggingFace.",
        "abstract": "With the skyrocketing costs of GPUs and their virtual instances in the cloud, there is a significant desire to use CPUs for large language model (LLM) inference. KV cache update, often implemented as allocation, copying, and in-place strided update for each generated token, incurs significant overhead. As the sequence length increases, the allocation and copy overheads dominate the performance. Alternate approaches may allocate large KV tensors upfront to enable in-place updates, but these matrices (with zero-padded rows) cause redundant computations. In this work, we propose a new KV cache allocation mechanism called Balancing Memory and Compute (BMC). BMC allocates, once every r iterations, KV tensors with r redundant rows, allowing in-place update without copy overhead for those iterations, but at the expense of a small amount of redundant computation. Second, we make an interesting observation that the extra rows allocated in the KV tensors and the resulting redundant computation can be repurposed for Speculative Decoding (SD) that improves token generation efficiency. Last, BMC represents a spectrum of design points with different values of r. To identify the best-performing design point(s), we derive a simple analytical model for BMC. The proposed BMC method achieves an average throughput acceleration of up to 3.2x over baseline HuggingFace (without SD). Importantly when we apply BMC with SD, it results in an additional speedup of up to 1.39x, over and above the speedup offered by SD. Further, BMC achieves a throughput acceleration of up to 1.36x and 2.29x over state-of-the-art inference servers vLLM and DeepSpeed, respectively. Although the BMC technique is evaluated extensively across different classes of CPUs (desktop and server class), we also evaluate the scheme with GPUs and demonstrate that it works well for GPUs.",
        "authors": [
            "Arun Ramachandran",
            "Ramaswamy Govindarajan",
            "Murali Annavaram",
            "Prakash Raghavendra",
            "Hossein Entezari Zarch",
            "Lei Gao",
            "Chaoyi Jiang"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-11-15"
    },
    "http://arxiv.org/abs/2511.12025v1": {
        "title": "A Quick and Exact Method for Distributed Quantile Computation",
        "link": "http://arxiv.org/abs/2511.12025v1",
        "abstract": "Quantile computation is a core primitive in large-scale data analytics. In Spark, practitioners typically rely on the Greenwald-Khanna (GK) Sketch, an approximate method. When exact quantiles are required, the default option is an expensive global sort. We present GK Select, an exact Spark algorithm that avoids full-data shuffles and completes in a constant number of actions. GK Select leverages GK Sketch to identify a near-target pivot, extracts all values within the error bound around this pivot in each partition in linear time, and then tree-reduces the resulting candidate sets. We show analytically that GK Select matches the executor-side time complexity of GK Sketch while returning the exact quantile. Empirically, GK Select achieves sketch-level latency and outperforms Spark's full sort by approximately 10.5x on 10^9 values across 120 partitions on a 30-core AWS EMR cluster.",
        "authors": [
            "Ivan Cao",
            "Jaromir J. Saloni",
            "David A. G. Harrison"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.12025v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-15"
    },
    "http://arxiv.org/abs/2511.12009v1": {
        "title": "High-Performance N-Queens Solver on GPU: Iterative DFS with Zero Bank Conflicts",
        "link": "http://arxiv.org/abs/2511.12009v1",
        "abstract": "The counting of solutions to the N-Queens problem is a classic NP-complete problem with extremely high computational complexity. As of now, the academic community has rigorously verified the number of solutions only up to N <= 26. In 2016, the research team led by PreuBer solved the 27-Queens problem using FPGA hardware, which took approximately one year, though the result remains unverified independently. Recent studies on GPU parallel computing suggest that verifying the 27-Queens solution would still require about 17 months, indicating excessively high time and computational resource costs. To address this challenge, we propose an innovative parallel computing method on NVIDIA GPU platform, with the following core contributions: (1) An iterative depth-first search (DFS) algorithm for solving the N-Queens problem; (2) Complete mapping of the required stack structure to GPU shared memory; (3) Effective avoidance of bank conflicts through meticulously designed memory access patterns; (4) Various optimization techniques are employed to achieve optimal performance. Under the proposed optimization framework, we successfully verified the 27-Queens problem in just 28.4 days using eight RTX 5090 GPUs, thereby confirming the correctness of PreuBer's computational results. Moreover, we have reduced the projected solving time for the next open case-the 28-Queens problem-to approximately 11 months, making its resolution computationally feasible. Compared to the state-of-the-art GPU methods, our method achieves over 10x speedup on identical hardware configurations (8 A100), while delivering over 26x acceleration when utilizing 8 RTX 5090 GPUs, and brings fresh perspectives to this long-stagnant problem.",
        "authors": [
            "Guangchao Yao",
            "Yali Li"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.12009v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-15"
    },
    "http://arxiv.org/abs/2511.11939v1": {
        "id": "http://arxiv.org/abs/2511.11939v1",
        "title": "Modular GPU Programming with Typed Perspectives",
        "link": "http://arxiv.org/abs/2511.11939v1",
        "tags": [
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Addresses modular GPU programming challenges for collective operations like Tensor Core instructions. Introduces Prism, a GPU language with typed perspectives tracking thread granularity control. Maintains performance parity without safety compromises for state-of-the-art kernels.",
        "abstract": "To achieve peak performance on modern GPUs, one must balance two frames of mind: issuing instructions to individual threads to control their behavior, while simultaneously tracking the convergence of many threads acting in concert to perform collective operations like Tensor Core instructions. The tension between these two mindsets makes modular programming error prone. Functions that encapsulate collective operations, despite being called per-thread, must be executed cooperatively by groups of threads.   In this work, we introduce Prism, a new GPU language that restores modularity while still giving programmers the low-level control over collective operations necessary for high performance. Our core idea is typed perspectives, which materialize, at the type level, the granularity at which the programmer is controlling the behavior of threads. We describe the design of Prism, implement a compiler for it, and lay its theoretical foundations in a core calculus called Bundl. We implement state-of-the-art GPU kernels in Prism and find that it offers programmers the safety guarantees needed to confidently write modular code without sacrificing performance.",
        "authors": [
            "Manya Bansal",
            "Daniel Sainati",
            "Joseph W. Cutler",
            "Saman Amarasinghe",
            "Jonathan Ragan-Kelley"
        ],
        "categories": [
            "cs.PL",
            "cs.DC"
        ],
        "submit_date": "2025-11-14"
    },
    "http://arxiv.org/abs/2511.11907v2": {
        "id": "http://arxiv.org/abs/2511.11907v2",
        "title": "KVSwap: Disk-aware KV Cache Offloading for Long-Context On-device Inference",
        "link": "http://arxiv.org/abs/2511.11907v2",
        "tags": [
            "offloading",
            "edge",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Addresses on-device KV cache memory bottleneck for long-context inference. Proposes KVSwap, a disk-aware offloading framework with predictive preloading, compute-disk overlap, and I/O-optimized access patterns. Achieves higher throughput under memory constraints with no quality loss.",
        "abstract": "Language models (LMs) underpin emerging mobile and embedded AI applications like meeting and video summarization and document analysis, which often require processing multiple long-context inputs. Running an LM locally on-device improves privacy, enables offline use, and reduces cost, but long-context inference quickly hits a \\emph{memory capacity wall} as the key-value (KV) cache grows linearly with context length and batch size. Existing KV-cache offloading schemes are designed to transfer cache data from GPU memory to CPU memory; however, they are not suitable for embedded and mobile systems, where the CPU and GPU (or NPU) typically share a unified memory and the non-volatile secondary storage (disk) offers limited I/O bandwidth. We present KVSwap, a software framework tailored for local devices that achieves high memory efficiency while effectively leveraging disk storage. KVSwap stores the full cache on disk, uses highly compact in-memory metadata to predict which entries to preload, overlaps computation with hardware-aware disk access, and orchestrates read patterns to match storage device characteristics. Our evaluation shows that across representative LMs and storage types, KVSwap delivers higher throughput under tight memory budgets while maintaining generation quality over existing KV cache offloading schemes.",
        "authors": [
            "Huawei Zhang",
            "Chunwei Xia",
            "Zheng Wang"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-11-14"
    },
    "http://arxiv.org/abs/2511.11890v1": {
        "title": "Advancing Annotat3D with Harpia: A CUDA-Accelerated Library For Large-Scale Volumetric Data Segmentation",
        "link": "http://arxiv.org/abs/2511.11890v1",
        "abstract": "High-resolution volumetric imaging techniques, such as X-ray tomography and advanced microscopy, generate increasingly large datasets that challenge existing tools for efficient processing, segmentation, and interactive exploration. This work introduces new capabilities to Annotat3D through Harpia, a new CUDA-based processing library designed to support scalable, interactive segmentation workflows for large 3D datasets in high-performance computing (HPC) and remote-access environments. Harpia features strict memory control, native chunked execution, and a suite of GPU-accelerated filtering, annotation, and quantification tools, enabling reliable operation on datasets exceeding single-GPU memory capacity. Experimental results demonstrate significant improvements in processing speed, memory efficiency, and scalability compared to widely used frameworks such as NVIDIA cuCIM and scikit-image. The system's interactive, human-in-the-loop interface, combined with efficient GPU resource management, makes it particularly suitable for collaborative scientific imaging workflows in shared HPC infrastructures.",
        "authors": [
            "Camila Machado de Araujo",
            "Egon P. B. S. Borges",
            "Ricardo Marcelo Canteiro Grangeiro",
            "Allan Pinto"
        ],
        "categories": [
            "cs.CV",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.11890v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-14"
    },
    "http://arxiv.org/abs/2511.11885v1": {
        "id": "http://arxiv.org/abs/2511.11885v1",
        "title": "Flash-Fusion: Enabling Expressive, Low-Latency Queries on IoT Sensor Streams with LLMs",
        "link": "http://arxiv.org/abs/2511.11885v1",
        "tags": [
            "edge",
            "serving",
            "RAG"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes Flash-Fusion, an edge-cloud system for low-latency LLM queries on IoT sensor streams. Uses edge summarization for data reduction and cloud-based query planning with clustered data for prompt assembly. Achieves 95% latency reduction and 98% cost decrease compared to raw data feeding.",
        "abstract": "Smart cities and pervasive IoT deployments have generated interest in IoT data analysis across transportation and urban planning. At the same time, Large Language Models offer a new interface for exploring IoT data - particularly through natural language. Users today face two key challenges when working with IoT data using LLMs: (1) data collection infrastructure is expensive, producing terabytes of low-level sensor readings that are too granular for direct use, and (2) data analysis is slow, requiring iterative effort and technical expertise. Directly feeding all IoT telemetry to LLMs is impractical due to finite context windows, prohibitive token costs at scale, and non-interactive latencies. What is missing is a system that first parses a user's query to identify the analytical task, then selects the relevant data slices, and finally chooses the right representation before invoking an LLM.   We present Flash-Fusion, an end-to-end edge-cloud system that reduces the IoT data collection and analysis burden on users. Two principles guide its design: (1) edge-based statistical summarization (achieving 73.5% data reduction) to address data volume, and (2) cloud-based query planning that clusters behavioral data and assembles context-rich prompts to address data interpretation. We deploy Flash-Fusion on a university bus fleet and evaluate it against a baseline that feeds raw data to a state-of-the-art LLM. Flash-Fusion achieves a 95% latency reduction and 98% decrease in token usage and cost while maintaining high-quality responses. It enables personas across disciplines - safety officers, urban planners, fleet managers, and data scientists - to efficiently iterate over IoT data without the burden of manual query authoring or preprocessing.",
        "authors": [
            "Kausar Patherya",
            "Ashutosh Dhekne",
            "Francisco Romero"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.DB"
        ],
        "submit_date": "2025-11-14"
    },
    "http://arxiv.org/abs/2511.11843v2": {
        "title": "TD-Orch: Scalable Load-Balancing for Distributed Systems with Applications to Graph Processing",
        "link": "http://arxiv.org/abs/2511.11843v2",
        "abstract": "In this paper, we introduce a task-data orchestration abstraction that supports a range of distributed applications, including graph processing and key-value stores. Given a batch of lambda tasks each requesting one or more data items, where both tasks and data are distributed across multiple machines, each task must be co-located with its target data (by moving tasks and/or data) and then executed. We present TD-Orch, an efficient and scalable orchestration framework featuring a simple application developer interface. TD-Orch employs a distributed push-pull technique, leveraging the bidirectional flow of both tasks and data to achieve scalable load balance across machines even under highly skewed data requests (data hot spots), with minimal communication overhead. Experimental results show that TD-Orch achieves up to 2.8x speedup over existing distributed scheduling baselines. Building on TD-Orch, we present TDO-GP, a distributed graph processing system for general graph problems, demonstrating the effectiveness of the underlying framework. We design three families of implementation techniques to fully leverage the execution flow provided by TD-Orch. Experimental results show that TDO-GP achieves an average speedup of 4.1x over the best prior open-source distributed graph systems for general graph processing.",
        "authors": [
            "Yiwei Zhao",
            "Qiushi Lin",
            "Hongbo Kang",
            "Guy E. Blelloch",
            "Laxman Dhulipala",
            "Yan Gu",
            "Charles McGuffey",
            "Phillip B. Gibbons"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.11843v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-14"
    },
    "http://arxiv.org/abs/2511.11560v2": {
        "title": "A Unified Convergence Analysis for Semi-Decentralized Learning: Sampled-to-Sampled vs. Sampled-to-All Communication",
        "link": "http://arxiv.org/abs/2511.11560v2",
        "abstract": "In semi-decentralized federated learning, devices primarily rely on device-to-device communication but occasionally interact with a central server. Periodically, a sampled subset of devices uploads their local models to the server, which computes an aggregate model. The server can then either (i) share this aggregate model only with the sampled clients (sampled-to-sampled, S2S) or (ii) broadcast it to all clients (sampled-to-all, S2A). Despite their practical significance, a rigorous theoretical and empirical comparison of these two strategies remains absent. We address this gap by analyzing S2S and S2A within a unified convergence framework that accounts for key system parameters: sampling rate, server aggregation frequency, and network connectivity. Our results, both analytical and experimental, reveal distinct regimes where one strategy outperforms the other, depending primarily on the degree of data heterogeneity across devices. These insights lead to concrete design guidelines for practical semi-decentralized FL deployments.",
        "authors": [
            "Angelo Rodio",
            "Giovanni Neglia",
            "Zheng Chen",
            "Erik G. Larsson"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.11560v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-14"
    },
    "http://arxiv.org/abs/2511.11542v2": {
        "title": "Beyond Exascale: Dataflow Domain Translation on a Cerebras Cluster",
        "link": "http://arxiv.org/abs/2511.11542v2",
        "abstract": "Simulation of physical systems is essential across scientific and engineering domains. Commonly used domain decomposition methods are unable to simultaneously deliver both high simulation rate and high utilization in network computing environments. In particular, Exascale systems deliver only a small fraction their peak performance for these workloads. This paper introduces the novel Domain Translation algorithm, designed to overcome these limitations. On a cluster of 64 Cerebras CS-3 systems, we use this method to demonstrate unprecedented cluster performance across a range of metrics: we show simulations running in excess of 1.6 million time steps per second; we also demonstrate perfect weak scaling at 88% of peak performance. At this cluster scale, our implementation provides 112 PFLOP/s in a power-unconstrained environment, and 57 GFLOP/J in a power-limited environment. We illustrate the method by applying the shallow-water equations to model a tsunami following an asteroid impact at 460m-resolution on a planetary scale.",
        "authors": [
            "Tomas Oppelstrup",
            "Nicholas Giamblanco",
            "Delyan Z. Kalchev",
            "Ilya Sharapov",
            "Mark Taylor",
            "Dirk Van Essendelft",
            "Sivasankaran Rajamanickam",
            "Michael James"
        ],
        "categories": [
            "cs.DC",
            "physics.comp-ph"
        ],
        "id": "http://arxiv.org/abs/2511.11542v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-14"
    },
    "http://arxiv.org/abs/2511.11332v1": {
        "id": "http://arxiv.org/abs/2511.11332v1",
        "title": "UFO$^3$: Weaving the Digital Agent Galaxy",
        "link": "http://arxiv.org/abs/2511.11332v1",
        "tags": [
            "agentic",
            "edge",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-11-14",
        "tldr": "Presents UFO3, a system for orchestrating LLM-powered agents across heterogeneous devices. Models user requests as mutable distributed DAGs with asynchronous execution and dynamic optimization. Achieves 31% lower end-to-end latency compared to a sequential baseline.",
        "abstract": "Large language model (LLM)-powered agents are transforming digital devices from passive tools into proactive intelligent collaborators. However, most existing frameworks remain confined to a single OS or device, making cross-device workflows brittle and largely manual. We present UFO$^3$, a system that unifies heterogeneous endpoints, desktops, servers, mobile devices, and edge, into a single orchestration fabric. UFO$^3$ models each user request as a mutable TaskConstellation: a distributed DAG of atomic subtasks (TaskStars) with explicit control and data dependencies (TaskStarLines). The TaskConstellation continuously evolves as results stream in from distributed devices, enabling asynchronous execution, adaptive recovery, and dynamic optimization. A Constellation Orchestrator} executes tasks safely and asynchronously while applying dynamic DAG updates, and the Agent Interaction Protocol (AIP) provides persistent, low-latency channels for reliable task dispatch and result streaming. These designs dissolve the traditional boundaries between devices and platforms, allowing agents to collaborate seamlessly and amplify their collective intelligence.   We evaluate UFO$^3$ on NebulaBench, a benchmark of 55 cross-device tasks across 5 machines and 10 categories. UFO$^3$ achieves 83.3% subtask completion, 70.9% task success, exposes parallelism with an average width of 1.72, and reduces end-to-end latency by 31% relative to a sequential baseline. Fault-injection experiments demonstrate graceful degradation and recovery under transient and permanent agent failures. These results show that UFO$^3$ achieves accurate, efficient, and resilient task orchestration across heterogeneous devices, uniting isolated agents into a coherent, adaptive computing fabric that extends across the landscape of ubiquitous computing.",
        "authors": [
            "Chaoyun Zhang",
            "Liqun Li",
            "He Huang",
            "Chiming Ni",
            "Bo Qiao",
            "Si Qin",
            "Yu Kang",
            "Minghua Ma",
            "Qingwei Lin",
            "Saravan Rajmohan",
            "Dongmei Zhang"
        ],
        "categories": [
            "cs.DC",
            "cs.MA"
        ],
        "submit_date": "2025-11-14"
    },
    "http://arxiv.org/abs/2511.13761v1": {
        "id": "http://arxiv.org/abs/2511.13761v1",
        "title": "What happens when nanochat meets DiLoCo?",
        "link": "http://arxiv.org/abs/2511.13761v1",
        "tags": [
            "training",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Studies performance impact of DiLoCo's communication-efficient decentralized training on LLMs. Implements DiLoCo with inner-outer optimizer into nanochat baseline, reducing communication by orders of magnitude vs data-parallel training. Shows irreversible representation drift causing 5-10% accuracy drops on downstream tasks despite pretraining convergence.",
        "abstract": "Although LLM training is typically centralized with high-bandwidth interconnects and large compute budgets, emerging methods target communication-constrained training in distributed environments. The model trade-offs introduced by this shift remain underexplored, and our goal is to study them.   We use the open-source nanochat project, a compact 8K-line full-stack ChatGPT-like implementation containing tokenization, pretraining, fine-tuning, and serving, as a controlled baseline. We implement the DiLoCo algorithm as a lightweight wrapper over nanochat's training loop, performing multiple local steps per worker before synchronization with an outer optimizer, effectively reducing communication by orders of magnitude. This inner-outer training is compared against a standard data-parallel (DDP) setup. Because nanochat is small and inspectable, it enables controlled pipeline adaptations and allows direct comparison with the conventional centralized baseline.   DiLoCo achieves stable convergence and competitive loss in pretraining but yields worse MMLU, GSM8K, and HumanEval scores after mid-training and SFT. We discover that using DiLoCo-pretrained weights and running mid- and post-training with DDP fails to recover performance, revealing irreversible representation drift from asynchronous updates that impairs downstream alignment. We provide this implementation as an official fork of nanochat on GitHub.",
        "authors": [
            "Alexander Acker",
            "Soeren Becker",
            "Sasho Nedelkoski",
            "Dominik Scheinert",
            "Odej Kao",
            "Philipp Wiesner"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.LG"
        ],
        "submit_date": "2025-11-14"
    },
    "http://arxiv.org/abs/2511.11111v1": {
        "title": "SMART: A Surrogate Model for Predicting Application Runtime in Dragonfly Systems",
        "link": "http://arxiv.org/abs/2511.11111v1",
        "abstract": "The Dragonfly network, with its high-radix and low-diameter structure, is a leading interconnect in high-performance computing. A major challenge is workload interference on shared network links. Parallel discrete event simulation (PDES) is commonly used to analyze workload interference. However, high-fidelity PDES is computationally expensive, making it impractical for large-scale or real-time scenarios. Hybrid simulation that incorporates data-driven surrogate models offers a promising alternative, especially for forecasting application runtime, a task complicated by the dynamic behavior of network traffic. We present \\ourmodel, a surrogate model that combines graph neural networks (GNNs) and large language models (LLMs) to capture both spatial and temporal patterns from port level router data. \\ourmodel outperforms existing statistical and machine learning baselines, enabling accurate runtime prediction and supporting efficient hybrid simulation of Dragonfly networks.",
        "authors": [
            "Xin Wang",
            "Pietro Lodi Rizzini",
            "Sourav Medya",
            "Zhiling Lan"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.11111v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-14"
    },
    "http://arxiv.org/abs/2511.11038v1": {
        "id": "http://arxiv.org/abs/2511.11038v1",
        "title": "SemanticNN: Compressive and Error-Resilient Semantic Offloading for Extremely Weak Devices",
        "link": "http://arxiv.org/abs/2511.11038v1",
        "tags": [
            "edge",
            "offloading",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-11-14",
        "tldr": "Addresses how to perform accurate AI inference offloading for resource-constrained IoT devices over unreliable networks. Proposes SemanticNN, a semantic codec with a BER-aware decoder and soft quantization, that is resilient to bit errors. Reduces feature transmission volume by 56.82-344.83x while maintaining accuracy.",
        "abstract": "With the rapid growth of the Internet of Things (IoT), integrating artificial intelligence (AI) on extremely weak embedded devices has garnered significant attention, enabling improved real-time performance and enhanced data privacy. However, the resource limitations of such devices and unreliable network conditions necessitate error-resilient device-edge collaboration systems. Traditional approaches focus on bit-level transmission correctness, which can be inefficient under dynamic channel conditions. In contrast, we propose SemanticNN, a semantic codec that tolerates bit-level errors in pursuit of semantic-level correctness, enabling compressive and resilient collaborative inference offloading under strict computational and communication constraints. It incorporates a Bit Error Rate (BER)-aware decoder that adapts to dynamic channel conditions and a Soft Quantization (SQ)-based encoder to learn compact representations. Building on this architecture, we introduce Feature-augmentation Learning, a novel training strategy that enhances offloading efficiency. To address encoder-decoder capability mismatches from asymmetric resources, we propose XAI-based Asymmetry Compensation to enhance decoding semantic fidelity. We conduct extensive experiments on STM32 using three models and six datasets across image classification and object detection tasks. Experimental results demonstrate that, under varying transmission error rates, SemanticNN significantly reduces feature transmission volume by 56.82-344.83x while maintaining superior inference accuracy.",
        "authors": [
            "Jiaming Huang",
            "Yi Gao",
            "Fuchang Pan",
            "Renjie Li",
            "Wei Dong"
        ],
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.DC"
        ],
        "submit_date": "2025-11-14"
    },
    "http://arxiv.org/abs/2511.10938v1": {
        "title": "Cascading Bandits With Feedback",
        "link": "http://arxiv.org/abs/2511.10938v1",
        "abstract": "Motivated by the challenges of edge inference, we study a variant of the cascade bandit model in which each arm corresponds to an inference model with an associated accuracy and error probability. We analyse four decision-making policies-Explore-then-Commit, Action Elimination, Lower Confidence Bound (LCB), and Thompson Sampling-and provide sharp theoretical regret guarantees for each. Unlike in classical bandit settings, Explore-then-Commit and Action Elimination incur suboptimal regret because they commit to a fixed ordering after the exploration phase, limiting their ability to adapt. In contrast, LCB and Thompson Sampling continuously update their decisions based on observed feedback, achieving constant O(1) regret. Simulations corroborate these theoretical findings, highlighting the crucial role of adaptivity for efficient edge inference under uncertainty.",
        "authors": [
            "R Sri Prakash",
            "Nikhil Karamchandani",
            "Sharayu Moharir"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.10938v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-14"
    },
    "http://arxiv.org/abs/2511.17566v1": {
        "title": "Root Cause Analysis for Microservice Systems via Cascaded Conditional Learning with Hypergraphs",
        "link": "http://arxiv.org/abs/2511.17566v1",
        "abstract": "Root cause analysis in microservice systems typically involves two core tasks: root cause localization (RCL) and failure type identification (FTI). Despite substantial research efforts, conventional diagnostic approaches still face two key challenges. First, these methods predominantly adopt a joint learning paradigm for RCL and FTI to exploit shared information and reduce training time. However, this simplistic integration neglects the causal dependencies between tasks, thereby impeding inter-task collaboration and information transfer. Second, these existing methods primarily focus on point-to-point relationships between instances, overlooking the group nature of inter-instance influences induced by deployment configurations and load balancing. To overcome these limitations, we propose CCLH, a novel root cause analysis framework that orchestrates diagnostic tasks based on cascaded conditional learning. CCLH provides a three-level taxonomy for group influences between instances and incorporates a heterogeneous hypergraph to model these relationships, facilitating the simulation of failure propagation. Extensive experiments conducted on datasets from three microservice benchmarks demonstrate that CCLH outperforms state-of-the-art methods in both RCL and FTI.",
        "authors": [
            "Shuaiyu Xie",
            "Hanbin He",
            "Jian Wang",
            "Bing Li"
        ],
        "categories": [
            "cs.LG",
            "cs.DC",
            "cs.SE"
        ],
        "id": "http://arxiv.org/abs/2511.17566v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-14"
    },
    "http://arxiv.org/abs/2511.10860v1": {
        "id": "http://arxiv.org/abs/2511.10860v1",
        "title": "HPCAgentTester: A Multi-Agent LLM Approach for Enhanced HPC Unit Test Generation",
        "link": "http://arxiv.org/abs/2511.10860v1",
        "tags": [
            "agentic",
            "RL",
            "offline"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes a multi-agent LLM framework for automated HPC unit test generation. Uses specialized agents in iterative critique loops to generate tests targeting parallelism and communication patterns. Increases test compilation rates by 30% and correctness by 25% compared to standalone LLMs.",
        "abstract": "Unit testing in High-Performance Computing (HPC) is critical but challenged by parallelism, complex algorithms, and diverse hardware. Traditional methods often fail to address non-deterministic behavior and synchronization issues in HPC applications. This paper introduces HPCAgentTester, a novel multi-agent Large Language Model (LLM) framework designed to automate and enhance unit test generation for HPC software utilizing OpenMP and MPI. HPCAgentTester employs a unique collaborative workflow where specialized LLM agents (Recipe Agent and Test Agent) iteratively generate and refine test cases through a critique loop. This architecture enables the generation of context-aware unit tests that specifically target parallel execution constructs, complex communication patterns, and hierarchical parallelism. We demonstrate HPCAgentTester's ability to produce compilable and functionally correct tests for OpenMP and MPI primitives, effectively identifying subtle bugs that are often missed by conventional techniques. Our evaluation shows that HPCAgentTester significantly improves test compilation rates and correctness compared to standalone LLMs, offering a more robust and scalable solution for ensuring the reliability of parallel software systems.",
        "authors": [
            "Rabimba Karanjai",
            "Lei Xu",
            "Weidong Shi"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.SE"
        ],
        "submit_date": "2025-11-13"
    },
    "http://arxiv.org/abs/2511.13751v1": {
        "title": "Inside VOLT: Designing an Open-Source GPU Compiler",
        "link": "http://arxiv.org/abs/2511.13751v1",
        "abstract": "Recent efforts in open-source GPU research are opening new avenues in a domain that has long been tightly coupled with a few commercial vendors. Emerging open GPU architectures define SIMT functionality through their own ISAs, but executing existing GPU programs and optimizing performance on these ISAs relies on a compiler framework that is technically complex and often undercounted in open hardware development costs.   To address this challenge, the Vortex-Optimized Lightweight Toolchain (VOLT) has been proposed. This paper presents its design principles, overall structure, and the key compiler transformations required to support SIMT execution on Vortex. VOLT enables SIMT code generation and optimization across multiple levels of abstraction through a hierarchical design that accommodates diverse front-end languages and open GPU hardware. To ensure extensibility as GPU architectures evolve, VOLT centralizes fundamental SIMT-related analyses and optimizations in the middle-end, allowing them to be reused across front-ends and easily adapted to emerging open-GPU variants. Through two case studies on ISA extensions and host-runtime API, this paper also demonstrates how VOLT can support extensions",
        "authors": [
            "Shinnung Jeong",
            "Chihyo Ahn",
            "Huanzhi Pu",
            "Jisheng Zhao",
            "Hyesoon Kim",
            "Blaise Tine"
        ],
        "categories": [
            "cs.DC",
            "cs.AR",
            "cs.PL"
        ],
        "id": "http://arxiv.org/abs/2511.13751v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-13"
    },
    "http://arxiv.org/abs/2511.10834v1": {
        "id": "http://arxiv.org/abs/2511.10834v1",
        "title": "EarthSight: A Distributed Framework for Low-Latency Satellite Intelligence",
        "link": "http://arxiv.org/abs/2511.10834v1",
        "tags": [
            "edge",
            "serving",
            "multi-modal"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Addresses high latency in satellite image analysis by introducing EarthSight, a distributed runtime for multi-task inference with shared backbones, query scheduling, and dynamic filtering. Achieves 1.9× faster compute time and reduces 90th percentile latency from 51 to 21 minutes.",
        "abstract": "Low-latency delivery of satellite imagery is essential for time-critical applications such as disaster response, intelligence, and infrastructure monitoring. However, traditional pipelines rely on downlinking all captured images before analysis, introducing delays of hours to days due to restricted communication bandwidth. To address these bottlenecks, emerging systems perform onboard machine learning to prioritize which images to transmit. However, these solutions typically treat each satellite as an isolated compute node, limiting scalability and efficiency. Redundant inference across satellites and tasks further strains onboard power and compute costs, constraining mission scope and responsiveness. We present EarthSight, a distributed runtime framework that redefines satellite image intelligence as a distributed decision problem between orbit and ground. EarthSight introduces three core innovations: (1) multi-task inference on satellites using shared backbones to amortize computation across multiple vision tasks; (2) a ground-station query scheduler that aggregates user requests, predicts priorities, and assigns compute budgets to incoming imagery; and (3) dynamic filter ordering, which integrates model selectivity, accuracy, and execution cost to reject low-value images early and conserve resources. EarthSight leverages global context from ground stations and resource-aware adaptive decisions in orbit to enable constellations to perform scalable, low-latency image analysis within strict downlink bandwidth and onboard power budgets. Evaluations using a prior established satellite simulator show that EarthSight reduces average compute time per image by 1.9x and lowers 90th percentile end-to-end latency from first contact to delivery from 51 to 21 minutes compared to the state-of-the-art baseline.",
        "authors": [
            "Ansel Kaplan Erol",
            "Seungjun Lee",
            "Divya Mahajan"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "submit_date": "2025-11-13"
    },
    "http://arxiv.org/abs/2511.10753v1": {
        "id": "http://arxiv.org/abs/2511.10753v1",
        "title": "FengHuang: Next-Generation Memory Orchestration for AI Inferencing",
        "link": "http://arxiv.org/abs/2511.10753v1",
        "tags": [
            "inference",
            "offloading",
            "hardware"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes FengHuang, a disaggregated infrastructure with multi-tier memory and active tensor paging to overcome GPU memory and scaling limits for LLM inference. Achieves 93% local memory reduction, 50% GPU savings, and 16x-70x faster inter-GPU communication while maintaining performance.",
        "abstract": "This document presents a vision for a novel AI infrastructure design that has been initially validated through inference simulations on state-of-the-art large language models. Advancements in deep learning and specialized hardware have driven the rapid growth of large language models (LLMs) and generative AI systems. However, traditional GPU-centric architectures face scalability challenges for inference workloads due to limitations in memory capacity, bandwidth, and interconnect scaling. To address these issues, the FengHuang Platform, a disaggregated AI infrastructure platform, is proposed to overcome memory and communication scaling limits for AI inference. FengHuang features a multi-tier shared-memory architecture combining high-speed local memory with centralized disaggregated remote memory, enhanced by active tensor paging and near-memory compute for tensor operations. Simulations demonstrate that FengHuang achieves up to 93% local memory capacity reduction, 50% GPU compute savings, and 16x to 70x faster inter-GPU communication compared to conventional GPU scaling. Across workloads such as GPT-3, Grok-1, and QWEN3-235B, FengHuang enables up to 50% GPU reductions while maintaining end-user performance, offering a scalable, flexible, and cost-effective solution for AI inference infrastructure. FengHuang provides an optimal balance as a rack-level AI infrastructure scale-up solution. Its open, heterogeneous design eliminates vendor lock-in and enhances supply chain flexibility, enabling significant infrastructure and power cost reductions.",
        "authors": [
            "Jiamin Li",
            "Lei Qu",
            "Tao Zhang",
            "Grigory Chirkov",
            "Shuotao Xu",
            "Peng Cheng",
            "Lidong Zhou"
        ],
        "categories": [
            "cs.DC",
            "cs.AR"
        ],
        "submit_date": "2025-11-13"
    },
    "http://arxiv.org/abs/2511.10480v2": {
        "id": "http://arxiv.org/abs/2511.10480v2",
        "title": "STAGE: A Symbolic Tensor grAph GEnerator for distributed AI system co-design",
        "link": "http://arxiv.org/abs/2511.10480v2",
        "tags": [
            "training",
            "hardware",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-11-13",
        "tldr": "Introduces STAGE, a symbolic tensor graph generator for modeling distributed LLM workloads. It synthesizes high-fidelity execution traces to explore system configurations and parallelization strategies. Successfully scales to 32K GPUs with tensor-level accuracy in compute, memory, and communication.",
        "abstract": "Optimizing the performance of large language models (LLMs) on large-scale AI training and inference systems requires a scalable and expressive mechanism to model distributed workload execution. Such modeling is essential for pre-deployment system-level optimizations (e.g., parallelization strategies) and design-space explorations. While recent efforts have proposed collecting execution traces from real systems, access to large-scale infrastructure remains limited to major cloud providers. Moreover, traces obtained from existing platforms cannot be easily adapted to study future larger-scale system configurations. We introduce Symbolic Tensor grAph GEnerator(STAGE), a framework that synthesizes high-fidelity execution traces to accurately model LLM workloads. STAGE supports a comprehensive set of parallelization strategies, allowing users to systematically explore a wide spectrum of LLM architectures and system configurations. STAGE demonstrates its scalability by synthesizing high-fidelity LLM traces spanning over 32K GPUs, while preserving tensor-level accuracy in compute, memory, and communication. STAGE is publicly available to facilitate further research in distributed machine learning systems: https://github.com/astra-sim/symbolic tensor graph",
        "authors": [
            "Changhai Man",
            "Joongun Park",
            "Hanjiang Wu",
            "Huan Xu",
            "Srinivas Sridharan",
            "Tushar Krishna"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-11-13"
    },
    "http://arxiv.org/abs/2511.11749v1": {
        "title": "How Machine Learning-Data Driven Replication Strategies Enhance Fault Tolerance in Large-Scale Distributed Systems",
        "link": "http://arxiv.org/abs/2511.11749v1",
        "abstract": "This research paper investigates how machine learning-driven data replication strategies can enhance fault tolerance in large-scale distributed systems. Traditional replication methods, which rely on static configurations, often struggle to adapt to dynamic workloads and unexpected failures, leading to inefficient resource utilization and prolonged downtime. By integrating machine learning techniques-specifically predictive analytics and reinforcement learning. The study proposes adaptive replication mechanisms capable of forecasting system failures and optimizing data placement in real time. Through an extensive literature review, qualitative analysis, and comparative evaluations with traditional approaches, the paper identifies key limitations in existing replication strategies and highlights the transformative potential of machine learning in creating more resilient, self-optimizing systems. The findings underscore both the promise and the challenges of implementing ML-driven solutions in real-world environments, offering recommendations for future research and practical deployment in cloud-based and enterprise systems.",
        "authors": [
            "Almond Kiruthu Murimi"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.11749v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-13"
    },
    "http://arxiv.org/abs/2511.10442v1": {
        "title": "FastGraph: Optimized GPU-Enabled Algorithms for Fast Graph Building and Message Passing",
        "link": "http://arxiv.org/abs/2511.10442v1",
        "abstract": "We introduce FastGraph, a novel GPU-optimized k-nearest neighbor algorithm specifically designed to accelerate graph construction in low-dimensional spaces (2-10 dimensions), critical for high-performance graph neural networks. Our method employs a GPU-resident, bin-partitioned approach with full gradient-flow support and adaptive parameter tuning, significantly enhancing both computational and memory efficiency. Benchmarking demonstrates that FastGraph achieves a 20-40x speedup over state-of-the-art libraries such as FAISS, ANNOY, and SCANN in dimensions less than 10 with virtually no memory overhead. These improvements directly translate into substantial performance gains for GNN-based workflows, particularly benefiting computationally intensive applications in low dimensions such as particle clustering in high-energy physics, visual object tracking, and graph clustering.",
        "authors": [
            "Aarush Agarwal",
            "Raymond He",
            "Jan Kieseler",
            "Matteo Cremonesi",
            "Shah Rukh Qasim"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.10442v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-13"
    },
    "http://arxiv.org/abs/2511.10434v1": {
        "title": "Unlocking Dynamic Inter-Client Spatial Dependencies: A Federated Spatio-Temporal Graph Learning Method for Traffic Flow Forecasting",
        "link": "http://arxiv.org/abs/2511.10434v1",
        "abstract": "Spatio-temporal graphs are powerful tools for modeling complex dependencies in traffic time series. However, the distributed nature of real-world traffic data across multiple stakeholders poses significant challenges in modeling and reconstructing inter-client spatial dependencies while adhering to data locality constraints. Existing methods primarily address static dependencies, overlooking their dynamic nature and resulting in suboptimal performance. In response, we propose Federated Spatio-Temporal Graph with Dynamic Inter-Client Dependencies (FedSTGD), a framework designed to model and reconstruct dynamic inter-client spatial dependencies in federated learning. FedSTGD incorporates a federated nonlinear computation decomposition module to approximate complex graph operations. This is complemented by a graph node embedding augmentation module, which alleviates performance degradation arising from the decomposition. These modules are coordinated through a client-server collective learning protocol, which decomposes dynamic inter-client spatial dependency learning tasks into lightweight, parallelizable subtasks. Extensive experiments on four real-world datasets demonstrate that FedSTGD achieves superior performance over state-of-the-art baselines in terms of RMSE, MAE, and MAPE, approaching that of centralized baselines. Ablation studies confirm the contribution of each module in addressing dynamic inter-client spatial dependencies, while sensitivity analysis highlights the robustness of FedSTGD to variations in hyperparameters.",
        "authors": [
            "Feng Wang",
            "Tianxiang Chen",
            "Shuyue Wei",
            "Qian Chu",
            "Yi Zhang",
            "Yifan Sun",
            "Zhiming Zheng"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.10434v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-13"
    },
    "http://arxiv.org/abs/2511.10363v1": {
        "id": "http://arxiv.org/abs/2511.10363v1",
        "title": "On The Performance of Prefix-Sum Parallel Kalman Filters and Smoothers on GPUs",
        "link": "http://arxiv.org/abs/2511.10363v1",
        "tags": [
            "kernel",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Evaluates parallel scan algorithms for GPU-based Kalman filters and smoothers. Implements all-prefix-sum algorithms for temporal parallelization and proposes a novel two-filter smoother. Achieves reduced run times on CUDA/Metal GPUs through optimized kernel operations and offloading techniques.",
        "abstract": "This paper presents an experimental evaluation of parallel-in-time Kalman filters and smoothers using graphics processing units (GPUs). In particular, the paper evaluates different all-prefix-sum algorithms, that is, parallel scan algorithms for temporal parallelization of Kalman filters and smoothers in two ways: by calculating the required number of operations via simulation, and by measuring the actual run time of the algorithms on real GPU hardware. In addition, a novel parallel-in-time two-filter smoother is proposed and experimentally evaluated. Julia code for Metal and CUDA implementations of all the algorithms is made publicly available.",
        "authors": [
            "Simo Särkkä",
            "Ángel F. García-Fernández"
        ],
        "categories": [
            "stat.CO",
            "cs.DC",
            "math.DS"
        ],
        "submit_date": "2025-11-13"
    },
    "http://arxiv.org/abs/2511.10339v1": {
        "title": "Massively Parallel Proof-Number Search for Impartial Games and Beyond",
        "link": "http://arxiv.org/abs/2511.10339v1",
        "abstract": "Proof-Number Search is a best-first search algorithm with many successful applications, especially in game solving. As large-scale computing clusters become increasingly accessible, parallelization is a natural way to accelerate computation. However, existing parallel versions of Proof-Number Search are known to scale poorly on many CPU cores. Using two parallelized levels and shared information among workers, we present the first massively parallel version of Proof-Number Search that scales efficiently even on a large number of CPUs. We apply our solver, enhanced with Grundy numbers for reducing game trees, to the Sprouts game, a case study motivated by the long-standing Sprouts Conjecture. Our solver achieves a significantly improved 332.9$\\times$ speedup when run on 1024 cores, enabling it to outperform the state-of-the-art Sprouts solver GLOP by four orders of magnitude in runtime and to generate proofs 1,000$\\times$ more complex. Despite exponential growth in game tree size, our solver verified the Sprouts Conjecture for 42 new positions, nearly doubling the number of known outcomes.",
        "authors": [
            "Tomáš Čížek",
            "Martin Balko",
            "Martin Schmid"
        ],
        "categories": [
            "cs.AI",
            "cs.DC",
            "cs.GT"
        ],
        "id": "http://arxiv.org/abs/2511.10339v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-13"
    },
    "http://arxiv.org/abs/2511.10258v1": {
        "title": "Workload Schedulers -- Genesis, Algorithms and Differences",
        "link": "http://arxiv.org/abs/2511.10258v1",
        "abstract": "This paper presents a novel approach to categorization of modern workload schedulers. We provide descriptions of three classes of schedulers: Operating Systems Process Schedulers, Cluster Systems Jobs Schedulers and Big Data Schedulers. We describe their evolution from early adoptions to modern implementations, considering both the use and features of algorithms. In summary, we discuss differences between all presented classes of schedulers and discuss their chronological development. In conclusion we highlight similarities in the focus of scheduling strategies design, applicable to both local and distributed systems.",
        "authors": [
            "Leszek Sliwko",
            "Vladimir Getov"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "id": "http://arxiv.org/abs/2511.10258v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-13"
    },
    "http://arxiv.org/abs/2511.10248v1": {
        "title": "Pk-IOTA: Blockchain empowered Programmable Data Plane to secure OPC UA communications in Industry 4.0",
        "link": "http://arxiv.org/abs/2511.10248v1",
        "abstract": "The OPC UA protocol is becoming the de facto standard for Industry 4.0 machine-to-machine communication. It stands out as one of the few industrial protocols that provide robust security features designed to prevent attackers from manipulating and damaging critical infrastructures. However, prior works showed that significant challenges still exists to set up secure OPC UA deployments in practice, mainly caused by the complexity of certificate management in industrial scenarios and the inconsistent implementation of security features across industrial OPC UA devices. In this paper, we present Pk-IOTA, an automated solution designed to secure OPC UA communications by integrating programmable data plane switches for in-network certificate validation and leveraging the IOTA Tangle for decen- tralized certificate distribution. Our evaluation is performed on a physical testbed representing a real-world industrial scenario and shows that Pk-IOTA introduces a minimal overhead while providing a scalable and tamper-proof mechanism for OPC UA certificate management.",
        "authors": [
            "Rinieri Lorenzo",
            "Gori Giacomo",
            "Melis Andrea",
            "Girau Roberto",
            "Prandini Marco",
            "Callegati Franco"
        ],
        "categories": [
            "cs.CR",
            "cs.DC",
            "cs.NI"
        ],
        "id": "http://arxiv.org/abs/2511.10248v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-13"
    },
    "http://arxiv.org/abs/2511.10180v1": {
        "title": "Selection of Supervised Learning-based Sparse Matrix Reordering Algorithms",
        "link": "http://arxiv.org/abs/2511.10180v1",
        "abstract": "Sparse matrix ordering is a vital optimization technique often employed for solving large-scale sparse matrices. Its goal is to minimize the matrix bandwidth by reorganizing its rows and columns, thus enhancing efficiency. Conventional methods for algorithm selection usually depend on brute-force search or empirical knowledge, lacking the ability to adjust to diverse sparse matrix structures.As a result, we have introduced a supervised learning-based model for choosing sparse matrix reordering algorithms. This model grasps the correlation between matrix characteristics and commonly utilized reordering algorithms, facilitating the automated and intelligent selection of the suitable sparse matrix reordering algorithm. Experiments conducted on the Florida sparse matrix dataset reveal that our model can accurately predict the optimal reordering algorithm for various matrices, leading to a 55.37% reduction in solution time compared to solely using the AMD reordering algorithm, with an average speedup ratio of 1.45.",
        "authors": [
            "Tao Tang",
            "Youfu Jiang",
            "Yingbo Cui",
            "Jianbin Fang",
            "Peng Zhang",
            "Lin Peng",
            "Chun Huang"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.10180v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-13"
    },
    "http://arxiv.org/abs/2511.11739v1": {
        "title": "Noise-Aware Optimization in Nominally Identical Manufacturing and Measuring Systems for High-Throughput Parallel Workflows",
        "link": "http://arxiv.org/abs/2511.11739v1",
        "abstract": "Device-to-device variability in experimental noise critically impacts reproducibility, especially in automated, high-throughput systems like additive manufacturing farms. While manageable in small labs, such variability can escalate into serious risks at larger scales, such as architectural 3D printing, where noise may cause structural or economic failures. This contribution presents a noise-aware decision-making algorithm that quantifies and models device-specific noise profiles to manage variability adaptively. It uses distributional analysis and pairwise divergence metrics with clustering to choose between single-device and robust multi-device Bayesian optimization strategies. Unlike conventional methods that assume homogeneous devices or generic robustness, this framework explicitly leverages inter-device differences to enhance performance, reproducibility, and efficiency. An experimental case study involving three nominally identical 3D printers (same brand, model, and close serial numbers) demonstrates reduced redundancy, lower resource usage, and improved reliability. Overall, this framework establishes a paradigm for precision- and resource-aware optimization in scalable, automated experimental platforms.",
        "authors": [
            "Christina Schenk",
            "Miguel Hernández-del-Valle",
            "Luis Calero-Lumbreras",
            "Marcus Noack",
            "Maciej Haranczyk"
        ],
        "categories": [
            "cs.DC",
            "cond-mat.mtrl-sci",
            "cs.LG",
            "math.OC",
            "stat.CO"
        ],
        "id": "http://arxiv.org/abs/2511.11739v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-13"
    },
    "http://arxiv.org/abs/2511.10146v1": {
        "id": "http://arxiv.org/abs/2511.10146v1",
        "title": "Dynamic Edge Server Selection in Time-Varying Environments: A Reliability-Aware Predictive Approach",
        "link": "http://arxiv.org/abs/2511.10146v1",
        "tags": [
            "edge",
            "serving",
            "RL"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes MO-HAN, a lightweight edge server selection method for embedded applications. It fuses latency prediction with adaptive reliability and hysteresis-based handover to balance predicted latency and reliability. Reduces handovers by 50% and lowers mean/tail latencies compared to baselines.",
        "abstract": "Latency-sensitive embedded applications increasingly rely on edge computing, yet dynamic network congestion in multi-server architectures challenges proper edge server selection. This paper proposes a lightweight server-selection method for edge applications that fuses latency prediction with adaptive reliability and hysteresis-based handover. Using passive measurements (arrival rate, utilization, payload size) and an exponentially modulated rational delay model, the proposed Moderate Handover (MO-HAN) method computes a score that balances predicted latency and reliability to ensure handovers occur only when the expected gain is meaningful and maintain reduced end-to-end latency. Results show that MO-HAN consistently outperforms static and fair-distribution baselines by lowering mean and tail latencies, while reducing handovers by nearly 50% compared to pure opportunistic selection. These gains arise without intrusive instrumentation or heavy learning infrastructure, making MO-HAN practical for resource-constrained embedded devices.",
        "authors": [
            "Jaime Sebastian Burbano",
            "Arnova Abdullah",
            "Eldiyar Zhantileuov",
            "Mohan Liyanage",
            "Rolf Schuster"
        ],
        "categories": [
            "cs.DC",
            "cs.NI"
        ],
        "submit_date": "2025-11-13"
    },
    "http://arxiv.org/abs/2511.10069v1": {
        "title": "dHPR: A Distributed Halpern Peaceman--Rachford Method for Non-smooth Distributed Optimization Problems",
        "link": "http://arxiv.org/abs/2511.10069v1",
        "abstract": "This paper introduces the distributed Halpern Peaceman--Rachford (dHPR) method, an efficient algorithm for solving distributed convex composite optimization problems with non-smooth objectives, which achieves a non-ergodic $O(1/k)$ iteration complexity regarding Karush--Kuhn--Tucker residual. By leveraging the symmetric Gauss--Seidel decomposition, the dHPR effectively decouples the linear operators in the objective functions and consensus constraints while maintaining parallelizability and avoiding additional large proximal terms, leading to a decentralized implementation with provably fast convergence. The superior performance of dHPR is demonstrated through comprehensive numerical experiments on distributed LASSO, group LASSO, and $L_1$-regularized logistic regression problems.",
        "authors": [
            "Zhangcheng Feng",
            "Defeng Sun",
            "Yancheng Yuan",
            "Guojun Zhang"
        ],
        "categories": [
            "math.OC",
            "cs.DC",
            "cs.MA"
        ],
        "id": "http://arxiv.org/abs/2511.10069v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-13"
    },
    "http://arxiv.org/abs/2511.11733v1": {
        "id": "http://arxiv.org/abs/2511.11733v1",
        "title": "Speculative Decoding in Decentralized LLM Inference: Turning Communication Latency into Computation Throughput",
        "link": "http://arxiv.org/abs/2511.11733v1",
        "tags": [
            "serving",
            "offloading",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes Decentralized Speculative Decoding (DSD) to turn network latency into computation throughput for distributed LLM inference. Uses parallel verification of candidate tokens across nodes and adaptive thresholding by token importance. Achieves 2.59x speedup on GSM8K with preserved accuracy.",
        "abstract": "Speculative decoding accelerates large language model (LLM) inference by using a lightweight draft model to propose tokens that are later verified by a stronger target model. While effective in centralized systems, its behavior in decentralized settings, where network latency often dominates compute, remains under-characterized. We present Decentralized Speculative Decoding (DSD), a plug-and-play framework for decentralized inference that turns communication delay into useful computation by verifying multiple candidate tokens in parallel across distributed nodes. We further introduce an adaptive speculative verification strategy that adjusts acceptance thresholds by token-level semantic importance, delivering an additional 15% to 20% end-to-end speedup without retraining. In theory, DSD reduces cross-node communication cost by approximately (N-1)t1(k-1)/k, where t1 is per-link latency and k is the average number of tokens accepted per round. In practice, DSD achieves up to 2.56x speedup on HumanEval and 2.59x on GSM8K, surpassing the Eagle3 baseline while preserving accuracy. These results show that adapting speculative decoding for decentralized execution provides a system-level optimization that converts network stalls into throughput, enabling faster distributed LLM inference with no model retraining or architectural changes.",
        "authors": [
            "Jingwei Song",
            "Wanyi Chen",
            "Xinyuan Song",
            "Max",
            "Chris Tong",
            "Gufeng Chen",
            "Tianyi Zhao",
            "Eric Yang",
            "Bill Shi",
            "Lynn Ai"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-11-13"
    },
    "http://arxiv.org/abs/2511.11729v2": {
        "id": "http://arxiv.org/abs/2511.11729v2",
        "title": "Harli: SLO-Aware Co-location of LLM Inference and PEFT-based Finetuning on Model-as-a-Service Platforms",
        "link": "http://arxiv.org/abs/2511.11729v2",
        "tags": [
            "serving",
            "offline",
            "RL"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes Harli, a system for co-locating PEFT-based finetuning with LLM inference decode. Integrates memory reuse, latency prediction, and QoS-aware scheduling to maximize throughput. Achieves 46.2% average finetune throughput gain while maintaining QoS.",
        "abstract": "Large language models (LLMs) are increasingly deployed under the Model-as-a-Service (MaaS) paradigm. To meet stringent quality-of-service (QoS) requirements, existing LLM serving systems disaggregate the prefill and decode phases of inference. However, decode instances often experience low GPU utilization due to their memory-bound nature and insufficient batching in dynamic workloads, leaving compute resources underutilized.   We introduce Harli, a serving system that improves GPU utilization by co-locating parameter-efficient finetuning (PEFT) tasks with LLM decode instances. PEFT tasks are compute-bound and memory-efficient, making them ideal candidates for safe co-location. Specifically, Harli addresses key challenges--limited memory and unpredictable interference--using three components: a unified memory allocator for runtime memory reuse, a two-stage latency predictor for decode latency modeling, and a QoS-guaranteed throughput-maximizing scheduler for throughput maximization. Experimental results show that Harli improves the finetune throughput by 46.2% on average (up to 92.0%) over state-of-the-art serving systems, while maintaining strict QoS guarantees for inference decode.",
        "authors": [
            "Ao Xu",
            "Han Zhao",
            "Weihao Cui",
            "Quan Chen",
            "Yukang Chen",
            "Shulai Zhang",
            "Shuang Chen",
            "Jiemin Jiang",
            "Zhibin Yu",
            "Minyi Guo"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-11-13"
    },
    "http://arxiv.org/abs/2511.09956v2": {
        "title": "Optimizing CPU Cache Utilization in Cloud VMs with Accurate Cache Abstraction",
        "link": "http://arxiv.org/abs/2511.09956v2",
        "abstract": "This paper shows that cache-based optimizations are often ineffective in cloud virtual machines (VMs) due to limited visibility into and control over provisioned caches. In public clouds, CPU caches can be partitioned or shared among VMs, but a VM is unaware of cache provisioning details. Moreover, a VM cannot influence cache usage via page placement policies, as memory-to-cache mappings are hidden. The paper proposes a novel solution, CacheX, which probes accurate and fine-grained cache abstraction within VMs using eviction sets without requiring hardware or hypervisor support, and showcases the utility of the probed information with two new techniques: LLC contention-aware task scheduling and virtual color-aware page cache management. Our evaluation of CacheX's implementation in x86 Linux kernel demonstrates that it can effectively improve cache utilization for various workloads in public cloud VMs.",
        "authors": [
            "Mani Tofigh",
            "Edward Guo",
            "Weiwei Jia",
            "Xiaoning Ding",
            "Zirui Neil Zhao",
            "Jianchen Shan"
        ],
        "categories": [
            "cs.DC",
            "cs.OS",
            "cs.PF"
        ],
        "id": "http://arxiv.org/abs/2511.09956v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-13"
    },
    "http://arxiv.org/abs/2511.09861v2": {
        "id": "http://arxiv.org/abs/2511.09861v2",
        "title": "Lit Silicon: A Case Where Thermal Imbalance Couples Concurrent Execution in Multiple GPUs",
        "link": "http://arxiv.org/abs/2511.09861v2",
        "tags": [
            "training",
            "quantization",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-11-13",
        "tldr": "Investigates GPU thermal imbalance-induced straggling in LLM training. Proposes detection, mitigation, and three power optimization techniques under node-level constraints. Achieves up to 6% performance and 4% power improvements on multi-GPU systems.",
        "abstract": "GPU systems are increasingly powering modern datacenters at scale. Despite being highly performant, GPU systems suffer from performance variation at the node and cluster levels. Such performance variation significantly impacts both high-performance computing and artificial intelligence workloads, such as cutting-edge large language models (LLMs). We analyze the performance of a single-node multi-GPU system running LLM training, and observe that the kernel-level performance variation is highly correlated with concurrent computation communication (C3), a technique to overlap computation and communication across GPUs for performance gains. We then take a further step to reason that thermally induced straggling coupling with C3 impacts performance variation, coined as the Lit Silicon effect. Lit Silicon describes that in a multi-GPU node, thermal imbalance across GPUs introduces node-level straggler GPUs, which in turn slow down the leader GPUs. Lit Silicon leads to node-level performance variation and inefficiency, impacting the entire datacenter from the bottom up. We propose analytical performance and power models for Lit Silicon, to understand the potential system-level gains. We further design simple detection and mitigation techniques to effectively address the Lit Silicon problem, and evaluate three different power management solutions, including power optimization under GPU thermal design power, performance optimization under node-level GPU power capping, and performance optimization under node-level CPU power sloshing. We conduct experiments on two workloads on two AMD InstinctTM MI300X GPU systems under two LLM training frameworks, and observe up to 6% performance and 4% power improvements, potentially saving hundreds of millions of dollars in datacenters. Our solution is almost free lunch and can be effortlessly adopted in datacenters as a new node-level power management layer.",
        "authors": [
            "Marco Kurzynski",
            "Shaizeen Aga",
            "Di Wu"
        ],
        "categories": [
            "cs.DC",
            "cs.AR"
        ],
        "submit_date": "2025-11-13"
    },
    "http://arxiv.org/abs/2511.09837v2": {
        "id": "http://arxiv.org/abs/2511.09837v2",
        "title": "MoFa: A Unified Performance Modeling Framework for LLM Pretraining",
        "link": "http://arxiv.org/abs/2511.09837v2",
        "tags": [
            "training",
            "quantization",
            "autoscaling"
        ],
        "relevant": true,
        "indexed_date": "2025-11-13",
        "tldr": "Proposes MoFa, a performance modeling framework for distributed LLM pretraining that unifies optimization features and fault tolerance. Incorporates enhanced cost and fault tolerance models to automate strategy tuning. Achieves high prediction accuracy and identifies key performance factors across configurations.",
        "abstract": "The exponential growth in LLM scales, with parameters soaring from billions to trillions, has necessitated distributed pretraining across large clusters comprising thousands to tens of thousands of devices. While hybrid parallelization strategies enable such pretraining, the vast combinatorial strategy space introduces significant optimization challenges. Traditional manual tuning methods incur prohibitive trial-and-error costs, and existing performance modeling approaches exhibit critical limitations: they fail to comprehensively account for prevalent optimization features and ignore the substantial overhead imposed by essential fault tolerance mechanisms like checkpoint recovery in long-duration pretraining. To address these gaps, we propose MoFa, a novel pretraining performance modeling framework that unifies multi-dimensional optimization features and fault tolerance. MoFa incorporates an enhanced cost model to accurately capture the effects of key optimizations and integrates a fault tolerance model based on historical cluster reliability data. Besides, a MoFa-based tuning system is developed to explore optimal pretraining performance and potential bottlenecks in various scenarios. Extensive modeling evaluations demonstrate that MoFa can achieve high prediction accuracy across various scenarios. In addition, through comprehensive tuning experiments, our framework systematically reveals the key factors influencing pretraining performance under different configurations, which provides solid a priori guidance for LLM pretraining system design and deployment.",
        "authors": [
            "Lu Zhao",
            "Rong Shi",
            "Shaoqing Zhang",
            "Shangchao Su",
            "Ziqing Yin",
            "Zhiyan Cui",
            "Hongfeng Sun",
            "Baoguo He",
            "Yueqiang Chen",
            "Liang Dong",
            "Xiyuan Li",
            "Lingbin Wang",
            "Lijun Ma",
            "Qiang Huang",
            "Ting Liu",
            "Chong Wang",
            "Can Wei"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-11-13"
    },
    "http://arxiv.org/abs/2511.11721v2": {
        "title": "A Meta-Heuristic Load Balancer for Cloud Computing Systems",
        "link": "http://arxiv.org/abs/2511.11721v2",
        "abstract": "This paper presents a strategy to allocate services on a Cloud system without overloading nodes and maintaining the system stability with minimum cost. We specify an abstract model of cloud resources utilization, including multiple types of resources as well as considerations for the service migration costs. A prototype meta-heuristic load balancer is demonstrated and experimental results are presented and discussed. We also propose a novel genetic algorithm, where population is seeded with the outputs of other meta-heuristic algorithms.",
        "authors": [
            "Leszek Sliwko",
            "Vladimir Getov"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "id": "http://arxiv.org/abs/2511.11721v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-13"
    },
    "http://arxiv.org/abs/2511.09828v2": {
        "title": "SMoFi: Step-wise Momentum Fusion for Split Federated Learning on Heterogeneous Data",
        "link": "http://arxiv.org/abs/2511.09828v2",
        "abstract": "Split Federated Learning is a system-efficient federated learning paradigm that leverages the rich computing resources at a central server to train model partitions. Data heterogeneity across silos, however, presents a major challenge undermining the convergence speed and accuracy of the global model. This paper introduces Step-wise Momentum Fusion (SMoFi), an effective and lightweight framework that counteracts gradient divergence arising from data heterogeneity by synchronizing the momentum buffers across server-side optimizers. To control gradient divergence over the training process, we design a staleness-aware alignment mechanism that imposes constraints on gradient updates of the server-side submodel at each optimization step. Extensive validations on multiple real-world datasets show that SMoFi consistently improves global model accuracy (up to 7.1%) and convergence speed (up to 10.25$\\times$). Furthermore, SMoFi has a greater impact with more clients involved and deeper learning models, making it particularly suitable for model training in resource-constrained contexts.",
        "authors": [
            "Mingkun Yang",
            "Ran Zhu",
            "Qing Wang",
            "Jie Yang"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.09828v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-13"
    },
    "http://arxiv.org/abs/2511.11719v1": {
        "id": "http://arxiv.org/abs/2511.11719v1",
        "title": "ECCENTRIC: Edge-Cloud Collaboration Framework for Distributed Inference Using Knowledge Adaptation",
        "link": "http://arxiv.org/abs/2511.11719v1",
        "tags": [
            "edge",
            "offloading",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes ECCENTRIC, an edge-cloud collaboration framework for distributed inference that uses knowledge adaptation to balance computation, communication, and performance. Achieves reduced costs while maintaining accuracy for classification and object detection tasks.",
        "abstract": "The massive growth in the utilization of edge AI has made the applications of machine learning models ubiquitous in different domains. Despite the computation and communication efficiency of these systems, due to limited computation resources on edge devices, relying on more computationally rich systems on the cloud side is inevitable in most cases. Cloud inference systems can achieve the best performance while the computation and communication cost is dramatically increasing by the expansion of a number of edge devices relying on these systems. Hence, there is a trade-off between the computation, communication, and performance of these systems. In this paper, we propose a novel framework, dubbed as Eccentric that learns models with different levels of trade-offs between these conflicting objectives. This framework, based on an adaptation of knowledge from the edge model to the cloud one, reduces the computation and communication costs of the system during inference while achieving the best performance possible. The Eccentric framework can be considered as a new form of compression method suited for edge-cloud inference systems to reduce both computation and communication costs. Empirical studies on classification and object detection tasks corroborate the efficacy of this framework.",
        "authors": [
            "Mohammad Mahdi Kamani",
            "Zhongwei Cheng",
            "Lin Chen"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-11-12"
    },
    "http://arxiv.org/abs/2511.09776v1": {
        "title": "A Poly-Log Approximation for Transaction Scheduling in Fog-Cloud Computing and Beyond",
        "link": "http://arxiv.org/abs/2511.09776v1",
        "abstract": "Transaction scheduling is crucial to efficiently allocate shared resources in a conflict-free manner in distributed systems. We investigate the efficient scheduling of transactions in a network of fog-cloud computing model, where transactions and their associated shared objects can move within the network. The schedule may require objects to move to transaction nodes, or the transactions to move to the object nodes. Moreover, the schedule may determine intermediate nodes where both objects and transactions meet. Our goal is to minimize the total combined cost of the schedule. We focus on networks of constant doubling dimension, which appear frequently in practice. We consider a batch problem where an arbitrary set of nodes has transactions that need to be scheduled. First, we consider a single shared object required by all the transactions and present a scheduling algorithm that gives an $O(\\log n \\cdot \\log D)$ approximation of the optimal schedule, where $n$ is the number of nodes and $D$ is the diameter of the network. Later, we consider transactions accessing multiple shared objects (at most $k$ objects per transaction) and provide a scheduling algorithm that gives an $O(k \\cdot \\log n \\cdot \\log D)$ approximation. We also provide a fully distributed version of the scheduling algorithms where the nodes do not need global knowledge of transactions.",
        "authors": [
            "Ramesh Adhikari",
            "Costas Busch",
            "Pavan Poudel"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.09776v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-12"
    },
    "http://arxiv.org/abs/2511.09766v1": {
        "title": "Ksurf-Drone: Attention Kalman Filter for Contextual Bandit Optimization in Cloud Resource Allocation",
        "link": "http://arxiv.org/abs/2511.09766v1",
        "abstract": "Resource orchestration and configuration parameter search are key concerns for container-based infrastructure in cloud data centers. Large configuration search space and cloud uncertainties are often mitigated using contextual bandit techniques for resource orchestration including the state-of-the-art Drone orchestrator. Complexity in the cloud provider environment due to varying numbers of virtual machines introduces variability in workloads and resource metrics, making orchestration decisions less accurate due to increased nonlinearity and noise. Ksurf, a state-of-the-art variance-minimizing estimator method ideal for highly variable cloud data, enables optimal resource estimation under conditions of high cloud variability.   This work evaluates the performance of Ksurf on estimation-based resource orchestration tasks involving highly variable workloads when employed as a contextual multi-armed bandit objective function model for cloud scenarios using Drone. Ksurf enables significantly lower latency variance of $41\\%$ at p95 and $47\\%$ at p99, demonstrates a $4\\%$ reduction in CPU usage and 7 MB reduction in master node memory usage on Kubernetes, resulting in a $7\\%$ cost savings in average worker pod count on VarBench Kubernetes benchmark.",
        "authors": [
            "Michael Dang'ana",
            "Yuqiu Zhang",
            "Hans-Arno Jacobsen"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "id": "http://arxiv.org/abs/2511.09766v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-12"
    },
    "http://arxiv.org/abs/2511.09741v1": {
        "id": "http://arxiv.org/abs/2511.09741v1",
        "title": "TawPipe: Topology-Aware Weight Pipeline Parallelism for Accelerating Long-Context Large Models Training",
        "link": "http://arxiv.org/abs/2511.09741v1",
        "tags": [
            "training",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-11-12",
        "tldr": "Proposes TawPipe, a topology-aware weight pipeline parallelism method for LLM training that optimizes communication hierarchy to reduce cross-node traffic. Groups devices by topology, assigns fixed weight shards, and overlaps communication. Achieves 1.56x higher throughput than WeiPipe on 24 GPUs with long sequences.",
        "abstract": "Training large language models (LLMs) is fundamentally constrained by limited device memory and costly inter-device communication. Although pipeline parallelism alleviates memory pressure by partitioning models across devices, it incurs activation communication overhead that scales linearly with sequence length, limiting efficiency in long-context training. Recent weight-passing approaches (e.g., WeiPipe) mitigate this by transmitting model weights instead of activations, but suffer from redundant peer-to-peer (P2P) transfers and underutilized intra-node bandwidth. We propose TawPipe--topology-aware weight pipeline parallelism, which exploits hierarchical bandwidth in distributed clusters for improved communication efficiency. TawPipe: (i) groups devices based on topology to optimize intra-node collective and inter-node P2P communication; (ii) assigns each device a fixed shard of model weights and gradients, avoiding redundant transfers; and (iii) overlaps communication with computation to hide latency. Unlike global collective operations used in fully sharded data parallelism (FSDP), TawPipe confines most communication within node boundaries, significantly reducing cross-node traffic. Extensive experiments on up to 24 GPUs with LLaMA-style models show that TawPipe achieves superior throughput and scalability compared to state-of-the-art baselines.",
        "authors": [
            "Houming Wu",
            "Ling Chen"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC"
        ],
        "submit_date": "2025-11-12"
    },
    "http://arxiv.org/abs/2511.09557v3": {
        "id": "http://arxiv.org/abs/2511.09557v3",
        "title": "LLM Inference Beyond a Single Node: From Bottlenecks to Mitigations with Fast All-Reduce Communication",
        "link": "http://arxiv.org/abs/2511.09557v3",
        "tags": [
            "serving",
            "training",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-11-12",
        "tldr": "Investigates communication bottlenecks in multi-node distributed LLM inference. Proposes NVRAR, a hierarchical all-reduce algorithm using NVSHMEM for faster collective communication. Achieves 1.72x lower latency in Llama 3.1 405B decode-heavy workloads compared to NCCL.",
        "abstract": "As large language models (LLMs) continue to grow in size, distributed inference has become increasingly important. Model-parallel strategies must now efficiently scale not only across multiple GPUs but also across multiple nodes. In this work, we present a detailed performance study of multi-node distributed inference using LLMs on GPU-based supercomputers. We conduct experiments with several state-of-the-art inference engines alongside YALIS, a research-oriented prototype engine designed for controlled experimentation. We analyze the strong-scaling behavior of different model-parallel schemes and identify key bottlenecks. Since all-reduce operations are a common performance bottleneck, we develop NVRAR, a hierarchical all-reduce algorithm based on recursive doubling with NVSHMEM. NVRAR achieves up to 1.9x-3.6x lower latency than NCCL for message sizes between 128 KB and 2 MB on HPE Slingshot and InfiniBand interconnects. Integrated into YALIS, NVRAR achieves up to a 1.72x reduction in end-to-end batch latency for the Llama 3.1 405B model in multi-node decode-heavy workloads using tensor parallelism.",
        "authors": [
            "Prajwal Singhania",
            "Siddharth Singh",
            "Lannie Dalton Hough",
            "Akarsh Srivastava",
            "Harshitha Menon",
            "Charles Fredrick Jekel",
            "Abhinav Bhatele"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-11-12"
    },
    "http://arxiv.org/abs/2511.11714v1": {
        "title": "Federated Learning for Pediatric Pneumonia Detection: Enabling Collaborative Diagnosis Without Sharing Patient Data",
        "link": "http://arxiv.org/abs/2511.11714v1",
        "abstract": "Early and accurate pneumonia detection from chest X-rays (CXRs) is clinically critical to expedite treatment and isolation, reduce complications, and curb unnecessary antibiotic use. Although artificial intelligence (AI) substantially improves CXR-based detection, development is hindered by globally distributed data, high inter-hospital variability, and strict privacy regulations (e.g., HIPAA, GDPR) that make centralization impractical. These constraints are compounded by heterogeneous imaging protocols, uneven data availability, and the costs of transferring large medical images across geographically dispersed sites.   In this paper, we evaluate Federated Learning (FL) using the Sherpa.ai FL platform, enabling multiple hospitals (nodes) to collaboratively train a CXR classifier for pneumonia while keeping data in place and private. Using the Pediatric Pneumonia Chest X-ray dataset, we simulate cross-hospital collaboration with non-independent and non-identically distributed (non-IID) data, reproducing real-world variability across institutions and jurisdictions. Our experiments demonstrate that collaborative and privacy-preserving training across multiple hospitals via FL led to a dramatic performance improvement achieving 0.900 Accuracy and 0.966 ROC-AUC, corresponding to 47.5% and 50.0% gains over single-hospital models (0.610; 0.644), without transferring any patient CXR. These results indicate that FL delivers high-performing, generalizable, secure and private pneumonia detection across healthcare networks, with data kept local. This is especially relevant for rare diseases, where FL enables secure multi-institutional collaboration without data movement, representing a breakthrough for accelerating diagnosis and treatment development in low-data domains.",
        "authors": [
            "Daniel M. Jimenez-Gutierrez",
            "Enrique Zuazua",
            "Joaquin Del Rio",
            "Oleksii Sliusarenko",
            "Xabi Uribe-Etxebarria"
        ],
        "categories": [
            "cs.LG",
            "cs.CR",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.11714v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-12"
    },
    "http://arxiv.org/abs/2511.09485v1": {
        "title": "Formal Verification of a Generic Algorithm for TDM Communication Over Inter Satellite Links",
        "link": "http://arxiv.org/abs/2511.09485v1",
        "abstract": "The Python Testbed for Federated Learning Algorithms is a simple FL framework targeting edge systems, which provides the three generic algorithms: the centralized federated learning, the decentralized federated learning, and the universal TDM communication in the current time slot. The first two were formally verified in a previous paper using the CSP process algebra, and in this paper, we use the same approach to formally verify the third one, in two phases. In the first phase, we construct the CSP model as a faithful representation of the real Python code. In the second phase, the model checker PAT automatically proves correctness of the third generic algorithm by proving its deadlock freeness (safety property) and successful termination (liveness property).",
        "authors": [
            "Miroslav Popovic",
            "Marko Popovic",
            "Pavle Vasiljevic",
            "Miodrag Djukic"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.09485v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-12"
    },
    "http://arxiv.org/abs/2511.09447v1": {
        "title": "SPADA: A Spatial Dataflow Architecture Programming Language",
        "link": "http://arxiv.org/abs/2511.09447v1",
        "abstract": "Spatial dataflow architectures like the Cerebras Wafer-Scale Engine achieve exceptional performance in AI and scientific applications by leveraging distributed memory across processing elements (PEs) and localized computation. However, programming these architectures remains challenging due to the need for explicit orchestration of data movement through reconfigurable networks-on-chip and asynchronous computation triggered by data arrival. Existing FPGA and CGRA programming models emphasize loop scheduling but overlook the unique capabilities of spatial dataflow architectures, particularly efficient dataflow over regular grids and intricate routing management.   We present SPADA, a programming language that provides precise control over data placement, dataflow patterns, and asynchronous operations while abstracting architecture-specific low-level details. We introduce a rigorous dataflow semantics framework for SPADA that defines routing correctness, data races, and deadlocks. Additionally, we design and implement a compiler targeting Cerebras CSL with multi-level lowering.   SPADA serves as both a high-level programming interface and an intermediate representation for domain-specific languages (DSLs), which we demonstrate with the GT4Py stencil DSL. SPADA enables developers to express complex parallel patterns -- including pipelined reductions and multi-dimensional stencils -- in 6--8x less code than CSL with near-ideal weak scaling across three orders of magnitude. By unifying programming for spatial dataflow architectures under a single model, SPADA advances both the theoretical foundations and practical usability of these emerging high-performance computing platforms.",
        "authors": [
            "Lukas Gianinazzi",
            "Tal Ben-Nun",
            "Torsten Hoefler"
        ],
        "categories": [
            "cs.DC",
            "cs.PL"
        ],
        "id": "http://arxiv.org/abs/2511.09447v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-12"
    },
    "http://arxiv.org/abs/2511.09410v1": {
        "id": "http://arxiv.org/abs/2511.09410v1",
        "title": "No Cords Attached: Coordination-Free Concurrent Lock-Free Queues",
        "link": "http://arxiv.org/abs/2511.09410v1",
        "tags": [
            "training",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-11-12",
        "tldr": "Addresses high coordination overhead in concurrent queues for massively parallel AI workloads. Proposes Cyclic Memory Protection (CMP), a coordination-free lock-free queue using bounded protection windows. Outperforms state-of-the-art queues by 1.72-4x under high contention with hundreds of threads.",
        "abstract": "The queue is conceptually one of the simplest data structures-a basic FIFO container. However, ensuring correctness in the presence of concurrency makes existing lock-free implementations significantly more complex than their original form. Coordination mechanisms introduced to prevent hazards such as ABA, use-after-free, and unsafe reclamation often dominate the design, overshadowing the queue itself. Many schemes compromise strict FIFO ordering, unbounded capacity, or lock-free progress to mask coordination overheads. Yet the true source of complexity lies in the pursuit of infinite protection against reclamation hazards--theoretically sound but impractical and costly. This pursuit not only drives unnecessary complexity but also creates a protection paradox where excessive protection reduces system resilience rather than improving it. While such costs may be tolerable in conventional workloads, the AI era has shifted the paradigm: training and inference pipelines involve hundreds to thousands of concurrent threads per node, and at this scale, protection and coordination overheads dominate, often far heavier than the basic queue operations themselves.   This paper introduces Cyclic Memory Protection (CMP), a coordination-free queue that preserves strict FIFO semantics, unbounded capacity, and lock-free progress while restoring simplicity. CMP reclaims the strict FIFO that other approaches sacrificed through bounded protection windows that provide practical reclamation guarantees. We prove strict FIFO and safety via linearizability and bounded reclamation analysis, and show experimentally that CMP outperforms state-of-the-art lock-free queues by up to 1.72-4x under high contention while maintaining scalability to hundreds of threads. Our work demonstrates that highly concurrent queues can return to their fundamental simplicity without weakening queue semantics.",
        "authors": [
            "Yusuf Motiwala"
        ],
        "categories": [
            "cs.DC",
            "cs.DS",
            "cs.PF"
        ],
        "submit_date": "2025-11-12"
    },
    "http://arxiv.org/abs/2511.09262v1": {
        "title": "CheetahGIS: Architecting a Scalable and Efficient Streaming Spatial Query Processing System",
        "link": "http://arxiv.org/abs/2511.09262v1",
        "abstract": "Spatial data analytics systems are widely studied in both the academia and industry. However, existing systems are limited when handling a large number of moving objects and real time spatial queries. In this work, we architect a scalable and efficient system CheetahGIS to process streaming spatial queries over massive moving objects. In particular, CheetahGIS is built upon Apache Flink Stateful Functions (StateFun), an API for building distributed streaming applications with an actor-like model. CheetahGIS enjoys excellent scalability due to its modular architecture, which clearly decomposes different components and allows scaling individual components. To improve the efficiency and scalability of CheetahGIS, we devise a suite of optimizations, e.g., lightweight global grid-based index, metadata synchroniza tion strategies, and load balance mechanisms. We also formulate a generic paradigm for spatial query processing in CheetahGIS, and verify its generality by processing three representative streaming queries (i.e., object query, range count query, and k nearest neighbor query). We conduct extensive experiments on both real and synthetic datasets to evaluate CheetahGIS.",
        "authors": [
            "Jiaping Cao",
            "Ting Sun",
            "Man Lung Yiu",
            "Xiao Yan",
            "Bo Tang"
        ],
        "categories": [
            "cs.DB",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.09262v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-12"
    },
    "http://arxiv.org/abs/2511.09582v2": {
        "title": "Revisit to the Bai-Galbraith signature scheme",
        "link": "http://arxiv.org/abs/2511.09582v2",
        "abstract": "Dilithium is one of the NIST approved lattice-based signature schemes. In this short note we describe the Bai-Galbraith signature scheme proposed in BG14, which differs to Dilithium, due to the fact that there is no public key compression. This lattice-based signature scheme is based on Learning with Errors (LWE).",
        "authors": [
            "Banhirup Sengupta",
            "Peenal Gupta",
            "Souvik Sengupta"
        ],
        "categories": [
            "cs.CR",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.09582v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-12"
    },
    "http://arxiv.org/abs/2511.09194v1": {
        "title": "Minimize Your Critical Path with Combine-and-Exchange Locks",
        "link": "http://arxiv.org/abs/2511.09194v1",
        "abstract": "Coroutines are experiencing a renaissance as many modern programming languages support the use of cooperative multitasking for highly parallel or asynchronous applications. One of the greatest advantages of this is that concurrency and synchronization is manged entirely in the userspace, omitting heavy-weight system calls. However, we find that state-of-the-art userspace synchronization primitives approach synchronization in the userspace from the perspective of kernel-level scheduling. This introduces unnecessary delays on the critical path of the application, limiting throughput. In this paper, we re-think synchronization for tasks that are scheduled entirely in the userspace (e.g., coroutines, fibers, etc.). We develop Combine-and-Exchange Scheduling (CES), a novel scheduling approach that ensures contended critical sections stay on the same thread of execution while parallelizable work is evenly spread across the remaining threads. We show that our approach can be applied to many existing languages and libraries, resulting in 3-fold performance improvements in application benchmarks as well as 8-fold performance improvements in microbenchmarks.",
        "authors": [
            "Simon König",
            "Lukas Epple",
            "Christian Becker"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.09194v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-12"
    },
    "http://arxiv.org/abs/2511.09143v2": {
        "id": "http://arxiv.org/abs/2511.09143v2",
        "title": "Flex-MIG: Enabling Distributed Execution on MIG",
        "link": "http://arxiv.org/abs/2511.09143v2",
        "tags": [
            "hardware",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-11-12",
        "tldr": "Addresses GPU underutilization from rigid NVIDIA MIG partitioning. Proposes Flex-MIG, a software framework enabling one-to-many allocation and shared-memory collectives for distributed execution across MIG instances. Improves cluster makespan by up to 17% on diverse traces.",
        "abstract": "GPU clusters in multi-tenant settings often suffer from underutilization, making GPU-sharing technologies essential for efficient resource use. Among them, NVIDIA Multi-Instance GPU (MIG) has gained traction for providing hardware-level isolation that enables concurrent workloads without interference. However, MIG's hardware rigidity and the conventional one-to-one allocation model jointly lead to severe fragmentation and cluster-wide underutilization. We present Flex-MIG, a software-only framework that replaces one-to-one with a one-to-many allocation model and enables host-shared-memory collectives across MIG instances without hardware modification. Flex-MIG eliminates drain-required reconfiguration, reduces fragmentation, and improves makespan by up to 17% across diverse traces, showing that rethinking MIG's operational model as a software-coordinated layer substantially improves cluster efficiency.",
        "authors": [
            "Myeongsu Kim",
            "Ikjun Yeom",
            "Younghoon Kim"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-11-12"
    },
    "http://arxiv.org/abs/2511.09100v1": {
        "title": "FedPM: Federated Learning Using Second-order Optimization with Preconditioned Mixing of Local Parameters",
        "link": "http://arxiv.org/abs/2511.09100v1",
        "abstract": "We propose Federated Preconditioned Mixing (FedPM), a novel Federated Learning (FL) method that leverages second-order optimization. Prior methods--such as LocalNewton, LTDA, and FedSophia--have incorporated second-order optimization in FL by performing iterative local updates on clients and applying simple mixing of local parameters on the server. However, these methods often suffer from drift in local preconditioners, which significantly disrupts the convergence of parameter training, particularly in heterogeneous data settings. To overcome this issue, we refine the update rules by decomposing the ideal second-order update--computed using globally preconditioned global gradients--into parameter mixing on the server and local parameter updates on clients. As a result, our FedPM introduces preconditioned mixing of local parameters on the server, effectively mitigating drift in local preconditioners.   We provide a theoretical convergence analysis demonstrating a superlinear rate for strongly convex objectives in scenarios involving a single local update. To demonstrate the practical benefits of FedPM, we conducted extensive experiments. The results showed significant improvements with FedPM in the test accuracy compared to conventional methods incorporating simple mixing, fully leveraging the potential of second-order optimization.",
        "authors": [
            "Hiro Ishii",
            "Kenta Niwa",
            "Hiroshi Sawada",
            "Akinori Fujino",
            "Noboru Harada",
            "Rio Yokota"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.09100v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-12"
    },
    "http://arxiv.org/abs/2511.09051v1": {
        "title": "Attack-Centric by Design: A Program-Structure Taxonomy of Smart Contract Vulnerabilities",
        "link": "http://arxiv.org/abs/2511.09051v1",
        "abstract": "Smart contracts concentrate high value assets and complex logic in small, immutable programs, where even minor bugs can cause major losses. Existing taxonomies and tools remain fragmented, organized around symptoms such as reentrancy rather than structural causes. This paper introduces an attack-centric, program-structure taxonomy that unifies Solidity vulnerabilities into eight root-cause families covering control flow, external calls, state integrity, arithmetic safety, environmental dependencies, access control, input validation, and cross-domain protocol assumptions. Each family is illustrated through concise Solidity examples, exploit mechanics, and mitigations, and linked to the detection signals observable by static, dynamic, and learning-based tools. We further cross-map legacy datasets (SmartBugs, SolidiFI) to this taxonomy to reveal label drift and coverage gaps. The taxonomy provides a consistent vocabulary and practical checklist that enable more interpretable detection, reproducible audits, and structured security education for both researchers and practitioners.",
        "authors": [
            "Parsa Hedayatnia",
            "Tina Tavakkoli",
            "Hadi Amini",
            "Mohammad Allahbakhsh",
            "Haleh Amintoosi"
        ],
        "categories": [
            "cs.CR",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.09051v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-12"
    },
    "http://arxiv.org/abs/2511.08998v1": {
        "title": "Experiences Building Enterprise-Level Privacy-Preserving Federated Learning to Power AI for Science",
        "link": "http://arxiv.org/abs/2511.08998v1",
        "abstract": "Federated learning (FL) is a promising approach to enabling collaborative model training without centralized data sharing, a crucial requirement in scientific domains where data privacy, ownership, and compliance constraints are critical. However, building user-friendly enterprise-level FL frameworks that are both scalable and privacy-preserving remains challenging, especially when bridging the gap between local prototyping and distributed deployment across heterogeneous client computing infrastructures. In this paper, based on our experiences building the Advanced Privacy-Preserving Federated Learning (APPFL) framework, we present our vision for an enterprise-grade, privacy-preserving FL framework designed to scale seamlessly across computing environments. We identify several key capabilities that such a framework must provide: (1) Scalable local simulation and prototyping to accelerate experimentation and algorithm design; (2) seamless transition from simulation to deployment; (3) distributed deployment across diverse, real-world infrastructures, from personal devices to cloud clusters and HPC systems; (4) multi-level abstractions that balance ease of use and research flexibility; and (5) comprehensive privacy and security through techniques such as differential privacy, secure aggregation, robust authentication, and confidential computing. We further discuss architectural designs to realize these goals. This framework aims to bridge the gap between research prototypes and enterprise-scale deployment, enabling scalable, reliable, and privacy-preserving AI for science.",
        "authors": [
            "Zilinghan Li",
            "Aditya Sinha",
            "Yijiang Li",
            "Kyle Chard",
            "Kibaek Kim",
            "Ravi Madduri"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.08998v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-12"
    },
    "http://arxiv.org/abs/2511.08948v1": {
        "title": "Evaluating HPC-Style CPU Performance and Cost in Virtualized Cloud Infrastructures",
        "link": "http://arxiv.org/abs/2511.08948v1",
        "abstract": "This paper evaluates HPC-style CPU performance and cost in virtualized cloud infrastructures using a subset of OpenMP workloads in the SPEC ACCEL suite. Four major cloud providers by market share AWS, Azure, Google Cloud Platform (GCP), and Oracle Cloud Infrastructure (OCI) are compared across Intel, AMD, and ARM general purpose instance types under both on-demand and one-year discounted pricing. AWS consistently delivers the shortest runtime in all three instance types, yet charges a premium, especially for on-demand usage. OCI emerges as the most economical option across all CPU families, although it generally runs workloads more slowly than AWS. Azure often exhibits mid-range performance and cost, while GCP presents a mixed profile: it sees a notable boost when moving from Intel to AMD. On the other hand, its ARM instance is more than twice as slow as its own AMD offering and remains significantly more expensive. AWS's internal comparisons reveal that its ARM instance can outperform its Intel and AMD siblings by up to 49 percent in runtime. These findings highlight how instance choices and provider selection can yield substantial variations in both runtime and price, indicating that workload priorities, whether raw speed or cost minimization, should guide decisions on instance types.",
        "authors": [
            "Jay Tharwani",
            "Shobhit Aggarwal",
            "Arnab A Purkayastha"
        ],
        "categories": [
            "cs.DC",
            "cs.PF"
        ],
        "id": "http://arxiv.org/abs/2511.08948v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-12"
    },
    "http://arxiv.org/abs/2511.08936v1": {
        "title": "Distribution and Management of Datacenter Load Decoupling",
        "link": "http://arxiv.org/abs/2511.08936v1",
        "abstract": "The exploding power consumption of AI and cloud datacenters (DCs) intensifies the long-standing concerns about their carbon footprint, especially because DCs' need for constant power clashes with volatile renewable generation needed for grid decarbonization. DC flexibility (a.k.a. load adaptation) is a key to reducing DC carbon emissions by improving grid renewable absorption.   DC flexibility can be created, without disturbing datacenter capacity by decoupling a datacenter's power capacity and grid load with a collection of energy resources. Because decoupling can be costly, we study how to best distribute and manage decoupling to maximize benefits for all. Key considerations include site variation and datacenter-grid cooperation.   We first define and compute the power and energy needs of datacenter load decoupling, and then we evaluate designed distribution and management approaches. Evaluation shows that optimized distribution can deliver >98% of the potential grid carbon reduction with 70% of the total decoupling need. For management, DC-grid cooperation (2-way sharing and control vs. 1-way info sharing) enables 1.4x grid carbon reduction. Finally, we show that decoupling may be economically viable, as on average datacenters can get power cost and carbon emissions benefits greater than their local costs of decoupling. However, skew across sites suggests grid intervention may be required.",
        "authors": [
            "Liuzixuan Lin",
            "Andrew A. Chien"
        ],
        "categories": [
            "cs.DC",
            "eess.SY"
        ],
        "id": "http://arxiv.org/abs/2511.08936v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-12"
    },
    "http://arxiv.org/abs/2511.11678v1": {
        "id": "http://arxiv.org/abs/2511.11678v1",
        "title": "A Structure-Agnostic Co-Tuning Framework for LLMs and SLMs in Cloud-Edge Systems",
        "link": "http://arxiv.org/abs/2511.11678v1",
        "tags": [
            "edge",
            "training",
            "RL"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes Co-PLMs, a co-tuning framework for collaborative training of cloud LLMs and edge SLMs via distilled proxy models, enabling structure-agnostic knowledge exchange. Achieves 5.38% Rouge-L and 4.88% EM gains over SOTA.",
        "abstract": "The surge in intelligent applications driven by large language models (LLMs) has made it increasingly difficult for bandwidth-limited cloud servers to process extensive LLM workloads in real time without compromising user data privacy. To solve these problems, recent research has focused on constructing cloud-edge consortia that integrate server-based LLM with small language models (SLMs) on mobile edge devices. Furthermore, designing collaborative training mechanisms within such consortia to enhance inference performance has emerged as a promising research direction. However, the cross-domain deployment of SLMs, coupled with structural heterogeneity in SLMs architectures, poses significant challenges to enhancing model performance. To this end, we propose Co-PLMs, a novel co-tuning framework for collaborative training of large and small language models, which integrates the process of structure-agnostic mutual learning to realize knowledge exchange between the heterogeneous language models. This framework employs distilled proxy models (DPMs) as bridges to enable collaborative training between the heterogeneous server-based LLM and on-device SLMs, while preserving the domain-specific insights of each device. The experimental results show that Co-PLMs outperform state-of-the-art methods, achieving average increases of 5.38% in Rouge-L and 4.88% in EM.",
        "authors": [
            "Yuze Liu",
            "Yunhan Wang",
            "Tiehua Zhang",
            "Zhishu Shen",
            "Cheng Peng",
            "Libing Wu",
            "Feng Xia",
            "Jiong Jin"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "submit_date": "2025-11-12"
    },
    "http://arxiv.org/abs/2511.11672v2": {
        "id": "http://arxiv.org/abs/2511.11672v2",
        "title": "OSGym: Super-Scalable Distributed Data Engine for Generalizable Computer Agents",
        "link": "http://arxiv.org/abs/2511.11672v2",
        "tags": [
            "training",
            "edge",
            "hardware"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Presents OSGym, a scalable and affordable distributed engine for training computer agents. Utilizes parallelized OS replicas for dynamic environments, optimizing resource use and cost. Generates up to 1420 trajectories/min at $0.2-0.3/day per instance.",
        "abstract": "We introduce OSGym, a super-scalable distributed data engine for training agents across diverse computer-related tasks. OSGym efficiently scales to over a thousand operating system (OS) replicas at an academia-affordable cost, serving as dynamic runtime environments for intelligent agents. It offers three key advantages. (1) Scalability: Despite the intensive resource requirements of running multiple OS replicas, OSGym parallelizes over a thousand instances while maintaining operational efficiency under constrained resources, generating up to 1420 multi-turn trajectories per minute. (2) Generality and Customizability: OSGym supports a broad spectrum of tasks that run on OS platforms, including tool use, browser interactions, software engineering, and office applications, with flexible support for diverse model training algorithms. (3) Economic Viability: OSGym operates at only 0.2-0.3 USD per day per OS replica using accessible on-demand compute providers. It is fully open-source and freely available for both research and commercial use. Experiments show that OSGym enables comprehensive data collection, supervised fine-tuning, and reinforcement learning pipelines for computer agents. Models trained with OSGym outperform state-of-the-art baselines, demonstrating its potential to advance scalability and universality in future agent research.",
        "authors": [
            "Zengyi Qin",
            "Jinyuan Chen",
            "Yunze Man",
            "Shengcao Cao",
            "Ziqi Pang",
            "Zhuoyuan Wang",
            "Xin Sun",
            "Gen Lin",
            "Han Fang",
            "Ling Zhu",
            "Zixin Xie",
            "Zibu Wei",
            "Tianshu Ran",
            "Haoran Geng",
            "Xander Wu",
            "Zachary Bright",
            "Qizhen Sun",
            "Rui Wang",
            "Yuyang Cai",
            "Song Wang",
            "Jiace Zhao",
            "Han Cao",
            "Yeyang Zhou",
            "Tianrui Liu",
            "Ray Pan",
            "Chongye Yang",
            "Xiang Ren",
            "Bo Zhang",
            "Yutong Ban",
            "Jitendra Malik",
            "Pieter Abbeel"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-11-11"
    },
    "http://arxiv.org/abs/2511.08713v1": {
        "id": "http://arxiv.org/abs/2511.08713v1",
        "title": "An MLIR pipeline for offloading Fortran to FPGAs via OpenMP",
        "link": "http://arxiv.org/abs/2511.08713v1",
        "tags": [
            "offloading",
            "hardware",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes an MLIR-based pipeline for offloading Fortran code to FPGAs via OpenMP directives. Combines OpenMP and HLS dialects to enable portable compilation and kernel optimization. Achieves reduced development effort while supporting manual kernel tuning for FPGA acceleration.",
        "abstract": "With the slowing of Moore's Law, heterogeneous computing platforms such as Field Programmable Gate Arrays (FPGAs) have gained increasing interest for accelerating HPC workloads. In this work we present, to the best of our knowledge, the first implementation of selective code offloading to FPGAs via the OpenMP target directive within MLIR. Our approach combines the MLIR OpenMP dialect with a High-Level Synthesis (HLS) dialect to provide a portable compilation flow targeting FPGAs. Unlike prior OpenMP FPGA efforts that rely on custom compilers, by contrast we integrate with MLIR and so support any MLIR-compatible front end, demonstrated here with Flang. Building upon a range of existing MLIR building blocks significantly reduces the effort required and demonstrates the composability benefits of the MLIR ecosystem. Our approach supports manual optimisation of offloaded kernels through standard OpenMP directives, and this work establishes a flexible and extensible path for directive-based FPGA acceleration integrated within the MLIR ecosystem.",
        "authors": [
            "Gabriel Rodriguez-Canal",
            "David Katz",
            "Nick Brown"
        ],
        "categories": [
            "cs.DC",
            "cs.PL"
        ],
        "submit_date": "2025-11-11"
    },
    "http://arxiv.org/abs/2511.08373v1": {
        "title": "Priority Matters: Optimising Kubernetes Clusters Usage with Constraint-Based Pod Packing",
        "link": "http://arxiv.org/abs/2511.08373v1",
        "abstract": "Distributed applications employ Kubernetes for scalable, fault-tolerant deployments over computer clusters, where application components run in groups of containers called pods. The scheduler, at the heart of Kubernetes' architecture, determines the placement of pods given their priority and resource requirements on cluster nodes. To quickly allocate pods, the scheduler uses lightweight heuristics that can lead to suboptimal placements and resource fragmentation, preventing allocations of otherwise deployable pods on the available nodes.   We propose the usage of constraint programming to find the optimal allocation of pods satisfying all their priorities and resource requests. Implementation-wise, our solution comes as a plug-in to the default scheduler that operates as a fallback mechanism when some pods cannot be allocated. Using the OR-Tools constraint solver, our experiments on small-to-mid-sized clusters indicate that, within a 1-second scheduling window, our approach places more higher-priority pods than the default scheduler (possibly demonstrating allocation optimality) in over 44\\% of realisable allocation scenarios where the default scheduler fails, while certifying that the default scheduler's placement is already optimal in over 19\\% of scenarios. With a 10-second window, our approach improves placements in over 73\\% and still certifies that the default scheduler's placement is already optimal in over 19\\% of scenarios.",
        "authors": [
            "Henrik Daniel Christensen",
            "Saverio Giallorenzo",
            "Jacopo Mauro"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.08373v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-11"
    },
    "http://arxiv.org/abs/2511.08222v1": {
        "title": "Gathering in Vertex- and Edge-Transitive Graphs without Multiplicity Detection under Round Robin",
        "link": "http://arxiv.org/abs/2511.08222v1",
        "abstract": "In the field of swarm robotics, one of the most studied problem is Gathering. It asks for a distributed algorithm that brings the robots to a common location, not known in advance. We consider the case of robots constrained to move along the edges of a graph under the well-known OBLOT model. Gathering is then accomplished once all the robots occupy a same vertex. Differently from classical settings, we assume: i) the initial configuration may contain multiplicities, i.e. more than one robot may occupy the same vertex; ii) robots cannot detect multiplicities; iii) robots move along the edges of vertex- and edge-transitive graphs, i.e. graphs where all the vertices (and the edges, resp.) belong to a same class of equivalence. To balance somehow such a `hostile' setting, as a scheduler for the activation of the robots, we consider the round-robin, where robots are cyclically activated one at a time.   We provide some basic impossibility results and we design two different algorithms approaching the Gathering for robots moving on two specific topologies belonging to edge- and vertex-transitive graphs: infinite grids and hypercubes. The two algorithms are both time-optimal and heavily exploit the properties of the underlying topologies. Because of this, we conjecture that no general algorithm can exist for all the solvable cases.",
        "authors": [
            "Serafino Cicerone",
            "Alessia Di Fonso",
            "Gabriele Di Stefano",
            "Alfredo Navarra"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.08222v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-11"
    },
    "http://arxiv.org/abs/2511.08210v1": {
        "title": "Forgetting Alternation and Blossoms: A New Framework for Fast Matching Augmentation and Its Applications to Sequential/Distributed/Streaming Computation",
        "link": "http://arxiv.org/abs/2511.08210v1",
        "abstract": "Finding a maximum cardinality matching in a graph is one of the most fundamental problems. An algorithm proposed by Micali and Vazirani (1980) is well-known to solve the problem in $O(m\\sqrt{n})$ time, which is still one of the fastest algorithms in general. While the MV algorithm itself is not so complicated and is indeed convincing, its correctness proof is extremely challenging, which can be seen from the history: after the first algorithm paper had appeared in 1980, Vazirani has made several attempts to give a complete proof for more than 40 years. It seems, roughly speaking, caused by the nice but highly complex structure of the shortest alternating paths in general graphs that are deeply intertwined with the so-called (nested) blossoms.   In this paper, we propose a new structure theorem on the shortest alternating paths in general graphs without taking into the details of blossoms. The high-level idea is to forget the alternation (of matching and non-matching edges) as early as possible. A key ingredient is a notion of alternating base trees (ABTs) introduced by Izumi, Kitamura, and Yamaguchi (2024) to develop a nearly linear-time distributed algorithm. Our structure theorem refines the properties of ABTs exploited in their algorithm, and we also give simpler alternative proofs for them. Based on our structure theorem, we propose a new algorithm, which is slightly slower but more implementable and much easier to confirm its correctness than the MV algorithm.   As applications of our framework, we also present new $(1 - ε)$-approximation algorithms in the distributed and semi-streaming settings. Both algorithms are deterministic, and substantially improve the best known upper bounds on the running time. The algorithms are built on the top of a novel framework of amplifying approximation factors of given matchings, which is of independent interest.",
        "authors": [
            "Taisuke Izumi",
            "Naoki Kitamura",
            "Yutaro Yamaguchi"
        ],
        "categories": [
            "cs.DS",
            "cs.DC",
            "math.CO"
        ],
        "id": "http://arxiv.org/abs/2511.08210v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-11"
    },
    "http://arxiv.org/abs/2511.11664v1": {
        "id": "http://arxiv.org/abs/2511.11664v1",
        "title": "Range Asymmetric Numeral Systems-Based Lightweight Intermediate Feature Compression for Split Computing of Deep Neural Networks",
        "link": "http://arxiv.org/abs/2511.11664v1",
        "tags": [
            "edge",
            "offloading",
            "storage"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Addresses communication overhead in split DNN inference by proposing a lightweight compression framework using rANS encoding, asymmetric integer quantization, and sparse tensor representation. Achieves near-baseline accuracy on vision and language models (e.g., ResNet, Llama2) with sub-millisecond encoding latency.",
        "abstract": "Split computing distributes deep neural network inference between resource-constrained edge devices and cloud servers but faces significant communication bottlenecks when transmitting intermediate features. To this end, in this paper, we propose a novel lightweight compression framework that leverages Range Asymmetric Numeral Systems (rANS) encoding with asymmetric integer quantization and sparse tensor representation to reduce transmission overhead dramatically. Specifically, our approach combines asymmetric integer quantization with a sparse representation technique, eliminating the need for complex probability modeling or network modifications. The key contributions include: (1) a distribution-agnostic compression pipeline that exploits inherent tensor sparsity to achieve bandwidth reduction with minimal computational overhead; (2) an approximate theoretical model that optimizes tensor reshaping dimensions to maximize compression efficiency; and (3) a GPU-accelerated implementation with sub-millisecond encoding/decoding latency. Extensive evaluations across diverse neural architectures (ResNet, VGG16, MobileNetV2, SwinT, DenseNet121, EfficientNetB0) demonstrate that the proposed framework consistently maintains near-baseline accuracy across CIFAR100 and ImageNet benchmarks. Moreover, we validated the framework's effectiveness on advanced natural language processing tasks by employing Llama2 7B and 13B on standard benchmarks such as MMLU, HellaSwag, ARC, PIQA, Winogrande, BoolQ, and OpenBookQA, demonstrating its broad applicability beyond computer vision. Furthermore, this method addresses a fundamental bottleneck in deploying sophisticated artificial intelligence systems in bandwidth-constrained environments without compromising model performance.",
        "authors": [
            "Mingyu Sung",
            "Suhwan Im",
            "Vikas Palakonda",
            "Jae-Mo Kang"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.CV"
        ],
        "submit_date": "2025-11-11"
    },
    "http://arxiv.org/abs/2511.08158v2": {
        "id": "http://arxiv.org/abs/2511.08158v2",
        "title": "LOw-cOst yet High-Performant Sparse Matrix-Matrix Multiplication on Arm SME Architectures",
        "link": "http://arxiv.org/abs/2511.08158v2",
        "tags": [
            "kernel",
            "quantization",
            "hardware"
        ],
        "relevant": true,
        "indexed_date": "2025-11-11",
        "tldr": "Proposes LOOPS, a hybrid execution framework exploiting Arm SME and vector instructions for efficient multi-precision SpMM kernels. Uses adaptive parallelization guided by a performance model. Achieves up to 33.5× speedup over GPU methods and better energy efficiency.",
        "abstract": "Sparse matrix-dense matrix multiplication (SpMM) is a critical kernel in both scientific computing and emerging graph learning workloads. The recent Armv9 architecture introduces Scalable Matrix Extension (SME), enabling tile-based matrix operations with high throughput. However, effectively exploiting both SME and traditional SIMD resources for unstructured sparse workloads remains an open challenge. To address this, we propose LOOPS, a hybrid execution framework that combines row-wise CSR-part with vector-wise BCSR-part layout, enabling cooperative utilization of vector instructions (NEON) and Scalable Matrix Extension (SME) resources. LOOPS supports multi-precision SpMM across FP64, FP32, and FP16 via an adaptive two-level parallelization scheme guided by a lightweight performance model. Experimental results on the entire SuiteSparse on an Apple's M4Pro CPU show that LOOPS achieves average speedups of 9.93$\\times$ (FP32)/14.4$\\times$ (FP64) against the CPU baseline TACO and 71.3$\\times$ (FP32)/54.8$\\times$ (FP64) with respect to Armadillo. A comparison of LOOPS running on the same CPU with two GPU methods (cuSPARSE, Magicube) executed on an NVIDIA A100 GPU show average speedups for LOOPS between 19.8$\\times$ and 33.5$\\times$, depending on the precision. Notably, LOOPS delivers significantly better energy efficiency than the GPU codes on the A100 GPU.",
        "authors": [
            "Kelun Lei",
            "Hailong Yang",
            "Kaige Zhang",
            "Kejie Ma",
            "Yiqing Wang",
            "Xin You",
            "Yufan Xu",
            "Enrique S. Quintana-Orti",
            "Zhongzhi Luan",
            "Yi Liu",
            "Depei Qian"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-11-11"
    },
    "http://arxiv.org/abs/2511.08147v1": {
        "title": "ProbSelect: Stochastic Client Selection for GPU-Accelerated Compute Devices in the 3D Continuum",
        "link": "http://arxiv.org/abs/2511.08147v1",
        "abstract": "Integration of edge, cloud and space devices into a unified 3D continuum imposes significant challenges for client selection in federated learning systems. Traditional approaches rely on continuous monitoring and historical data collection, which becomes impractical in dynamic environments where satellites and mobile devices frequently change operational conditions. Furthermore, existing solutions primarily consider CPU-based computation, failing to capture complex characteristics of GPU-accelerated training that is prevalent across the 3D continuum. This paper introduces ProbSelect, a novel approach utilizing analytical modeling and probabilistic forecasting for client selection on GPU-accelerated devices, without requiring historical data or continuous monitoring. We model client selection within user-defined SLOs. Extensive evaluation across diverse GPU architectures and workloads demonstrates that ProbSelect improves SLO compliance by 13.77% on average while achieving 72.5% computational waste reduction compared to baseline approaches.",
        "authors": [
            "Andrija Stanisic",
            "Stefan Nastic"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "id": "http://arxiv.org/abs/2511.08147v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-11"
    },
    "http://arxiv.org/abs/2511.08142v1": {
        "title": "BIPPO: Budget-Aware Independent PPO for Energy-Efficient Federated Learning Services",
        "link": "http://arxiv.org/abs/2511.08142v1",
        "abstract": "Federated Learning (FL) is a promising machine learning solution in large-scale IoT systems, guaranteeing load distribution and privacy. However, FL does not natively consider infrastructure efficiency, a critical concern for systems operating in resource-constrained environments. Several Reinforcement Learning (RL) based solutions offer improved client selection for FL; however, they do not consider infrastructure challenges, such as resource limitations and device churn. Furthermore, the training of RL methods is often not designed for practical application, as these approaches frequently do not consider generalizability and are not optimized for energy efficiency. To fill this gap, we propose BIPPO (Budget-aware Independent Proximal Policy Optimization), which is an energy-efficient multi-agent RL solution that improves performance. We evaluate BIPPO on two image classification tasks run in a highly budget-constrained setting, with FL clients training on non-IID data, a challenging context for vanilla FL. The improved sampler of BIPPO enables it to increase the mean accuracy compared to non-RL mechanisms, traditional PPO, and IPPO. In addition, BIPPO only consumes a negligible proportion of the budget, which stays consistent even if the number of clients increases. Overall, BIPPO delivers a performant, stable, scalable, and sustainable solution for client selection in IoT-FL.",
        "authors": [
            "Anna Lackinger",
            "Andrea Morichetta",
            "Pantelis A. Frangoudis",
            "Schahram Dustdar"
        ],
        "categories": [
            "cs.LG",
            "cs.DC",
            "cs.MA"
        ],
        "id": "http://arxiv.org/abs/2511.08142v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-11"
    },
    "http://arxiv.org/abs/2511.08135v1": {
        "id": "http://arxiv.org/abs/2511.08135v1",
        "title": "UniFormer: Unified and Efficient Transformer for Reasoning Across General and Custom Computing",
        "link": "http://arxiv.org/abs/2511.08135v1",
        "tags": [
            "training",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes UniFormer, a unified Transformer architecture for efficient deployment on both general-purpose GPUs and custom hardware. Unifies parallelism and compute-storage fusion to optimize performance. Achieves SOTA accuracy and latency on GPUs with improved FPGA adaptability.",
        "abstract": "The success of neural networks such as convolutional neural networks (CNNs) has been largely attributed to their effective and widespread deployment on customised computing platforms, including field-programmable gate arrays (FPGAs) and application-specific integrated circuits (ASICs). In the current era, Transformer-based architectures underpin the majority of state-of-the-art (SOTA) larger models that are also increasingly deployed on customised computing hardware for low-power and real-time applications. However, the fundamentally different parallel computation paradigms between general-purpose and customised computing often lead to compromises in model transfer and deployability, which typically come at the cost of complexity, efficiency or accuracy. Moreover, many cross-platform optimisation principles have also remained underexplored in existing studies. This paper introduces UniFormer, a unified and efficient Transformer architecture for both general-purpose and customised computing platforms. By enabling higher parallelism and compute-storage fusion, UniFormer achieves state-of-the-art (SOTA) accuracy and latency on GPUs while exhibiting strong adaptability on FPGAs. To the best of our knowledge, this paper is the first efficient Transformer work that jointly considers both general-purpose and customised computing architectures.",
        "authors": [
            "Zhuoheng Ran",
            "Chong Wu",
            "Renjie Xu",
            "Maolin Che",
            "Hong Yan"
        ],
        "categories": [
            "cs.DC",
            "cs.AR"
        ],
        "submit_date": "2025-11-11"
    },
    "http://arxiv.org/abs/2511.08130v2": {
        "title": "Foam Segmentation in Wastewater Treatment Plants: A Federated Learning Approach with Segment Anything Model 2",
        "link": "http://arxiv.org/abs/2511.08130v2",
        "abstract": "Foam formation in Wastewater Treatment Plants (WTPs) is a major challenge that can reduce treatment efficiency and increase costs. The ability to automatically examine changes in real-time with respect to the percentage of foam can be of great benefit to the plant. However, large amounts of labeled data are required to train standard Machine Learning (ML) models. The development of these systems is slow due to the scarcity and heterogeneity of labeled data. Additionally, the development is often hindered by the fact that different WTPs do not share their data due to privacy concerns. This paper proposes a new framework to address these challenges by combining Federated Learning (FL) with the state-of-the-art base model for image segmentation, Segment Anything Model 2 (SAM2). The FL paradigm enables collaborative model training across multiple WTPs without centralizing sensitive operational data, thereby ensuring privacy. The framework accelerates training convergence and improves segmentation performance even with limited local datasets by leveraging SAM2's strong pre-trained weights for initialization. The methodology involves fine-tuning SAM2 on distributed clients (edge nodes) using the Flower framework, where a central Fog server orchestrates the process by aggregating model weights without accessing private data. The model was trained and validated using various data collections, including real-world images captured at a WTPs in Granada, Spain, a synthetically generated foam dataset, and images from publicly available datasets to improve generalization. This research offers a practical, scalable, and privacy-aware solution for automatic foam tracking in WTPs. The findings highlight the significant potential of integrating large-scale foundational models into FL systems to solve real-world industrial challenges characterized by distributed and sensitive data.",
        "authors": [
            "Mehmet Batuhan Duman",
            "Alejandro Carnero",
            "Cristian Martín",
            "Daniel Garrido",
            "Manuel Díaz"
        ],
        "categories": [
            "cs.CV",
            "cs.DC",
            "cs.LG"
        ],
        "id": "http://arxiv.org/abs/2511.08130v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-11"
    },
    "http://arxiv.org/abs/2511.08034v1": {
        "title": "Generic Algorithm for Universal TDM Communication Over Inter Satellite Links",
        "link": "http://arxiv.org/abs/2511.08034v1",
        "abstract": "The original Python Testbed for Federated Learning Algorithms is a light FL framework, which provides the three generic algorithms: the centralized federated learning, the decentralized federated learning, and the TDM communication (i.e., peer data exchange) in the current time slot. The limitation of the latter is that it allows communication only between pairs of network nodes. This paper presents the new generic algorithm for the universal TDM communication that overcomes this limitation, such that a node can communicate with an arbitrary number of peers (assuming the peers also want to communicate with it). The paper covers: (i) the algorithm's theoretical foundation, (ii) the system design, and (iii) the system validation. The main advantage of the new algorithm is that it supports real-world TDM communications over inter satellite links.",
        "authors": [
            "Miroslav Popovic",
            "Marko Popovic",
            "Pavle Vasiljevic",
            "Ilija Basicevic"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.08034v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-11"
    },
    "http://arxiv.org/abs/2511.07886v1": {
        "title": "ACGraph: An Efficient Asynchronous Out-of-Core Graph Processing Framework",
        "link": "http://arxiv.org/abs/2511.07886v1",
        "abstract": "Graphs are a ubiquitous data structure in diverse domains such as machine learning, social networks, and data mining. As real-world graphs continue to grow beyond the memory capacity of single machines, out-of-core graph processing systems have emerged as a viable solution. Yet, existing systems that rely on strictly synchronous, iteration-by-iteration execution incur significant overheads. In particular, their scheduling mechanisms lead to I/O inefficiencies, stemming from read and work amplification, and induce costly synchronization stalls hindering sustained disk utilization. To overcome these limitations, we present {\\em ACGraph}, a novel asynchronous graph processing system optimized for SSD-based environments with constrained memory resources. ACGraph employs a dynamic, block-centric priority scheduler that adjusts in real time based on workload, along with an online asynchronous worklist that minimizes redundant disk accesses by efficiently reusing active blocks in memory. Moreover, ACGraph unifies asynchronous I/O with computation in a pipelined execution model that maintains sustained I/O activation, and leverages a highly optimized hybrid storage format to expedite access to low-degree vertices. We implement popular graph algorithms, such as Breadth-First Search (BFS), Weakly Connected Components (WCC), personalized PageRank (PPR), PageRank (PR), and $k$-core on ACGraph and demonstrate that ACGraph substantially outperforms state-of-the-art out-of-core graph processing systems in both runtime and I/O efficiency.",
        "authors": [
            "Dechuang Chen",
            "Sibo Wang",
            "Qintian Guo"
        ],
        "categories": [
            "cs.DB",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.07886v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-11"
    },
    "http://arxiv.org/abs/2511.07885v2": {
        "id": "http://arxiv.org/abs/2511.07885v2",
        "title": "Intelligence per Watt: Measuring Intelligence Efficiency of Local AI",
        "link": "http://arxiv.org/abs/2511.07885v2",
        "tags": [
            "edge",
            "serving",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes intelligence per watt (IPW) to measure efficiency of local LLM inference on edge devices. Evaluates 20+ models across 8 accelerators using 1M queries, finding local accelerators achieve 1.4x better IPW than cloud counterparts, enabling feasible redistribution of inference demand.",
        "abstract": "Large language model (LLM) queries are predominantly processed by frontier models in centralized cloud infrastructure. Rapidly growing demand strains this paradigm, and cloud providers struggle to scale infrastructure at pace. Two advances enable us to rethink this paradigm: small LMs (<=20B active parameters) now achieve competitive performance to frontier models on many tasks, and local accelerators (e.g., Apple M4 Max) run these models at interactive latencies. This raises the question: can local inference viably redistribute demand from centralized infrastructure? Answering this requires measuring whether local LMs can accurately answer real-world queries and whether they can do so efficiently enough to be practical on power-constrained devices (i.e., laptops). We propose intelligence per watt (IPW), task accuracy divided by unit of power, as a metric for assessing capability and efficiency of local inference across model-accelerator pairs. We conduct a large-scale empirical study across 20+ state-of-the-art local LMs, 8 accelerators, and a representative subset of LLM traffic: 1M real-world single-turn chat and reasoning queries. For each query, we measure accuracy, energy, latency, and power. Our analysis reveals $3$ findings. First, local LMs can accurately answer 88.7% of single-turn chat and reasoning queries with accuracy varying by domain. Second, from 2023-2025, IPW improved 5.3x and local query coverage rose from 23.2% to 71.3%. Third, local accelerators achieve at least 1.4x lower IPW than cloud accelerators running identical models, revealing significant headroom for optimization. These findings demonstrate that local inference can meaningfully redistribute demand from centralized infrastructure, with IPW serving as the critical metric for tracking this transition. We release our IPW profiling harness for systematic intelligence-per-watt benchmarking.",
        "authors": [
            "Jon Saad-Falcon",
            "Avanika Narayan",
            "Hakki Orhun Akengin",
            "J. Wes Griffin",
            "Herumb Shandilya",
            "Adrian Gamarra Lafuente",
            "Medhya Goel",
            "Rebecca Joseph",
            "Shlok Natarajan",
            "Etash Kumar Guha",
            "Shang Zhu",
            "Ben Athiwaratkun",
            "John Hennessy",
            "Azalia Mirhoseini",
            "Christopher Ré"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "submit_date": "2025-11-11"
    },
    "http://arxiv.org/abs/2511.07869v1": {
        "id": "http://arxiv.org/abs/2511.07869v1",
        "title": "Parallel Sampling via Autospeculation",
        "link": "http://arxiv.org/abs/2511.07869v1",
        "tags": [
            "serving",
            "training",
            "quantization"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes parallel sampling via autospeculation to reduce sampling time for autoregressive and diffusion models. Uses speculative rejection sampling with sequence-level speculation from the same oracle. Achieves expected sampling time of ̃O(n^{1/2}), improving over sequential O(n).",
        "abstract": "We present parallel algorithms to accelerate sampling via counting in two settings: any-order autoregressive models and denoising diffusion models. An any-order autoregressive model accesses a target distribution $μ$ on $[q]^n$ through an oracle that provides conditional marginals, while a denoising diffusion model accesses a target distribution $μ$ on $\\mathbb{R}^n$ through an oracle that provides conditional means under Gaussian noise. Standard sequential sampling algorithms require $\\widetilde{O}(n)$ time to produce a sample from $μ$ in either setting. We show that, by issuing oracle calls in parallel, the expected sampling time can be reduced to $\\widetilde{O}(n^{1/2})$. This improves the previous $\\widetilde{O}(n^{2/3})$ bound for any-order autoregressive models and yields the first parallel speedup for diffusion models in the high-accuracy regime, under the relatively mild assumption that the support of $μ$ is bounded.   We introduce a novel technique to obtain our results: speculative rejection sampling. This technique leverages an auxiliary ``speculative'' distribution~$ν$ that approximates~$μ$ to accelerate sampling. Our technique is inspired by the well-studied ``speculative decoding'' techniques popular in large language models, but differs in key ways. Firstly, we use ``autospeculation,'' namely we build the speculation $ν$ out of the same oracle that defines~$μ$. In contrast, speculative decoding typically requires a separate, faster, but potentially less accurate ``draft'' model $ν$. Secondly, the key differentiating factor in our technique is that we make and accept speculations at a ``sequence'' level rather than at the level of single (or a few) steps. This last fact is key to unlocking our parallel runtime of $\\widetilde{O}(n^{1/2})$.",
        "authors": [
            "Nima Anari",
            "Carlo Baronio",
            "CJ Chen",
            "Alireza Haqi",
            "Frederic Koehler",
            "Anqi Li",
            "Thuy-Duong Vuong"
        ],
        "categories": [
            "cs.DS",
            "cs.DC",
            "cs.LG",
            "math.PR"
        ],
        "submit_date": "2025-11-11"
    },
    "http://arxiv.org/abs/2511.11660v1": {
        "id": "http://arxiv.org/abs/2511.11660v1",
        "title": "HeteroSTA: A CPU-GPU Heterogeneous Static Timing Analysis Engine with Holistic Industrial Design Support",
        "link": "http://arxiv.org/abs/2511.11660v1",
        "tags": [
            "training",
            "hardware",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes HeteroSTA, a CPU-GPU heterogeneous timing analysis engine supporting various delay models and industry formats. Features end-to-end GPU acceleration for graph-based and path-based timing queries. Achieves remarkable runtime speed-up in industrial use cases.",
        "abstract": "We introduce in this paper, HeteroSTA, the first CPU-GPU heterogeneous timing analysis engine that efficiently supports: (1) a set of delay calculation models providing versatile accuracy-speed choices without relying on an external golden tool, (2) robust support for industry formats, including especially the .sdc constraints containing all common timing exceptions, clock domains, and case analysis modes, and (3) end-to-end GPU-acceleration for both graph-based and path-based timing queries, all exposed as a zero-overhead flattened heterogeneous application programming interface (API). HeteroSTA is publicly available with both a standalone binary executable and an embeddable shared library targeting ubiquitous academic and industry applications. Example use cases as a standalone tool, a timing-driven DREAMPlace 4.0 integration, and a timing-driven global routing integration have all demonstrated remarkable runtime speed-up and comparable quality.",
        "authors": [
            "Zizheng Guo",
            "Haichuan Liu",
            "Xizhe Shi",
            "Shenglu Hua",
            "Zuodong Zhang",
            "Chunyuan Zhao",
            "Runsheng Wang",
            "Yibo Lin"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-11-11"
    },
    "http://arxiv.org/abs/2511.07584v1": {
        "id": "http://arxiv.org/abs/2511.07584v1",
        "title": "SemanticForge: Repository-Level Code Generation through Semantic Knowledge Graphs and Constraint Satisfaction",
        "link": "http://arxiv.org/abs/2511.07584v1",
        "tags": [
            "thinking",
            "kernel",
            "RAG"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes SemanticForge for repository-level code generation by integrating semantic knowledge graphs and constraint solving. Combines graph query learning, beam search with SMT solving, and incremental graph maintenance to address logical/schematic hallucination. Achieves 73% query precision versus 51% baseline.",
        "abstract": "Large language models (LLMs) have transformed software development by enabling automated code generation, yet they frequently suffer from systematic errors that limit practical deployment. We identify two critical failure modes: \\textit{logical hallucination} (incorrect control/data-flow reasoning) and \\textit{schematic hallucination} (type mismatches, signature violations, and architectural inconsistencies). These errors stem from the absence of explicit, queryable representations of repository-wide semantics.   This paper presents \\textbf{SemanticForge}, which introduces four fundamental algorithmic advances for semantically-aware code generation: (1) a novel automatic reconciliation algorithm for dual static-dynamic knowledge graphs, unifying compile-time and runtime program semantics; (2) a neural approach that learns to generate structured graph queries from natural language, achieving 73\\% precision versus 51\\% for traditional retrieval; (3) a novel beam search algorithm with integrated SMT solving, enabling real-time constraint verification during generation rather than post-hoc validation; and (4) an incremental maintenance algorithm that updates knowledge graphs in $O(|ΔR| \\cdot \\log n)$ time while maintaining semantic equivalence.",
        "authors": [
            "Wuyang Zhang",
            "Chenkai Zhang",
            "Zhen Luo",
            "Jianming Ma",
            "Wangming Yuan",
            "Chuqiao Gu",
            "Chenwei Feng"
        ],
        "categories": [
            "cs.SE",
            "cs.AI",
            "cs.DC"
        ],
        "submit_date": "2025-11-10"
    },
    "http://arxiv.org/abs/2511.07574v1": {
        "title": "HyProv: Hybrid Provenance Management for Scientific Workflows",
        "link": "http://arxiv.org/abs/2511.07574v1",
        "abstract": "Provenance plays a crucial role in scientific workflow execution, for instance by providing data for failure analysis, real-time monitoring, or statistics on resource utilization for right-sizing allocations. The workflows themselves, however, become increasingly complex in terms of involved components. Furthermore, they are executed on distributed cluster infrastructures, which makes the real-time collection, integration, and analysis of provenance data challenging. Existing provenance systems struggle to balance scalability, real-time processing, online provenance analytics, and integration across different components and compute resources. Moreover, most provenance solutions are not workflow-aware; by focusing on arbitrary workloads, they miss opportunities for workflow systems where optimization and analysis can exploit the availability of a workflow specification that dictates, to some degree, task execution orders and provides abstractions for physical tasks at a logical level.   In this paper, we present HyProv, a hybrid provenance management system that combines centralized and federated paradigms to offer scalable, online, and workflow-aware queries over workflow provenance traces. HyProv uses a centralized component for efficient management of the small and stable workflow-specification-specific provenance, and complements this with federated querying over different scalable monitoring and provenance databases for the large-scale execution logs. This enables low-latency access to current execution data. Furthermore, the design supports complex provenance queries, which we exemplify for the workflow system Airflow in combination with the resource manager Kubernetes. Our experiments indicate that HyProv scales to large workflows, answers provenance queries with sub-second latencies, and adds only modest CPU and memory overhead to the cluster.",
        "authors": [
            "Vasilis Bountris",
            "Lauritz Thamsen",
            "Ulf Leser"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.07574v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-10"
    },
    "http://arxiv.org/abs/2511.07418v1": {
        "title": "Lightning Grasp: High Performance Procedural Grasp Synthesis with Contact Fields",
        "link": "http://arxiv.org/abs/2511.07418v1",
        "abstract": "Despite years of research, real-time diverse grasp synthesis for dexterous hands remains an unsolved core challenge in robotics and computer graphics. We present Lightning Grasp, a novel high-performance procedural grasp synthesis algorithm that achieves orders-of-magnitude speedups over state-of-the-art approaches, while enabling unsupervised grasp generation for irregular, tool-like objects. The method avoids many limitations of prior approaches, such as the need for carefully tuned energy functions and sensitive initialization. This breakthrough is driven by a key insight: decoupling complex geometric computation from the search process via a simple, efficient data structure - the Contact Field. This abstraction collapses the problem complexity, enabling a procedural search at unprecedented speeds. We open-source our system to propel further innovation in robotic manipulation.",
        "authors": [
            "Zhao-Heng Yin",
            "Pieter Abbeel"
        ],
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.DC",
            "cs.GR"
        ],
        "id": "http://arxiv.org/abs/2511.07418v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-10"
    },
    "http://arxiv.org/abs/2511.07229v1": {
        "id": "http://arxiv.org/abs/2511.07229v1",
        "title": "LLMServingSim2.0: A Unified Simulator for Heterogeneous Hardware and Serving Techniques in LLM Infrastructure",
        "link": "http://arxiv.org/abs/2511.07229v1",
        "tags": [
            "serving",
            "hardware",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Presents LLMServingSim2.0, a trace-driven simulator for heterogeneous hardware integration in LLM serving systems. Features operator-level latency profiling and flexible interfaces for serving techniques like request routing and cache management. Achieves 18.5x fewer LoC in TPU integration with 1.9% error in GPU serving simulation.",
        "abstract": "This paper introduces LLMServingSim2.0, a system simulator designed for exploring heterogeneous hardware in large-scale LLM serving systems. LLMServingSim2.0 addresses two key limitations of its predecessor: (1) integrating hardware models into system-level simulators is non-trivial due to the lack of a clear abstraction, and (2) existing simulators support only a narrow subset of serving techniques, leaving no infrastructure that captures the breadth of approaches in modern LLM serving. To overcome these issues, LLMServingSim2.0 adopts trace-driven performance modeling, accompanied by an operator-level latency profiler, enabling the integration of new accelerators with a single command. It further embeds up-to-date serving techniques while exposing flexible interfaces for request routing, cache management, and scheduling policies. In a TPU case study, our profiler requires 18.5x fewer LoC and outperforms the predecessor's hardware-simulator integration, demonstrating LLMServingSim2.0's low-effort hardware extensibility. Our experiments further show that LLMServingSim2.0 reproduces GPU-based LLM serving with 1.9% error, while maintaining practical simulation time, making it a comprehensive platform for both hardware developers and LLM service providers.",
        "authors": [
            "Jaehong Cho",
            "Hyunmin Choi",
            "Jongse Park"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-11-10"
    },
    "http://arxiv.org/abs/2511.07202v2": {
        "title": "Resilient by Design -- Active Inference for Distributed Continuum Intelligence",
        "link": "http://arxiv.org/abs/2511.07202v2",
        "abstract": "Failures are the norm in highly complex and heterogeneous devices spanning the distributed computing continuum (DCC), from resource-constrained IoT and edge nodes to high-performance computing systems. Ensuring reliability and global consistency across these layers remains a major challenge, especially for AI-driven workloads requiring real-time, adaptive coordination. This work-in-progress paper introduces a Probabilistic Active Inference Resilience Agent (PAIR-Agent) to achieve resilience in DCC systems. PAIR-Agent performs three core operations: (i) constructing a causal fault graph from device logs, (ii) identifying faults while managing certainties and uncertainties using Markov blankets and the free energy principle, and (iii) autonomously healing issues through active inference. Through continuous monitoring and adaptive reconfiguration, the agent maintains service continuity and stability under diverse failure conditions. Theoretical validations confirm the reliability and effectiveness of the proposed framework.",
        "authors": [
            "Praveen Kumar Donta",
            "Alfreds Lapkovskis",
            "Enzo Mingozzi",
            "Schahram Dustdar"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.MA",
            "cs.NI"
        ],
        "id": "http://arxiv.org/abs/2511.07202v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-10"
    },
    "http://arxiv.org/abs/2511.06824v1": {
        "title": "A GPU-boosted high-performance multi-working condition joint analysis framework for predicting dynamics of textured axial piston pump",
        "link": "http://arxiv.org/abs/2511.06824v1",
        "abstract": "Accurate simulation to dynamics of axial piston pump (APP) is essential for its design, manufacture and maintenance. However, limited by computation capacity of CPU device and traditional solvers, conventional iteration methods are inefficient in complicated case with textured surface requiring refined mesh, and could not handle simulation during multiple periods. To accelerate Picard iteration for predicting dynamics of APP, a GPU-boosted high-performance Multi-working condition joint Analysis Framework (GMAF) is designed, which adopts Preconditioned Conjugate Gradient method (PCG) using Approximate Symmetric Successive Over-Relaxation preconditioner (ASSOR). GMAF abundantly utilizes GPU device via elevating computational intensity and expanding scale of massive parallel computation. Therefore, it possesses novel performance in analyzing dynamics of both smooth and textured APPs during multiple periods, as the establishment and solution to joint algebraic system for pressure field are accelerated magnificently, as well as numerical integral for force and moment due to oil flow. Compared with asynchronized convergence strategy pursuing local convergence, synchronized convergence strategy targeting global convergence is adopted in PCG solver for the joint system. Revealed by corresponding results, oil force in axial direction and moment in circumferential directly respond to input pressure, while other components evolve in sinusoidal patterns. Specifically, force and moment due to normal pressure instantly reach their steady state initially, while ones due to viscous shear stress evolve during periods. After simulating dynamics of APP and pressure distribution via GMAF, the promotion of pressure capacity and torsion resistance due to textured surface is revealed numerically, as several 'steps' exist in the pressure field corresponding to textures.",
        "authors": [
            "Xin Yao",
            "Yang Liu",
            "Jin Jiang",
            "Yesen Chen",
            "Zhilong Chen",
            "Hongkang Dong",
            "Xiaofeng Wei",
            "Teng Zhang",
            "Dongyun Wang"
        ],
        "categories": [
            "cs.DC",
            "cs.CE"
        ],
        "id": "http://arxiv.org/abs/2511.06824v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-10"
    },
    "http://arxiv.org/abs/2511.06735v1": {
        "title": "Wireless Sensor Networks Nodes Clustering and Optimization Based on Fuzzy C-Means and Water Strider Algorithms",
        "link": "http://arxiv.org/abs/2511.06735v1",
        "abstract": "Wireless sensor networks (WSNs) face critical challenges in energy management and network lifetime optimization due to limited battery resources and communication overhead. This study introduces a novel hybrid clustering protocol that integrates the Water Strider Algorithm (WSA) with Fuzzy C-Means (FCM) clustering to achieve superior energy efficiency and network longevity. The proposed WSA-FCM method employs WSA for global optimization of cluster-head positions and FCM for refined node membership assignment with fuzzy boundaries. Through extensive experimentation across networks of 200-800 nodes with 10 independent simulation runs, the method demonstrates significant improvements: First Node Death (FND) delayed by 16.1% ($678\\pm12$ vs $584\\pm18$ rounds), Last Node Death (LND) extended by 11.9% ($1,262\\pm8$ vs $1,128\\pm11$ rounds), and 37.4% higher residual energy retention ($5.47\\pm0.09$ vs $3.98\\pm0.11$ J) compared to state-of-the-art hybrid methods. Intra-cluster distances are reduced by 19.4% with statistical significance (p < 0.001). Theoretical analysis proves convergence guarantees and complexity bounds of $O(n\\times c\\times T)$, while empirical scalability analysis demonstrates near-linear scaling behaviour. The method outperforms recent hybrid approaches including MOALO-FCM, MSSO-MST, Fuzzy+HHO, and GWO-FCM across all performance metrics with rigorous statistical validation.",
        "authors": [
            "Raya Majid Alsharfa",
            "Mahmood Mohassel Feghhi",
            "Majid Hameed Majeed"
        ],
        "categories": [
            "cs.DC",
            "eess.SP",
            "math.OC"
        ],
        "id": "http://arxiv.org/abs/2511.06735v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-10"
    },
    "http://arxiv.org/abs/2511.06724v1": {
        "id": "http://arxiv.org/abs/2511.06724v1",
        "title": "Argus: Quality-Aware High-Throughput Text-to-Image Inference Serving System",
        "link": "http://arxiv.org/abs/2511.06724v1",
        "tags": [
            "diffusion",
            "serving",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Focuses on high-throughput serving for text-to-image diffusion models. Proposes Argus, a system that dynamically selects approximation levels per prompt to balance quality and throughput. Achieves 40% higher throughput with 10% higher average quality.",
        "abstract": "Text-to-image (T2I) models have gained significant popularity. Most of these are diffusion models with unique computational characteristics, distinct from both traditional small-scale ML models and large language models. They are highly compute-bound and use an iterative denoising process to generate images, leading to very high inference time. This creates significant challenges in designing a high-throughput system. We discovered that a large fraction of prompts can be served using faster, approximated models. However, the approximation setting must be carefully calibrated for each prompt to avoid quality degradation. Designing a high-throughput system that assigns each prompt to the appropriate model and compatible approximation setting remains a challenging problem. We present Argus, a high-throughput T2I inference system that selects the right level of approximation for each prompt to maintain quality while meeting throughput targets on a fixed-size cluster. Argus intelligently switches between different approximation strategies to satisfy both throughput and quality requirements. Overall, Argus achieves 10x fewer latency service-level objective (SLO) violations, 10% higher average quality, and 40% higher throughput compared to baselines on two real-world workload traces.",
        "authors": [
            "Shubham Agarwal",
            "Subrata Mitra",
            "Saud Iqbal"
        ],
        "categories": [
            "cs.CV",
            "cs.DC"
        ],
        "submit_date": "2025-11-10"
    },
    "http://arxiv.org/abs/2511.06605v1": {
        "id": "http://arxiv.org/abs/2511.06605v1",
        "title": "DMA Collectives for Efficient ML Communication Offloads",
        "link": "http://arxiv.org/abs/2511.06605v1",
        "tags": [
            "training",
            "networking",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Analyses DMA offloading for ML communication collectives to improve training efficiency. Optimizes DMA collective implementations on AMD GPUs by reducing synchronization costs. Achieves up to 16% better performance and 32% lower power at large sizes, improving small-size performance by 20-30%.",
        "abstract": "Offloading machine learning (ML) communication collectives to direct memory access (DMA) engines has emerged as an interesting and low-cost solution to efficiently overlap computation and communication in inference and training. Doing so delivers superior concurrent performance by freeing up all GPU cores for computation and also lowers interference in the memory sub-system (caches). While DMA collectives show strong promise, prior works have only studied them in limited context (bandwidth-bound transfer sizes only, performance-only).   To address this, we provide a comprehensive performance, power/energy and synchronization costs analysis of offloading ML communication collectives (all-gather, all-to-all) to DMA engines on state-of-the-art AMD Instinct MI300X GPUs. Our analysis reveals that, compared to the state-of-the-art RCCL communication collectives library, DMA collectives are at-par or better for large sizes (10s of MB to GB) in terms of both performance (16% better) and power (32% better). However, they significantly lag for latency-bound small sizes; 4.5X and 2.5X slower for all-gather and all-to-all, respectively. We provide a detailed latency breakdown of a DMA transfer and identify that DMA command scheduling and synchronization costs can limit DMA collective performance. To tackle this, we harness existing DMA architecture innovations, hitherto untapped, to build optimized DMA collectives and demonstrate their efficacy on real hardware. Our optimized implementations considerably close the performance gap for DMA collectives at smaller sizes (30% slower and 20% faster all-gather and all-to-all, respectively) and further improves performance (by 7%) and power savings at larger sizes (3-10%). Overall, this work represents a significant step toward making DMA collectives suitable for adoption in mainstream collective libraries.",
        "authors": [
            "Suchita Pati",
            "Mahzabeen Islam",
            "Shaizeen Aga",
            "Mohamed Assem Ibrahim"
        ],
        "categories": [
            "cs.DC",
            "cs.AR"
        ],
        "submit_date": "2025-11-10"
    },
    "http://arxiv.org/abs/2511.06599v1": {
        "title": "Saarthi: An End-to-End Intelligent Platform for Optimising Distributed Serverless Workloads",
        "link": "http://arxiv.org/abs/2511.06599v1",
        "abstract": "FaaS offers significant advantages with its infrastructure abstraction, on-demand execution, and attractive no idle resource pricing for modern cloud applications. Despite these benefits, challenges such as startup latencies, static configurations, sub-optimal resource allocation and scheduling still exist due to coupled resource offering and workload-agnostic generic scheduling behaviour. These issues often lead to inconsistent function performance and unexpected operational costs for users and service providers. This paper introduces Saarthi, a novel, end-to-end serverless framework that intelligently manages the dynamic resource needs of function workloads, representing a significant step toward self-driving serverless platforms. Unlike platforms that rely on static resource configurations, Saarthi is input-aware, allowing it to intelligently anticipate resource requirements based on the characteristics of an incoming request payload. This input-driven approach reinforces function right-sizing and enables smart request orchestration across available function configurations. Saarthi further integrates a proactive fault-tolerant redundancy mechanism and employs a multi-objective Integer Linear Programming (ILP) model to maintain an optimal function quantity. This optimisation aims to maximise system throughput while simultaneously reducing overall operational costs. We validate the effectiveness of Saarthi by implementing it as a framework atop OpenFaaS. Our results demonstrate Saarthi's ability to achieve up to 1.45x better throughput, 1.84x reduced costs, while maintaining up to 98.3% service level targets with an overhead of up to 0.2 seconds as compared to the baseline OpenFaaS.",
        "authors": [
            "Siddharth Agarwal",
            "Maria A. Rodriguez",
            "Rajkumar Buyya"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.06599v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-10"
    },
    "http://arxiv.org/abs/2511.06565v1": {
        "title": "FPGA or GPU? Analyzing comparative research for application-specific guidance",
        "link": "http://arxiv.org/abs/2511.06565v1",
        "abstract": "The growing complexity of computational workloads has amplified the need for efficient and specialized hardware accelerators. Field Programmable Gate Arrays (FPGAs) and Graphics Processing Units (GPUs) have emerged as prominent solutions, each excelling in specific domains. Although there is substantial research comparing FPGAs and GPUs, most of the work focuses primarily on performance metrics, offering limited insight into the specific types of applications that each accelerator benefits the most. This paper aims to bridge this gap by synthesizing insights from various research articles to guide users in selecting the appropriate accelerator for domain-specific applications. By categorizing the reviewed studies and analyzing key performance metrics, this work highlights the strengths, limitations, and ideal use cases for FPGAs and GPUs. The findings offer actionable recommendations, helping researchers and practitioners navigate trade-offs in performance, energy efficiency, and programmability.",
        "authors": [
            "Arnab A Purkayastha",
            "Jay Tharwani",
            "Shobhit Aggarwal"
        ],
        "categories": [
            "cs.AR",
            "cs.CL",
            "cs.DC",
            "cs.PL"
        ],
        "id": "http://arxiv.org/abs/2511.06565v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-09"
    },
    "http://arxiv.org/abs/2511.06372v1": {
        "id": "http://arxiv.org/abs/2511.06372v1",
        "title": "Towards Optimal Constellation Design for Digital Over-the-Air Computation",
        "link": "http://arxiv.org/abs/2511.06372v1",
        "tags": [
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Addresses optimal constellation design for digital over-the-air computation to minimize mean-squared error. Proposes a modulation framework with non-linear equations for unique solutions and high-SNR closed-form analysis. Achieves reduced MSE under power constraints for efficient wireless function aggregation.",
        "abstract": "Over-the-air computation (OAC) has emerged as a key technique for efficient function computation over multiple-access channels (MACs) by exploiting the waveform superposition property of the wireless domain. While conventional OAC methods rely on analog amplitude modulation, their performance is often limited by noise sensitivity and hardware constraints, motivating the use of digital modulation schemes. This paper proposes a novel digital modulation framework optimized for computation over additive white Gaussian noise (AWGN) channels. The design is formulated as an additive mapping problem to determine the optimal constellation that minimizes the mean-squared error (MSE) under a transmit power constraint. We express the optimal constellation design as a system of nonlinear equations and establish the conditions guaranteeing the uniqueness of its solution. In the high signal-to-noise-ratio (SNR) regime, we derive closed-form expressions for the optimal modulation parameters using the generalized Lambert function, providing analytical insight into the system's behavior. Furthermore, we discuss extensions of the framework to higher-dimensional grids corresponding to multiple channel uses, to non-Gaussian noise models, and to computation over real-valued domains via hybrid digital-analog modulation.",
        "authors": [
            "Saeed Razavikia",
            "Deniz Gündüz",
            "Carlo Fischione"
        ],
        "categories": [
            "cs.IT",
            "cs.DC"
        ],
        "submit_date": "2025-11-09"
    },
    "http://arxiv.org/abs/2511.06345v2": {
        "id": "http://arxiv.org/abs/2511.06345v2",
        "title": "PRAGMA: A Profiling-Reasoned Multi-Agent Framework for Automatic Kernel Optimization",
        "link": "http://arxiv.org/abs/2511.06345v2",
        "tags": [
            "kernel",
            "training"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "PRAGMA proposes a profiling-reasoned multi-agent framework for automatic GPU/CPU kernel generation. It integrates hardware profiling into the LLM reasoning loop to identify bottlenecks and iteratively refine kernels. Achieves 2.81× and 2.30× speedups on CPU and GPU platforms compared to Torch.",
        "abstract": "Designing high-performance kernels requires expert-level tuning and a deep understanding of hardware characteristics. Recent advances in large language models (LLMs) have enabled automated kernel generation, yet most existing systems rely solely on correctness or execution time feedback, lacking the ability to reason about low-level performance bottlenecks. In this paper, we introduce PRAGMA, a profile-guided AI kernel generation framework that integrates execution feedback and fine-grained hardware profiling into the reasoning loop. PRAGMA enables LLMs to identify performance bottlenecks, preserve historical best versions, and iteratively refine code quality. We evaluate PRAGMA on KernelBench, covering GPU and CPU backends. Results show that PRAGMA consistently outperforms baseline AIKG without profiling enabled and achieves 2.81$\\times$ and 2.30$\\times$ averaged speedups against Torch on CPU and GPU platforms, respectively.",
        "authors": [
            "Kelun Lei",
            "Hailong Yang",
            "Huaitao Zhang",
            "Xin You",
            "Kaige Zhang",
            "Zhongzhi Luan",
            "Yi Liu",
            "Depei Qian"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-11-09"
    },
    "http://arxiv.org/abs/2511.06247v2": {
        "id": "http://arxiv.org/abs/2511.06247v2",
        "title": "Optimizing Long-context LLM Serving via Fine-grained Sequence Parallelism",
        "link": "http://arxiv.org/abs/2511.06247v2",
        "tags": [
            "serving",
            "sparse",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Optimizes long-context LLM serving with fine-grained sequence parallelism. Proposes Tetris system that dynamically allocates parallelism per token segments in intra-request chunks. Achieves 4.35× lower TTFT, 40.1% median TBT reduction, and 45% higher request capacity.",
        "abstract": "With the advancement of large language models (LLMs), their context windows have rapidly expanded. To meet diverse demands from varying-length requests in online services, existing state-of-the-art systems tune the sequence parallelism (SP) allocation. However, current dynamic SP allocation lacks flexibility to (1) support stage-specific parallelism requirements in LLM inference, (2) mitigate the global latency degradation from excessive SP allocation, and (3) exploit resource fragments arising from SP size variation.   To tackle this problem, we propose Chunkwise Dynamic Sequence Parallelism (CDSP), a fine-grained parallelism strategy that assigns SP sizes across \\textit{intra-request} token segments. Based on CDSP, we build Tetris, an LLM serving system that (1) efficiently integrates CDSP into disaggregated cluster to satisfy parallelism heterogeneity, (2) dynamically regulates SP size expansion based on real-time load conditions, and (3) adaptively explores chunking plans to utilize fragmented resources while meeting per-request demands. Compared with state-of-the-art systems, Tetris achieves up to 4.35$\\times$ lower time-to-first-token (TTFT) under max sustainable loads, reduces median time-between-tokens (TBT) by up to 40.1\\%, and increases the max request capacity by up to 45\\%.",
        "authors": [
            "Cong Li",
            "Yuzhe Yang",
            "Xuegui Zheng",
            "Qifan Yang",
            "Yijin Guan",
            "Size Zheng",
            "Li-Wen Chang",
            "Shufan Liu",
            "Xin Liu",
            "Guangyu Sun"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-11-09"
    },
    "http://arxiv.org/abs/2511.11640v1": {
        "id": "http://arxiv.org/abs/2511.11640v1",
        "title": "Exploring Parallelism in FPGA-Based Accelerators for Machine Learning Applications",
        "link": "http://arxiv.org/abs/2511.11640v1",
        "tags": [
            "training",
            "hardware",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes speculative backpropagation for neural network training acceleration by overlapping forward and backward passes. Implemented using OpenMP multi-threading on CPUs and targeted for FPGA synthesis. Achieves up to 35% speedup in step execution time while maintaining comparable accuracy on MNIST.",
        "abstract": "Speculative backpropagation has emerged as a promising technique to accelerate the training of neural networks by overlapping the forward and backward passes. Leveraging speculative weight updates when error gradients fall within a specific threshold reduces training time without substantially compromising accuracy. In this work, we implement speculative backpropagation on the MNIST dataset using OpenMP as the parallel programming platform. OpenMP's multi-threading capabilities enable simultaneous execution of forward and speculative backpropagation steps, significantly improving training speed. The application is planned for synthesis on a state-of-the-art FPGA to demonstrate its potential for hardware acceleration. Our CPU-based experimental results demonstrate that speculative backpropagation achieves a maximum speedup of 24% in execution time when using a threshold of 0.25, and accuracy remaining within 3-4% of the baseline across various epochs. Additionally, when comparing individual step execution time, speculative backpropagation yields a maximum speedup of 35% over the baseline, demonstrating the effectiveness of overlapping forward and backward passes.",
        "authors": [
            "Sed Centeno",
            "Christopher Sprague",
            "Arnab A Purkayastha",
            "Ray Simar",
            "Neeraj Magotra"
        ],
        "categories": [
            "cs.DC",
            "cs.AR",
            "cs.LG"
        ],
        "submit_date": "2025-11-09"
    },
    "http://arxiv.org/abs/2511.06187v2": {
        "title": "LiteCast: A Lightweight Forecaster for Carbon Optimizations",
        "link": "http://arxiv.org/abs/2511.06187v2",
        "abstract": "Over recent decades, electricity demand has experienced sustained growth through widespread electrification of transportation and the accelerated expansion of Artificial Intelligence (AI). Grids have managed the resulting surges by scaling generation capacity, incorporating additional resources such as solar and wind, and implementing demand-response mechanisms. Altogether, these policies influence a region's carbon intensity by affecting its energy mix. To mitigate the environmental impacts of consumption, carbon-aware optimizations often rely on long-horizon, high-accuracy forecasts of the grid's carbon intensity that typically use compute intensive models with extensive historical energy mix data. In addition to limiting scalability, accuracy improvements do not necessarily translate into proportional increases in savings. Highlighting the need for more efficient forecasting strategies, we argue that carbon forecasting solutions can achieve the majority of savings without requiring highly precise and complex predictions. Instead, it is the preservation of the ranking of forecasts relative to the ground-truth that drives realized savings. In this paper, we present LiteCast, a lightweight time series forecasting method capable of quickly modeling a region's energy mix to estimate its carbon intensity. LiteCast requires only a few days of historical energy and weather data, delivering fast forecasts that can quickly adapt to sudden changes in the electrical grid. Our evaluation in 50 worldwide regions under various real-world workloads shows that LiteCast outperforms state-of-the-art forecasters, delivering 20% higher savings with near-optimal performance, achieving 97% of the maximum attainable average savings, while remaining lightweight, efficient to run, and adaptive to new data.",
        "authors": [
            "Mathew Joseph",
            "Tanush Savadi",
            "Abel Souza"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.06187v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-09"
    },
    "http://arxiv.org/abs/2511.06159v2": {
        "id": "http://arxiv.org/abs/2511.06159v2",
        "title": "Elastic Data Transfer Optimization with Hybrid Reinforcement Learning",
        "link": "http://arxiv.org/abs/2511.06159v2",
        "tags": [
            "networking",
            "offline",
            "storage"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Optimizes large-scale data transfer for scientific datasets using hybrid heuristics and deep reinforcement learning. Combines parallelism, pipelining, and a lightweight simulator for fast concurrency tuning. Achieves 9.5x higher throughput compared to state-of-the-art solutions.",
        "abstract": "Modern scientific data acquisition generates petabytes of data that must be transferred to geographically distant computing clusters. Conventional tools either rely on preconfigured sessions, which are difficult to tune for users without domain expertise, or they adaptively optimize only concurrency while ignoring other important parameters. We present \\name, an adaptive data transfer method that jointly considers multiple parameters. Our solution incorporates heuristic-based parallelism, infinite pipelining, and a deep reinforcement learning based concurrency optimizer. To make agent training practical, we introduce a lightweight network simulator that reduces training time to less than four minutes and provides a $2750\\times$ speedup compared to online training. Experimental evaluation shows that \\name consistently outperforms existing methods across diverse datasets, achieving up to 9.5x higher throughput compared to state-of-the-art solutions.",
        "authors": [
            "Rasman Mubtasim Swargo",
            "Md Arifuzzaman"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-11-08"
    },
    "http://arxiv.org/abs/2511.06130v1": {
        "title": "Reliablocks: Developing Reliability Scores for Optimistic Rollups",
        "link": "http://arxiv.org/abs/2511.06130v1",
        "abstract": "Introducing Reliablocks, an on-chain reliability index for non-finalized blocks in Optimistic Rollups. This was built during the EigenLayer Infinite Hackathon at the Infinite Hacker House at DevCon 2024. As part of this research, we delivered a working Layer AVS WASMI component, a working Eigen Layer AVS component, EigenLayer Solidity smart contracts that work with the AVS component, a UI dashboard illustrating the reliability score and a derived interest rate for further utilization.",
        "authors": [
            "Souradeep Das",
            "Ethan Lam",
            "Varun Vaidya",
            "Sanjay Amirthraj"
        ],
        "categories": [
            "cs.CR",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.06130v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-08"
    },
    "http://arxiv.org/abs/2511.15712v1": {
        "title": "Secure Autonomous Agent Payments: Verifying Authenticity and Intent in a Trustless Environment",
        "link": "http://arxiv.org/abs/2511.15712v1",
        "abstract": "Artificial intelligence (AI) agents are increasingly capable of initiating financial transactions on behalf of users or other agents. This evolution introduces a fundamental challenge: verifying both the authenticity of an autonomous agent and the true intent behind its transactions in a decentralized, trustless environment. Traditional payment systems assume human authorization, but autonomous, agent-led payments remove that safeguard. This paper presents a blockchain-based framework that cryptographically authenticates and verifies the intent of every AI-initiated transaction. The proposed system leverages decentralized identity (DID) standards and verifiable credentials to establish agent identities, on-chain intent proofs to record user authorization, and zero-knowledge proofs (ZKPs) to preserve privacy while ensuring policy compliance. Additionally, secure execution environments (TEE-based attestations) guarantee the integrity of agent reasoning and execution. The hybrid on-chain/off-chain architecture provides an immutable audit trail linking user intent to payment outcome. Through qualitative analysis, the framework demonstrates strong resistance to impersonation, unauthorized transactions, and misalignment of intent. This work lays the foundation for secure, auditable, and intent-aware autonomous economic agents, enabling a future of verifiable trust and accountability in AI-driven financial ecosystems.",
        "authors": [
            "Vivek Acharya"
        ],
        "categories": [
            "cs.CR",
            "cs.AI",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.15712v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-08"
    },
    "http://arxiv.org/abs/2511.06052v1": {
        "title": "Inductive Loop Analysis for Practical HPC Application Optimization",
        "link": "http://arxiv.org/abs/2511.06052v1",
        "abstract": "Scientific computing applications heavily rely on multi-level loop nests operating on multidimensional arrays. This presents multiple optimization opportunities from exploiting parallelism to reducing data movement through prefetching and improved register usage. HPC frameworks often delegate fine-grained data movement optimization to compilers, but their low-level representations hamper analysis of common patterns, such as strided data accesses and loop-carried dependencies. In this paper, we introduce symbolic, inductive loop optimization (SILO), a novel technique that models data accesses and dependencies as functions of loop nest strides. This abstraction enables the automatic parallelization of sequentially-dependent loops, as well as data movement optimizations including software prefetching and pointer incrementation to reduce register spills. We demonstrate SILO on fundamental kernels from scientific applications with a focus on atmospheric models and numerical solvers, achieving up to 12$\\times$ speedup over the state of the art.",
        "authors": [
            "Philipp Schaad",
            "Tal Ben-Nun",
            "Patrick Iff",
            "Torsten Hoefler"
        ],
        "categories": [
            "cs.DC",
            "cs.PF"
        ],
        "id": "http://arxiv.org/abs/2511.06052v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-08"
    },
    "http://arxiv.org/abs/2511.06010v1": {
        "id": "http://arxiv.org/abs/2511.06010v1",
        "title": "MoSKA: Mixture of Shared KV Attention for Efficient Long-Sequence LLM Inference",
        "link": "http://arxiv.org/abs/2511.06010v1",
        "tags": [
            "serving",
            "offloading",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Addresses KV cache bottlenecks in long-sequence LLM inference. Proposes MoSKA with Shared KV Attention to batch shared data processing (GEMM instead of GEMV), sparse attention pruning, and disaggregated hardware. Achieves up to 538.7x higher throughput in high-sharing scenarios.",
        "abstract": "The escalating context length in Large Language Models (LLMs) creates a severe performance bottleneck around the Key-Value (KV) cache, whose memory-bound nature leads to significant GPU under-utilization. This paper introduces Mixture of Shared KV Attention (MoSKA), an architecture that addresses this challenge by exploiting the heterogeneity of context data. It differentiates between per-request unique and massively reused shared sequences. The core of MoSKA is a novel Shared KV Attention mechanism that transforms the attention on shared data from a series of memory-bound GEMV operations into a single, compute-bound GEMM by batching concurrent requests. This is supported by an MoE-inspired sparse attention strategy that prunes the search space and a tailored Disaggregated Infrastructure that specializes hardware for unique and shared data. This comprehensive approach demonstrates a throughput increase of up to 538.7x over baselines in workloads with high context sharing, offering a clear architectural path toward scalable LLM inference.",
        "authors": [
            "Myunghyun Rhee",
            "Sookyung Choi",
            "Euiseok Kim",
            "Joonseop Sim",
            "Youngpyo Joo",
            "Hoshik Kim"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC"
        ],
        "submit_date": "2025-11-08"
    },
    "http://arxiv.org/abs/2511.06006v1": {
        "title": "Distributed Deep Learning for Medical Image Denoising with Data Obfuscation",
        "link": "http://arxiv.org/abs/2511.06006v1",
        "abstract": "Medical image denoising is essential for improving image quality while minimizing the exposure of sensitive information, particularly when working with large-scale clinical datasets. This study explores distributed deep learning for denoising chest X-ray images from the NIH Chest X-ray14 dataset, using additive Gaussian noise as a lightweight obfuscation technique. We implement and evaluate U-Net and U-Net++ architectures under single-GPU, standard multi-GPU (DataParallel), and optimized multi-GPU training configurations using PyTorch's DistributedDataParallel (DDP) and Automatic Mixed Precision (AMP). Our results show that U-Net++ consistently delivers superior denoising performance, achieving competitive Peak Signal to Noise Ratio (PSNR) and Structured Similarity Index Method (SSIM) scores, though with less performance in Learned Perceptual Image Patch Similarity (LPIPS) compared to U-Net under low and moderate noise levels. This indicates U-Net++'s enhanced structural fidelity and low perceptual similarity. Meanwhile, our optimized training pipeline reduces training time by over 60% for both models compared to single-GPU training, and outperforms standard DataParallel by over 40%, with only a minor accuracy drop for both models (trading some accuracy for speed). These findings highlight the effectiveness of software-level optimization in distributed learning for medical imaging. This work demonstrates the practical viability of combining architectural design, lightweight obfuscation, and advanced distributed training strategies to accelerate and enhance medical image processing pipelines in real-world clinical and research environments. The full implementation is publicly available at: https://github.com/Suadey/medical-image-denoising-ddp.",
        "authors": [
            "Sulaimon Oyeniyi Adebayo",
            "Ayaz H. Khan"
        ],
        "categories": [
            "cs.CV",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.06006v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-08"
    },
    "http://arxiv.org/abs/2511.05978v1": {
        "id": "http://arxiv.org/abs/2511.05978v1",
        "title": "Kunlun Anomaly Troubleshooter: Enabling Kernel-Level Anomaly Detection and Causal Reasoning for Large Model Distributed Inference",
        "link": "http://arxiv.org/abs/2511.05978v1",
        "tags": [
            "serving",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Presents Kunlun Anomaly Troubleshooter (KAT) for kernel-level anomaly detection in distributed LLM inference. Uses GPU trace data and domain-adapted LLM for causal reasoning. Achieves 0.884 precision and 0.936 recall in anomaly detection.",
        "abstract": "Anomaly troubleshooting for large model distributed inference (LMDI) remains a critical challenge. Resolving anomalies such as inference performance degradation or latency jitter in distributed system demands significant manual efforts from domain experts, resulting in extremely time-consuming diagnosis processes with relatively low accuracy. In this paper, we introduce Kunlun Anomaly Troubleshooter (KAT), the first anomaly troubleshooting framework tailored for LMDI. KAT addresses this problem through two core innovations. First, KAT exploits the synchronicity and consistency of GPU workers, innovatively leverages function trace data to precisely detect kernel-level anomalies and associated hardware components at nanosecond resolution. Second, KAT integrates these detection results into a domain-adapted LLM, delivering systematic causal reasoning and natural language interpretation of complex anomaly symptoms. Evaluations conducted in Alibaba Cloud Service production environment indicate that KAT achieves over 0.884 precision and 0.936 recall in anomaly detection, providing detail anomaly insights that significantly narrow down the diagnostic scope and improve both the efficiency and success rate of troubleshooting.",
        "authors": [
            "Yuyang Liu",
            "Jingjing Cai",
            "Jiayi Ren",
            "Peng Zhou",
            "Danyang Zhang",
            "Yin Du",
            "Shijian Li"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC",
            "cs.PF"
        ],
        "submit_date": "2025-11-08"
    },
    "http://arxiv.org/abs/2511.05972v1": {
        "id": "http://arxiv.org/abs/2511.05972v1",
        "title": "DWM-RO: Decentralized World Models with Reasoning Offloading for SWIPT-enabled Satellite-Terrestrial HetNets",
        "link": "http://arxiv.org/abs/2511.05972v1",
        "tags": [
            "offloading",
            "RL",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes DWM-RO framework for decentralized MARL in satellite-terrestrial hetnets with SWIPT. Uses world models for predictive representations and offloading gate for selective edge coordination. Achieves 5x faster convergence, 34.7% higher spectral efficiency, and 40% lower violations.",
        "abstract": "Wireless networks are undergoing a paradigm shift toward massive connectivity with energy-efficient operation, driving the integration of satellite-terrestrial architectures with simultaneous wireless information and power transfer (SWIPT). Optimizing transmit beamforming and power splitting in such systems faces formidable challenges, e.g., time-varying channels and multi-tier interference, which create a complex decision landscape where conventional model-free multi-agent reinforcement learning (MARL) suffers from sample inefficiency due to rarely-encountered state transitions and poor coordination as decentralized agents act independently. This paper proposes the Decentralized World Model with Reasoning Offloading (DWM-RO) framework to address these fundamental limitations. Specifically, each agent employs a world model to learn compact predictive representations of environment dynamics, enabling imagination-based policy training that dramatically reduces required environment interactions. An uncertainty-aware offloading gate monitors local interference levels and model reconstruction errors to trigger selective edge coordination. When activated, a lightweight latent decorrelation mechanism at the edge refines agents' strategic representations, guiding them toward orthogonal actions that minimize resource conflicts. Extensive simulations demonstrate that DWM-RO converges 5 times faster than state-of-the-art baselines while achieving 34.7% higher spectral efficiency and reducing constraint violations by 40%. In dense network scenarios with 10 users, DWM-RO maintains violation rates below 20% while baselines exceed 70%, validating superior robustness.",
        "authors": [
            "Guangyuan Liu",
            "Yinqiu Liu",
            "Ruichen Zhang",
            "Dusit Niyato",
            "Jiawen Kang",
            "Sumei Sun",
            "Abbas Jamalipour",
            "Ping Zhang"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-11-08"
    },
    "http://arxiv.org/abs/2511.05958v1": {
        "id": "http://arxiv.org/abs/2511.05958v1",
        "title": "MT4G: A Tool for Reliable Auto-Discovery of NVIDIA and AMD GPU Compute and Memory Topologies",
        "link": "http://arxiv.org/abs/2511.05958v1",
        "tags": [
            "hardware",
            "training",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes MT4G, a vendor-agnostic tool for auto-discovering GPU compute/memory topologies via microbenchmarks and statistical methods. Enables performance modeling, bottleneck analysis, and resource partitioning, demonstrated across NVIDIA/AMD GPUs to optimize HPC/AI system utilization.",
        "abstract": "Understanding GPU topology is essential for performance-related tasks in HPC or AI. Yet, unlike for CPUs with tools like hwloc, GPU information is hard to come by, incomplete, and vendor-specific.   In this work, we address this gap and present MT4G, an open-source and vendor-agnostic tool that automatically discovers GPU compute and memory topologies and configurations, including cache sizes, bandwidths, and physical layouts. MT4G combines existing APIs with a suite of over 50 microbenchmarks, applying statistical methods, such as the Kolmogorov-Smirnov test, to automatically and reliably identify otherwise programmatically unavailable topological attributes.   We showcase MT4G's universality on ten different GPUs and demonstrate its impact through integration into three workflows: GPU performance modeling, GPUscout bottleneck analysis, and dynamic resource partitioning. These scenarios highlight MT4G's role in understanding system performance and characteristics across NVIDIA and AMD GPUs, providing an automated, portable solution for modern HPC and AI systems.",
        "authors": [
            "Stepan Vanecek",
            "Manuel Walter Mussbacher",
            "Dominik Groessler",
            "Urvij Saroliya",
            "Martin Schulz"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-11-08"
    },
    "http://arxiv.org/abs/2511.05915v1": {
        "id": "http://arxiv.org/abs/2511.05915v1",
        "title": "CoEdge-RAG: Optimizing Hierarchical Scheduling for Retrieval-Augmented LLMs in Collaborative Edge Computing",
        "link": "http://arxiv.org/abs/2511.05915v1",
        "tags": [
            "edge",
            "RAG",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes CoEdge-RAG, a hierarchical scheduling framework for RAG-enhanced LLMs in collaborative edge environments. Uses online PPO-based query identification, dynamic inter-node workload balancing, and intra-node resource optimization to handle privacy constraints and heterogeneity. Achieves 4.23% to 91.39% performance gains over baselines.",
        "abstract": "Motivated by the imperative for real-time responsiveness and data privacy preservation, large language models (LLMs) are increasingly deployed on resource-constrained edge devices to enable localized inference. To improve output quality, retrieval-augmented generation (RAG) is an efficient technique that seamlessly integrates local data into LLMs. However, existing edge computing paradigms primarily focus on single-node optimization, neglecting opportunities to holistically exploit distributed data and heterogeneous resources through cross-node collaboration. To bridge this gap, we propose CoEdge-RAG, a hierarchical scheduling framework for retrieval-augmented LLMs in collaborative edge computing. In general, privacy constraints preclude accurate a priori acquisition of heterogeneous data distributions across edge nodes, directly impeding RAG performance optimization. Thus, we first design an online query identification mechanism using proximal policy optimization (PPO), which autonomously infers query semantics and establishes cross-domain knowledge associations in an online manner. Second, we devise a dynamic inter-node scheduling strategy that balances workloads across heterogeneous edge nodes by synergizing historical performance analytics with real-time resource thresholds. Third, we develop an intra-node scheduler based on online convex optimization, adaptively allocating query processing ratios and memory resources to optimize the latency-quality trade-off under fluctuating assigned loads. Comprehensive evaluations across diverse QA benchmarks demonstrate that our proposed method significantly boosts the performance of collaborative retrieval-augmented LLMs, achieving performance gains of 4.23\\% to 91.39\\% over baseline methods across all tasks.",
        "authors": [
            "Guihang Hong",
            "Tao Ouyang",
            "Kongyange Zhao",
            "Zhi Zhou",
            "Xu Chen"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-11-08"
    },
    "http://arxiv.org/abs/2511.05895v1": {
        "title": "Efficient Dynamic MaxFlow Computation on GPUs",
        "link": "http://arxiv.org/abs/2511.05895v1",
        "abstract": "Maxflow is a fundamental problem in graph theory and combinatorial optimisation, used to determine the maximum flow from a source node to a sink node in a flow network. It finds applications in diverse domains, including computer networks, transportation, and image segmentation. The core idea is to maximise the total flow across the network without violating capacity constraints on edges and ensuring flow conservation at intermediate nodes. The rapid growth of unstructured and semi-structured data has motivated the development of parallel solutions to compute MaxFlow. However, due to the higher computational complexity, computing Maxflow for real-world graphs is time-consuming in practice. In addition, these graphs are dynamic and constantly evolve over time. In this work, we propose two Push-Relabel based algorithms for processing dynamic graphs on GPUs. The key novelty of our algorithms is their ability to efficiently handle both increments and decrements in edge capacities together when they appear in a batch. We illustrate the efficacy of our algorithms with a suite of real-world graphs. Overall, we find that for small updates, dynamic recomputation is significantly faster than a static GPU-based Maxflow.",
        "authors": [
            "Shruthi Kannappan",
            "Ashwina Kumar",
            "Rupesh Nasre"
        ],
        "categories": [
            "cs.DS",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.05895v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-08"
    },
    "http://arxiv.org/abs/2511.05843v1": {
        "title": "HYDRA: Breaking the Global Ordering Barrier in Multi-BFT Consensus",
        "link": "http://arxiv.org/abs/2511.05843v1",
        "abstract": "Multi-Byzantine Fault Tolerant (Multi-BFT) consensus, which runs multiple BFT instances in parallel, has recently emerged as a promising approach to overcome the leader bottleneck in classical BFT protocols. However, existing designs rely on a global ordering layer to serialize blocks across instances, an intuitive yet costly mechanism that constrains scalability, amplifies failure propagation, and complicates deployment. In this paper, we challenge this conventional wisdom. We present HYDRA, the first Multi-BFT consensus framework that eliminates global ordering altogether. HYDRA introduces an object-centric execution model that partitions transactions by their accessed objects, enabling concurrent yet deterministic execution across instances. To ensure consistency, HYDRA combines lightweight lock-based coordination with a deadlock resolution mechanism, achieving both scalability and correctness. We implement HYDRA and evaluate it on up to 128 replicas in both LAN and WAN environments. Experimental results show HYDRA outperforms several state-of-the-art Multi-BFT protocols in the presence of a straggler. These results demonstrate strong consistency and high performance by removing global ordering, opening a new direction toward scalable Multi-BFT consensus design.",
        "authors": [
            "Hanzheng Lyu",
            "Shaokang Xie",
            "Jianyu Niu",
            "Mohammad Sadoghi",
            "Yinqian Zhang",
            "Cong Wang",
            "Ivan Beschastnikh",
            "Chen Feng"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.05843v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-08"
    },
    "http://arxiv.org/abs/2511.05770v1": {
        "id": "http://arxiv.org/abs/2511.05770v1",
        "title": "An Efficient Gradient-Aware Error-Bounded Lossy Compressor for Federated Learning",
        "link": "http://arxiv.org/abs/2511.05770v1",
        "tags": [
            "training",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes an error-bounded lossy compressor for federated learning gradients to reduce communication overhead. The method exploits temporal correlations and convolutional kernel structures via two predictors: magnitude and sign predictors. Achieves up to 1.53x higher compression ratio than SZ3 and reduces communication time by 76.1%-96.2%.",
        "abstract": "Federated learning (FL) enables collaborative model training without exposing clients' private data, but its deployment is often constrained by the communication cost of transmitting gradients between clients and the central server, especially under system heterogeneity where low-bandwidth clients bottleneck overall performance. Lossy compression of gradient data can mitigate this overhead, and error-bounded lossy compression (EBLC) is particularly appealing for its fine-grained utility-compression tradeoff. However, existing EBLC methods (e.g., SZ), originally designed for smooth scientific data with strong spatial locality, rely on generic predictors such as Lorenzo and interpolation for entropy reduction to improve compression ratio. Gradient tensors, in contrast, exhibit low smoothness and weak spatial correlation, rendering these predictors ineffective and leading to poor compression ratios. To address this limitation, we propose an EBLC framework tailored for FL gradient data to achieve high compression ratios while preserving model accuracy. The core of it is an innovative prediction mechanism that exploits temporal correlations across FL training rounds and structural regularities within convolutional kernels to reduce residual entropy. The predictor is compatible with standard quantizers and entropy coders and comprises (1) a cross-round magnitude predictor based on a normalized exponential moving average, and (2) a sign predictor that leverages gradient oscillation and kernel-level sign consistency. Experiments show that this new EBLC yields up to 1.53x higher compression ratios than SZ3 with lower accuracy loss. Integrated into a real-world FL framework, APPFL, it reduces end-to-end communication time by 76.1%-96.2% under various constrained-bandwidth scenarios, demonstrating strong scalability for real-world FL deployments.",
        "authors": [
            "Zhijing Ye",
            "Sheng Di",
            "Jiamin Wang",
            "Zhiqing Zhong",
            "Zhaorui Zhang",
            "Xiaodong Yu"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "submit_date": "2025-11-07"
    },
    "http://arxiv.org/abs/2511.05762v1": {
        "title": "Distributed Recoverable Sketches (Extended Version)",
        "link": "http://arxiv.org/abs/2511.05762v1",
        "abstract": "Sketches are commonly used in computer systems and network monitoring tools to provide efficient query executions while maintaining a compact data representation. Switches and routers maintain sketches to track statistical characteristics of network traffic. The availability of such data is essential for the network analysis as a whole. Consequently, being able to recover sketches is critical after a switch crash. In this work, we explore how nodes in a network environment can cooperate to recover sketch data whenever any subset of them crashes. In particular, we focus on frequency estimation linear sketches, such as the Count-Min Sketch. We consider various approaches to ensure data reliability and explore the trade-offs between space consumption, runtime overheads, and traffic during recovery, which we point out as design guidelines. Besides different aspects of efficacy, we design a modular system for ease of maintenance and further scaling. A key aspect we examine is how the nodes update each other regarding their sketch content as it evolves over time. In particular, we compare periodic full updates vs incremental updates. We also examine several data structures to economically represent and encode a batch of latest changes. Our framework is generic, and other data structures can be plugged-in via an abstract API as long as they implement the corresponding API methods.",
        "authors": [
            "Diana Cohen",
            "Roy Friedman",
            "Rana Shahout"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.05762v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-07"
    },
    "http://arxiv.org/abs/2511.07466v1": {
        "id": "http://arxiv.org/abs/2511.07466v1",
        "title": "Optimal Multi-Constrained Workflow Scheduling for Cyber-Physical Systems in the Edge-Cloud Continuum",
        "link": "http://arxiv.org/abs/2511.07466v1",
        "tags": [
            "edge",
            "offline",
            "hardware"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes an optimal multi-constrained workflow scheduler for edge-cloud continuum CPS to minimize latency. Uses continuous-time MILP formulation with heterogeneous multicore and capability constraints. Achieves 13.54% average latency reduction vs. enhanced heuristic in real-world use case.",
        "abstract": "The emerging edge-hub-cloud paradigm has enabled the development of innovative latency-critical cyber-physical applications in the edge-cloud continuum. However, this paradigm poses multiple challenges due to the heterogeneity of the devices at the edge of the network, their limited computational, communication, and energy capacities, as well as their different sensing and actuating capabilities. To address these issues, we propose an optimal scheduling approach to minimize the overall latency of a workflow application in an edge-hub-cloud cyber-physical system. We consider multiple edge devices cooperating with a hub device and a cloud server. All devices feature heterogeneous multicore processors and various sensing, actuating, or other specialized capabilities. We present a comprehensive formulation based on continuous-time mixed integer linear programming, encapsulating multiple constraints often overlooked by existing approaches. We conduct a comparative experimental evaluation between our method and a well-established and effective scheduling heuristic, which we enhanced to consider the constraints of the specific problem. The results reveal that our technique outperforms the heuristic, achieving an average latency improvement of 13.54% in a relevant real-world use case, under varied system configurations. In addition, the results demonstrate the scalability of our method under synthetic workflows of varying sizes, attaining a 33.03% average latency decrease compared to the heuristic.",
        "authors": [
            "Andreas Kouloumpris",
            "Georgios L. Stavrinides",
            "Maria K. Michael",
            "Theocharis Theocharides"
        ],
        "categories": [
            "cs.NI",
            "cs.DC",
            "cs.ET"
        ],
        "submit_date": "2025-11-07"
    },
    "http://arxiv.org/abs/2512.00029v1": {
        "title": "An optimization framework for task allocation in the edge/hub/cloud paradigm",
        "link": "http://arxiv.org/abs/2512.00029v1",
        "abstract": "With the advent of the Internet of Things (IoT), novel critical applications have emerged that leverage the edge/hub/cloud paradigm, which diverges from the conventional edge computing perspective. A growing number of such applications require a streamlined architecture for their effective execution, often comprising a single edge device with sensing capabilities, a single hub device (e.g., a laptop or smartphone) for managing and assisting the edge device, and a more computationally capable cloud server. Typical examples include the utilization of an unmanned aerial vehicle (UAV) for critical infrastructure inspection or a wearable biomedical device (e.g., a smartwatch) for remote patient monitoring. Task allocation in this streamlined architecture is particularly challenging, due to the computational, communication, and energy limitations of the devices at the network edge. Consequently, there is a need for a comprehensive framework that can address the specific task allocation problem optimally and efficiently. To this end, we propose a complete, binary integer linear programming (BILP) based formulation for an application-driven design-time approach, capable of providing an optimal task allocation in the targeted edge/hub/cloud environment. The proposed method minimizes the desired objective, either the overall latency or overall energy consumption, while considering several crucial parameters and constraints often overlooked in related literature. We evaluate our framework using a real-world use-case scenario, as well as appropriate synthetic benchmarks. Our extensive experimentation reveals that the proposed approach yields optimal and scalable results, enabling efficient design space exploration for different applications and computational devices.",
        "authors": [
            "Andreas Kouloumpris",
            "Georgios L. Stavrinides",
            "Maria K. Michael",
            "Theocharis Theocharides"
        ],
        "categories": [
            "cs.NI",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.00029v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-07"
    },
    "http://arxiv.org/abs/2511.11628v1": {
        "title": "Mixture-of-Schedulers: An Adaptive Scheduling Agent as a Learned Router for Expert Policies",
        "link": "http://arxiv.org/abs/2511.11628v1",
        "abstract": "Modern operating system schedulers employ a single, static policy, which struggles to deliver optimal performance across the diverse and dynamic workloads of contemporary systems. This \"one-policy-fits-all\" approach leads to significant compromises in fairness, throughput, and latency, particularly with the rise of heterogeneous hardware and varied application architectures.   This paper proposes a new paradigm: dynamically selecting the optimal policy from a portfolio of specialized schedulers rather than designing a single, monolithic one. We present the Adaptive Scheduling Agent (ASA), a lightweight framework that intelligently matches workloads to the most suitable \"expert\" scheduling policy at runtime. ASA's core is a novel, low-overhead offline/online approach. First, an offline process trains a universal, hardware-agnostic machine learning model to recognize abstract workload patterns from system behaviors. Second, at runtime, ASA continually processes the model's predictions using a time-weighted probability voting algorithm to identify the workload, then makes a scheduling decision by consulting a pre-configured, machine-specific mapping table to switch to the optimal scheduler via Linux's sched_ext framework. This decoupled architecture allows ASA to adapt to new hardware platforms rapidly without expensive retraining of the core recognition model.   Our evaluation, based on a novel benchmark focused on user-experience metrics, demonstrates that ASA consistently outperforms the default Linux scheduler (EEVDF), achieving superior results in 86.4% of test scenarios. Furthermore, ASA's selections are near-optimal, ranking among the top three schedulers in 78.6% of all scenarios. This validates our approach as a practical path toward more intelligent, adaptive, and responsive operating system schedulers.",
        "authors": [
            "Xinbo Wang",
            "Shian Jia",
            "Ziyang Huang",
            "Jing Cao",
            "Mingli Song"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "id": "http://arxiv.org/abs/2511.11628v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-07"
    },
    "http://arxiv.org/abs/2511.05209v1": {
        "title": "CUNQA: a Distributed Quantum Computing emulator for HPC",
        "link": "http://arxiv.org/abs/2511.05209v1",
        "abstract": "The challenge of scaling quantum computers to gain computational power is expected to lead to architectures with multiple connected quantum processing units (QPUs), commonly referred to as Distributed Quantum Computing (DQC). In parallel, there is a growing momentum toward treating quantum computers as accelerators, integrating them into the heterogeneous architectures of high-performance computing (HPC) environments. This work combines these two foreseeable futures in CUNQA, an open-source DQC emulator designed for HPC environments that allows testing, evaluating and studying DQC in HPC before it even becomes real. It implements the three DQC models of no-communication, classical-communication and quantum-communication; which will be examined in this work. Addressing programming considerations, explaining emulation and simulation details, and delving into the specifics of the implementation will be part of the effort. The well-known Quantum Phase Estimation (QPE) algorithm is used to demonstrate and analyze the emulation of the models. To the best of our knowledge, CUNQA is the first tool designed to emulate the three DQC schemes in an HPC environment.",
        "authors": [
            "Jorge Vázquez-Pérez",
            "Daniel Expósito-Patiño",
            "Marta Losada",
            "Álvaro Carballido",
            "Andrés Gómez",
            "Tomás F. Pena"
        ],
        "categories": [
            "quant-ph",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.05209v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-07"
    },
    "http://arxiv.org/abs/2511.05067v1": {
        "id": "http://arxiv.org/abs/2511.05067v1",
        "title": "GPU Under Pressure: Estimating Application's Stress via Telemetry and Performance Counters",
        "link": "http://arxiv.org/abs/2511.05067v1",
        "tags": [
            "kernel",
            "hardware"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Studies estimating GPU stress under sustained workloads using telemetry and performance counters. Combines telemetry data with specific counters (throughput, instructions issued, stalls) to assess stress. Demonstrates effectiveness for CNN workloads, aiming to predict reliability and aging effects.",
        "abstract": "Graphics Processing Units (GPUs) are specialized accelerators in data centers and high-performance computing (HPC) systems, enabling the fast execution of compute-intensive applications, such as Convolutional Neural Networks (CNNs). However, sustained workloads can impose significant stress on GPU components, raising reliability concerns due to potential faults that corrupt the intermediate application computations, leading to incorrect results. Estimating the stress induced by an application is thus crucial to predict reliability (with\\,special\\,emphasis\\,on\\,aging\\,effects). In this work, we combine online telemetry parameters and hardware performance counters to assess GPU stress induced by different applications. The experimental results indicate the stress induced by a parallel workload can be estimated by combining telemetry data and Performance Counters that reveal the efficiency in the resource usage of the target workload. For this purpose the selected performance counters focus on measuring the i) throughput, ii) amount of issued instructions and iii) stall events.",
        "authors": [
            "Giuseppe Esposito",
            "Juan-David Guerrero-Balaguera",
            "Josie Esteban Rodriguez Condia",
            "Matteo Sonza Reorda",
            "Marco Barbiero",
            "Rossella Fortuna"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-11-07"
    },
    "http://arxiv.org/abs/2511.05053v1": {
        "id": "http://arxiv.org/abs/2511.05053v1",
        "title": "Accelerating HDC-CNN Hybrid Models Using Custom Instructions on RISC-V GPUs",
        "link": "http://arxiv.org/abs/2511.05053v1",
        "tags": [
            "video",
            "edge",
            "hardware"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Explores optimizing hybrid HDC-CNN models for energy-efficient ML. Designs custom instructions on RISC-V GPUs for accelerated processing. Achieves 56.2x speedup in microbenchmarks.",
        "abstract": "Machine learning based on neural networks has advanced rapidly, but the high energy consumption required for training and inference remains a major challenge. Hyperdimensional Computing (HDC) offers a lightweight, brain-inspired alternative that enables high parallelism but often suffers from lower accuracy on complex visual tasks. To overcome this, hybrid accelerators combining HDC and Convolutional Neural Networks (CNNs) have been proposed, though their adoption is limited by poor generalizability and programmability. The rise of open-source RISC-V architectures has created new opportunities for domain-specific GPU design. Unlike traditional proprietary GPUs, emerging RISC-V-based GPUs provide flexible, programmable platforms suitable for custom computation models such as HDC. In this study, we design and implement custom GPU instructions optimized for HDC operations, enabling efficient processing for hybrid HDC-CNN workloads. Experimental results using four types of custom HDC instructions show a performance improvement of up to 56.2 times in microbenchmark tests, demonstrating the potential of RISC-V GPUs for energy-efficient, high-performance computing.",
        "authors": [
            "Wakuto Matsumi",
            "Riaz-Ul-Haque Mian"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.GR"
        ],
        "submit_date": "2025-11-07"
    },
    "http://arxiv.org/abs/2511.13738v1": {
        "title": "TT-Edge: A Hardware-Software Co-Design for Energy-Efficient Tensor-Train Decomposition on Edge AI",
        "link": "http://arxiv.org/abs/2511.13738v1",
        "abstract": "The growing demands of distributed learning on resource constrained edge devices underscore the importance of efficient on device model compression. Tensor Train Decomposition (TTD) offers high compression ratios with minimal accuracy loss, yet repeated singular value decompositions (SVDs) and matrix multiplications can impose significant latency and energy costs on low power processors. In this work, we present TT-Edge, a hardware software co designed framework aimed at overcoming these challenges. By splitting SVD into two phases--bidiagonalization and diagonalization--TT-Edge offloads the most compute intensive tasks to a specialized TTD Engine. This engine integrates tightly with an existing GEMM accelerator, thereby curtailing the frequent matrix vector transfers that often undermine system performance and energy efficiency. Implemented on a RISC-V-based edge AI processor, TT-Edge achieves a 1.7x speedup compared to a GEMM only baseline when compressing a ResNet 32 model via TTD, while reducing overall energy usage by 40.2 percent. These gains come with only a 4 percent increase in total power and minimal hardware overhead, enabled by a lightweight design that reuses GEMM resources and employs a shared floating point unit. Our experimental results on both FPGA prototypes and post-synthesis power analysis at 45 nm demonstrate that TT-Edge effectively addresses the latency and energy bottlenecks of TTD based compression in edge environments.",
        "authors": [
            "Hyunseok Kwak",
            "Kyeongwon Lee",
            "Kyeongpil Min",
            "Chaebin Jung",
            "Woojoo Lee"
        ],
        "categories": [
            "cs.DC",
            "cs.AR"
        ],
        "id": "http://arxiv.org/abs/2511.13738v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-07"
    },
    "http://arxiv.org/abs/2511.04946v1": {
        "title": "The Future of Fully Homomorphic Encryption System: from a Storage I/O Perspective",
        "link": "http://arxiv.org/abs/2511.04946v1",
        "abstract": "Fully Homomorphic Encryption (FHE) allows computations to be performed on encrypted data, significantly enhancing user privacy. However, the I/O challenges associated with deploying FHE applications remains understudied. We analyze the impact of storage I/O on the performance of FHE applications and summarize key lessons from the status quo. Key results include that storage I/O can degrade the performance of ASICs by as much as 357$\\times$ and reduce GPUs performance by up to 22$\\times$.",
        "authors": [
            "Lei Chen",
            "Erci Xu",
            "Yiming Sun",
            "Shengyu Fan",
            "Xianglong Deng",
            "Guiming Shi",
            "Guang Fan",
            "Liang Kong",
            "Yilan Zhu",
            "Shoumeng Yan",
            "Mingzhe Zhang"
        ],
        "categories": [
            "cs.CR",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.04946v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-07"
    },
    "http://arxiv.org/abs/2511.11624v1": {
        "id": "http://arxiv.org/abs/2511.11624v1",
        "title": "Characterizing and Understanding Energy Footprint and Efficiency of Small Language Model on Edges",
        "link": "http://arxiv.org/abs/2511.11624v1",
        "tags": [
            "edge",
            "serving",
            "quantization"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "This paper analyzes the power efficiency of small language models on edge devices, comparing CPU and GPU configurations. It evaluates energy-to-performance ratios across Raspberry Pi, Jetson Nano, and Jetson Orin platforms. Jetson Orin Nano with GPU achieves highest efficiency, with Llama 3.2 offering best accuracy-power balance.",
        "abstract": "Cloud-based large language models (LLMs) and their variants have significantly influenced real-world applications. Deploying smaller models (i.e., small language models (SLMs)) on edge devices offers additional advantages, such as reduced latency and independence from network connectivity. However, edge devices' limited computing resources and constrained energy budgets challenge efficient deployment. This study evaluates the power efficiency of five representative SLMs - Llama 3.2, Phi-3 Mini, TinyLlama, and Gemma 2 on Raspberry Pi 5, Jetson Nano, and Jetson Orin Nano (CPU and GPU configurations). Results show that Jetson Orin Nano with GPU acceleration achieves the highest energy-to-performance ratio, significantly outperforming CPU-based setups. Llama 3.2 provides the best balance of accuracy and power efficiency, while TinyLlama is well-suited for low-power environments at the cost of reduced accuracy. In contrast, Phi-3 Mini consumes the most energy despite its high accuracy. In addition, GPU acceleration, memory bandwidth, and model architecture are key in optimizing inference energy efficiency. Our empirical analysis offers practical insights for AI, smart systems, and mobile ad-hoc platforms to leverage tradeoffs from accuracy, inference latency, and power efficiency in energy-constrained environments.",
        "authors": [
            "Md Romyull Islam",
            "Bobin Deng",
            "Nobel Dhar",
            "Tu N. Nguyen",
            "Selena He",
            "Yong Shi",
            "Kun Suo"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "submit_date": "2025-11-07"
    },
    "http://arxiv.org/abs/2511.05626v1": {
        "title": "LLMs as Packagers of HPC Software",
        "link": "http://arxiv.org/abs/2511.05626v1",
        "abstract": "High performance computing (HPC) software ecosystems are inherently heterogeneous, comprising scientific applications that depend on hundreds of external packages, each with distinct build systems, options, and dependency constraints. Tools such as Spack automate dependency resolution and environment management, but their effectiveness relies on manually written build recipes. As these ecosystems grow, maintaining existing specifications and creating new ones becomes increasingly labor-intensive. While large language models (LLMs) have shown promise in code generation, automatically producing correct and maintainable Spack recipes remains a significant challenge. We present a systematic analysis of how LLMs and context-augmentation methods can assist in the generation of Spack recipes. To this end, we introduce SpackIt, an end-to-end framework that combines repository analysis, retrieval of relevant examples, and iterative refinement through diagnostic feedback. We apply SpackIt to a representative subset of 308 open-source HPC packages to assess its effectiveness and limitations. Our results show that SpackIt increases installation success from 20% in a zero-shot setting to over 80% in its best configuration, demonstrating the value of retrieval and structured feedback for reliable package synthesis.",
        "authors": [
            "Caetano Melone",
            "Daniel Nichols",
            "Konstantinos Parasyris",
            "Todd Gamblin",
            "Harshitha Menon"
        ],
        "categories": [
            "cs.SE",
            "cs.AI",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.05626v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-07"
    },
    "http://arxiv.org/abs/2511.04853v1": {
        "id": "http://arxiv.org/abs/2511.04853v1",
        "title": "Marionette: Data Structure Description and Management for Heterogeneous Computing",
        "link": "http://arxiv.org/abs/2511.04853v1",
        "tags": [
            "kernel",
            "hardware",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes Marionette, a C++17 library for flexible and efficient data structure management in heterogeneous computing. It decouples data layout from interface description, supports multiple memory strategies, and enables efficient data transfers across devices. Achieves minimal runtime overhead, demonstrated in a CUDA case study.",
        "abstract": "Adapting large, object-oriented C++ codebases for hardware acceleration might be extremely challenging, particularly when targeting heterogeneous platforms such as GPUs. Marionette is a C++17 library designed to address this by enabling flexible, efficient, and portable data structure definitions. It decouples data layout from the description of the interface, supports multiple memory management strategies, and provides efficient data transfers and conversions across devices, all of this with minimal runtime overhead due to the compile-time nature of its abstractions. By allowing interfaces to be augmented with arbitrary functions, Marionette maintains compatibility with existing code and offers a streamlined interface that supports both straightforward and advanced use cases. This paper outlines its design, usage, and performance, including a CUDA-based case study demonstrating its efficiency and flexibility.",
        "authors": [
            "Nuno dos Santos Fernandes",
            "Pedro Tomás",
            "Nuno Roma",
            "Frank Winklmeier",
            "Patricia Conde-Muíño"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-11-06"
    },
    "http://arxiv.org/abs/2511.04631v2": {
        "title": "Resolving Conflicts with Grace: Dynamically Concurrent Universality",
        "link": "http://arxiv.org/abs/2511.04631v2",
        "abstract": "Synchronization is the major obstacle to scalability in distributed computing. Concurrent operations on the shared data engage in synchronization when they encounter a \\emph{conflict}, i.e., their effects depend on the order in which they are applied. Ideally, one would like to detect conflicts in a \\emph{dynamic} manner, i.e., adjusting to the current system state. Indeed, it is very common that two concurrent operations conflict only in some rarely occurring states. In this paper, we define the notion of \\emph{dynamic concurrency}: an operation employs strong synchronization primitives only if it \\emph{has} to arbitrate with concurrent operations, given the current system state. We then present a dynamically concurrent universal construction.",
        "authors": [
            "Petr Kuznetsov",
            "Nathan Josia Schrodt"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.04631v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-06"
    },
    "http://arxiv.org/abs/2511.04523v1": {
        "title": "A New Probabilistic Mobile Byzantine Failure Model for Self-Protecting Systems",
        "link": "http://arxiv.org/abs/2511.04523v1",
        "abstract": "Modern distributed systems face growing security threats, as attackers continuously enhance their skills and vulnerabilities span across the entire system stack, from hardware to the application layer. In the system design phase, fault tolerance techniques can be employed to safeguard systems. From a theoretical perspective, an attacker attempting to compromise a system can be abstracted by considering the presence of Byzantine processes in the system. Although this approach enhances the resilience of the distributed system, it introduces certain limitations regarding the accuracy of the model in reflecting real-world scenarios. In this paper, we consider a self-protecting distributed system based on the \\emph{Monitoring-Analyse-Plan-Execute over a shared Knowledge} (MAPE-K) architecture, and we propose a new probabilistic Mobile Byzantine Failure (MBF) that can be plugged into the Analysis component. Our new model captures the dynamics of evolving attacks and can be used to drive the self-protection and reconfiguration strategy. We analyze mathematically the time that it takes until the number of Byzantine nodes crosses given thresholds, or for the system to self-recover back into a safe state, depending on the rates of Byzantine infection spreading \\emph{vs.} the rate of self-recovery. We also provide simulation results that illustrate the behavior of the system under such assumptions.",
        "authors": [
            "Silvia Bonomi",
            "Giovanni Farina",
            "Roy Friedman",
            "Eviatar B. Procaccia",
            "Sebastien Tixeuil"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.04523v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-06"
    },
    "http://arxiv.org/abs/2511.04489v1": {
        "title": "Scalable Domain-decomposed Monte Carlo Neutral Transport for Nuclear Fusion",
        "link": "http://arxiv.org/abs/2511.04489v1",
        "abstract": "EIRENE [1] is a Monte Carlo neutral transport solver heavily used in the fusion community. EIRENE does not implement domain decomposition, making it impossible to use for simulations where the grid data does not fit on one compute node (see e.g. [2]). This paper presents a domain-decomposed Monte Carlo (DDMC) algorithm implemented in a new open source Monte Carlo code, Eiron. Two parallel algorithms currently used in EIRENE are also implemented in Eiron, and the three algorithms are compared by running strong scaling tests, with DDMC performing better than the other two algorithms in nearly all cases. On the supercomputer Mahti [3], DDMC strong scaling is superlinear for grids that do not fit into an L3 cache slice (4 MiB). The DDMC algorithm is also scaled up to 16384 cores in weak scaling tests, with a weak scaling efficiency of 45% in a high-collisional (heavier compute load) case, and 26% in a low-collisional (lighter compute load) case. We conclude that implementing this domain decomposition algorithm in EIRENE would improve performance and enable simulations that are currently impossible due to memory constraints.",
        "authors": [
            "Oskar Lappi",
            "Huw Leggate",
            "Yannick Marandet",
            "Jan Åström",
            "Keijo Heljanko",
            "Dmitriy V. Borodin"
        ],
        "categories": [
            "physics.comp-ph",
            "cs.DC",
            "cs.PF"
        ],
        "id": "http://arxiv.org/abs/2511.04489v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-06"
    },
    "http://arxiv.org/abs/2511.04477v1": {
        "id": "http://arxiv.org/abs/2511.04477v1",
        "title": "Enabling Dynamic Sparsity in Quantized LLM Inference",
        "link": "http://arxiv.org/abs/2511.04477v1",
        "tags": [
            "quantization",
            "sparse",
            "edge"
        ],
        "relevant": true,
        "indexed_date": "2025-11-06",
        "tldr": "Proposes techniques to enable dynamic sparsity in quantized LLM inference for edge devices. Key designs include a zigzag-patterned quantization layout and a specialized GEMV kernel for commodity GPUs. Achieves up to 1.55x faster decoding throughput while maintaining accuracy.",
        "abstract": "Deploying large language models (LLMs) on end-user devices is gaining importance due to benefits in responsiveness, privacy, and operational cost. Yet the limited memory and compute capability of mobile and desktop GPUs make efficient execution difficult. Recent observations suggest that the internal activations of LLMs are often dynamically sparse, meaning that for each input, only part of the network contributes significantly to the output. Such sparsity could reduce computation, but it interacts poorly with group-wise quantization, which remains the dominant approach for fitting LLMs onto resource-constrained hardware. To reconcile these two properties, this study proposes a set of techniques that realize dynamic sparse inference under low-bit quantization. The method features: (1) a zigzag-patterned quantization layout that organizes weights in a way consistent with activation sparsity and improves GPU memory locality; (2) a specialized GEMV kernel designed for this layout to fully utilize parallel compute units; and (3) a compact runtime mechanism that gathers sparse indices with minimal overhead. Across several model scales and hardware configurations, the approach achieves up to 1.55x faster decoding throughput while maintaining accuracy comparable to dense quantized inference, showing that structured sparsity and quantization can effectively coexist on commodity GPUs.",
        "authors": [
            "Rongxiang Wang",
            "Kangyuan Shu",
            "Felix Xiaozhu Lin"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-11-06"
    },
    "http://arxiv.org/abs/2511.11621v1": {
        "id": "http://arxiv.org/abs/2511.11621v1",
        "title": "AIvailable: A Software-Defined Architecture for LLM-as-a-Service on Heterogeneous and Legacy GPUs",
        "link": "http://arxiv.org/abs/2511.11621v1",
        "tags": [
            "serving",
            "offloading",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes AIvailable, a software-defined LLM-as-a-Service platform for heterogeneous GPUs. Uses VRAM-aware dynamic allocation to run GPU-accelerated inference without CPU fallbacks on mixed NVIDIA/AMD nodes. Achieves efficient resource utilization by repurposing legacy GPUs for scalable serving.",
        "abstract": "The rise of Large Language Models (LLM) has increased the need for scalable, high-performance inference systems, yet most existing frameworks assume homogeneous, resource-rich hardware, often unrealistic in academic, or resource-constrained settings. We introduce AIvailable, a low-cost, highly available LLM-as-a-Service (LLMaaS) platform, that uses a software-defined approach for running LLMs across heterogeneous and legacy GPU nodes, including NVIDIA and AMD devices, with a focus on fully utilizing each node's VRAM. AIvailable operates as a fully GPU-accelerated inference without CPU fallbacks, featuring a unified client interface that allows seamless interaction with all deployed LLMs through a single logical unit. The architecture comprises four main components: the Client Interface for user access, the Service Frontend for secure request routing and load balancing, the SDAI Controller for orchestration, deployment, and monitoring, and the Service Backend of heterogeneous GPU nodes executing workloads. By abstracting GPU-specific details and providing dynamic, VRAM-aware allocation and reallocation of models, AIvailable ensures efficient use of resources and resilience against failures or workload fluctuations. Targeting academic labs, private companies, and other constrained organizations, it supports diverse open LLMs helping democratize generative AI through the repurposing of legacy GPUs.",
        "authors": [
            "Pedro Antunes",
            "Ana Rita Ortigoso",
            "Gabriel Vieira",
            "Daniel Fuentes",
            "Luís Frazão",
            "Nuno Costa",
            "António Pereira"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.NI"
        ],
        "submit_date": "2025-11-06"
    },
    "http://arxiv.org/abs/2511.04268v1": {
        "id": "http://arxiv.org/abs/2511.04268v1",
        "title": "Parallel Spawning Strategies for Dynamic-Aware MPI Applications",
        "link": "http://arxiv.org/abs/2511.04268v1",
        "tags": [
            "training",
            "sparse",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Addresses the high cost of reconfiguration for malleable MPI applications. Proposes a cooperative parallel spawning strategy that reuses original processes and fully releases unneeded nodes during shrinkage. Reduces shrink operation cost by at least 20× while maintaining expansion overhead below 1.25×.",
        "abstract": "Dynamic resource management is an increasingly important capability of High Performance Computing systems, as it enables jobs to adjust their resource allocation at runtime. This capability has been shown to reduce workload makespan, substantially decrease job waiting times and improve overall system utilization. In this context, malleability refers to the ability of applications to adapt to new resource allocations during execution. Although beneficial, malleability incurs significant reconfiguration costs, making the reduction of these costs an important research topic.   Some existing methods for MPI applications respawn the entire application, which is an expensive solution that avoids the reuse of original processes. Other MPI methods reuse them, but fail to fully release unneeded processes when shrinking, since some ranks within the same communicator remain active across nodes, preventing the application from returning those nodes to the system. This work overcomes both limitations by proposing a novel parallel spawning strategy, in which all processes cooperate in spawning before redistribution, thereby reducing execution time. Additionally, it removes shrinkage limitations, allowing better adaptation of parallel systems to workload and reducing their makespan. As a result, it preserves competitive expansion times with at most a $1.25\\times$ overhead, while enabling fast shrink operations that reduce their cost by at least $20\\times$. This strategy has been validated on both homogeneous and heterogeneous systems and can also be applied in shared-resource environments.",
        "authors": [
            "Iker Martín-Álvarez",
            "José I. Aliaga",
            "Maribel Castillo",
            "Sergio Iserte"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-11-06"
    },
    "http://arxiv.org/abs/2511.04183v1": {
        "title": "A Reinforced Evolution-Based Approach to Multi-Resource Load Balancing",
        "link": "http://arxiv.org/abs/2511.04183v1",
        "abstract": "This paper presents a reinforced genetic approach to a defined d-resource system optimization problem. The classical evolution schema was ineffective due to a very strict feasibility function in the studied problem. Hence, the presented strategy has introduced several modifications and adaptations to standard genetic routines, e.g.: a migration operator which is an analogy to the biological random genetic drift.",
        "authors": [
            "Leszek Sliwko"
        ],
        "categories": [
            "cs.NE",
            "cs.AI",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.04183v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-06"
    },
    "http://arxiv.org/abs/2511.11619v1": {
        "title": "DIAP: A Decentralized Agent Identity Protocol with Zero-Knowledge Proofs and a Hybrid P2P Stack",
        "link": "http://arxiv.org/abs/2511.11619v1",
        "abstract": "The absence of a fully decentralized, verifiable, and privacy-preserving communication protocol for autonomous agents remains a core challenge in decentralized computing. Existing systems often rely on centralized intermediaries, which reintroduce trust bottlenecks, or lack decentralized identity-resolution mechanisms, limiting persistence and cross-network interoperability.   We propose the Decentralized Interstellar Agent Protocol (DIAP), a novel framework for agent identity and communication that enables persistent, verifiable, and trustless interoperability in fully decentralized environments. DIAP binds an agent's identity to an immutable IPFS or IPNS content identifier and uses zero-knowledge proofs (ZKP) to dynamically and statelessly prove ownership, removing the need for record updates.   We present a Rust SDK that integrates Noir (for zero-knowledge proofs), DID-Key, IPFS, and a hybrid peer-to-peer stack combining Libp2p GossipSub for discovery and Iroh for high-performance, QUIC based data exchange. DIAP introduces a zero-dependency ZKP deployment model through a universal proof manager and compile-time build script that embeds a precompiled Noir circuit, eliminating the need for external ZKP toolchains. This enables instant, verifiable, and privacy-preserving identity proofs.   This work establishes a practical, high-performance foundation for next-generation autonomous agent ecosystems and agent-to-agent (A to A) economies.",
        "authors": [
            "Yuanjie Liu",
            "Wenpeng Xing",
            "Ye Zhou",
            "Gaowei Chang",
            "Changting Lin",
            "Meng Han"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "id": "http://arxiv.org/abs/2511.11619v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-06"
    },
    "http://arxiv.org/abs/2511.03941v1": {
        "id": "http://arxiv.org/abs/2511.03941v1",
        "title": "Stochastic Modeling for Energy-Efficient Edge Infrastructure",
        "link": "http://arxiv.org/abs/2511.03941v1",
        "tags": [
            "edge"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Addresses energy efficiency in edge computing via AI-driven predictive power scaling. Uses Markov Chains for modeling power state transitions and optimizes workload distribution to minimize unnecessary transitions. Reduces energy consumption disparities by 30% and improves system responsiveness.",
        "abstract": "Edge Computing enables low-latency processing for real-time applications but introduces challenges in power management due to the distributed nature of edge devices and their limited energy resources. This paper proposes a stochastic modeling approach using Markov Chains to analyze power state transitions in Edge Computing. By deriving steady-state probabilities and evaluating energy consumption, we demonstrate the benefits of AI-driven predictive power scaling over conventional reactive methods. Monte Carlo simulations validate the model, showing strong alignment between theoretical and empirical results. Sensitivity analysis highlights how varying transition probabilities affect power efficiency, confirming that predictive scaling minimizes unnecessary transitions and improves overall system responsiveness. Our findings suggest that AI-based power management strategies significantly enhance energy efficiency by anticipating workload demands and optimizing state transitions. Experimental results indicate that AI-based power management optimizes workload distribution across heterogeneous edge nodes, reducing energy consumption disparities between devices, improving overall efficiency, and enhancing adaptive power coordination in multi-node environments.",
        "authors": [
            "Fabio Diniz Rossi"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-11-06"
    },
    "http://arxiv.org/abs/2511.03866v2": {
        "title": "OMPILOT: Harnessing Transformer Models for Auto Parallelization to Shared Memory Computing Paradigms",
        "link": "http://arxiv.org/abs/2511.03866v2",
        "abstract": "Recent advances in large language models (LLMs) have significantly accelerated progress in code translation, enabling more accurate and efficient transformation across programming languages. While originally developed for natural language processing, LLMs have shown strong capabilities in modeling programming language syntax and semantics, outperforming traditional rule-based systems in both accuracy and flexibility. These models have streamlined cross-language conversion, reduced development overhead, and accelerated legacy code migration. In this paper, we introduce OMPILOT, a novel domain-specific encoder-decoder transformer tailored for translating C++ code into OpenMP, enabling effective shared-memory parallelization. OMPILOT leverages custom pre-training objectives that incorporate the semantics of parallel constructs and combines both unsupervised and supervised learning strategies to improve code translation robustness. Unlike previous work that focused primarily on loop-level transformations, OMPILOT operates at the function level to capture a wider semantic context. To evaluate our approach, we propose OMPBLEU, a novel composite metric specifically crafted to assess the correctness and quality of OpenMP parallel constructs, addressing limitations in conventional translation metrics.",
        "authors": [
            "Arijit Bhattacharjee",
            "Ali TehraniJamsaz",
            "Le Chen",
            "Niranjan Hasabnis",
            "Mihai Capota",
            "Nesreen Ahmed",
            "Ali Jannesari"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.LG",
            "cs.PF",
            "cs.PL"
        ],
        "id": "http://arxiv.org/abs/2511.03866v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-05"
    },
    "http://arxiv.org/abs/2511.03662v1": {
        "title": "A General Input-Dependent Colorless Computability Theorem and Applications to Core-Dependent Adversaries",
        "link": "http://arxiv.org/abs/2511.03662v1",
        "abstract": "Distributed computing tasks can be presented with a triple $(\\I,\\Ou,Δ)$. The solvability of a colorless task on the Iterated Immediate Snapshot model (IIS) has been characterized by the Colorless Computability Theorem \\cite[Th.4.3.1]{HKRbook}. A recent paper~\\cite{CG-24} generalizes this theorem for any message adversaries $\\ma \\subseteq IIS$ by geometric methods. In 2001, Mostéfaoui, Rajsbaum, Raynal, and Roy \\cite{condbased} introduced \\emph{condition-based adversaries}. This setting considers a particular adversary that will be applied only to a subset of input configurations. In this setting, they studied the $k$-set agreement task with condition-based $t$-resilient adversaries and obtained a sufficient condition on the conditions that make $k$-Set Agreement solvable. In this paper we have three contributions:   -We generalize the characterization of~\\cite{CG-24} to \\emph{input-dependent} adversaries, which means that the adversaries can change depending on the input configuration.   - We show that core-resilient adversaries of $IIS_n$ have the same computability power as the core-resilient adversaries of $IIS_n$ where crashes only happen at the start.   - Using the two previous contributions, we provide a necessary and sufficient characterization of the condition-based, core-dependent adversaries that can solve $k$-Set Agreement. We also distinguish four settings that may appear when presenting a distributed task as $(\\I,\\Ou,Δ)$. Finally, in a later section, we present structural properties on the carrier map $Δ$. Such properties allow simpler proof, without changing the computability power of the task. Most of the proofs in this article leverage the topological framework used in distributed computing by using simple geometric constructions.",
        "authors": [
            "Yannis Coutouly",
            "Emmanuel Godard"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.03662v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-05"
    },
    "http://arxiv.org/abs/2511.03609v2": {
        "title": "Stone Duality Proofs for Colorless Distributed Computability Theorems",
        "link": "http://arxiv.org/abs/2511.03609v2",
        "abstract": "We introduce a new topological encoding of executions of round-based, full-information distributed protocols via spectral spaces. Such protocols constitute a model of distributed computations which are functorially presented and englobe message adversaries. We give a characterization of the solvability of colorless tasks against compact adversaries. Colorless tasks are an important class of distributed tasks, examples thereof including the ubiquitous agreement tasks. Therefore, our result is a significant step toward unifying topological methods in distributed computing.   The main insight of this work is in considering global states obtained after finite executions of a distributed protocol not as abstract simplicial complexes as was previously done, but as spectral spaces, considering the Alexandrov topology on the associated face posets. Given an adversary $\\mathcal{M}$ with a set of inputs $\\mathcal{I}$, we define a limit object $Π^\\infty_{\\mathcal{M}}(\\mathcal{I})$ by a projective limit in the category of spectral spaces. This encodes all distributed information about the adversary, allowing us to derive a new distributed computability theorem using Stone duality: there exists an algorithm solving a colorless task $(\\mathcal{I},\\mathcal{O},Δ)$ against the compact adversary $\\mathcal{M}$ if and only if there exists a spectral map $Π^\\infty_{\\mathcal{M}}(\\mathcal{I})\\rightarrow\\mathcal{O}$ compatible with $Δ$.   From this characterization, we derive the known colorless computability theorems for (colored or uncolored) Iterated Immediate Snapshot. Quite surprisingly, colored and uncolored models have the same distributed computability power, i.e. they solve the same tasks. Our new proofs give topological reasons for this equivalence, previously known through algorithmic reductions.",
        "authors": [
            "Cameron Calk",
            "Emmanuel Godard"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.03609v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-05"
    },
    "http://arxiv.org/abs/2511.03533v1": {
        "title": "Investigating the Impact of Isolation on Synchronized Benchmarks",
        "link": "http://arxiv.org/abs/2511.03533v1",
        "abstract": "Benchmarking in cloud environments suffers from performance variability from multi-tenant resource contention. Duet benchmarking mitigates this by running two workload versions concurrently on the same VM, exposing them to identical external interference. However, intra-VM contention between synchronized workloads necessitates additional isolation mechanisms.   This work evaluates three such strategies: cgroups and CPU pinning, Docker containers, and Firecracker MicroVMs. We compare all strategies with an unisolated baseline experiment, by running benchmarks with a duet setup alongside a noise generator. This noise generator \"steals\" compute resources to degrade performance measurements.   All experiments showed different latency distributions while under the effects of noise generation, but results show that process isolation generally lowered false positives, except for our experiments with Docker containers. Even though Docker containers rely internally on cgroups and CPU pinning, they were more susceptible to performance degradation due to noise influence. Therefore, we recommend to use process isolation for synchronized workloads, with the exception of Docker containers.",
        "authors": [
            "Nils Japke",
            "Furat Hamdan",
            "Diana Baumann",
            "David Bermbach"
        ],
        "categories": [
            "cs.DC",
            "cs.SE"
        ],
        "id": "http://arxiv.org/abs/2511.03533v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-05"
    },
    "http://arxiv.org/abs/2511.11617v1": {
        "id": "http://arxiv.org/abs/2511.11617v1",
        "title": "AnchorTP: Resilient LLM Inference with State-Preserving Elastic Tensor Parallelism",
        "link": "http://arxiv.org/abs/2511.11617v1",
        "tags": [
            "offloading",
            "serving",
            "MoE"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes AnchorTP, a state-preserving elastic tensor parallelism framework for resilient LLM inference. Preserves model parameters and KV caches via a daemon and minimizes recovery time with a bandwidth-aware planner and pipelined transfers. Reduces Time to First Success by 11x and Time to Peak by 59%.",
        "abstract": "Large Language Model (LLM) inference services demand exceptionally high availability and low latency, yet multi-GPU Tensor Parallelism (TP) makes them vulnerable to single-GPU failures. We present AnchorTP, a state-preserving elastic TP framework for fast recovery. It (i) enables Elastic Tensor Parallelism (ETP) with unequal-width partitioning over any number of GPUs and compatibility with Mixture-of-Experts (MoE), and (ii) preserves model parameters and KV caches in GPU memory via a daemon decoupled from the inference process. To minimize downtime, we propose a bandwidth-aware planner based on a Continuous Minimal Migration (CMM) algorithm that minimizes reload bytes under a byte-cost dominance assumption, and an execution scheduler that pipelines P2P transfers with reloads. These components jointly restore service quickly with minimal data movement and without changing service interfaces. In typical failure scenarios, AnchorTP reduces Time to First Success (TFS) by up to 11x and Time to Peak (TTP) by up to 59% versus restart-and-reload.",
        "authors": [
            "Wendong Xu",
            "Chujie Chen",
            "He Xiao",
            "Kuan Li",
            "Jing Xiong",
            "Chen Zhang",
            "Wenyong Zhou",
            "Chaofan Tao",
            "Yang Bai",
            "Bei Yu",
            "Ngai Wong"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-11-05"
    },
    "http://arxiv.org/abs/2511.03359v2": {
        "title": "Universal Quantum Simulation of 50 Qubits on Europe`s First Exascale Supercomputer Harnessing Its Heterogeneous CPU-GPU Architecture",
        "link": "http://arxiv.org/abs/2511.03359v2",
        "abstract": "We have developed a new version of the high-performance Jülich universal quantum computer simulator (JUQCS-50) that leverages key features of the GH200 superchips as used in the JUPITER supercomputer, enabling simulations of a 50-qubit universal quantum computer for the first time. JUQCS-50 achieves this through three key innovations: (1) extending usable memory beyond GPU limits via high-bandwidth CPU-GPU interconnects and LPDDR5 memory; (2) adaptive data encoding to reduce memory footprint with acceptable trade-offs in precision and compute effort; and (3) an on-the-fly network traffic optimizer. These advances result in an 11.4-fold speedup over the previous 48-qubit record on the K computer.",
        "authors": [
            "Hans De Raedt",
            "Jiri Kraus",
            "Andreas Herten",
            "Vrinda Mehta",
            "Mathis Bode",
            "Markus Hrywniak",
            "Kristel Michielsen",
            "Thomas Lippert"
        ],
        "categories": [
            "quant-ph",
            "cs.DC",
            "physics.comp-ph"
        ],
        "id": "http://arxiv.org/abs/2511.03359v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-05"
    },
    "http://arxiv.org/abs/2511.03293v2": {
        "id": "http://arxiv.org/abs/2511.03293v2",
        "title": "UMDAM: A Unified Data Layout and DRAM Address Mapping for Heterogenous NPU-PIM",
        "link": "http://arxiv.org/abs/2511.03293v2",
        "tags": [
            "edge",
            "serving",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-11-05",
        "tldr": "Proposes UMDAM, a unified data layout and DRAM address mapping scheme for NPU-PIM co-execution on edge devices. Uses column-major tile-based layout and configurable mapping to eliminate layout mismatches and reduce bandwidth loss. Achieves up to 3.0x lower TTFT and 2.18x lower TTLT in OPT model inference.",
        "abstract": "Large Language Models (LLMs) are increasingly deployed on edge devices with Neural Processing Units (NPUs), yet the decode phase remains memory-intensive, limiting performance. Processing-in-Memory (PIM) offers a promising solution, but co-executing NPU-PIM systems face challenges such as data layout mismatches, bandwidth loss, and redundant storage. To address these issues, we propose UMDAM, a unified memory-affinity data layout and DRAM address mapping scheme tailored for NPU-PIM co-execution. UMDAM employs a column-major, tile-based layout and a configurable DRAM mapping strategy to ensure compatibility with NPU computation while maximizing PIM efficiency -- without introducing extra memory overhead or bandwidth loss. Comprehensive evaluations on OPT models demonstrate that UMDAM reduces time-to-first-token (TTFT) by up to 3.0x and time-to-last-token (TTLT) by 2.18x, significantly improving end-to-end LLM inference efficiency on edge devices.",
        "authors": [
            "Hai Huang",
            "Xuhong Qiang",
            "Weisheng Zhao",
            "Chenchen Liu"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-11-05"
    },
    "http://arxiv.org/abs/2511.03286v2": {
        "title": "Characterising Global Platforms: Centralised, Decentralised, Federated, and Grassroots",
        "link": "http://arxiv.org/abs/2511.03286v2",
        "abstract": "Global digital platforms are software systems designed to serve entire populations, with some already serving billions of people. We propose atomic transactions-based multiagent transition systems and protocols as a formal framework to study them; introduce essential agents -- minimal sets of agents the removal of which makes communication impossible; and show that the cardinality of essential agents partitions all global platforms into four classes:   1. Centralised -- one (the server)   2. Decentralised -- finite $>1$ (bootstrap nodes)   3. Federated -- infinite but not universal (all servers)   4. Grassroots -- universal (all agents)   Our illustrative formal example is a global social network, for which we provide centralised, decentralised, federated, and grassroots specifications via multiagent atomic transactions, and prove they all satisfy the same basic correctness properties. We discuss informally additional global platforms -- currencies, ``sharing economy'' apps, AI, and more. While this may be the first characterisation of centralised, decentralised, and federated global platforms, grassroots platforms have been formally defined previously, but using different notions. Here, we prove that their original definition implies that all agents are essential, placing grassroots platforms in a distinct class within the broader formal context that includes all global platforms. This work provides the first mathematical framework for classifying any global platform -- existing or imagined -- by providing a multiagent atomic-transactions specification of it and determining the cardinality of the minimal set of essential agents in the ensuing multiagent protocol. It thus provides a unifying mathematical approach for the study of global digital platforms, perhaps the most important class of computer systems today.",
        "authors": [
            "Ehud Shapiro"
        ],
        "categories": [
            "cs.DC",
            "cs.MA",
            "cs.SE",
            "cs.SI"
        ],
        "id": "http://arxiv.org/abs/2511.03286v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-05"
    },
    "http://arxiv.org/abs/2511.03092v5": {
        "id": "http://arxiv.org/abs/2511.03092v5",
        "title": "SnapStream: Efficient Long Sequence Decoding on Dataflow Accelerators",
        "link": "http://arxiv.org/abs/2511.03092v5",
        "tags": [
            "serving",
            "offloading",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-11-05",
        "tldr": "Proposes SnapStream, a KV cache compression technique for efficient long-sequence LLM inference on accelerators. Reduces on-chip memory usage by 4× while maintaining accuracy, achieving 1832 tokens/sec in a 16-way tensor-parallel deployment of DeepSeek-671B at 128k context length.",
        "abstract": "The proliferation of 100B+ parameter Large Language Models (LLMs) with 100k+ context length support have resulted in increasing demands for on-chip memory to support large KV caches. Techniques such as StreamingLLM and SnapKV demonstrate how to control KV cache size while maintaining model accuracy. Yet, these techniques are not commonly used within industrial deployments using frameworks like vLLM or SGLang. The reason is twofold: on one hand, the static graphs and continuous batching methodology employed by these frameworks make it difficult to admit modifications to the standard multi-head attention algorithm, while on the other hand, the accuracy implications of such techniques on modern instruction-following and reasoning models are not well understood, obfuscating the need for implementing these techniques. In this paper, we explore these accuracy implications on Llama-3.1-8B-Instruct and DeepSeek-R1, and develop SnapStream, a KV cache compression method that can be deployed at scale. We demonstrate the efficacy of SnapStream in a 16-way tensor-parallel deployment of DeepSeek-671B on SambaNova SN40L accelerators running at 128k context length and up to 1832 tokens per second in a real production setting. SnapStream enables $4\\times$ improved on-chip memory usage and introduces minimal accuracy degradation on LongBench-v2, AIME24 and LiveCodeBench. To the best of our knowledge, this is the first implementation of sparse KV attention techniques deployed in a production inference system with static graphs and continuous batching.",
        "authors": [
            "Jonathan Li",
            "Nasim Farahini",
            "Evgenii Iuliugin",
            "Magnus Vesterlund",
            "Christian Häggström",
            "Guangtao Wang",
            "Shubhangi Upasani",
            "Ayush Sachdeva",
            "Rui Li",
            "Faline Fu",
            "Chen Wu",
            "Ayesha Siddiqua",
            "John Long",
            "Tuowen Zhao",
            "Matheen Musaddiq",
            "Håkan Zeffer",
            "Yun Du",
            "Mingran Wang",
            "Qinghua Li",
            "Bo Li",
            "Urmish Thakker",
            "Raghu Prabhakar"
        ],
        "categories": [
            "cs.AI",
            "cs.AR",
            "cs.DC"
        ],
        "submit_date": "2025-11-05"
    },
    "http://arxiv.org/abs/2511.03029v1": {
        "title": "Harvesting energy consumption on European HPC systems: Sharing Experience from the CEEC project",
        "link": "http://arxiv.org/abs/2511.03029v1",
        "abstract": "Energy efficiency has emerged as a central challenge for modern high-performance computing (HPC) systems, where escalating computational demands and architectural complexity have led to significant energy footprints. This paper presents the collective experience of the EuroHPC JU Center of Excellence in Exascale CFD (CEEC) in measuring, analyzing, and optimizing energy consumption across major European HPC systems. We briefly review key methodologies and tools for energy measurement as well as define metrics for reporting results. Through case studies using representative CFD applications (waLBerla, FLEXI/GALÆXI, Neko, and NekRS), we evaluate energy-to-solution and time-to-solution metrics on diverse architectures, including CPU- and GPU-based partitions of LUMI, MareNostrum5, MeluXina, and JUWELS Booster. Our results highlight the advantages of accelerators and mixed-precision techniques for reducing energy consumption while maintaining computational accuracy. Finally, we advocate the need to facilitate energy measurements on HPC systems in order to raise awareness, teach the community, and take actions toward more sustainable exascale computing.",
        "authors": [
            "Kajol Kulkarni",
            "Samuel Kemmler",
            "Anna Schwarz",
            "Gulcin Gedik",
            "Yanxiang Chen",
            "Dimitrios Papageorgiou",
            "Ioannis Kavroulakis",
            "Roman Iakymchuk"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.03029v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-04"
    },
    "http://arxiv.org/abs/2511.02743v1": {
        "title": "Making Democracy Work: Fixing and Simplifying Egalitarian Paxos (Extended Version)",
        "link": "http://arxiv.org/abs/2511.02743v1",
        "abstract": "Classical state-machine replication protocols, such as Paxos, rely on a distinguished leader process to order commands. Unfortunately, this approach makes the leader a single point of failure and increases the latency for clients that are not co-located with it. As a response to these drawbacks, Egalitarian Paxos introduced an alternative, leaderless approach, that allows replicas to order commands collaboratively. Not relying on a single leader allows the protocol to maintain non-zero throughput with up to $f$ crashes of any processes out of a total of $n = 2f+1$. The protocol furthermore allows any process to execute a command $c$ fast, in $2$ message delays, provided no more than $e = \\lceil\\frac{f+1}{2}\\rceil$ other processes fail, and all concurrently submitted commands commute with $c$; the latter condition is often satisfied in practical systems.   Egalitarian Paxos has served as a foundation for many other replication protocols. But unfortunately, the protocol is very complex, ambiguously specified and suffers from nontrivial bugs. In this paper, we present EPaxos* -- a simpler and correct variant of Egalitarian Paxos. Our key technical contribution is a simpler failure-recovery algorithm, which we have rigorously proved correct. Our protocol also generalizes Egalitarian Paxos to cover the whole spectrum of failure thresholds $f$ and $e$ such that $n \\ge \\max\\{2e+f-1, 2f+1\\}$ -- the number of processes that we show to be optimal.",
        "authors": [
            "Fedor Ryabinin",
            "Alexey Gotsman",
            "Pierre Sutra"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.02743v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-04"
    },
    "http://arxiv.org/abs/2511.02655v1": {
        "id": "http://arxiv.org/abs/2511.02655v1",
        "title": "Implementing Multi-GPU Scientific Computing Miniapps Across Performance Portable Frameworks",
        "link": "http://arxiv.org/abs/2511.02655v1",
        "tags": [
            "training",
            "hardware",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Evaluates performance portability frameworks (Kokkos, OpenMP, RAJA, OCCA) for multi-GPU scientific computing miniapps. Measures time-to-solution on NVIDIA A100 GPUs for N-body and structured grid simulations. Shows OCCA achieves faster execution for small problems but lacks optimized reductions.",
        "abstract": "Scientific computing in the exascale era demands increased computational power to solve complex problems across various domains. With the rise of heterogeneous computing architectures the need for vendor-agnostic, performance portability frameworks has been highlighted. Libraries like Kokkos have become essential for enabling high-performance computing applications to execute efficiently across different hardware platforms with minimal code changes. In this direction, this paper presents preliminary time-to-solution results for two representative scientific computing applications: an N-body simulation and a structured grid simulation. Both applications used a distributed memory approach and hardware acceleration through four performance portability frameworks: Kokkos, OpenMP, RAJA, and OCCA. Experiments conducted on a single node of the Polaris supercomputer using four NVIDIA A100 GPUs revealed significant performance variability among frameworks. OCCA demonstrated faster execution times for small-scale validation problems, likely due to JIT compilation, however its lack of optimized reduction algorithms may limit scalability for larger simulations while using its out of the box API. OpenMP performed poorly in the structured grid simulation most likely due to inefficiencies in inter-node data synchronization and communication. These findings highlight the need for further optimization to maximize each framework's capabilities. Future work will focus on enhancing reduction algorithms, data communication, memory management, as wells as performing scalability studies, and a comprehensive statistical analysis to evaluate and compare framework performance.",
        "authors": [
            "Johansell Villalobos",
            "Josef Ruzicka",
            "Silvio Rizzi"
        ],
        "categories": [
            "cs.DC",
            "cs.MS"
        ],
        "submit_date": "2025-11-04"
    },
    "http://arxiv.org/abs/2511.02647v1": {
        "id": "http://arxiv.org/abs/2511.02647v1",
        "title": "Federated Attention: A Distributed Paradigm for Collaborative LLM Inference over Edge Networks",
        "link": "http://arxiv.org/abs/2511.02647v1",
        "tags": [
            "serving",
            "edge",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-11-04",
        "tldr": "Proposes Federated Attention (FedAttn), a distributed LLM inference framework for edge networks that performs local self-attention and periodic KV matrix aggregation to preserve privacy and reduce communication. Achieves a 3.2x reduction in communication cost while maintaining response quality.",
        "abstract": "Large language models (LLMs) are proliferating rapidly at the edge, delivering intelligent capabilities across diverse application scenarios. However, their practical deployment in collaborative scenarios confronts fundamental challenges: privacy vulnerabilities, communication overhead, and computational bottlenecks. To address these, we propose Federated Attention (FedAttn), which integrates the federated paradigm into the self-attention mechanism, creating a new distributed LLM inference framework that simultaneously achieves privacy protection, communication efficiency, and computational efficiency. FedAttn enables participants to perform local self-attention over their own token representations while periodically exchanging and aggregating Key-Value (KV) matrices across multiple Transformer blocks, collaboratively generating LLM responses without exposing private prompts. Further, we identify a structural duality between contextual representation refinement in FedAttn and parameter optimization in FL across private data, local computation, and global aggregation. This key insight provides a principled foundation for systematically porting federated optimization techniques to collaborative LLM inference. Building on this framework, we theoretically analyze how local self-attention computation within participants and heterogeneous token relevance among participants shape error propagation dynamics across Transformer blocks. Moreover, we characterize the fundamental trade-off between response quality and communication/computation efficiency, which is governed by the synchronization interval and the number of participants. Experimental results validate our theoretical analysis, and reveal significant optimization opportunities through sparse attention and adaptive KV aggregation, highlighting FedAttn's potential to deliver scalability and efficiency in real-world edge deployments.",
        "authors": [
            "Xiumei Deng",
            "Zehui Xiong",
            "Binbin Chen",
            "Dong In Kim",
            "Merouane Debbah",
            "H. Vincent Poor"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.LG"
        ],
        "submit_date": "2025-11-04"
    },
    "http://arxiv.org/abs/2511.02501v1": {
        "title": "Lightweight Latency Prediction Scheme for Edge Applications: A Rational Modelling Approach",
        "link": "http://arxiv.org/abs/2511.02501v1",
        "abstract": "Accurately predicting end-to-end network latency is essential for enabling reliable task offloading in real-time edge computing applications. This paper introduces a lightweight latency prediction scheme based on rational modelling that uses features such as frame size, arrival rate, and link utilization, eliminating the need for intrusive active probing. The model achieves state-of-the-art prediction accuracy through extensive experiments and 5-fold cross-validation (MAE = 0.0115, R$^2$ = 0.9847) with competitive inference time, offering a substantial trade-off between precision and efficiency compared to traditional regressors and neural networks.",
        "authors": [
            "Mohan Liyanage",
            "Eldiyar Zhantileuov",
            "Ali Kadhum Idrees",
            "Rolf Schuster"
        ],
        "categories": [
            "cs.NI",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.02501v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-04"
    },
    "http://arxiv.org/abs/2511.02293v1": {
        "title": "3D Point Cloud Object Detection on Edge Devices for Split Computing",
        "link": "http://arxiv.org/abs/2511.02293v1",
        "abstract": "The field of autonomous driving technology is rapidly advancing, with deep learning being a key component. Particularly in the field of sensing, 3D point cloud data collected by LiDAR is utilized to run deep neural network models for 3D object detection. However, these state-of-the-art models are complex, leading to longer processing times and increased power consumption on edge devices. The objective of this study is to address these issues by leveraging Split Computing, a distributed machine learning inference method. Split Computing aims to lessen the computational burden on edge devices, thereby reducing processing time and power consumption. Furthermore, it minimizes the risk of data breaches by only transmitting intermediate data from the deep neural network model. Experimental results show that splitting after voxelization reduces the inference time by 70.8% and the edge device execution time by 90.0%. When splitting within the network, the inference time is reduced by up to 57.1%, and the edge device execution time is reduced by up to 69.5%.",
        "authors": [
            "Taisuke Noguchi",
            "Takuya Azumi"
        ],
        "categories": [
            "cs.DC",
            "cs.CV"
        ],
        "id": "http://arxiv.org/abs/2511.02293v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-04"
    },
    "http://arxiv.org/abs/2511.02257v1": {
        "title": "Fast Algorithms for Scheduling Many-body Correlation Functions on Accelerators",
        "link": "http://arxiv.org/abs/2511.02257v1",
        "abstract": "Computation of correlation functions is a key operation in Lattice quantum chromodynamics (LQCD) simulations to extract nuclear physics observables. These functions involve many binary batch tensor contractions, each tensor possibly occupying hundreds of MBs of memory. Performing these contractions on GPU accelerators poses the challenge of scheduling them as to optimize tensor reuse and reduce data traffic. In this work we propose two fast novel scheduling algorithms that reorder contractions to increase temporal locality via input/intermediate tensor reuse. Our schedulers take advantage of application-specific features, such as contractions being binary and locality within contraction trees, to optimize the objective of minimizing peak memory. We integrate them into the LQCD analysis software suite Redstar and improve time-to-solution. Our schedulers attain upto 2.1x improvement in peak memory, which is reflected by a reduction of upto 4.2x in evictions, upto 1.8x in data traffic, resulting in upto 1.9x faster correlation function computation time.",
        "authors": [
            "Oguz Selvitopi",
            "Emin Ozturk",
            "Jie Chen",
            "Ponnuswamy Sadayappan",
            "Robert G. Edwards",
            "Aydın Buluç"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.02257v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-04"
    },
    "http://arxiv.org/abs/2511.02248v1": {
        "id": "http://arxiv.org/abs/2511.02248v1",
        "title": "From Models to Operators: Rethinking Autoscaling Granularity for Large Generative Models",
        "link": "http://arxiv.org/abs/2511.02248v1",
        "tags": [
            "serving",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-11-04",
        "tldr": "Proposes operator-level autoscaling for large generative models to replace inefficient model-level scaling. The framework profiles individual model operators and independently scales resources for each. Achieves up to 40% fewer GPUs while preserving SLOs, or 1.6x higher throughput under fixed resources.",
        "abstract": "Serving large generative models such as LLMs and multi- modal transformers requires balancing user-facing SLOs (e.g., time-to-first-token, time-between-tokens) with provider goals of efficiency and cost reduction. Existing solutions rely on static provisioning or model-level autoscaling, both of which treat the model as a monolith. This coarse-grained resource management leads to degraded performance or significant resource underutilization due to poor adaptability to dynamic inference traffic that is common online.   The root cause of this inefficiency lies in the internal structure of generative models: they are executed as graphs of interconnected operators. Through detailed characterization and systematic analysis, we find that operators are heterogeneous in their compute and memory footprints and exhibit diverse sensitivity to workload and resource factors such as batch size, sequence length, and traffic rate. This heterogeneity suggests that the operator, rather than the entire model, is the right granularity for scaling decisions.   We propose an operator-level autoscaling framework, which allocates resources at finer (operator)-granularity, optimizing the scaling, batching, and placement based on individual operator profiles. Evaluated on production-scale traces, our approach preserves SLOs with up to 40% fewer GPUs and 35% less energy, or under fixed resources achieves 1.6x higher throughput with 5% less energy. These results show that the operator, rather than the model, is fundamentally a more effective unit for scaling large generative workloads.",
        "authors": [
            "Xingqi Cui",
            "Chieh-Jan Mike Liang",
            "Jiarong Xing",
            "Haoran Qiu"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-11-04"
    },
    "http://arxiv.org/abs/2511.11614v1": {
        "id": "http://arxiv.org/abs/2511.11614v1",
        "title": "Beyond the GPU: The Strategic Role of FPGAs in the Next Wave of AI",
        "link": "http://arxiv.org/abs/2511.11614v1",
        "tags": [
            "hardware",
            "inference",
            "edge"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Explores FPGA-based acceleration for AI to address GPU limitations in latency and energy efficiency. Advocates for direct algorithm-to-logic mapping with parallel pipelines, deterministic timing, and local inference near sensors. Achieves up to 40% lower power consumption compared to GPUs.",
        "abstract": "AI acceleration has been dominated by GPUs, but the growing need for lower latency, energy efficiency, and fine-grained hardware control exposes the limits of fixed architectures. In this context, Field-Programmable Gate Arrays (FPGAs) emerge as a reconfigurable platform that allows mapping AI algorithms directly into device logic. Their ability to implement parallel pipelines for convolutions, attention mechanisms, and post-processing with deterministic timing and reduced power consumption makes them a strategic option for workloads that demand predictable performance and deep customization.   Unlike CPUs and GPUs, whose architecture is immutable, an FPGA can be reconfigured in the field to adapt its physical structure to a specific model, integrate as a SoC with embedded processors, and run inference near the sensor without sending raw data to the cloud. This reduces latency and required bandwidth, improves privacy, and frees GPUs from specialized tasks in data centers. Partial reconfiguration and compilation flows from AI frameworks are shortening the path from prototype to deployment, enabling hardware--algorithm co-design.",
        "authors": [
            "Arturo Urías Jiménez"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-11-04"
    },
    "http://arxiv.org/abs/2511.11612v1": {
        "title": "Evaluating Large Language Models for Workload Mapping and Scheduling in Heterogeneous HPC Systems",
        "link": "http://arxiv.org/abs/2511.11612v1",
        "abstract": "Large language models (LLMs) are increasingly explored for their reasoning capabilities, yet their ability to perform structured, constraint-based optimization from natural language remains insufficiently understood. This study evaluates twenty-one publicly available LLMs on a representative heterogeneous high-performance computing (HPC) workload mapping and scheduling problem. Each model received the same textual description of system nodes, task requirements, and scheduling constraints, and was required to assign tasks to nodes, compute the total makespan, and explain its reasoning. A manually derived analytical optimum of nine hours and twenty seconds served as the ground truth reference. Three models exactly reproduced the analytical optimum while satisfying all constraints, twelve achieved near-optimal results within two minutes of the reference, and six produced suboptimal schedules with arithmetic or dependency errors. All models generated feasible task-to-node mappings, though only about half maintained strict constraint adherence. Nineteen models produced partially executable verification code, and eighteen provided coherent step-by-step reasoning, demonstrating strong interpretability even when logical errors occurred. Overall, the results define the current capability boundary of LLM reasoning in combinatorial optimization: leading models can reconstruct optimal schedules directly from natural language, but most still struggle with precise timing, data transfer arithmetic, and dependency enforcement. These findings highlight the potential of LLMs as explainable co-pilots for optimization and decision-support tasks rather than autonomous solvers.",
        "authors": [
            "Aasish Kumar Sharma",
            "Julian Kunkel"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "id": "http://arxiv.org/abs/2511.11612v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-04"
    },
    "http://arxiv.org/abs/2511.02168v1": {
        "id": "http://arxiv.org/abs/2511.02168v1",
        "title": "Eliminating Multi-GPU Performance Taxes: A Systems Approach to Efficient Distributed LLMs",
        "link": "http://arxiv.org/abs/2511.02168v1",
        "tags": [
            "training",
            "kernel",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-11-04",
        "tldr": "Addresses performance inefficiencies in distributed LLM execution across multiple GPUs due to the Bulk Synchronous Parallel (BSP) model. Proposes fine-grained programming patterns using in-kernel communication primitives to replace global barriers with dataflow synchronization. Achieves a 10-20% speedup in end-to-end latency.",
        "abstract": "As large language models (LLMs) continue to scale, their workloads increasingly rely on distributed execution across multiple GPUs. However, the conventional bulk synchronous parallel~(BSP) model used in such settings introduces significant performance inefficiencies. To characterize these bottlenecks, we introduce the ''Three Taxes'' (Bulk Synchronous, Inter-Kernel Data Locality, and Kernel Launch Overhead) as an analytical framework. We propose moving beyond the rigid BSP model to address key inefficiencies in distributed GPU execution. By exploiting libraries like Iris for Triton, we gain access to in-kernel communication primitives that enable the design of novel fine-grained programming patterns, offering greater flexibility and performance than traditional BSP-based approaches. These patterns systematically eliminate the three taxes by creating direct, tile-level producer-consumer pipelines and replacing global barriers with fine-grained dataflow synchronization. Applying this methodology to critical kernels, from the foundational All-Gather + general matrix multiplication operation to the complex Flash Decode algorithm, we observe a 10-20% speedup in end-to-end latency over BSP-based approaches, establishing a more programmable and efficient paradigm for distributed LLM workloads.",
        "authors": [
            "Octavian Alexandru Trifan",
            "Karthik Sangaiah",
            "Muhammad Awad",
            "Muhammad Osama",
            "Sumanth Gudaparthi",
            "Alexandru Nicolau",
            "Alexander Veidenbaum",
            "Ganesh Dasika"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-11-04"
    },
    "http://arxiv.org/abs/2511.02132v1": {
        "id": "http://arxiv.org/abs/2511.02132v1",
        "title": "Optimizing Attention on GPUs by Exploiting GPU Architectural NUMA Effects",
        "link": "http://arxiv.org/abs/2511.02132v1",
        "tags": [
            "kernel",
            "training",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-11-03",
        "tldr": "Addresses performance degradation of attention kernels on disaggregated GPUs due to NUMA effects. Proposes Swizzled Head-first Mapping, a scheduling strategy aligning attention heads with NUMA domains for cache reuse. Achieves 50% higher performance on AMD MI300X with L2 cache hit rates of 80-97%.",
        "abstract": "The rise of disaggregated AI GPUs has exposed a critical bottleneck in large-scale attention workloads: non-uniform memory access (NUMA). As multi-chiplet designs become the norm for scaling compute capabilities, memory latency and bandwidth vary sharply across compute regions, undermining the performance of traditional GPU kernel scheduling strategies that assume uniform memory access. We identify how these NUMA effects distort locality in multi-head attention (MHA) and present Swizzled Head-first Mapping, a spatially-aware scheduling strategy that aligns attention heads with GPU NUMA domains to exploit intra-chiplet cache reuse. On AMD's MI300X architecture, our method achieves up to 50% higher performance over state-of-the-art attention algorithms using conventional scheduling techniques and sustains consistently high L2 cache hit rates of 80-97%. These results demonstrate that NUMA-aware scheduling is now fundamental to achieving full efficiency on next-generation disaggregated GPUs, offering a path forward for scalable AI training and inference.",
        "authors": [
            "Mansi Choudhary",
            "Karthik Sangaiah",
            "Sonali Singh",
            "Muhammad Osama",
            "Lisa Wu Wills",
            "Ganesh Dasika"
        ],
        "categories": [
            "cs.AR",
            "cs.DC",
            "cs.LG",
            "cs.PF"
        ],
        "submit_date": "2025-11-03"
    },
    "http://arxiv.org/abs/2511.02042v1": {
        "title": "Quantum-Enhanced Generative Models for Rare Event Prediction",
        "link": "http://arxiv.org/abs/2511.02042v1",
        "abstract": "Rare events such as financial crashes, climate extremes, and biological anomalies are notoriously difficult to model due to their scarcity and heavy-tailed distributions. Classical deep generative models often struggle to capture these rare occurrences, either collapsing low-probability modes or producing poorly calibrated uncertainty estimates. In this work, we propose the Quantum-Enhanced Generative Model (QEGM), a hybrid classical-quantum framework that integrates deep latent-variable models with variational quantum circuits. The framework introduces two key innovations: (1) a hybrid loss function that jointly optimizes reconstruction fidelity and tail-aware likelihood, and (2) quantum randomness-driven noise injection to enhance sample diversity and mitigate mode collapse. Training proceeds via a hybrid loop where classical parameters are updated through backpropagation while quantum parameters are optimized using parameter-shift gradients. We evaluate QEGM on synthetic Gaussian mixtures and real-world datasets spanning finance, climate, and protein structure. Results demonstrate that QEGM reduces tail KL divergence by up to 50 percent compared to state-of-the-art baselines (GAN, VAE, Diffusion), while improving rare-event recall and coverage calibration. These findings highlight the potential of QEGM as a principled approach for rare-event prediction, offering robustness beyond what is achievable with purely classical methods.",
        "authors": [
            "M. Z. Haider",
            "M. U. Ghouri",
            "Tayyaba Noreen",
            "M. Salman"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CR",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.02042v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-03"
    },
    "http://arxiv.org/abs/2511.02034v1": {
        "title": "GPoS: Geospatially-aware Proof of Stake",
        "link": "http://arxiv.org/abs/2511.02034v1",
        "abstract": "Geospatial decentralization is essential for blockchains, ensuring regulatory resilience, robustness, and fairness. We empirically analyze five major Proof of Stake (PoS) blockchains: Aptos, Avalanche, Ethereum, Solana, and Sui, revealing that a few geographic regions dominate consensus voting power, resulting in limited geospatial decentralization. To address this, we propose Geospatially aware Proof of Stake (GPoS), which integrates geospatial diversity with stake-based voting power. Experimental evaluation demonstrates an average 45% improvement in geospatial decentralization, as measured by the Gini coefficient of Eigenvector centrality, while incurring minimal performance overhead in BFT protocols, including HotStuff and CometBFT. These results demonstrate that GPoS can improve geospatial decentralization {while, in our experiments, incurring minimal overhead} to consensus performance.",
        "authors": [
            "Shashank Motepalli",
            "Naman Garg",
            "Gengrui Zhang",
            "Hans-Arno Jacobsen"
        ],
        "categories": [
            "cs.DC",
            "cs.ET",
            "cs.NI"
        ],
        "id": "http://arxiv.org/abs/2511.02034v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-03"
    },
    "http://arxiv.org/abs/2511.02029v2": {
        "title": "RobustFSM: Submodular Maximization in Federated Setting with Malicious Clients",
        "link": "http://arxiv.org/abs/2511.02029v2",
        "abstract": "Submodular maximization is an optimization problem benefiting many machine learning applications, where we seek a small subset best representing an extremely large dataset. We focus on the federated setting where the data are locally owned by decentralized clients who have their own definitions for the quality of representability. This setting requires repetitive aggregation of local information computed by the clients. While the main motivation is to respect the privacy and autonomy of the clients, the federated setting is vulnerable to client misbehaviors: malicious clients might share fake information. An analogy is backdoor attack in conventional federated learning, but our challenge differs freshly due to the unique characteristics of submodular maximization. We propose RobustFSM, a federated submodular maximization solution that is robust to various practical client attacks. Its performance is substantiated with an empirical evaluation study using real-world datasets. Numerical results show that the solution quality of RobustFSM substantially exceeds that of the conventional federated algorithm when attacks are severe. The degree of this improvement depends on the dataset and attack scenarios, which can be as high as 200%",
        "authors": [
            "Duc A. Tran",
            "Dung Truong",
            "Duy Le"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.02029v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-03"
    },
    "http://arxiv.org/abs/2511.01843v1": {
        "title": "LARK -- Linearizability Algorithms for Replicated Keys in Aerospike",
        "link": "http://arxiv.org/abs/2511.01843v1",
        "abstract": "We present LARK (Linearizability Algorithms for Replicated Keys), a synchronous replication protocol that achieves linearizability while minimizing latency and infrastructure cost, at significantly higher availability than traditional quorum-log consensus. LARK introduces Partition Availability Conditions (PAC) that reason over the entire database cluster rather than fixed replica sets, improving partition availability under independent failures by roughly 3x when tolerating one failure and 10x when tolerating two. Unlike Raft, Paxos, and Viewstamped Replication, LARK eliminates ordered logs, enabling immediate partition readiness after leader changes -- with at most a per-key duplicate-resolution round trip when the new leader lacks the latest copy. Under equal storage budgets -- where both systems maintain only f+1 data copies to tolerate f failures -- LARK continues committing through data-node failures while log-based protocols must pause commits for replica rebuilding. These properties also enable zero-downtime rolling restarts even when maintaining only two copies. We provide formal safety arguments and a TLA+ specification, and we demonstrate through analysis and experiments that LARK achieves significant availability gains.",
        "authors": [
            "Andrew Goodng",
            "Kevin Porter",
            "Thomas Lopatic",
            "Ashish Shinde",
            "Sunil Sayyaparaju",
            "Srinivasan Seshadri",
            "V. Srinivasan"
        ],
        "categories": [
            "cs.DC",
            "cs.DB"
        ],
        "id": "http://arxiv.org/abs/2511.01843v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-03"
    },
    "http://arxiv.org/abs/2511.01737v1": {
        "title": "Edge AI in Highly Volatile Environments: Is Fairness Worth the Accuracy Trade-off?",
        "link": "http://arxiv.org/abs/2511.01737v1",
        "abstract": "Federated learning (FL) has emerged as a transformative paradigm for edge intelligence, enabling collaborative model training while preserving data privacy across distributed personal devices. However, the inherent volatility of edge environments, characterized by dynamic resource availability and heterogeneous client capabilities, poses significant challenges for achieving high accuracy and fairness in client participation. This paper investigates the fundamental trade-off between model accuracy and fairness in highly volatile edge environments. This paper provides an extensive empirical evaluation of fairness-based client selection algorithms such as RBFF and RBCSF against random and greedy client selection regarding fairness, model performance, and time, in three benchmarking datasets (CIFAR10, FashionMNIST, and EMNIST). This work aims to shed light on the fairness-performance and fairness-speed trade-offs in a volatile edge environment and explore potential future research opportunities to address existing pitfalls in \\textit{fair client selection} strategies in FL. Our results indicate that more equitable client selection algorithms, while providing a marginally better opportunity among clients, can result in slower global training in volatile environments\\footnote{The code for our experiments can be found at https://github.com/obaidullahzaland/FairFL_FLTA.",
        "authors": [
            "Obaidullah Zaland",
            "Feras M. Awaysheh",
            "Sawsan Al Zubi",
            "Abdul Rahman Safi",
            "Monowar Bhuyan"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.01737v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-03"
    },
    "http://arxiv.org/abs/2511.01583v1": {
        "title": "Federated Cyber Defense: Privacy-Preserving Ransomware Detection Across Distributed Systems",
        "link": "http://arxiv.org/abs/2511.01583v1",
        "abstract": "Detecting malware, especially ransomware, is essential to securing today's interconnected ecosystems, including cloud storage, enterprise file-sharing, and database services. Training high-performing artificial intelligence (AI) detectors requires diverse datasets, which are often distributed across multiple organizations, making centralization necessary. However, centralized learning is often impractical due to security, privacy regulations, data ownership issues, and legal barriers to cross-organizational sharing. Compounding this challenge, ransomware evolves rapidly, demanding models that are both robust and adaptable.   In this paper, we evaluate Federated Learning (FL) using the Sherpa.ai FL platform, which enables multiple organizations to collaboratively train a ransomware detection model while keeping raw data local and secure. This paradigm is particularly relevant for cybersecurity companies (including both software and hardware vendors) that deploy ransomware detection or firewall systems across millions of endpoints. In such environments, data cannot be transferred outside the customer's device due to strict security, privacy, or regulatory constraints. Although FL applies broadly to malware threats, we validate the approach using the Ransomware Storage Access Patterns (RanSAP) dataset.   Our experiments demonstrate that FL improves ransomware detection accuracy by a relative 9% over server-local models and achieves performance comparable to centralized training. These results indicate that FL offers a scalable, high-performing, and privacy-preserving framework for proactive ransomware detection across organizational and regulatory boundaries.",
        "authors": [
            "Daniel M. Jimenez-Gutierrez",
            "Enrique Zuazua",
            "Joaquin Del Rio",
            "Oleksii Sliusarenko",
            "Xabi Uribe-Etxebarria"
        ],
        "categories": [
            "cs.CR",
            "cs.AI",
            "cs.DC",
            "cs.LG"
        ],
        "id": "http://arxiv.org/abs/2511.01583v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-03"
    },
    "http://arxiv.org/abs/2511.01573v1": {
        "title": "Adaptive Multidimensional Quadrature on Multi-GPU Systems",
        "link": "http://arxiv.org/abs/2511.01573v1",
        "abstract": "We introduce a distributed adaptive quadrature method that formulates multidimensional integration as a hierarchical domain decomposition problem on multi-GPU architectures. The integration domain is recursively partitioned into subdomains whose refinement is guided by local error estimators. Each subdomain evolves independently on a GPU, which exposes a significant load imbalance as the adaptive process progresses. To address this challenge, we introduce a decentralised load redistribution schemes based on a cyclic round-robin policy. This strategy dynamically rebalance subdomains across devices through non-blocking, CUDA-aware MPI communication that overlaps with computation. The proposed strategy has two main advantages compared to a state-of-the-art GPU-tailored package: higher efficiency in high dimensions; and improved robustness w.r.t the integrand regularity and the target accuracy.",
        "authors": [
            "Melanie Tonarelli",
            "Simone Riva",
            "Pietro Benedusi",
            "Fabrizio Ferrandi",
            "Rolf Krause"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.01573v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-03"
    },
    "http://arxiv.org/abs/2511.01553v1": {
        "title": "Real-time Continual Learning on Intel Loihi 2",
        "link": "http://arxiv.org/abs/2511.01553v1",
        "abstract": "AI systems on edge devices face a critical challenge in open-world environments: adapting when data distributions shift and novel classes emerge. While offline training dominates current paradigms, online continual learning (OCL)--where models learn incrementally from non-stationary streams without catastrophic forgetting--remains challenging in power-constrained settings. We present a neuromorphic solution called CLP-SNN: a spiking neural network architecture for Continually Learning Prototypes and its implementation on Intel's Loihi 2 chip. Our approach introduces three innovations: (1) event-driven and spatiotemporally sparse local learning, (2) a self-normalizing three-factor learning rule maintaining weight normalization, and (3) integrated neurogenesis and metaplasticity for capacity expansion and forgetting mitigation. On OpenLORIS few-shot learning experiments, CLP-SNN achieves accuracy competitive with replay methods while being rehearsal-free. CLP-SNN delivers transformative efficiency gains: 70\\times faster (0.33ms vs 23.2ms), and 5,600\\times more energy efficient (0.05mJ vs 281mJ) than the best alternative OCL on edge GPU. This demonstrates that co-designed brain-inspired algorithms and neuromorphic hardware can break traditional accuracy-efficiency trade-offs for future edge AI systems.",
        "authors": [
            "Elvin Hajizada",
            "Danielle Rager",
            "Timothy Shea",
            "Leobardo Campos-Macias",
            "Andreas Wild",
            "Eyke Hüllermeier",
            "Yulia Sandamirskaya",
            "Mike Davies"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC",
            "cs.NE"
        ],
        "id": "http://arxiv.org/abs/2511.01553v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-03"
    },
    "http://arxiv.org/abs/2511.01420v1": {
        "title": "Gradient Clock Synchronization with Practically Constant Local Skew",
        "link": "http://arxiv.org/abs/2511.01420v1",
        "abstract": "Gradient Clock Synchronization (GCS) is the task of minimizing the local skew, i.e., the clock offset between neighboring clocks, in a larger network. While asymptotically optimal bounds are known, from a practical perspective they have crucial shortcomings:   - Local skew bounds are determined by upper bounds on offset estimation that need to be guaranteed throughout the entire lifetime of the system.   - Worst-case frequency deviations of local oscillators from their nominal rate are assumed, yet frequencies tend to be much more stable in the (relevant) short term.   State-of-the-art deployed synchronization methods adapt to the true offset measurement and frequency errors, but achieve no non-trivial guarantees on the local skew.   In this work, we provide a refined model and novel analysis of existing techniques for solving GCS in this model. By requiring only stability of measurement and frequency errors, we can circumvent existing lower bounds, leading to dramatic improvements under very general conditions. For example, if links exhibit a uniform worst-case estimation error of $Δ$ and a change in estimation errors of $δ\\ll Δ$ on relevant time scales, we bound the local skew by $O(Δ+δ\\log D)$ for networks of diameter $D$, effectively ``breaking'' the established $Ω(Δ\\log D)$ lower bound, which holds when $δ=Δ$. Similarly, we show how to limit the influence of local oscillators on $δ$ to scale with the change of frequency of an individual oscillator on relevant time scales, rather than a worst-case bound over all oscillators and the lifetime of the system.   Moreover, we show how to ensure self-stabilization in this challenging setting. Last, but not least, we extend all of our results to the scenario of external synchronization, at the cost of a limited increase in stabilization time.",
        "authors": [
            "Christoph Lenzen"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.01420v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-03"
    },
    "http://arxiv.org/abs/2511.11608v1": {
        "id": "http://arxiv.org/abs/2511.11608v1",
        "title": "Why Should the Server Do It All?: A Scalable, Versatile, and Model-Agnostic Framework for Server-Light DNN Inference over Massively Distributed Clients via Training-Free Intermediate Feature Compression",
        "link": "http://arxiv.org/abs/2511.11608v1",
        "tags": [
            "serving",
            "offloading",
            "edge"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes SLICER, a training-free framework to compress intermediate features for distributed DNN inference, reducing communication and server load. Uses asymmetric top-K filtering, magnitude-splitting, and adaptive quantization to achieve up to 10x lower uplink volume and 4.4x server GPU time reduction.",
        "abstract": "Modern DNNs often rely on edge-cloud model partitioning (MP), but widely used schemes fix shallow, static split points that underutilize edge compute and concentrate latency and energy on the server. The problem is exacerbated in autoregressive (AR) LLM inference, where per-token forward passes repeatedly generate bulky intermediate features (IFs). We introduce SLICER, a retraining-free, architecture-agnostic framework that compresses IFs to reduce both communication and server load in split computing. SLICER combines (i) asymmetric top-K filtering (ATKF) to sparsify low-magnitude activations, (ii) magnitude-splitting (MS) to group the remaining non-zeros into equal-cardinality blocks, and (iii) adaptive bit quantization (ABQ) that selects per-block bitwidths under a distortion budget. Across standard vision and LLM workloads (e.g., ImageNet/COCO; HellaSwag, PIQA, ARC-E/C, GSM8K, HumanEval), SLICER reduces uplink volume by up to 10x and server GPU time by up to 4.4x, while keeping task quality within ~0-3 pp of baseline. In multi-device settings and AR LLMs, SLICER scales by shifting meaningful compute to the edge and lowering bits-per-token and server time per token, stabilizing per-step traffic. The codec attaches to off-the-shelf models without retraining or architectural changes, offering a plug-and-play path to scalable, low-latency distributed inference. Code is provided in the supplementary material.",
        "authors": [
            "Mingyu Sung",
            "Suhwan Im",
            "Daeho Bang",
            "Il-Min Kim",
            "Sangseok Yun",
            "Jae-Mo Kang"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-11-03"
    },
    "http://arxiv.org/abs/2511.01333v1": {
        "title": "Transformer-Based Sparse CSI Estimation for Non-Stationary Channels",
        "link": "http://arxiv.org/abs/2511.01333v1",
        "abstract": "Accurate and efficient estimation of Channel State Information (CSI) is critical for next-generation wireless systems operating under non-stationary conditions, where user mobility, Doppler spread, and multipath dynamics rapidly alter channel statistics. Conventional pilot aided estimators incur substantial overhead, while deep learning approaches degrade under dynamic pilot patterns and time varying fading. This paper presents a pilot-aided Flash-Attention Transformer framework that unifies model-driven pilot acquisition with data driven CSI reconstruction through patch-wise self-attention and a physics aware composite loss function enforcing phase alignment, correlation consistency, and time frequency smoothness. Under a standardized 3GPP NR configuration, the proposed framework outperforms LMMSE and LSTM baselines by approximately 13 dB in phase invariant normalized mean-square error (NMSE) with markedly lower bit-error rate (BER), while reducing pilot overhead by 16 times. These results demonstrate that attention based architectures enable reliable CSI recovery and enhanced spectral efficiency without compromising link quality, addressing a fundamental bottleneck in adaptive, low-overhead channel estimation for non-stationary 5G and beyond-5G networks.",
        "authors": [
            "Muhammad Ahmed Mohsin",
            "Muhammad Umer",
            "Ahsan Bilal",
            "Hassan Rizwan",
            "Sagnik Bhattacharya",
            "Muhammad Ali Jamshed",
            "John M. Cioffi"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.01333v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-03"
    },
    "http://arxiv.org/abs/2511.01255v1": {
        "title": "Design of quasi phase matching crystal based on differential gray wolf algorithm",
        "link": "http://arxiv.org/abs/2511.01255v1",
        "abstract": "This paper focuses on the key problem in the development of nonlinear optical technology, the performance optimization of aperiodically polarized crystals. The performance of the crystal depends on the precise control of the micro distribution of crystal domains, but its optimization belongs to the high-dimensional discrete combination \"NP hard\" problem. The traditional algorithm has the bottleneck of slow convergence and easy to fall into local optimization, while the heuristic methods such as genetic algorithm are limited by the CPU serial calculation and inefficient. In order to solve the above challenges, this paper proposes the fusion scheme of hwsda hybrid optimization algorithm and GPU parallel acceleration technology: the differential evolution algorithm (DE) is used to realize the global search, and the gray wolf optimization algorithm (GWO) is used to strengthen the local search and convergence speed, and the two coordinate to balance the global and local optimization requirements; At the same time, it relies on GPU multi-core architecture to realize thread level parallel computing and improve optimization efficiency. This scheme effectively breaks through the optimization problem of high-dimensional discrete space, improves the accuracy of crystal domain control, improves the efficiency of quasi phase matching design by hundreds to thousands of times compared with traditional CPU serial computing, provides a new paradigm for the design of complex nonlinear optical devices, and helps promote the performance breakthrough and industrial application of related devices in the fields of quantum optics and laser processing.",
        "authors": [
            "He Chen",
            "ZiHua Zheng",
            "JingHua Sun"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.01255v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-03"
    },
    "http://arxiv.org/abs/2511.01235v1": {
        "title": "Scalable Maxflow Processing for Dynamic Graphs",
        "link": "http://arxiv.org/abs/2511.01235v1",
        "abstract": "The Maximum Flow (Max-Flow) problem is a cornerstone in graph theory and combinatorial optimization, aiming to determine the largest possible flow from a designated source node to a sink node within a capacitated flow network. It has extensive applications across diverse domains such as computer networking, transportation systems, and image segmentation. The objective is to maximize the total throughput while respecting edge capacity constraints and maintaining flow conservation at all intermediate vertices.   Among the various algorithms proposed for solving the Max-Flow problem, the Push--Relabel algorithm is particularly notable for its efficiency and suitability for parallelization, owing to its localized vertex-based operations. This property has motivated extensive research into GPU-accelerated Max-Flow computation, leveraging the high degree of parallelism inherent to modern GPU architectures.   In this paper, we present a novel GPU-parallel Max-Flow algorithm capable of incrementally recomputing the maximum flow of a dynamic graph following a batch of edge updates. In addition, we introduce a high-performance static GPU algorithm designed for efficiently computing the initial Max-Flow on static graphs. We further describe a series of CUDA-specific implementation optimizations that enhance performance, scalability, and memory efficiency on GPU platforms.",
        "authors": [
            "Shruthi Kannappan",
            "Ashwina Kumar",
            "Rupesh Nasre"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.01235v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-03"
    },
    "http://arxiv.org/abs/2511.01129v1": {
        "id": "http://arxiv.org/abs/2511.01129v1",
        "title": "Boosting performance of computer vision applications through embedded GPUs on the edge",
        "link": "http://arxiv.org/abs/2511.01129v1",
        "tags": [
            "edge"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes using embedded GPUs on edge devices to boost computer vision application performance. Designs a system offloading intensive tasks to embedded GPUs for efficient execution. Achieves performance gains over CPU-only, improving user experience through faster processing.",
        "abstract": "Computer vision applications, especially those using augmented reality technology, are becoming quite popular in mobile devices. However, this type of application is known as presenting significant demands regarding resources. In order to enable its utilization in devices with more modest resources, edge computing can be used to offload certain high intensive tasks. Still, edge computing is usually composed of devices with limited capacity, which may impact in users quality of experience when using computer vision applications. This work proposes the use of embedded devices with graphics processing units (GPUs) to overcome such limitation. Experiments performed shown that GPUs can attain a performance gain when compared to using only CPUs, which guarantee a better experience to users using such kind of application.",
        "authors": [
            "Fabio Diniz Rossi"
        ],
        "categories": [
            "cs.CV",
            "cs.DC"
        ],
        "submit_date": "2025-11-03"
    },
    "http://arxiv.org/abs/2511.01127v1": {
        "id": "http://arxiv.org/abs/2511.01127v1",
        "title": "Neuro-Inspired Task Offloading in Edge-IoT Networks Using Spiking Neural Networks",
        "link": "http://arxiv.org/abs/2511.01127v1",
        "tags": [
            "offloading",
            "edge",
            "sparse"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Explores energy-efficient task offloading in edge-IoT networks. Uses spiking neural networks (SNNs) for real-time, adaptive task orchestration. Achieves 32% lower energy consumption and 25% higher success rate versus baselines under high load.",
        "abstract": "Traditional task offloading strategies in edge computing often rely on static heuristics or data-intensive machine learning models, which are not always suitable for highly dynamic and resource-constrained environments. In this paper, we propose a novel task-offloading framework based on Spiking Neural Networks inspired by the efficiency and adaptability of biological neural systems. Our approach integrates an SNN-based decision module into edge nodes to perform real-time, energy-efficient task orchestration. We evaluate the model under various IoT workload scenarios using a hybrid simulation environment composed of YAFS and Brian2. The results demonstrate that our SNN-based framework significantly reduces task processing latency and energy consumption while improving task success rates. Compared to traditional heuristic and ML-based strategies, our model achieves up to 26% lower latency, 32% less energy consumption, and 25\\% higher success rate under high-load conditions.",
        "authors": [
            "Fabio Diniz Rossi"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-11-03"
    },
    "http://arxiv.org/abs/2511.01001v1": {
        "title": "Towards Portability at Scale: A Cross-Architecture Performance Evaluation of a GPU-enabled Shallow Water Solver",
        "link": "http://arxiv.org/abs/2511.01001v1",
        "abstract": "Current climate change has posed a grand challenge in the field of numerical modeling due to its complex, multiscale dynamics. In hydrological modeling, the increasing demand for high-resolution, real-time simulations has led to the adoption of GPU-accelerated platforms and performance portable programming frameworks such as Kokkos. In this work, we present a comprehensive performance study of the SERGHEI-SWE solver, a shallow water equations code, across four state-of-the-art heterogeneous HPC systems: Frontier (AMD MI250X), JUWELS Booster (NVIDIA A100), JEDI (NVIDIA H100), and Aurora (Intel Max 1550). We assess strong scaling up to 1024 GPUs and weak scaling upwards of 2048 GPUs, demonstrating consistent scalability with a speedup of 32 and an efficiency upwards of 90\\% for most almost all the test range. Roofline analysis reveals that memory bandwidth is the dominant performance bottleneck, with key solver kernels residing in the memory-bound region. To evaluate performance portability, we apply both harmonic and arithmetic mean-based metrics while varying problem size. Results indicate that while SERGHEI-SWE achieves portability across devices with tuned problem sizes (<70\\%), there is room for kernel optimization within the solver with more granular control of the architecture specifically by using Kokkos teams and architecture specific tunable parameters. These findings position SERGHEI-SWE as a robust, scalable, and portable simulation tool for large-scale geophysical applications under evolving HPC architectures with potential to enhance its performance.",
        "authors": [
            "Johansell Villalobos",
            "Daniel Caviedes-Voullième",
            "Silvio Rizzi",
            "Esteban Meneses"
        ],
        "categories": [
            "cs.DC",
            "cs.PF"
        ],
        "id": "http://arxiv.org/abs/2511.01001v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-02"
    },
    "http://arxiv.org/abs/2511.00870v1": {
        "title": "A Distributed Plug-and-Play MCMC Algorithm for High-Dimensional Inverse Problems",
        "link": "http://arxiv.org/abs/2511.00870v1",
        "abstract": "Markov Chain Monte Carlo (MCMC) algorithms are standard approaches to solve imaging inverse problems and quantify estimation uncertainties, a key requirement in absence of ground-truth data. To improve estimation quality, Plug-and-Play MCMC algorithms, such as PnP-ULA, have been recently developed to accommodate priors encoded by a denoising neural network. Designing scalable samplers for high-dimensional imaging inverse problems remains a challenge: drawing and storing high-dimensional samples can be prohibitive, especially for high-resolution images. To address this issue, this work proposes a distributed sampler based on approximate data augmentation and PnP-ULA to solve very large problems. The proposed sampler uses lightweight denoising convolutional neural network, to efficiently exploit multiple GPUs on a Single Program Multiple Data architecture. Reconstruction performance and scalability are evaluated on several imaging problems. Communication and computation overheads due to the denoiser are carefully discussed. The proposed distributed approach noticeably combines three very precious qualities: it is scalable, enables uncertainty quantification, for a reconstruction performance comparable to other PnP methods.",
        "authors": [
            "Maxime Bouton",
            "Pierre-Antoine Thouvenin",
            "Audrey Repetti",
            "Pierre Chainais"
        ],
        "categories": [
            "stat.ME",
            "cs.DC",
            "eess.SP"
        ],
        "id": "http://arxiv.org/abs/2511.00870v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-02"
    },
    "http://arxiv.org/abs/2511.00823v1": {
        "title": "TINC: Trusted Intelligent NetChain",
        "link": "http://arxiv.org/abs/2511.00823v1",
        "abstract": "Blockchain technology facilitates the development of decentralized systems that ensure trust and transparency without the need for expensive centralized intermediaries. However, existing blockchain architectures particularly consortium blockchains face critical challenges related to scalability and efficiency. State sharding has emerged as a promising approach to enhance blockchain scalability and performance. However, current shard-based solutions often struggle to guarantee fair participation and a balanced workload distribution among consortium members. To address these limitations, we propose Trusted Intelligent NetChain (TINC), a multi-plane sharding architecture specifically designed for consortium blockchains. TINC incorporates intelligent mechanisms for adaptive node assignment and dynamic workload balancing, enabling the system to respond effectively to changing network conditions while maintaining equitable shard utilization. By decoupling the control and data planes, TINC allows control nodes to focus on consensus operations, while data nodes handle large-scale storage, thus improving overall resource efficiency. Extensive experimental evaluation and formal analysis demonstrate that TINC significantly outperforms existing shard-based blockchain frameworks. It achieves higher throughput, lower latency, balanced node and transaction distributions, and reduced transaction failure rates. Furthermore, TINC maintains essential blockchain security guarantees, exhibiting resilience against Byzantine faults and dynamic network environments. The integration of Dynamic Decentralized Identifiers (DDIDs) further strengthens trust and security management within the consortium network.",
        "authors": [
            "Qi Xia",
            "Hu Xia",
            "Isaac Amankona Obiri",
            "Adjei-Arthur Bonsu",
            "Grace Mupoyi Ntuala",
            "Ansu Badjie",
            "Tienin Bole Wilfried",
            "Jiaqin Liu",
            "Lan Ma",
            "Jianbin Gao",
            "Feng Yao"
        ],
        "categories": [
            "cs.NI",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.00823v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-02"
    },
    "http://arxiv.org/abs/2511.00807v2": {
        "id": "http://arxiv.org/abs/2511.00807v2",
        "title": "FREESH: Fair, Resource- and Energy-Efficient Scheduling for LLM Serving on Heterogeneous GPUs",
        "link": "http://arxiv.org/abs/2511.00807v2",
        "tags": [
            "serving",
            "autoscaling",
            "edge"
        ],
        "relevant": true,
        "indexed_date": "2025-11-02",
        "tldr": "Proposes FREESH, a system for fair and energy-efficient LLM serving on heterogeneous GPUs by joint routing and scheduling. It matches GPU power profiles to query workloads, using optimized parallelism, routing, dynamic GPU scaling, and LLF scheduling. Achieves 28.6% lower energy and 45.45% lower emissions while improving SLO attainment.",
        "abstract": "The ever-increasing computation and energy demand for LLM and AI agents call for holistic and efficient optimization of LLM serving systems. In practice, heterogeneous GPU clusters can be deployed in a geographically distributed manner, while LLM load also observes diversity in terms of both query traffic and serving patterns. LLM queries running on advanced GPUs during a high-emission hour at one location can lead to significantly higher carbon footprints versus same queries running on mid-level GPUs at a low-emission time and location. By observing LLM serving requirements and leveraging spatiotemporal computation flexibility, we consider the joint routing and scheduling problem, and propose FREESH to cooperatively run a group of data centers while minimizing user-specified carbon or energy objectives. FREESH identifies the optimal configurations of balanced load serving by matching distinct GPU instance's power-throughput characteristics with predictable LLM query length and workloads. To ensure both latency and fairness requirements, FREESH identifies optimized parallelism and query routing schedules together with dynamic GPU frequency scaling for power saving, and Least-Laxity-First (LLF) serving strategy for query scheduling. During the 1-hour serving on production workloads, FREESH reduces energy by 28.6% and emissions by 45.45% together with improvements in SLO attainment and fairness.",
        "authors": [
            "Xuan He",
            "Zequan Fang",
            "Jinzhao Lian",
            "Danny H. K. Tsang",
            "Baosen Zhang",
            "Yize Chen"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-11-02"
    },
    "http://arxiv.org/abs/2511.00796v1": {
        "id": "http://arxiv.org/abs/2511.00796v1",
        "title": "AReaL-Hex: Accommodating Asynchronous RL Training over Heterogeneous GPUs",
        "link": "http://arxiv.org/abs/2511.00796v1",
        "tags": [
            "RL",
            "training"
        ],
        "relevant": true,
        "indexed_date": "2025-11-02",
        "tldr": "Introduces AReaL-Hex, a heterogeneity-aware system for asynchronous RL training on heterogeneous GPUs. It uses a two-phase scheduler (MILP-based planning and graph partitioning) to assign stages optimally. On reasoning tasks, it achieves 1.50x higher throughput or 1.46x cost reduction versus homogeneous deployments.",
        "abstract": "Maximizing training throughput and cost-efficiency of RL for LLMs is essential to democratize this advanced technique. One promising but challenging approach is to deploy such a computational workflow over heterogeneous GPUs. Unlike conventional large-scale LLM pretraining, RL training generally decomposes into three coupled stages, i.e., rollout generation, reward computation, and policy/value updates, which exhibit markedly different compute intensities, memory footprints, and communication patterns. Recent research shows that fully asynchronous RL training can disaggregate these stages across disjoint hardware pools without sacrificing training stability, creating a great opportunity for real-world heterogeneous deployment. To this end, we present AReaL-Hex, a heterogeneity-aware asynchronous RL training system that effectively schedules how to execute rollout generation and policy model training over heterogeneous GPUs while enforcing data staleness bounds. Concretely, we use a two-phase scheduler: (i) a constrained search with MILP to select per-stage parallelization strategies and workload assignments given a resource budget, and (ii) a graph-partitioning step that allocates heterogeneous GPUs and interconnects to maximize end-to-end throughput. Built atop a fully asynchronous RL architecture, AReaL-Hex maps HBM-I/O-bound generation and compute-bound optimization to more cost-efficient resources and balances their producer-consumer interactions to avoid both idleness and stale rollout trajectories. On the mathematical reasoning task with various model scales (1.5B, 7B, and 14B), compared to homogeneous deployments of state-of-the-art asynchronous RL systems: (i) When maintaining the same total budgets, AReaL-Hex delivers up to 1.50x higher training throughput; (ii) When achieving the same training throughput, AReaL-Hex results in up to 1.46x reduction in training cost.",
        "authors": [
            "Ran Yan",
            "Youhe Jiang",
            "Tianyuan Wu",
            "Jiaxuan Gao",
            "Zhiyu Mei",
            "Wei Fu",
            "Haohui Mai",
            "Wei Wang",
            "Yi Wu",
            "Binhang Yuan"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-11-02"
    },
    "http://arxiv.org/abs/2512.04089v1": {
        "title": "Serverless Everywhere: A Comparative Analysis of WebAssembly Workflows Across Browser, Edge, and Cloud",
        "link": "http://arxiv.org/abs/2512.04089v1",
        "abstract": "WebAssembly (Wasm) is a binary instruction format that enables portable, sandboxed, and near-native execution across heterogeneous platforms, making it well-suited for serverless workflow execution on browsers, edge nodes, and cloud servers. However, its performance and stability depend heavily on factors such as startup overhead, runtime execution model (e.g., Ahead-of-Time (AOT) and Just-in-Time (JIT) compilation), and resource variability across deployment contexts. This paper evaluates a Wasm-based serverless workflow executed consistently from the browser to edge and cloud instances. The setup uses wasm32-wasi modules: in the browser, execution occurs within a web worker, while on Edge and Cloud, an HTTP shim streams frames to the Wasm runtime. We measure cold- and warm-start latency, per-step delays, workflow makespan, throughput, and CPU/memory utilization to capture the end-to-end behavior across environments. Results show that AOT compilation and instance warming substantially reduce startup latency. For workflows with small payloads, the browser achieves competitive performance owing to fully in-memory data exchanges. In contrast, as payloads grow, the workflow transitions into a compute- and memory-intensive phase where AOT execution on edge and cloud nodes distinctly surpasses browser performance.",
        "authors": [
            "Mario Colosi",
            "Reza Farahani",
            "Lauri Loven",
            "Radu Prodan",
            "Massimo Villari"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.04089v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-01"
    },
    "http://arxiv.org/abs/2511.11605v1": {
        "title": "PACE Solver Description: twin_width_fmi",
        "link": "http://arxiv.org/abs/2511.11605v1",
        "abstract": "In this paper we present \\texttt{twin\\_width\\_fmi}'s solver for the heuristic track of PACE's 2025 competition on Minimum Dominating Set.   As a baseline, we implement \\texttt{greedy-ln}, a standard greedy dominating-set heuristic that repeatedly selects the vertex that newly dominates the largest number of currently undominated vertices. We then use this greedy solution as the starting point for a simulated annealing local search: we attempt vertex removals and exchanges and accept worsening moves with decaying probability, in order to escape local minima while preserving domination.   Our best-performing component, which we ultimately submitted, is \\texttt{hedom5}. The design of \\texttt{hedom5} is inspired by recent iterative-greedy style domination heuristics~\\cite{IterativeGreedy22} that alternate between constructive steps, pruning, and focused repair rather than relying on a single pass. In \\texttt{hedom5}, the input graph is first stored in a compact CSR structure and simplified using fast reductions such as forcing neighbors of leaves and handling isolates. We then run a lazy gain-based greedy stage using a priority queue: each candidate vertex is scored by how many currently undominated vertices its closed neighborhood would newly dominate, and scores are only recomputed when necessary. After this constructive phase, we perform an aggressive backward pruning pass that iterates over the chosen dominators in reverse insertion order and deletes any vertex whose closed neighborhood is still fully dominated by the remaining set. Finally, we run a budgeted 1-swap local improvement step that attempts to replace a dominator by an alternative vertex that covers all of its uniquely covered vertices, thereby reducing the size of the dominating set. A brief safety patch at the end guarantees full domination.",
        "authors": [
            "David Balaban",
            "Adrian Miclăuş"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.11605v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-01"
    },
    "http://arxiv.org/abs/2512.04088v1": {
        "id": "http://arxiv.org/abs/2512.04088v1",
        "title": "Toward Sustainability-Aware LLM Inference on Edge Clusters",
        "link": "http://arxiv.org/abs/2512.04088v1",
        "tags": [
            "edge",
            "serving",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Explores carbon- and latency-aware routing for LLM inference on edge clusters. Proposes strategies that balance latency and carbon footprint by routing prompts to specific hardware based on benchmarking. Achieves optimal batch size of 4 for throughput and energy efficiency without memory saturation.",
        "abstract": "Large language models (LLMs) require substantial computational resources, leading to significant carbon emissions and operational costs. Although training is energy-intensive, the long-term environmental burden arises from inference, amplified by the massive global query volume. Cloud-based inference offers scalability but suffers from latency and bandwidth constraints due to centralized processing and continuous data transfer. Edge clusters instead can mitigate these limitations by enabling localized execution, yet they face trade-offs between performance, energy efficiency, and device constraints. This short paper presents a sustainability-aware LLM inference for edge clusters comprising NVIDIA Jetson Orin NX (8GB) and Nvidia Ada 2000 (16GB) devices. It aims to balance inference latency and carbon footprint through carbon- and latency-aware routing strategies, guided by empirical benchmarking of energy consumption and execution time across diverse prompts and batch (i.e., group of prompts) configurations. We compared baseline greedy strategies to carbon-aware and latency-aware strategies in prompt routing to specific hardware based on benchmarking information. Experimental evaluation shows that a batch size of four prompts achieves a trade-off between throughput, energy efficiency, while larger batches risk GPU memory saturation.",
        "authors": [
            "Kolichala Rajashekar",
            "Nafiseh Sharghivand",
            "Radu Prodan",
            "Reza Farahani"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-11-01"
    },
    "http://arxiv.org/abs/2511.00603v1": {
        "id": "http://arxiv.org/abs/2511.00603v1",
        "title": "EPARA: Parallelizing Categorized AI Inference in Edge Clouds",
        "link": "http://arxiv.org/abs/2511.00603v1",
        "tags": [
            "serving",
            "edge",
            "multi-modal"
        ],
        "relevant": true,
        "indexed_date": "2025-11-01",
        "tldr": "Proposes EPARA, an end-to-end parallel inference framework for edge clouds that categorizes AI tasks by latency sensitivity and GPU needs. It uses a task allocator, distributed handler, and state-aware scheduler to improve resource allocation. Achieves up to 2.1x higher goodput compared to prior frameworks.",
        "abstract": "With the increasing adoption of AI applications such as large language models and computer vision AI, the computational demands on AI inference systems are continuously rising, making the enhancement of task processing capacity using existing hardware a primary objective in edge clouds. We propose EPARA, an end-to-end AI parallel inference framework in edge, aimed at enhancing the edge AI serving capability. Our key idea is to categorize tasks based on their sensitivity to latency/frequency and requirement for GPU resources, thereby achieving both request-level and service-level task-resource allocation. EPARA consists of three core components: 1) a task-categorized parallelism allocator that decides the parallel mode of each task, 2) a distributed request handler that performs the calculation for the specific request, and 3) a state-aware scheduler that periodically updates service placement in edge clouds. We implement a EPARA prototype and conduct a case study on the EPARA operation for LLMs and segmentation tasks. Evaluation through testbed experiments involving edge servers, embedded devices, and microcomputers shows that EPARA achieves up to 2.1$\\times$ higher goodput in production workloads compared to prior frameworks, while adapting to various edge AI inference tasks.",
        "authors": [
            "Yubo Wang",
            "Yubo Cui",
            "Tuo Shi",
            "Danyang Li",
            "Wenxin Li",
            "Lide Suo",
            "Tao Wang",
            "Xin Xie"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.NI"
        ],
        "submit_date": "2025-11-01"
    },
    "http://arxiv.org/abs/2511.00592v1": {
        "title": "Agentic Auto-Scheduling: An Experimental Study of LLM-Guided Loop Optimization",
        "link": "http://arxiv.org/abs/2511.00592v1",
        "abstract": "Automatic code optimization remains a difficult challenge, particularly for complex loop nests on modern hardware. This paper investigates a novel approach to code optimization where Large Language Models (LLMs) guide the process through a closed-loop interaction with a compiler. We present ComPilot, an experimental framework that leverages off-the-shelf LLMs, without any task-specific fine-tuning, as interactive optimization agents. ComPilot establishes a feedback loop where an LLM proposes transformations for a given loop nest to a compiler. The compiler attempts the transformations, reporting back legality status and measured speedup or slowdown. The LLM utilizes this concrete feedback to iteratively refine its optimization strategy. Our extensive evaluation across the PolyBench benchmark suite demonstrates the effectiveness of this zero-shot approach. ComPilot achieves geometric mean speedups of 2.66x (single run) and 3.54x (best-of-5 runs) over the original code. Furthermore, ComPilot demonstrates competitive performance against the state-of-the-art Pluto polyhedral optimizer, outperforming it in many cases. This experimental study demonstrates that general-purpose LLMs can effectively guide the code optimization process when grounded by compiler feedback, opening promising research directions for agentic AI in code optimization.",
        "authors": [
            "Massinissa Merouani",
            "Islem Kara Bernou",
            "Riyadh Baghdadi"
        ],
        "categories": [
            "cs.PL",
            "cs.DC",
            "cs.LG",
            "cs.PF"
        ],
        "id": "http://arxiv.org/abs/2511.00592v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-01"
    },
    "http://arxiv.org/abs/2511.13728v1": {
        "id": "http://arxiv.org/abs/2511.13728v1",
        "title": "Gaia: Hybrid Hardware Acceleration for Serverless AI in the 3D Compute Continuum",
        "link": "http://arxiv.org/abs/2511.13728v1",
        "tags": [
            "offloading",
            "edge",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes Gaia, a GPU-as-a-service architecture for serverless AI. Dynamically switches between CPU/GPU backends based on workload and SLOs via execution modes and runtime adjustments. Achieves up to 95% reduction in end-to-end latency.",
        "abstract": "Serverless computing offers elastic scaling and pay-per-use execution, making it well-suited for AI workloads. As these workloads run in heterogeneous environments such as the Edge-Cloud-Space 3D Continuum, they often require intensive parallel computation, which GPUs can perform far more efficiently than CPUs. However, current platforms struggle to manage hardware acceleration effectively, as static user-device assignments fail to ensure SLO compliance under varying loads or placements, and one-time dynamic selections often lead to suboptimal or cost-inefficient configurations. To address these issues, we present Gaia, a GPU-as-a-service model and architecture that makes hardware acceleration a platform concern. Gaia combines (i) a lightweight Execution Mode Identifier that inspects function code at deploy time to emit one of four execution modes, and a Dynamic Function Runtime that continuously reevaluates user-defined SLOs to promote or demote between CPU- and GPU backends. Our evaluation shows that it seamlessly selects the best hardware acceleration for the workload, reducing end-to-end latency by up to 95%. These results indicate that Gaia enables SLO-aware, cost-efficient acceleration for serverless AI across heterogeneous environments.",
        "authors": [
            "Maximilian Reisecker",
            "Cynthia Marcelino",
            "Thomas Pusztai",
            "Stefan Nastic"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-11-01"
    },
    "http://arxiv.org/abs/2511.00336v1": {
        "title": "Split Learning-Enabled Framework for Secure and Light-weight Internet of Medical Things Systems",
        "link": "http://arxiv.org/abs/2511.00336v1",
        "abstract": "The rapid growth of Internet of Medical Things (IoMT) devices has resulted in significant security risks, particularly the risk of malware attacks on resource-constrained devices. Conventional deep learning methods are impractical due to resource limitations, while Federated Learning (FL) suffers from high communication overhead and vulnerability to non-IID (heterogeneous) data. In this paper, we propose a split learning (SL) based framework for IoT malware detection through image-based classification. By dividing the neural network training between the clients and an edge server, the framework reduces computational burden on resource-constrained clients while ensuring data privacy. We formulate a joint optimization problem that balances computation cost and communication efficiency by using a game-theoretic approach for attaining better training performance. Experimental evaluations show that the proposed framework outperforms popular FL methods in terms of accuracy (+6.35%), F1-score (+5.03%), high convergence speed (+14.96%), and less resource consumption (33.83%). These results establish the potential of SL as a scalable and secure paradigm for next-generation IoT security.",
        "authors": [
            "Siva Sai",
            "Manish Prasad",
            "Animesh Bhargava",
            "Vinay Chamola",
            "Rajkumar Buyya"
        ],
        "categories": [
            "cs.CR",
            "cs.DC",
            "cs.LG"
        ],
        "id": "http://arxiv.org/abs/2511.00336v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-01"
    },
    "http://arxiv.org/abs/2511.00294v1": {
        "title": "Tetris: An SLA-aware Application Placement Strategy in the Edge-Cloud Continuum",
        "link": "http://arxiv.org/abs/2511.00294v1",
        "abstract": "An Edge-Cloud Continuum integrates edge and cloud resources to provide a flexible and scalable infrastructure. This paradigm can minimize latency by processing data closer to the source at the edge while leveraging the vast computational power of the cloud for more intensive tasks. In this context, module application placement requires strategic allocation plans that align user demands with infrastructure constraints, aiming for efficient resource use. Therefore, we propose Tetris, an application placement strategy that utilizes a heuristic algorithm to distribute computational services across edge and cloud resources efficiently. Tetris prioritizes services based on SLA urgencies and resource efficiency to avoid system overloading. Our results demonstrate that Tetris reduces SLA violations by approximately 76% compared to the baseline method, which serves as a reference point for benchmarking performance in this scenario. Therefore, Tetris offers an effective placement approach for managing latency-sensitive applications in Edge-Cloud Continuum environments, enhancing Quality of Service (QoS) for users.",
        "authors": [
            "Lucas Almeida",
            "Maycon Peixoto"
        ],
        "categories": [
            "cs.DC",
            "cs.NI"
        ],
        "id": "http://arxiv.org/abs/2511.00294v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-31"
    },
    "http://arxiv.org/abs/2511.00279v2": {
        "id": "http://arxiv.org/abs/2511.00279v2",
        "title": "LongCat-Flash-Omni Technical Report",
        "link": "http://arxiv.org/abs/2511.00279v2",
        "tags": [
            "training",
            "MoE",
            "multi-modal"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Addresses efficient large-scale omni-modal model training. Proposes modality-decoupled parallelism and efficient MoE architecture with progressive curriculum. Achieves over 90% of text-only training throughput for a 560B parameter model.",
        "abstract": "We introduce LongCat-Flash-Omni, a state-of-the-art open-source omni-modal model with 560 billion parameters, excelling at real-time audio-visual interaction. By adopting a curriculum-inspired progressive training strategy that transitions from simpler to increasingly complex modality sequence modeling tasks, LongCat-Flash-Omni attains comprehensive multimodal capabilities while maintaining strong unimodal capability. Building upon LongCat-Flash, which adopts a high-performance Shortcut-connected Mixture-of-Experts (MoE) architecture with zero-computation experts, LongCat-Flash-Omni integrates efficient multimodal perception and speech reconstruction modules. Despite its immense size of 560B parameters (with 27B activated), LongCat-Flash-Omni achieves low-latency real-time audio-visual interaction. For training infrastructure, we developed a modality-decoupled parallelism scheme specifically designed to manage the data and model heterogeneity inherent in large-scale multimodal training. This innovative approach demonstrates exceptional efficiency by sustaining over 90% of the throughput achieved by text-only training. Extensive evaluations show that LongCat-Flash-Omni achieves state-of-the-art performance on omni-modal benchmarks among open-source models. Furthermore, it delivers highly competitive results across a wide range of modality-specific tasks, including text, image, and video understanding, as well as audio understanding and generation. We provide a comprehensive overview of the model architecture design, training procedures, and data strategies, and open-source the model to foster future research and development in the community.",
        "authors": [
            "Meituan LongCat Team",
            "Bairui Wang",
            "Bayan",
            "Bin Xiao",
            "Bo Zhang",
            "Bolin Rong",
            "Borun Chen",
            "Chang Wan",
            "Chao Zhang",
            "Chen Huang",
            "Chen Chen",
            "Chen Chen",
            "Chengxu Yang",
            "Chengzuo Yang",
            "Cong Han",
            "Dandan Peng",
            "Delian Ruan",
            "Detai Xin",
            "Disong Wang",
            "Dongchao Yang",
            "Fanfan Liu",
            "Fengjiao Chen",
            "Fengyu Yang",
            "Gan Dong",
            "Gang Huang",
            "Gang Xu",
            "Guanglu Wan",
            "Guoqiang Tan",
            "Guoqiao Yu",
            "Haibo Qiu",
            "Hao Lu",
            "Hongbo Liu",
            "Hongyu Xiang",
            "Jiaheng Wu",
            "Jian Yang",
            "Jiaxing Liu",
            "Jing Huang",
            "Jingang Wang",
            "Jinrui Ding",
            "Juchao Jiang",
            "Jun Kuang",
            "Jun Wang",
            "Junhui Mei",
            "Ke Ding",
            "Kefeng Zhang",
            "Lei Chen",
            "Liang Shi",
            "Limeng Qiao",
            "Liming Zheng",
            "Lin Ma",
            "Liuyang Guo",
            "Liya Ma",
            "Luying Sun",
            "Man Gao",
            "Mengshen Zhu",
            "Miao Cao",
            "Minliang Lin",
            "Nuo Xu",
            "Peng Shi",
            "Qi Zhang",
            "Qian Fang",
            "Qian Wang",
            "Qian Yang",
            "Quanxiu Wang",
            "Rongxiang Weng",
            "Rongxin Guo",
            "Ruoxuan Liang",
            "Senbin Yang",
            "Shanbo Xu",
            "Shanglin Lei",
            "Shengze Ye",
            "Shimin Chen",
            "Shuaiqi Chen",
            "Shujie Hu",
            "Shuo Li",
            "Siqi Yang",
            "Siyu Xu",
            "Siyu Ren",
            "Song Li",
            "Songxiang Liu",
            "Tianhao Bai",
            "Tianye Dai",
            "Wei Hong",
            "Wei Wang",
            "Weixiao Zhao",
            "Wengang Cao",
            "Wenlong Zhu",
            "Wenlong He",
            "Xi Su",
            "Xi Nan",
            "Xiaohan Zhao",
            "Xiaohao Wang",
            "Xiaoyu Zhao",
            "Xiaoyu Wang",
            "Xiaoyu Li",
            "Xin Pan",
            "Xin Chen",
            "Xiusong Sun",
            "Xu Xiang",
            "Xudong Xing",
            "Xuezhi Cao",
            "Xunliang Cai",
            "Yang Yang",
            "Yanli Tan",
            "Yao Yao",
            "Yerui Sun",
            "Yi Chen",
            "Yifan Lu",
            "Yin Gong",
            "Yining Zhang",
            "Yitian Chen",
            "Yiyang Gan",
            "Yuchen Tang",
            "Yuchen Xie",
            "Yueqian Wang",
            "Yuewen Zheng",
            "Yufei Zhang",
            "Yufeng Zhong",
            "Yulei Qian",
            "Yuqi Peng",
            "Yuqian Li",
            "Yuwei Jiang",
            "Zeyang Hu",
            "Zheng Zhang",
            "Zhengkun Tian",
            "Zhiqing Hong",
            "Zhixiong Zeng",
            "Zhuqi Mi",
            "Ziran Li",
            "Ziwen Wang",
            "Ziyi Zhao",
            "Ziyuan Zhuang",
            "Zizhe Zhao"
        ],
        "categories": [
            "cs.MM",
            "cs.AI",
            "cs.CL",
            "cs.DC",
            "cs.LG",
            "cs.SD"
        ],
        "submit_date": "2025-10-31"
    },
    "http://arxiv.org/abs/2511.00263v1": {
        "title": "COOL Is Optimal in Error-Free Asynchronous Byzantine Agreement",
        "link": "http://arxiv.org/abs/2511.00263v1",
        "abstract": "COOL (Chen'21) is an error-free, information-theoretically secure Byzantine agreement (BA) protocol proven to achieve BA consensus in the synchronous setting for an $\\ell$-bit message, with a total communication complexity of $O(\\max\\{n\\ell, nt \\log q\\})$ bits, four communication rounds in the worst case, and a single invocation of a binary BA, under the optimal resilience assumption $n \\geq 3t + 1$ in a network of $n$ nodes, where up to $t$ nodes may behave dishonestly. Here, $q$ denotes the alphabet size of the error correction code used in the protocol.   In this work, we present an adaptive variant of COOL, called OciorACOOL, which achieves error-free, information-theoretically secure BA consensus in the asynchronous setting with total $O(\\max\\{n\\ell, n t \\log q\\})$ communication bits, $O(1)$ rounds, and a single invocation of an asynchronous binary BA protocol, still under the optimal resilience assumption $n \\geq 3t + 1$. Moreover, OciorACOOL retains the same low-complexity, traditional $(n, k)$ error-correction encoding and decoding as COOL, with $k=t/3$.",
        "authors": [
            "Jinyuan Chen"
        ],
        "categories": [
            "cs.DC",
            "cs.CR",
            "cs.IT"
        ],
        "id": "http://arxiv.org/abs/2511.00263v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-31"
    },
    "http://arxiv.org/abs/2511.11603v1": {
        "title": "Machine learning-based cloud resource allocation algorithms: a comprehensive comparative review",
        "link": "http://arxiv.org/abs/2511.11603v1",
        "abstract": "Cloud resource allocation has emerged as a major challenge in modern computing environments, with organizations struggling to manage complex, dynamic workloads while optimizing performance and cost efficiency. Traditional heuristic approaches prove inadequate for handling the multi-objective optimization demands of existing cloud infrastructures. This paper presents a comparative analysis of state-of-the-art artificial intelligence and machine learning algorithms for resource allocation. We systematically evaluate 10 algorithms across four categories: Deep Reinforcement Learning approaches, Neural Network architectures, Traditional Machine Learning enhanced methods, and Multi-Agent systems. Analysis of published results demonstrates significant performance improvements across multiple metrics including makespan reduction, cost optimization, and energy efficiency gains compared to traditional methods. The findings reveal that hybrid architectures combining multiple artificial intelligence and machine learning techniques consistently outperform single-method approaches, with edge computing environments showing the highest deployment readiness. Our analysis provides critical insights for both academic researchers and industry practitioners seeking to implement next-generation cloud resource allocation strategies in increasingly complex and dynamic computing environments.",
        "authors": [
            "Deep Bodra",
            "Sushil Khairnar"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "id": "http://arxiv.org/abs/2511.11603v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-31"
    },
    "http://arxiv.org/abs/2511.00205v1": {
        "id": "http://arxiv.org/abs/2511.00205v1",
        "title": "Fix: externalizing network I/O in serverless computing",
        "link": "http://arxiv.org/abs/2511.00205v1",
        "tags": [
            "networking",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes a serverless computing system that externalizes network I/O, enabling the platform to manage data transfers. Uses deterministic procedures with precise data dependencies to optimize scheduling and reduce starvation. Shifts billing model to pay-for-results with improved efficiency.",
        "abstract": "We describe a system for serverless computing where users, programs,   and the underlying platform share a common representation of a   computation: a deterministic procedure, run in an environment   of well-specified data or the outputs of other computations. This   representation externalizes I/O: data movement over the network is   performed exclusively by the platform. Applications can describe the   precise data needed at each stage, helping the provider schedule   tasks and network transfers to reduce starvation. The design   suggests an end-to-end argument for outsourced computing, shifting   the service model from ``pay-for-effort'' to ``pay-for-results.''",
        "authors": [
            "Yuhan Deng",
            "Akshay Srivatsan",
            "Sebastian Ingino",
            "Francis Chua",
            "Yasmine Mitchell",
            "Matthew Vilaysack",
            "Keith Winstein"
        ],
        "categories": [
            "cs.OS",
            "cs.DC"
        ],
        "submit_date": "2025-10-31"
    },
    "http://arxiv.org/abs/2510.27656v1": {
        "id": "http://arxiv.org/abs/2510.27656v1",
        "title": "RDMA Point-to-Point Communication for LLM Systems",
        "link": "http://arxiv.org/abs/2510.27656v1",
        "tags": [
            "networking",
            "serving",
            "RL"
        ],
        "relevant": true,
        "indexed_date": "2025-10-31",
        "tldr": "Addresses the need for flexible point-to-point communication in LLM systems using diverse NICs. Proposes TransferEngine, a uniform interface that exposes one-sided WriteImm operations with completion primitives. Achieves 400 Gbps throughput and enables 1.3-second RL weight updates for trillion-parameter models.",
        "abstract": "Emerging Large Language Model (LLM) system patterns, such as disaggregated inference, Mixture-of-Experts (MoE) routing, and asynchronous reinforcement fine-tuning, require flexible point-to-point communication beyond simple collectives. Existing implementations are locked to specific Network Interface Controllers (NICs), hindering integration into inference engines and portability across hardware providers. We present TransferEngine, which bridges the functionality of common NICs to expose a uniform interface. TransferEngine exposes one-sided WriteImm operations with a ImmCounter primitive for completion notification, without ordering assumptions of network transport, transparently managing multiple NICs per GPU. We demonstrate peak throughput of 400 Gbps on both NVIDIA ConnectX-7 and AWS Elastic Fabric Adapter (EFA). We showcase TransferEngine through three production systems: (1) KvCache transfer for disaggregated inference with dynamic scaling, (2) RL weight updates achieving 1.3 seconds for trillion-parameter models, and (3) MoE dispatch/combine implementation exceeding DeepEP decode latency on ConnectX-7, with the first viable latencies on EFA. We demonstrate that our portable point-to-point communication complements collectives while avoiding lock-in.",
        "authors": [
            "Nandor Licker",
            "Kevin Hu",
            "Vladimir Zaytsev",
            "Lequn Chen"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-10-31"
    },
    "http://arxiv.org/abs/2510.27351v1": {
        "id": "http://arxiv.org/abs/2510.27351v1",
        "title": "ML-Based Optimum Sub-system Size Heuristic for the GPU Implementation of the Tridiagonal Partition Method",
        "link": "http://arxiv.org/abs/2510.27351v1",
        "tags": [
            "kernel",
            "training"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes a ML-based heuristic to find optimal sub-system size for CUDA implementation of parallel partition algorithms. Uses kNN to predict optimal sizes for solving systems of linear equations, reducing empirical tuning. Achieves acceptably good predictions validated against actual data.",
        "abstract": "This paper presents a machine learning (ML)-based heuristic for finding the optimum sub-system size for the CUDA implementation of the parallel partition algorithm. Computational experiments for different system of linear algebraic equation (SLAE) sizes are conducted, and the optimum sub-system size for each of them is found empirically. To estimate a model for the sub-system size, we perform the k-nearest neighbors (kNN) classification method. Statistical analysis of the results is done. By comparing the predicted values with the actual data, the algorithm is deemed to be acceptably good. Next, the heuristic is expanded to work for the recursive parallel partition algorithm as well. An algorithm for determining the optimum sub-system size for each recursive step is formulated. A kNN model for predicting the optimum number of recursive steps for a particular SLAE size is built.",
        "authors": [
            "Milena Veneva"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-10-31"
    },
    "http://arxiv.org/abs/2510.27317v1": {
        "title": "Dynamic Service Scheduling and Resource Management in Energy-Harvesting Multi-access Edge Computing",
        "link": "http://arxiv.org/abs/2510.27317v1",
        "abstract": "Multi-access Edge Computing (MEC) delivers low-latency services by hosting applications near end-users. To promote sustainability, these systems are increasingly integrated with renewable Energy Harvesting (EH) technologies, enabling operation where grid electricity is unavailable. However, balancing the intermittent nature of harvested energy with dynamic user demand presents a significant resource allocation challenge. This work proposes an online strategy for an MEC system powered exclusively by EH to address this trade-off. Our strategy dynamically schedules computational tasks with dependencies and governs energy consumption through real-time decisions on server frequency scaling and service module migration. Experiments using real-world datasets demonstrate our algorithm's effectiveness in efficiently utilizing harvested energy while maintaining low service latency.",
        "authors": [
            "Shuyi Chen",
            "Panagiotis Oikonomou",
            "Zhengchang Hua",
            "Nikos Tziritas",
            "Karim Djemame",
            "Nan Zhang",
            "Georgios Theodoropoulos"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.27317v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-31"
    },
    "http://arxiv.org/abs/2510.27289v1": {
        "title": "A Digital Twin-based Multi-Agent Reinforcement Learning Framework for Vehicle-to-Grid Coordination",
        "link": "http://arxiv.org/abs/2510.27289v1",
        "abstract": "The coordination of large-scale, decentralised systems, such as a fleet of Electric Vehicles (EVs) in a Vehicle-to-Grid (V2G) network, presents a significant challenge for modern control systems. While collaborative Digital Twins have been proposed as a solution to manage such systems without compromising the privacy of individual agents, deriving globally optimal control policies from the high-level information they share remains an open problem. This paper introduces Digital Twin Assisted Multi-Agent Deep Deterministic Policy Gradient (DT-MADDPG) algorithm, a novel hybrid architecture that integrates a multi-agent reinforcement learning framework with a collaborative DT network. Our core contribution is a simulation-assisted learning algorithm where the centralised critic is enhanced by a predictive global model that is collaboratively built from the privacy-preserving data shared by individual DTs. This approach removes the need for collecting sensitive raw data at a centralised entity, a requirement of traditional multi-agent learning algorithms. Experimental results in a simulated V2G environment demonstrate that DT-MADDPG can achieve coordination performance comparable to the standard MADDPG algorithm while offering significant advantages in terms of data privacy and architectural decentralisation. This work presents a practical and robust framework for deploying intelligent, learning-based coordination in complex, real-world cyber-physical systems.",
        "authors": [
            "Zhengchang Hua",
            "Panagiotis Oikonomou",
            "Karim Djemame",
            "Nikos Tziritas",
            "Georgios Theodoropoulos"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.27289v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-31"
    },
    "http://arxiv.org/abs/2510.27257v1": {
        "id": "http://arxiv.org/abs/2510.27257v1",
        "title": "Synergistic Tensor and Pipeline Parallelism",
        "link": "http://arxiv.org/abs/2510.27257v1",
        "tags": [
            "training",
            "multi-modal"
        ],
        "relevant": true,
        "indexed_date": "2025-10-31",
        "tldr": "Proposes a synergistic schedule for tensor and pipeline parallelism to reduce communication and synchronization bubbles during LLM/MLLM training. Decouples forward/backward passes into fine-grained units and braids them into a composite sequence. Improves training throughput by up to 16% over baselines.",
        "abstract": "In the machine learning system, the hybrid model parallelism combining tensor parallelism (TP) and pipeline parallelism (PP) has become the dominant solution for distributed training of Large Language Models~(LLMs) and Multimodal LLMs (MLLMs). However, TP introduces significant collective communication overheads, while PP suffers from synchronization inefficiencies such as pipeline bubbles. Existing works primarily address these challenges from isolated perspectives, focusing either on overlapping TP communication or on flexible PP scheduling to mitigate pipeline bubbles. In this paper, we propose a new synergistic tensor and pipeline parallelism schedule that simultaneously reduces both types of bubbles. Our proposed schedule decouples the forward and backward passes in PP into fine-grained computation units, which are then braided to form a composite computation sequence. This compositional structure enables near-complete elimination of TP-related bubbles. Building upon this structure, we further design the PP schedule to minimize PP bubbles. Experimental results demonstrate that our approach improves training throughput by up to 12% for LLMs and 16% for MLLMs compared to existing scheduling methods. Our source code is avaiable at https://github.com/MICLAB-BUPT/STP.",
        "authors": [
            "Mengshi Qi",
            "Jiaxuan Peng",
            "Jie Zhang",
            "Juan Zhu",
            "Yong Li",
            "Huadong Ma"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-10-31"
    },
    "http://arxiv.org/abs/2510.27182v1": {
        "id": "http://arxiv.org/abs/2510.27182v1",
        "title": "SERFLOW: A Cross-Service Cost Optimization Framework for SLO-Aware Dynamic ML Inference",
        "link": "http://arxiv.org/abs/2510.27182v1",
        "tags": [
            "serving",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-10-31",
        "tldr": "Addresses cost-efficient, SLO-aware serving for dynamic ML inference where requests may exit early. Proposes SERFLOW, a framework that offloads model stages between IaaS VMs and FaaS functions using stage-specific provisioning and adaptive load balancing. Reduces cloud costs by over 23%.",
        "abstract": "Dynamic offloading of Machine Learning (ML) model partitions across different resource orchestration services, such as Function-as-a-Service (FaaS) and Infrastructure-as-a-Service (IaaS), can balance processing and transmission delays while minimizing costs of adaptive inference applications. However, prior work often overlooks real-world factors, such as Virtual Machine (VM) cold starts, requests under long-tail service time distributions, etc. To tackle these limitations, we model each ML query (request) as traversing an acyclic sequence of stages, wherein each stage constitutes a contiguous block of sparse model parameters ending in an internal or final classifier where requests may exit. Since input-dependent exit rates vary, no single resource configuration suits all query distributions. IaaS-based VMs become underutilized when many requests exit early, yet rapidly scaling to handle request bursts reaching deep layers is impractical. SERFLOW addresses this challenge by leveraging FaaS-based serverless functions (containers) and using stage-specific resource provisioning that accounts for the fraction of requests exiting at each stage. By integrating this provisioning with adaptive load balancing across VMs and serverless functions based on request ingestion, SERFLOW reduces cloud costs by over $23\\%$ while efficiently adapting to dynamic workloads.",
        "authors": [
            "Zongshun Zhang",
            "Ibrahim Matta"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "submit_date": "2025-10-31"
    },
    "http://arxiv.org/abs/2510.27176v3": {
        "id": "http://arxiv.org/abs/2510.27176v3",
        "title": "Glia: A Human-Inspired AI for Automated Systems Design and Optimization",
        "link": "http://arxiv.org/abs/2510.27176v3",
        "tags": [
            "autoscaling",
            "serving",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-10-31",
        "tldr": "Proposes Glia, an AI that autonomously designs systems mechanisms using multi-agent LLMs for reasoning and experimentation. Applied to distributed GPU clusters for LLM inference, it generates routing, scheduling, and auto-scaling algorithms achieving human-expert performance with reduced design time.",
        "abstract": "Can an AI autonomously design mechanisms for computer systems on par with the creativity and reasoning of human experts? We present Glia, an AI architecture for networked systems design that uses large language models (LLMs) in a human-inspired, multi-agent workflow. Each agent specializes in reasoning, experimentation, and analysis, collaborating through an evaluation framework that grounds abstract reasoning in empirical feedback. Unlike prior ML-for-systems methods that optimize black-box policies, Glia generates interpretable designs and exposes its reasoning process. When applied to a distributed GPU cluster for LLM inference, it produces new algorithms for request routing, scheduling, and auto-scaling that perform at human-expert levels in significantly less time, while yielding novel insights into workload behavior. Our results suggest that by combining reasoning LLMs with structured experimentation, an AI can produce creative and understandable designs for complex systems problems.",
        "authors": [
            "Pouya Hamadanian",
            "Pantea Karimi",
            "Arash Nasr-Esfahany",
            "Kimia Noorbakhsh",
            "Joseph Chandler",
            "Ali ParandehGheibi",
            "Mohammad Alizadeh",
            "Hari Balakrishnan"
        ],
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.DC"
        ],
        "submit_date": "2025-10-31"
    },
    "http://arxiv.org/abs/2510.27175v1": {
        "title": "Byzantine Attacks in RIS-Enhanced Cooperative Spectrum Sensing: A Decision Fusion Perspective",
        "link": "http://arxiv.org/abs/2510.27175v1",
        "abstract": "From the perspective of hard decision fusion, we investigate Byzantine attacks in Reconfigurable Intelligent Surface (RIS)-enhanced and decode-and-forward relay-assisted Cooperative Spectrum Sensing (CSS) for mobile Cognitive Radio Networks (CRNs) in this paper. Specially, a RIS-enhanced and decode-and-forward relay-assisted CSS configuration is first constructed under dynamic channel scenarios due to user mobility. Subsequently, the channel- and attack-aware hard decision fusion rules are developed, and the optimal channel-aware Byzantine attack strategies are then developed under both small-scale and large-scale attacking scenarios. The corresponding results depict that the optimal attack strategy does not require any a prior knowledge of the global instantaneous Channel State Information (ICSI) (e.g. false alarm probability and detection probability of all the secondary users), although perfect acquisition of ICSI is clearly always not affordable from the attacker perspective, which is further exacerbated by the RIS and decode-and-forward relays involved in CSS and the potential high mobility of secondary users that leads to fast fading channels. Furthermore, our counterintuitive results also indicate that, regardless of the attacker's awareness of the decision fusion rule, the optimal Byzantine attack can be achieved through a unifying framework, the explicit attack strategy may be not unique, and the attacking effectiveness is primarily determined by the fraction of the Byzantine nodes rather than the channel dynamics. That is, to make the channel-aware approach more practical, the challenge that the heavy reliance on the global ICSI and decision fusion rule in obtaining the Byzantine attacks is successfully relaxed. Finally, we empirically validate our theoretical analysis through extensive simulations across a wide range of attacking scenarios.",
        "authors": [
            "Gaoyuan Zhang",
            "Gaolei Song",
            "Boyuan Li",
            "Zijian Li",
            "Baofeng Ji",
            "Ruijuan Zheng",
            "Guoqiang Zheng",
            "Tony Q. S. Quek"
        ],
        "categories": [
            "cs.IT",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.27175v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-31"
    },
    "http://arxiv.org/abs/2510.27147v1": {
        "title": "Secure Communication in the Presence of an RIS-Enhanced Eavesdropper in MIMO Networks",
        "link": "http://arxiv.org/abs/2510.27147v1",
        "abstract": "We pay our attention towards secure and robust communication in the presence of a Reconfigurable Intelligent Surface (RIS)-enhanced mobile eavesdropping attacker in Multiple-Input Multiple-Output (MIMO)wireless networks.Specifically,we first provide a unifying framework that generalizes specific intelligent wiretap model wherein the passive eavesdropper configured with any number of antennas is potentially mobile and can actively optimize its received signal strength with the help of RIS by intelligently manipulating wiretap channel characteristics.To effectively mitigate this intractable threat,we then propose a novel and lightweight secure communication scheme from the perspective of information theory.The main idea is that the data processing can in some cases be observed as communication channel,and a random bit-flipping scheme is then carefully involved for the legitimate transmitter to minimize the mutual information between the secret message and the passive eavesdropper's received data.The Singular Value Decomposition (SVD)-based precoding strategy is also implemented to optimize power allocation,and thus ensure that the legitimate receiver is not subject to interference from this random bit-flipping.The corresponding results depict that our secure communication scheme is practically desired, which does not require any a prior knowledge of the eavesdropper's full instantaneous Channel State Information (ICSI). Furthermore,we consider the RIS optimization problem from the eavesdropper's perspective,and provide RIS phase shift design solutions under different attacking scenarios.Finally,the optimal detection schemes respectively for the legitimate user and the eavesdropper are provided,and comprehensive simulations are presented to verify our theoretical analysis and show the effectiveness and robustness of our secure communication scheme across a wide range of attacking scenarios.",
        "authors": [
            "Gaoyuan Zhang",
            "Ruisong Si",
            "Boyuan Li",
            "Zijian Li",
            "Baofeng Ji",
            "Chenqi Zhu",
            "Tony Q. S. Quek"
        ],
        "categories": [
            "cs.IT",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.27147v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-31"
    },
    "http://arxiv.org/abs/2510.27039v1": {
        "title": "A Cloud-Based Spatio-Temporal GNN-Transformer Hybrid Model for Traffic Flow Forecasting with External Feature Integration",
        "link": "http://arxiv.org/abs/2510.27039v1",
        "abstract": "Accurate traffic flow forecasting is essential for the development of intelligent transportation systems (ITS), supporting tasks such as traffic signal optimization, congestion management, and route planning. Traditional models often fail to effectively capture complex spatial-temporal dependencies in large-scale road networks, especially under the influence of external factors such as weather, holidays, and traffic accidents. To address this challenge, this paper proposes a cloud-based hybrid model that integrates Spatio-Temporal Graph Neural Networks (ST-GNN) with a Transformer architecture for traffic flow prediction. The model leverages the strengths of GNNs in modeling spatial correlations across road networks and the Transformers' ability to capture long-term temporal dependencies. External contextual features are incorporated via feature fusion to enhance predictive accuracy. The proposed model is deployed on a cloud computing platform to achieve scalability and real-time adaptability. Experimental evaluation of the dataset shows that our model outperforms baseline methods (LSTM, TCN, GCN, pure Transformer) with an RMSE of only 17.92 and a MAE of only 10.53. These findings suggest that the hybrid GNN-Transformer approach provides an effective and scalable solution for cloud-based ITS applications, offering methodological advancements for traffic flow forecasting and practical implications for congestion mitigation.",
        "authors": [
            "Zhuo Zheng",
            "Lingran Meng",
            "Ziyu Lin"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.27039v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-30"
    },
    "http://arxiv.org/abs/2511.13727v1": {
        "title": "Boosting performance: Gradient Clock Synchronisation with two-way measured links",
        "link": "http://arxiv.org/abs/2511.13727v1",
        "abstract": "This master thesis extends the formal model of the GCS algorithm as presented by (Fan and Lynch 2004, 325), (Lenzen, Locher and Wattenhofer 2008, 510) and (Függer et al. 2023) to operate under implementation-near assumptions by replacing the one-way measurement paradigm assumed in prior work by the two-way measurement paradigm. With this change of paradigm, we remove many restrictions previously enforced to allow provable performance. Most notability, while maintaining the core behaviour of GCS, we: 1. Lift the requirement for unitary link lengths and thereby create a realistic model for flexible deployment of implementations of GCS in practice. 2. Provide a formal model of frequency sources assumed in prior work. 3. Perform a fine grained distinction between the different components of the algorithm's estimation error and globally reduce its impact by multiple orders of magnitude. 4. Significantly reduce the contribution of the uncertainty to the algorithm's estimation error to be in the range of 10\\% to 0,1\\% of the delay per link instead of being in the oder of the delay per link as in prior work and show matching upper bounds on the local and global skew of GCS.",
        "authors": [
            "Sophie Wenning"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.13727v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-30"
    },
    "http://arxiv.org/abs/2511.11601v1": {
        "id": "http://arxiv.org/abs/2511.11601v1",
        "title": "Mind the Gap: Revealing Inconsistencies Across Heterogeneous AI Accelerators",
        "link": "http://arxiv.org/abs/2511.11601v1",
        "tags": [
            "hardware",
            "inference",
            "training"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Investigates inconsistencies in ML model execution across heterogeneous AI accelerators. Uses automated pipeline to test 100,000+ model variants on five accelerators. Finds newer platforms support 17% fewer operators and exhibit >5% output discrepancies due to implementation differences.",
        "abstract": "While NVIDIA remains the dominant provider of AI accelerators within cloud data center, emerging vendors such as AMD, Intel, Mac, and Huawei offer cost-effective alternatives with claims of compatibility and performance. This paper presents the first empirical study investigating divergence in machine learning model across heterogeneous AI accelerators. Utilizing an automated pipeline, we synthesize over 100,000 variant models derived from 4,000 real-world models and execute them across five different enterprise-grade accelerators. Our findings suggest that newer AI platforms from Mac and Huawei support at least 17\\% fewer operators than NVIDIA. These platforms also exhibit a higher rate of output discrepancies (exceeding 5\\%), which stem from differences in operator implementations, handling of exceptional numerical values, and instruction scheduling. They are also more susceptible to failures during model compilation-based acceleration, and in some cases, the compiled models produce outputs that differ noticeably from those generated using the standard execution mode. In addition, we identify 7 implementation flaws in PyTorch and 40 platform-specific issues across vendors. These results underscore the challenges of achieving consistent machine learning behavior in an increasingly diverse hardware ecosystem.",
        "authors": [
            "Elliott Wen",
            "Sean Ma",
            "Ewan Tempero",
            "Jens Dietrich",
            "Daniel Luo",
            "Jiaxing Shen",
            "Kaiqi Zhao",
            "Bruce Sham",
            "Yousong Song",
            "Jiayi Hua",
            "Jia Hong"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.LG"
        ],
        "submit_date": "2025-10-30"
    },
    "http://arxiv.org/abs/2510.26913v1": {
        "id": "http://arxiv.org/abs/2510.26913v1",
        "title": "FlowMesh: A Service Fabric for Composable LLM Workflows",
        "link": "http://arxiv.org/abs/2510.26913v1",
        "tags": [
            "training",
            "agentic",
            "RL"
        ],
        "relevant": true,
        "indexed_date": "2025-10-30",
        "tldr": "Proposes FlowMesh, a service fabric for fine-grained execution of composite LLM workflows (e.g., RLHF, agent workflows). It decomposes workflows into operators for cross-user deduplication, batching, and global scheduling. Achieves up to 3.8x cost reduction compared to baseline pipelines.",
        "abstract": "AI deployment increasingly resembles a pipeline of data transformation, fine-tuning, and agent interactions rather than a monolithic LLM job; recent examples include RLHF/RLAIF training and agentic workflows. To cope with this shift, we propose FlowMesh, a multi-tenant service fabric that executes and optimizes these workloads as one shared service instead of isolated pipelines. It decomposes workflows into fine-grained operators with recorded lineage, enabling de-duplication of work across users and batching requests on the same hardware while preserving per-workflow provenance. A global control plane maintains a cluster-wide pool of ready operators and uses a single utility function to pick both the batch and the worker, balancing throughput, cost, and data locality on heterogeneous GPUs. The data plane is an elastic fleet of stateless workers backed by a content-addressable store, enabling rapid, automatic scale-out, safe retry after preemption, and portability across managed clusters such as Kubernetes and geo-distributed GPU marketplaces such as Vast.ai. Compared with baseline solutions, FlowMesh achieves up to 3.8x cost reduction and 2.0x lower energy usage, provides a similar or better latency profile, and remains efficient under dynamic and failure-prone conditions.",
        "authors": [
            "Junyi Shen",
            "Noppanat Wadlom",
            "Lingfeng Zhou",
            "Dequan Wang",
            "Xu Miao",
            "Lei Fang",
            "Yao Lu"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-10-30"
    },
    "http://arxiv.org/abs/2510.26730v1": {
        "id": "http://arxiv.org/abs/2510.26730v1",
        "title": "ExpertFlow: Adaptive Expert Scheduling and Memory Coordination for Efficient MoE Inference",
        "link": "http://arxiv.org/abs/2510.26730v1",
        "tags": [
            "serving",
            "MoE",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-10-30",
        "tldr": "Proposes ExpertFlow, a runtime system for MoE inference that uses adaptive expert prefetching and cache-aware routing to reduce latency from parameter transfers. It dynamically predicts expert needs to minimize cache misses, reducing model stall time to <0.1% of the baseline.",
        "abstract": "The expansion of large language models is increasingly limited by the constrained memory capacity of modern GPUs. To mitigate this, Mixture-of-Experts (MoE) architectures activate only a small portion of parameters during inference, significantly lowering both memory demand and computational overhead. However, conventional MoE inference approaches, which select active experts independently at each layer, often introduce considerable latency because of frequent parameter transfers between host and GPU memory. In addition, current cross-layer prediction strategies, which are typically based on fixed steps, lack adaptability across different hardware platforms and workloads, thereby reducing their robustness and effectiveness.   To address these challenges, we present ExpertFlow, a runtime system for MoE inference that combines adaptive expert prefetching and cache-aware routing. ExpertFlow continuously adjusts its prediction horizon for expert activation by leveraging runtime statistics such as transfer bandwidth, parameter dimensionality, and model feedback signals. Furthermore, it incorporates a hybrid cross-layer prediction scheme that fuses pregating information with intermediate computational states to anticipate future expert needs. By adaptively refining prefetching decisions and aligning them with actual usage behavior, ExpertFlow effectively decreases cache misses and removes latency caused by expert swap-ins. Our evaluation demonstrates that ExpertFlow reduces model stall time to less than 0.1% of the baseline, highlighting its capability to optimize MoE inference under stringent memory constraints.",
        "authors": [
            "Zixu Shen",
            "Kexin Chu",
            "Yifan Zhang",
            "Dawei Xiang",
            "Runxin Wu",
            "Wei Zhang"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.PF"
        ],
        "submit_date": "2025-10-30"
    },
    "http://arxiv.org/abs/2510.26722v3": {
        "title": "Non-Convex Over-the-Air Heterogeneous Federated Learning: A Bias-Variance Trade-off",
        "link": "http://arxiv.org/abs/2510.26722v3",
        "abstract": "Over-the-air (OTA) federated learning (FL) has been well recognized as a scalable paradigm that exploits the waveform superposition of the wireless multiple-access channel to aggregate model updates in a single use. Existing OTA-FL designs largely enforce zero-bias model updates by either assuming \\emph{homogeneous} wireless conditions (equal path loss across devices) or forcing zero-bias updates to guarantee convergence. Under \\emph{heterogeneous} wireless scenarios, however, such designs are constrained by the weakest device and inflate the update variance. Moreover, prior analyses of biased OTA-FL largely address convex objectives, while most modern AI models are highly non-convex. Motivated by these gaps, we study OTA-FL with stochastic gradient descent (SGD) for general smooth non-convex objectives under wireless heterogeneity. We develop novel OTA-FL SGD updates that allow a structured, time-invariant model bias while facilitating reduced variance updates. We derive a finite-time stationarity bound (expected time average squared gradient norm) that explicitly reveals a bias-variance trade-off. To optimize this trade-off, we pose a non-convex joint OTA power-control design and develop an efficient successive convex approximation (SCA) algorithm that requires only statistical CSI at the base station. Experiments on a non-convex image classification task validate the approach: the SCA-based design accelerates convergence via an optimized bias and improves generalization over prior OTA-FL baselines.",
        "authors": [
            "Muhammad Faraz Ul Abrar",
            "Nicolò Michelusi"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC",
            "eess.SP",
            "eess.SY"
        ],
        "id": "http://arxiv.org/abs/2510.26722v3",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-30"
    },
    "http://arxiv.org/abs/2510.26709v3": {
        "id": "http://arxiv.org/abs/2510.26709v3",
        "title": "An All-Reduce Compatible Top-K Compressor for Communication-Efficient Distributed Learning",
        "link": "http://arxiv.org/abs/2510.26709v3",
        "tags": [
            "training",
            "sparse",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-10-30",
        "tldr": "Addresses gradient communication bottleneck in distributed training. Proposes ARC-Top-K, an All-Reduce compatible compressor combining gradient sketching and momentum error feedback. Reduces training time by up to 60.7% while matching Top-K accuracy.",
        "abstract": "Communication remains a central bottleneck in large-scale distributed machine learning, and gradient sparsification has emerged as a promising strategy to alleviate this challenge. However, existing gradient compressors face notable limitations: Rand-$K$ discards structural information and performs poorly in practice, while Top-$K$ preserves informative entries but loses the contraction property and requires costly All-Gather operations. In this paper, we propose ARC-Top-$K$, an {All-Reduce}-Compatible Top-$K$ compressor that aligns sparsity patterns across nodes using a lightweight sketch of the gradient, enabling index-free All-Reduce while preserving globally significant information. ARC-Top-$K$ is provably contractive and, when combined with momentum error feedback (EF21M), achieves linear speedup and sharper convergence rates than the original EF21M under standard assumptions. Empirically, ARC-Top-$K$ matches the accuracy of Top-$K$ while reducing wall-clock training time by up to 60.7\\%, offering an efficient and scalable solution that combines the robustness of Rand-$K$ with the strong performance of Top-$K$.",
        "authors": [
            "Chuyan Chen",
            "Chenyang Ma",
            "Zhangxin Li",
            "Yutong He",
            "Yanjie Dong",
            "Kun Yuan"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "submit_date": "2025-10-30"
    },
    "http://arxiv.org/abs/2510.26492v1": {
        "title": "Wireless Sensor Networks as Parallel and Distributed Hardware Platform for Artificial Neural Networks",
        "link": "http://arxiv.org/abs/2510.26492v1",
        "abstract": "We are proposing fully parallel and maximally distributed hardware realization of a generic neuro-computing system. More specifically, the proposal relates to the wireless sensor networks technology to serve as a massively parallel and fully distributed hardware platform to implement and realize artificial neural network (ANN) algorithms. A parallel and distributed (PDP) hardware realization of ANNs makes it possible to have real time computation of large-scale (and complex) problems in a highly robust framework. We will demonstrate how a network of hundreds of thousands of processing nodes (or motes of a wireless sensor network), which have on-board processing and wireless communication features, can be used to implement fully parallel and massively distributed computation of artificial neural network algorithms for solution of truly large-scale problems in real time. The realization of artificial neural network algorithms in a massively parallel and fully distributed hardware has been the goal of neural network computing researchers. This is because a parallel and distributed computation of artificial neural network algorithms could not have been achieved against all the advancements in silicon- or optics-based computing. Accordingly, artificial neural networks could not be applied to very large-scale problems for real time computation of solutions. This hindered the development of neural algorithms for affordable and practical solutions of challenging problems since often special-purpose computing approaches in hardware, software or hybrid (non-neural) had to be developed for and fine-tuned to specific problems that are very large-scale and highly complex. Successful implementation is likely to revolutionize computing as we know it by making it possible to solve very large scale scientific, engineering or technical problems in real time.",
        "authors": [
            "Gursel Serpen"
        ],
        "categories": [
            "cs.NE",
            "cs.AR",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.26492v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-30"
    },
    "http://arxiv.org/abs/2510.26475v1": {
        "id": "http://arxiv.org/abs/2510.26475v1",
        "title": "ReSpec: Towards Optimizing Speculative Decoding in Reinforcement Learning Systems",
        "link": "http://arxiv.org/abs/2510.26475v1",
        "tags": [
            "RL",
            "training",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-10-30",
        "tldr": "Adapts speculative decoding to accelerate generation in RL training systems. Proposes ReSpec with dynamic SD tuning, drafter distillation, and reward-weighted updates to mitigate staleness and policy drift. Achieves up to 4.5x speedup on Qwen models (3B-14B) while preserving reward convergence.",
        "abstract": "Adapting large language models (LLMs) via reinforcement learning (RL) is often bottlenecked by the generation stage, which can consume over 75\\% of the training time. Speculative decoding (SD) accelerates autoregressive generation in serving systems, but its behavior under RL training remains largely unexplored. We identify three critical gaps that hinder the naive integration of SD into RL systems: diminishing speedups at large batch sizes, drafter staleness under continual actor updates, and drafter-induced policy degradation.   To address these gaps, we present ReSpec, a system that adapts SD to RL through three complementary mechanisms: dynamically tuning SD configurations, evolving the drafter via knowledge distillation, and weighting updates by rollout rewards. On Qwen models (3B--14B), ReSpec achieves up to 4.5x speedup while preserving reward convergence and training stability, providing a practical solution for efficient RL-based LLM adaptation.",
        "authors": [
            "Qiaoling Chen",
            "Zijun Liu",
            "Peng Sun",
            "Shenggui Li",
            "Guoteng Wang",
            "Ziming Liu",
            "Yonggang Wen",
            "Siyuan Feng",
            "Tianwei Zhang"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "submit_date": "2025-10-30"
    },
    "http://arxiv.org/abs/2510.26413v2": {
        "title": "Environmental Impact of CI/CD Pipelines",
        "link": "http://arxiv.org/abs/2510.26413v2",
        "abstract": "CI/CD pipelines are widely used in software development, yet their environmental impact, particularly carbon and water footprints (CWF), remains largely unknown to developers, as CI service providers typically do not disclose such information. With the growing environmental impact of cloud computing, understanding the CWF of CI/CD services has become increasingly important.   This work investigates the CWF of using GitHub Actions, focusing on open-source repositories where usage is free and unlimited for standard runners. We build upon a methodology from the Cloud Carbon Footprint framework and we use the largest dataset of workflow runs reported in the literature to date, comprising over 2.2 million workflow runs from more than 18,000 repositories.   Our analysis reveals that the GitHub Actions ecosystem results in a substantial CWF. Our estimates for the carbon footprint in 2024 range from 150.5 MTCO2e in the most optimistic scenario to 994.9 MTCO2e in the most pessimistic scenario, while the water footprint ranges from 1,989.6 to 37,664.5 kiloliters. The most likely scenario estimates are 456.9 MTCO2e for carbon footprint and 5,738.2 kiloliters for water footprint. To provide perspective, the carbon footprint in the most likely scenario is equivalent to the carbon captured by 7,615 urban trees in a year, and the water footprint is comparable to the water consumed by an average American family over 5,053 years.   We explore strategies to mitigate this impact, primarily by reducing wasted computational resources. Key recommendations include deploying runners in regions whose energy production has a low environmental impact such as France and the United Kingdom, implementing stricter deactivation policies for scheduled runs and aligning their execution with periods when the regional energy mix is more environmentally favorable, and reducing the size of repositories.",
        "authors": [
            "Nuno Saavedra",
            "Alexandra Mendes",
            "João F. Ferreira"
        ],
        "categories": [
            "cs.SE",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.26413v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-30"
    },
    "http://arxiv.org/abs/2510.26008v2": {
        "id": "http://arxiv.org/abs/2510.26008v2",
        "title": "Detecting Anomalies in Machine Learning Infrastructure via Hardware Telemetry",
        "link": "http://arxiv.org/abs/2510.26008v2",
        "tags": [
            "training",
            "networking",
            "storage"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes Reveal, an unsupervised pipeline that uses hardware telemetry to detect anomalies in ML infrastructure without workload knowledge. Achieves 5.97% acceleration for DeepSeek model by identifying network and system configuration issues.",
        "abstract": "Modern machine learning (ML) has grown into a tightly coupled, full-stack ecosystem that combines hardware, software, network, and applications. Many users rely on cloud providers for elastic, isolated, and cost-efficient resources. Unfortunately, these platforms as a service use virtualization, which means operators have little insight into the users' workloads. This hinders resource optimizations by the operator, which is essential to ensure cost efficiency and minimize execution time. In this paper, we argue that workload knowledge is unnecessary for system-level optimization. We propose Reveal, which takes a hardware-centric approach, relying only on hardware signals - fully accessible by operators. Using low-level signals collected from the system, Reveal detects anomalies through an unsupervised learning pipeline. The pipeline is developed by analyzing over 30 popular ML models on various hardware platforms, ensuring adaptability to emerging workloads and unknown deployment patterns. Using Reveal, we successfully identified both network and system configuration issues, accelerating the DeepSeek model by 5.97%.",
        "authors": [
            "Ziji Chen",
            "Steven W. D. Chien",
            "Peng Qian",
            "Noa Zilberman"
        ],
        "categories": [
            "cs.PF",
            "cs.AR",
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-10-29"
    },
    "http://arxiv.org/abs/2510.25878v1": {
        "title": "Foundations of Fiat-Denominated Loans Collateralized by Cryptocurrencies",
        "link": "http://arxiv.org/abs/2510.25878v1",
        "abstract": "The rising importance of cryptocurrencies as financial assets pushed their applicability from an object of speculation closer to standard financial instruments such as loans. In this work, we initiate the study of secure protocols that enable fiat-denominated loans collateralized by cryptocurrencies such as Bitcoin. We provide limited-custodial protocols for such loans relying only on trusted arbitration and provide their game-theoretical analysis. We also highlight various interesting directions for future research.",
        "authors": [
            "Pavel Hubáček",
            "Jan Václavek",
            "Michelle Yeo"
        ],
        "categories": [
            "cs.CR",
            "cs.DC",
            "cs.GT"
        ],
        "id": "http://arxiv.org/abs/2510.25878v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-29"
    },
    "http://arxiv.org/abs/2510.25757v1": {
        "title": "Holon Streaming: Global Aggregations with Windowed CRDTs",
        "link": "http://arxiv.org/abs/2510.25757v1",
        "abstract": "Scaling global aggregations is a challenge for exactly-once stream processing systems. Current systems implement these either by computing the aggregation in a single task instance, or by static aggregation trees, which limits scalability and may become a bottleneck. Moreover, the end-to-end latency is determined by the slowest path in the tree, and failures and reconfiguration cause large latency spikes due to the centralized coordination. Towards these issues, we present Holon Streaming, an exactly-once stream processing system for global aggregations. Its deterministic programming model uses windowed conflict-free replicated data types (Windowed CRDTs), a novel abstraction for shared replicated state. Windowed CRDTs make computing global aggregations scalable. Furthermore, their guarantees such as determinism and convergence enable the design of efficient failure recovery algorithms by decentralized coordination. Our evaluation shows a 5x lower latency and 2x higher throughput than an existing stream processing system on global aggregation workloads, with an 11x latency reduction under failure scenarios. The paper demonstrates the effectiveness of decentralized coordination with determinism, and the utility of Windowed CRDTs for global aggregations.",
        "authors": [
            "Jonas Spenger",
            "Kolya Krafeld",
            "Ruben van Gemeren",
            "Philipp Haller",
            "Paris Carbone"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.25757v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-29"
    },
    "http://arxiv.org/abs/2510.25736v1": {
        "title": "Effect of Full Common Randomness Replication in Symmetric PIR on Graph-Based Replicated Systems",
        "link": "http://arxiv.org/abs/2510.25736v1",
        "abstract": "We revisit the problem of symmetric private information retrieval (SPIR) in settings where the database replication is modeled by a simple graph. Here, each vertex corresponds to a server, and a message is replicated on two servers if and only if there is an edge between them. To satisfy the requirement of database privacy, we let all the servers share some common randomness, independent of the messages. We aim to quantify the improvement in SPIR capacity, i.e., the maximum ratio of the number of desired and downloaded symbols, compared to the setting with graph-replicated common randomness. Towards this, we develop an algorithm to convert a class of PIR schemes into the corresponding SPIR schemes, thereby establishing a capacity lower bound on graphs for which such schemes exist. This includes the class of path and cyclic graphs for which we derive capacity upper bounds that are tighter than the trivial bounds given by the respective PIR capacities. For the special case of path graph with three vertices, we identify the SPIR capacity to be $\\frac{1}{2}$.",
        "authors": [
            "Shreya Meel",
            "Sennur Ulukus"
        ],
        "categories": [
            "cs.IT",
            "cs.CR",
            "cs.DC",
            "eess.SP"
        ],
        "id": "http://arxiv.org/abs/2510.25736v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-29"
    },
    "http://arxiv.org/abs/2511.11598v1": {
        "title": "Distributed Q-learning-based Shortest-Path Tree Construction in IoT Sensor Networks",
        "link": "http://arxiv.org/abs/2511.11598v1",
        "abstract": "Efficient routing in IoT sensor networks is critical for minimizing energy consumption and latency. Traditional centralized algorithms, such as Dijkstra's, are computationally intensive and ill-suited for dynamic, distributed IoT environments. We propose a novel distributed Q-learning framework for constructing shortest-path trees (SPTs), enabling sensor nodes to independently learn optimal next-hop decisions using only local information. States are defined based on node positions and routing history, with a reward function that incentivizes progression toward the sink while penalizing inefficient paths. Trained on diverse network topologies, the framework generalizes effectively to unseen networks. Simulations across 100 to 500 nodes demonstrate near-optimal routing accuracy (over 99% for networks with more than 300 nodes), with minor deviations (1-2 extra hops) in smaller networks having negligible impact on performance. Compared to centralized and flooding-based methods, our approach reduces communication overhead, adapts to topology changes, and enhances scalability and energy efficiency. This work underscores the potential of Q-learning for autonomous, robust routing in resource-constrained IoT networks, offering a scalable alternative to traditional protocols.",
        "authors": [
            "Van-Vi Vo",
            "Tien-Dung Nguyen",
            "Duc-Tai Le",
            "Hyunseung Choo"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.11598v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-29"
    },
    "http://arxiv.org/abs/2511.19438v1": {
        "id": "http://arxiv.org/abs/2511.19438v1",
        "title": "Opt4GPTQ: Co-Optimizing Memory and Computation for 4-bit GPTQ Quantized LLM Inference on Heterogeneous Platforms",
        "link": "http://arxiv.org/abs/2511.19438v1",
        "tags": [
            "quantization",
            "serving",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Optimizes memory and computation for 4-bit GPTQ quantized LLM inference on heterogeneous platforms. Proposes SMB-Opt, VML-Opt, and ILA-Opt in vLLM. Achieves 84.42% higher throughput and 51.35% latency reduction.",
        "abstract": "The increasing adoption of large language model (LLMs) on heterogeneous computing platforms poses significant challenges for achieving high inference efficiency. To address the low inference efficiency of LLMs across diverse heterogeneous platforms, this paper proposes a practical optimization method, Opt4GPTQ, designed for 4-bit GPTQ quantized LLMs inference on heterogeneous AI accelerators. Built upon the vLLM serving system, Opt4GPTQ integrates three platform-level optimization strategies: Shared Memory Buffering optimization (SMB-Opt), which caches data in shared memory and employs single-threaded writes; Vectorized Memory Loading optimization (VML-Opt), which utilizes vectorized memory operations for efficient data loading; and Inline Assembly optimization (ILAOpt), which directly leverages hardware-native vector halfprecision addition and fused multiply-accumulate instructions for efficient execution. Experimental results show that Opt4GPTQ effectively improves inference performance across different models, achieving up to 84.42% throughput improvement and up to 51.35% latency reduction. This work highlights the critical role of platform-level engineering optimizations in enabling efficient LLMs inference on emerging heterogeneous AI acceleration architectures and provides valuable deployment experience and methodologies for future heterogeneous platform adaptation.",
        "authors": [
            "Yaozheng Zhang",
            "Wei Wang",
            "Jie Kong",
            "Jiehan Zhou",
            "Huanqing Cui"
        ],
        "categories": [
            "cs.DC",
            "cs.PF"
        ],
        "submit_date": "2025-10-29"
    },
    "http://arxiv.org/abs/2510.25451v2": {
        "title": "Can Like Attract Like? A Study of Homonymous Gathering in Networks",
        "link": "http://arxiv.org/abs/2510.25451v2",
        "abstract": "A team of mobile agents, starting from distinct nodes of a network, have to meet at the same node and declare that they all met. Agents execute the same algorithm, which they start when activated by an adversary or by an agent entering their initial node. When activated, agents traverse edges of the network in synchronous rounds. Their perception and communication are strictly local. This task, known as gathering, is a central problem in distributed mobile systems. Most prior work focuses on minimizing its time complexity, i.e., the worst-case number of rounds between the start of the earliest agent and the task completion. To break possible symmetries, deterministic solutions typically assume that agents have pairwise distinct IDs, called labels, known only to themselves. But must all labels be pairwise distinct to guarantee deterministic gathering?   We address this question by considering agents that may share the same label. A team L is said to be gatherable if, for every initial setting of L, there is an algorithm that solves gathering. Our contribution is threefold. (1) We give a full characterization of the gatherable teams. (2) We design an algorithm that gathers all of them in poly$(n,\\logλ)$ time, where $n$ (resp. $λ$) is the graph order (resp. the smallest label in L). This algorithm requires the agents to initially share only $O(\\log \\log \\log μ)$ bits of common knowledge, where $μ$ is the largest label multiplicity in L. (3) We show this dependency is almost optimal to get a poly$(n,\\logλ)$-time complexity.   As a by-product, we get the first deterministic poly$(n,\\logλ)$-time algorithm requiring no common knowledge to gather any team when all labels are distinct. Known to be achievable for two-agent teams, extending this to any team size faced a major challenge: termination detection. Our techniques to address it may be of independent interest.",
        "authors": [
            "Stéphane Devismes",
            "Yoann Dieudonné",
            "Arnaud Labourel"
        ],
        "categories": [
            "cs.DC",
            "cs.DS"
        ],
        "id": "http://arxiv.org/abs/2510.25451v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-29"
    },
    "http://arxiv.org/abs/2510.25362v1": {
        "title": "Scheduling Data-Intensive Workloads in Large-Scale Distributed Systems: Trends and Challenges",
        "link": "http://arxiv.org/abs/2510.25362v1",
        "abstract": "With the explosive growth of big data, workloads tend to get more complex and computationally demanding. Such applications are processed on distributed interconnected resources that are becoming larger in scale and computational capacity. Data-intensive applications may have different degrees of parallelism and must effectively exploit data locality. Furthermore, they may impose several Quality of Service requirements, such as time constraints and resilience against failures, as well as other objectives, like energy efficiency. These features of the workloads, as well as the inherent characteristics of the computing resources required to process them, present major challenges that require the employment of effective scheduling techniques. In this chapter, a classification of data-intensive workloads is proposed and an overview of the most commonly used approaches for their scheduling in large-scale distributed systems is given. We present novel strategies that have been proposed in the literature and shed light on open challenges and future directions.",
        "authors": [
            "Georgios L. Stavrinides",
            "Helen D. Karatza"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.25362v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-29"
    },
    "http://arxiv.org/abs/2510.25277v1": {
        "title": "A Privacy-Preserving Ecosystem for Developing Machine Learning Algorithms Using Patient Data: Insights from the TUM.ai Makeathon",
        "link": "http://arxiv.org/abs/2510.25277v1",
        "abstract": "The integration of clinical data offers significant potential for the development of personalized medicine. However, its use is severely restricted by the General Data Protection Regulation (GDPR), especially for small cohorts with rare diseases. High-quality, structured data is essential for the development of predictive medical AI. In this case study, we propose a novel, multi-stage approach to secure AI training: (1) The model is designed on a simulated clinical knowledge graph (cKG). This graph is used exclusively to represent the structural characteristics of the real cKG without revealing any sensitive content. (2) The model is then integrated into the FeatureCloud (FC) federated learning framework, where it is prepared in a single-client configuration within a protected execution environment. (3) Training then takes place within the hospital environment on the real cKG, either under the direct supervision of hospital staff or via a fully automated pipeline controlled by the hospital. (4) Finally, verified evaluation scripts are executed, which only return aggregated performance metrics. This enables immediate performance feedback without sensitive patient data or individual predictions, leaving the clinic. A fundamental element of this approach involves the incorporation of a cKG, which serves to organize multi-omics and patient data within the context of real-world hospital environments. This approach was successfully validated during the TUM.ai Makeathon 2024 (TUMaiM24) challenge set by the Dr. von Hauner Children's Hospital (HCH-LMU): 50 students developed models for patient classification and diagnosis without access to real data. Deploying secure algorithms via federated frameworks, such as the FC framework, could be a practical way of achieving privacy-preserving AI in healthcare.",
        "authors": [
            "Simon Süwer",
            "Mai Khanh Mai",
            "Christoph Klein",
            "Nicola Götzenberger",
            "Denis Dalić",
            "Andreas Maier",
            "Jan Baumbach"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.25277v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-29"
    },
    "http://arxiv.org/abs/2510.25258v1": {
        "id": "http://arxiv.org/abs/2510.25258v1",
        "title": "MoEntwine: Unleashing the Potential of Wafer-scale Chips for Large-scale Expert Parallel Inference",
        "link": "http://arxiv.org/abs/2510.25258v1",
        "tags": [
            "MoE",
            "serving",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-10-29",
        "tldr": "Proposes ER-Mapping and NI-Balancer to optimize MoE inference on wafer-scale chips by co-designing expert and attention layer mappings and overlapping expert migration with idle network links, achieving 62% communication reduction and 39% higher per-device performance than NVL72.",
        "abstract": "As large language models (LLMs) continue to scale up, mixture-of-experts (MoE) has become a common technology in SOTA models. MoE models rely on expert parallelism (EP) to alleviate memory bottleneck, which introduces all-to-all communication to dispatch and combine tokens across devices. However, in widely-adopted GPU clusters, high-overhead cross-node communication makes all-to-all expensive, hindering the adoption of EP. Recently, wafer-scale chips (WSCs) have emerged as a platform integrating numerous devices on a wafer-sized interposer. WSCs provide a unified high-performance network connecting all devices, presenting a promising potential for hosting MoE models. Yet, their network is restricted to a mesh topology, causing imbalanced communication pressure and performance loss. Moreover, the lack of on-wafer disk leads to high-overhead expert migration on the critical path.   To fully unleash this potential, we first propose Entwined Ring Mapping (ER-Mapping), which co-designs the mapping of attention and MoE layers to balance communication pressure and achieve better performance. We find that under ER-Mapping, the distribution of cold and hot links in the attention and MoE layers is complementary. Therefore, to hide the migration overhead, we propose the Non-invasive Balancer (NI-Balancer), which splits a complete expert migration into multiple steps and alternately utilizes the cold links of both layers. Evaluation shows ER-Mapping achieves communication reduction up to 62%. NI-Balancer further delivers 54% and 22% improvements in MoE computation and communication, respectively. Compared with the SOTA NVL72 supernode, the WSC platform delivers an average 39% higher per-device MoE performance owing to its scalability to larger EP.",
        "authors": [
            "Xinru Tang",
            "Jingxiang Hou",
            "Dingcheng Jiang",
            "Taiquan Wei",
            "Jiaxin Liu",
            "Jinyi Deng",
            "Huizheng Wang",
            "Qize Yang",
            "Haoran Shang",
            "Chao Li",
            "Yang Hu",
            "Shouyi Yin"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-10-29"
    },
    "http://arxiv.org/abs/2511.01893v1": {
        "title": "mLR: Scalable Laminography Reconstruction based on Memoization",
        "link": "http://arxiv.org/abs/2511.01893v1",
        "abstract": "ADMM-FFT is an iterative method with high reconstruction accuracy for laminography but suffers from excessive computation time and large memory consumption. We introduce mLR, which employs memoization to replace the time-consuming Fast Fourier Transform (FFT) operations based on an unique observation that similar FFT operations appear in iterations of ADMM-FFT. We introduce a series of techniques to make the application of memoization to ADMM-FFT performance-beneficial and scalable. We also introduce variable offloading to save CPU memory and scale ADMM-FFT across GPUs within and across nodes. Using mLR, we are able to scale ADMM-FFT on an input problem of 2Kx2Kx2K, which is the largest input problem laminography reconstruction has ever worked on with the ADMM-FFT solution on limited memory; mLR brings 52.8% performance improvement on average (up to 65.4%), compared to the original ADMM-FFT.",
        "authors": [
            "Bin Ma",
            "Viktor Nikitin",
            "Xi Wang",
            "Tekin Bicer",
            "Dong Li"
        ],
        "categories": [
            "cs.DC",
            "cs.PF"
        ],
        "id": "http://arxiv.org/abs/2511.01893v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-29"
    },
    "http://arxiv.org/abs/2510.25176v1": {
        "id": "http://arxiv.org/abs/2510.25176v1",
        "title": "Machine Learning and CPU (Central Processing Unit) Scheduling Co-Optimization over a Network of Computing Centers",
        "link": "http://arxiv.org/abs/2510.25176v1",
        "tags": [
            "training",
            "networking",
            "quantization"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Co-optimizes CPU resource allocation and distributed ML training over networked computing nodes. Combines local training with consensus-based resource scheduling and log-quantized communication. Achieves over 50% better cost optimality than existing schedulers.",
        "abstract": "In the rapidly evolving research on artificial intelligence (AI) the demand for fast, computationally efficient, and scalable solutions has increased in recent years. The problem of optimizing the computing resources for distributed machine learning (ML) and optimization is considered in this paper. Given a set of data distributed over a network of computing-nodes/servers, the idea is to optimally assign the CPU (central processing unit) usage while simultaneously training each computing node locally via its own share of data. This formulates the problem as a co-optimization setup to (i) optimize the data processing and (ii) optimally allocate the computing resources. The information-sharing network among the nodes might be time-varying, but with balanced weights to ensure consensus-type convergence of the algorithm. The algorithm is all-time feasible, which implies that the computing resource-demand balance constraint holds at all iterations of the proposed solution. Moreover, the solution allows addressing possible log-scale quantization over the information-sharing channels to exchange log-quantized data. For some example applications, distributed support-vector-machine (SVM) and regression are considered as the ML training models. Results from perturbation theory, along with Lyapunov stability and eigen-spectrum analysis, are used to prove the convergence towards the optimal case. As compared to existing CPU scheduling solutions, the proposed algorithm improves the cost optimality gap by more than $50\\%$.",
        "authors": [
            "Mohammadreza Doostmohammadian",
            "Zulfiya R. Gabidullina",
            "Hamid R. Rabiee"
        ],
        "categories": [
            "cs.LG",
            "cs.DC",
            "cs.MA",
            "eess.SY",
            "math.OC"
        ],
        "submit_date": "2025-10-29"
    },
    "http://arxiv.org/abs/2510.25170v1": {
        "id": "http://arxiv.org/abs/2510.25170v1",
        "title": "Multi-Resolution Model Fusion for Accelerating the Convolutional Neural Network Training",
        "link": "http://arxiv.org/abs/2510.25170v1",
        "tags": [
            "training",
            "kernel",
            "sparse"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Introduces Multi-Resolution Model Fusion (MRMF) to accelerate CNN training time. Trains initial models on reduced-resolution data, then fuses and refines them with original-resolution data. Achieves up to 47% training time reduction while maintaining accuracy on CosmoFlow and Neuron Inverter benchmarks.",
        "abstract": "Neural networks are rapidly gaining popularity in scientific research, but training the models is often very time-consuming. Particularly when the training data samples are large high-dimensional arrays, efficient training methodologies that can reduce the computational costs are crucial. To reduce the training cost, we propose a Multi-Resolution Model Fusion (MRMF) method that combines models trained on reduced-resolution data and then refined with data in the original resolution. We demonstrate that these reduced-resolution models and datasets could be generated quickly. More importantly, the proposed approach reduces the training time by speeding up the model convergence in each fusion stage before switching to the final stage of finetuning with data in its original resolution. This strategy ensures the final model retains high-resolution insights while benefiting from the computational efficiency of lower-resolution training. Our experiment results demonstrate that the multi-resolution model fusion method can significantly reduce end-to-end training time while maintaining the same model accuracy. Evaluated using two real-world scientific applications, CosmoFlow and Neuron Inverter, the proposed method improves the training time by up to 47% and 44%, respectively, as compared to the original resolution training, while the model accuracy is not affected.",
        "authors": [
            "Kewei Wang",
            "Claire Songhyun Lee",
            "Sunwoo Lee",
            "Vishu Gupta",
            "Jan Balewski",
            "Alex Sim",
            "Peter Nugent",
            "Ankit Agrawal",
            "Alok Choudhary",
            "Kesheng Wu",
            "Wei-keng Liao"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-10-29"
    },
    "http://arxiv.org/abs/2510.25144v1": {
        "title": "Timing Games in Responsive Consensus Protocols",
        "link": "http://arxiv.org/abs/2510.25144v1",
        "abstract": "Optimistic responsiveness -- the ability of a consensus protocol to operate at the speed of the network -- is widely used in consensus protocol design to optimize latency and throughput. However, blockchain applications incentivize validators to play timing games by strategically delaying their proposals, since increased block time correlates with greater rewards. Consequently, it may appear that responsiveness (even under optimistic conditions) is impossible in blockchain protocols. In this work, we develop a model of timing games in responsive consensus protocols and find a prisoner's dilemma structure, where cooperation (proposing promptly) is in the validators' best interest, but individual incentives encourage validators to delay proposals selfishly. To attain desirable equilibria, we introduce dynamic block rewards that decrease with round time to explicitly incentivize faster proposals. Delays are measured through a voting mechanism, where other validators vote on the current leader's round time. By carefully setting the protocol parameters, the voting mechanism allows validators to coordinate and reach the cooperative equilibrium, benefiting all through a higher rate-of-reward. Thus, instead of responsiveness being an unattainable property due to timing games, we show that responsiveness itself can promote faster block proposals. One consequence of moving from a static to dynamic block reward is that validator utilities become more sensitive to latency, worsening the gap between the best- and worst-connected validators. Our analysis shows, however, that this effect is minor in both theoretical latency models and simulations based on real-world networks.",
        "authors": [
            "Kaya Alpturer",
            "Kushal Babel",
            "Aditya Saraf"
        ],
        "categories": [
            "cs.GT",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.25144v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-29"
    },
    "http://arxiv.org/abs/2510.25112v1": {
        "title": "The Singularity Theory of Concurrent Programs: A Topological Characterization and Detection of Deadlocks and Livelocks",
        "link": "http://arxiv.org/abs/2510.25112v1",
        "abstract": "This paper introduces a novel paradigm for the analysis and verification of concurrent programs -- the Singularity Theory. We model the execution space of a concurrent program as a branched topological space, where program states are points and state transitions are paths. Within this framework, we characterize deadlocks as attractors and livelocks as non-contractible loops in the execution space. By employing tools from algebraic topology, particularly homotopy and homology groups, we define a series of concurrent topological invariants to systematically detect and classify these concurrent \"singularities\" without exhaustively traversing all states. This work aims to establish a geometric and topological foundation for concurrent program verification, transcending the limitations of traditional model checking.",
        "authors": [
            "Di Zhang"
        ],
        "categories": [
            "cs.PL",
            "cs.DC",
            "cs.LO",
            "math.AT"
        ],
        "id": "http://arxiv.org/abs/2510.25112v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-29"
    },
    "http://arxiv.org/abs/2510.24943v1": {
        "title": "Radar DataTree: A FAIR and Cloud-Native Framework for Scalable Weather Radar Archives",
        "link": "http://arxiv.org/abs/2510.24943v1",
        "abstract": "We introduce Radar DataTree, the first dataset-level framework that extends the WMO FM-301 standard from individual radar volume scans to time-resolved, analysis-ready archives. Weather radar data are among the most scientifically valuable yet structurally underutilized Earth observation datasets. Despite widespread public availability, radar archives remain fragmented, vendor-specific, and poorly aligned with FAIR (Findable, Accessible, Interoperable, Reusable) principles, hindering large-scale research, reproducibility, and cloud-native computation. Radar DataTree addresses these limitations with a scalable, open-source architecture that transforms operational radar archives into FAIR-compliant, cloud-optimized datasets. Built on the FM-301/CfRadial 2.1 standard and implemented using xarray DataTree, Radar DataTree organizes radar volume scans as hierarchical, metadata-rich structures and serializes them to Zarr for scalable analysis. Coupled with Icechunk for ACID-compliant storage and versioning, this architecture enables efficient, parallel computation across thousands of radar scans with minimal preprocessing. We demonstrate significant performance gains in case studies including Quasi-Vertical Profile (QVP) and precipitation accumulation workflows, and release all tools and datasets openly via the Raw2Zarr repository. This work contributes a reproducible and extensible foundation for radar data stewardship, high-performance geoscience, and AI-ready weather infrastructure.",
        "authors": [
            "Alfonso Ladino-Rincon",
            "Stephen W. Nesbitt"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.24943v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-28"
    },
    "http://arxiv.org/abs/2510.24547v1": {
        "title": "In-Situ High Performance Visualization for Astronomy & Cosmology",
        "link": "http://arxiv.org/abs/2510.24547v1",
        "abstract": "The Astronomy & Cosmology (A&C) community is presently witnessing an unprecedented growth in the quality and quantity of data coming from simulations and observations. Writing results of numerical simulations to disk files has long been a bottleneck in high-performance computing. To access effectively and extract the scientific content of such large-scale data sets appropriate tools and techniques are needed. This is especially true for visualization tools, where petascale data size problems cannot be visualized without some data filtering, which reduces either the resolution or the amount of data volume managed by the visualization tool.   A solution to this problem is to run the analysis and visualization concurrently (in-situ) with the simulation and bypass the storage of the full results. In particular we use Hecuba, a framework offering a highly distributed database to stream A\\&C simulation data for on-line visualization. We will demonstrate the Hecuba platform integration with the Changa high performant cosmological simulator and the in-situ visualization of its N-body results with the ParaView and VisIVO tools.",
        "authors": [
            "Nicola Tuccari",
            "Eva Sciacca",
            "Yolanda Becerra",
            "Enric Sosa Cintero",
            "Robert Wissing",
            "Sijing Shen",
            "Emiliano Tramontana"
        ],
        "categories": [
            "astro-ph.IM",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.24547v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-28"
    },
    "http://arxiv.org/abs/2510.24545v1": {
        "title": "Exascale In-situ visualization for Astronomy & Cosmology",
        "link": "http://arxiv.org/abs/2510.24545v1",
        "abstract": "Modern simulations and observations in Astronomy & Cosmology (A&C) produce massively large data volumes, posing significant challenges for storage, access and data analysis. A long-standing bottleneck in high-performance computing, especially now in the exascale era, has been the requirement to write these large datasets to disks, which limits the performance. A promising solution to this challenge is in-situ processing, where analysis and visualization are performed concurrently with the simulation itself, bypassing the storage of the simulation data. In this work, we present new results from an approach for in-situ processing based on Hecuba, a framework that provides a highly distributed database for streaming A&C simulation data directly into the visualization pipeline to make possible on-line visualization. By integrating Hecuba with the high-performance cosmological simulator ChaNGa, we enable real-time, in-situ visualization of N-body simulation results using tools such as ParaView and VisIVO.",
        "authors": [
            "Nicola Tuccari",
            "Eva Sciacca",
            "Yolanda Becerra",
            "Enric Sosa Cintero",
            "Emiliano Tramontana"
        ],
        "categories": [
            "astro-ph.IM",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.24545v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-28"
    },
    "http://arxiv.org/abs/2510.24503v1": {
        "title": "Local Performance vs. Out-of-Distribution Generalization: An Empirical Analysis of Personalized Federated Learning in Heterogeneous Data Environments",
        "link": "http://arxiv.org/abs/2510.24503v1",
        "abstract": "In the context of Federated Learning with heterogeneous data environments, local models tend to converge to their own local model optima during local training steps, deviating from the overall data distributions. Aggregation of these local updates, e.g., with FedAvg, often does not align with the global model optimum (client drift), resulting in an update that is suboptimal for most clients. Personalized Federated Learning approaches address this challenge by exclusively focusing on the average local performances of clients' models on their own data distribution. Generalization to out-of-distribution samples, which is a substantial benefit of FedAvg and represents a significant component of robustness, appears to be inadequately incorporated into the assessment and evaluation processes. This study involves a thorough evaluation of Federated Learning approaches, encompassing both their local performance and their generalization capabilities. Therefore, we examine different stages within a single communication round to enable a more nuanced understanding of the considered metrics. Furthermore, we propose and incorporate a modified approach of FedAvg, designated as Federated Learning with Individualized Updates (FLIU), extending the algorithm by a straightforward individualization step with an adaptive personalization factor. We evaluate and compare the approaches empirically using MNIST and CIFAR-10 under various distributional conditions, including benchmark IID and pathological non-IID, as well as additional novel test environments with Dirichlet distribution specifically developed to stress the algorithms on complex data heterogeneity.",
        "authors": [
            "Mortesa Hussaini",
            "Jan Theiß",
            "Anthony Stein"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "cs.DC",
            "cs.MA"
        ],
        "id": "http://arxiv.org/abs/2510.24503v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-28"
    },
    "http://arxiv.org/abs/2510.24452v1": {
        "title": "ARIMA_PLUS: Large-scale, Accurate, Automatic and Interpretable In-Database Time Series Forecasting and Anomaly Detection in Google BigQuery",
        "link": "http://arxiv.org/abs/2510.24452v1",
        "abstract": "Time series forecasting and anomaly detection are common tasks for practitioners in industries such as retail, manufacturing, advertising and energy. Two unique challenges stand out: (1) efficiently and accurately forecasting time series or detecting anomalies in large volumes automatically; and (2) ensuring interpretability of results to effectively incorporate business insights. We present ARIMA_PLUS, a novel framework to overcome these two challenges by a unique combination of (a) accurate and interpretable time series models and (b) scalable and fully managed system infrastructure. The model has a sequential and modular structure to handle different components of the time series, including holiday effects, seasonality, trend, and anomalies, which enables high interpretability of the results. Novel enhancements are made to each module, and a unified framework is established to address both forecasting and anomaly detection tasks simultaneously. In terms of accuracy, its comprehensive benchmark on the 42 public datasets in the Monash forecasting repository shows superior performance over not only well-established statistical alternatives (such as ETS, ARIMA, TBATS, Prophet) but also newer neural network models (such as DeepAR, N-BEATS, PatchTST, TimeMixer). In terms of infrastructure, it is directly built into the query engine of BigQuery in Google Cloud. It uses a simple SQL interface and automates tedious technicalities such as data cleaning and model selection. It automatically scales with managed cloud computational and storage resources, making it possible to forecast 100 million time series using only 1.5 hours with a throughput of more than 18000 time series per second. In terms of interpretability, we present several case studies to demonstrate time series insights it generates and customizability it offers.",
        "authors": [
            "Xi Cheng",
            "Weijie Shen",
            "Haoming Chen",
            "Chaoyi Shen",
            "Jean Ortega",
            "Jiashang Liu",
            "Steve Thomas",
            "Honglin Zheng",
            "Haoyun Wu",
            "Yuxiang Li",
            "Casey Lichtendahl",
            "Jenny Ortiz",
            "Gang Liu",
            "Haiyang Qi",
            "Omid Fatemieh",
            "Chris Fry",
            "Jing Jing Long"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "id": "http://arxiv.org/abs/2510.24452v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-28"
    },
    "http://arxiv.org/abs/2510.24307v2": {
        "title": "Odyssey: An End-to-End System for Pareto-Optimal Serverless Query Processing",
        "link": "http://arxiv.org/abs/2510.24307v2",
        "abstract": "Running data analytics queries on serverless (FaaS) workers has been shown to be cost- and performance-efficient for a variety of real-world scenarios, including intermittent query arrival patterns, sudden load spikes and management challenges that afflict managed VM clusters. Alas, existing serverless data analytics works focus primarily on the serverless execution engine and assume the existence of a \"good\" query execution plan or rely on user guidance to construct such a plan. Meanwhile, even simple analytics queries on serverless have a huge space of possible plans, with vast differences in both performance and cost among plans.   This paper introduces Odyssey, an end-to-end serverless-native data analytics pipeline that integrates a query planner, cost model and execution engine. Odyssey automatically generates and evaluates serverless query plans, utilizing state space pruning heuristics and a novel search algorithm to identify Pareto-optimal plans that balance cost and performance with low latency even for complex queries. Our evaluations demonstrate that Odyssey accurately predicts both monetary cost and latency, and consistently outperforms AWS Athena on cost and/or latency.",
        "authors": [
            "Shyam Jesalpura",
            "Shengda Zhu",
            "Amir Shaikhha",
            "Antonio Barbalace",
            "Boris Grot"
        ],
        "categories": [
            "cs.DB",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.24307v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-28"
    },
    "http://arxiv.org/abs/2510.24205v1": {
        "title": "CoMPSeT: A Framework for Comparing Multiparty Session Types",
        "link": "http://arxiv.org/abs/2510.24205v1",
        "abstract": "Concurrent systems are often complex and difficult to design. Choreographic languages, such as Multiparty Session Types (MPST), allow the description of global protocols of interactions by capturing valid patterns of interactions between participants. Many variations of MPST exist, each one with its rather specific features and idiosyncrasies. Here we propose a tool (CoMPSeT) that provides clearer insights over different features in existing MPST. We select a representative set of MPST examples and provide mechanisms to combine different features and to animate and compare the semantics of concrete examples. CoMPSeT is open-source, compiled into JavaScript, and can be directly executed from any browser, becoming useful both for researchers who want to better understand the landscape of MPST and for teachers who want to explain global choreographies.",
        "authors": [
            "Telmo Ribeiro",
            "José Proença",
            "Mário Florido"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.24205v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-28"
    },
    "http://arxiv.org/abs/2510.24203v1": {
        "title": "Fault-Tolerant Multiparty Session Types with Global Escape Loops",
        "link": "http://arxiv.org/abs/2510.24203v1",
        "abstract": "Multiparty session types are designed to abstractly capture the structure of communication protocols and verify behavioural properties. One important such property is progress, i.e., the absence of deadlock. Distributed algorithms often resemble multiparty communication protocols. But proving their properties, in particular termination that is closely related to progress, can be elaborate. Since distributed algorithms are often designed to cope with faults, a first step towards using session types to verify distributed algorithms is to integrate fault-tolerance.   We extend FTMPST (a version of fault-tolerant multiparty session types with failure patterns to represent system requirements for system failures such as unreliable communication and process crashes) by a novel, fault-tolerant loop construct with global escapes that does not require global coordination. Each process runs its own local version of the loop. If a process finds a solution to the considered problem, it does not only terminate its own loop but also informs the other participants via exit-messages. Upon receiving an exit-message, a process immediately terminates its algorithm. To increase efficiency and model standard fault-tolerant algorithms, these messages are non-blocking, i.e., a process may continue until a possibly delayed exit-message is received. To illustrate our approach, we analyse a variant of the well-known rotating coordinator algorithm by Chandra and Toueg.",
        "authors": [
            "Lukas Bartl",
            "Julian Linne",
            "Kirstin Peters"
        ],
        "categories": [
            "cs.LO",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.24203v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-28"
    },
    "http://arxiv.org/abs/2510.24200v1": {
        "title": "SPEAR++: Scaling Gradient Inversion via Sparsely-Used Dictionary Learning",
        "link": "http://arxiv.org/abs/2510.24200v1",
        "abstract": "Federated Learning has seen an increased deployment in real-world scenarios recently, as it enables the distributed training of machine learning models without explicit data sharing between individual clients. Yet, the introduction of the so-called gradient inversion attacks has fundamentally challenged its privacy-preserving properties. Unfortunately, as these attacks mostly rely on direct data optimization without any formal guarantees, the vulnerability of real-world systems remains in dispute and requires tedious testing for each new federated deployment. To overcome these issues, recently the SPEAR attack was introduced, which is based on a theoretical analysis of the gradients of linear layers with ReLU activations. While SPEAR is an important theoretical breakthrough, the attack's practicality was severely limited by its exponential runtime in the batch size b. In this work, we fill this gap by applying State-of-the-Art techniques from Sparsely-Used Dictionary Learning to make the problem of gradient inversion on linear layers with ReLU activations tractable. Our experiments demonstrate that our new attack, SPEAR++, retains all desirable properties of SPEAR, such as robustness to DP noise and FedAvg aggregation, while being applicable to 10x bigger batch sizes.",
        "authors": [
            "Alexander Bakarsky",
            "Dimitar I. Dimitrov",
            "Maximilian Baader",
            "Martin Vechev"
        ],
        "categories": [
            "cs.LG",
            "cs.CR",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.24200v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-28"
    },
    "http://arxiv.org/abs/2510.24175v1": {
        "title": "Towards Exascale Computing for Astrophysical Simulation Leveraging the Leonardo EuroHPC System",
        "link": "http://arxiv.org/abs/2510.24175v1",
        "abstract": "Developing and redesigning astrophysical, cosmological, and space plasma numerical codes for existing and next-generation accelerators is critical for enabling large-scale simulations. To address these challenges, the SPACE Center of Excellence (SPACE-CoE) fosters collaboration between scientists, code developers, and high-performance computing experts to optimize applications for the exascale era. This paper presents our strategy and initial results on the Leonardo system at CINECA for three flagship codes, namely gPLUTO, OpenGadget3 and iPIC3D, using profiling tools to analyze performance on single and multiple nodes. Preliminary tests show all three codes scale efficiently, reaching 80% scalability up to 1,024 GPUs.",
        "authors": [
            "Nitin Shukla",
            "Alessandro Romeo",
            "Caterina Caravita",
            "Michael Redenti",
            "Radim Vavrik",
            "Lubomir Riha",
            "Andrea Mignone",
            "Marco Rossazza",
            "Stefano Truzzi",
            "Luca Tornatore",
            "Antonio Ragagnin",
            "Tiago Castro",
            "Geray S. Karademir",
            "Klaus Dolag",
            "Pranab J. Deka",
            "Fabio Bacchini",
            "Rostislav-Paul Wilhelm",
            "Daniele Gregori",
            "Elisabetta Boella"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.24175v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-28"
    },
    "http://arxiv.org/abs/2510.24155v2": {
        "title": "Distributed Stochastic Momentum Tracking with Local Updates: Achieving Optimal Communication and Iteration Complexities",
        "link": "http://arxiv.org/abs/2510.24155v2",
        "abstract": "We propose Local Momentum Tracking (LMT), a novel distributed stochastic gradient method for solving distributed optimization problems over networks. To reduce communication overhead, LMT enables each agent to perform multiple local updates between consecutive communication rounds. Specifically, LMT integrates local updates with the momentum tracking strategy and the Loopless Chebyshev Acceleration (LCA) technique. We demonstrate that LMT achieves linear speedup with respect to the number of local updates as well as the number of agents for minimizing smooth objective functions with and without the Polyak-Łojasiewicz (PL) condition. Notably, with sufficiently many local updates $Q\\geq Q^*$, LMT attains the optimal communication complexity. For a moderate number of local updates $Q\\in[1,Q^*]$, LMT achieves the optimal iteration complexity. To our knowledge, LMT is the first distributed stochastic gradient method with local updates that enjoys such properties.",
        "authors": [
            "Kun Huang",
            "Shi Pu"
        ],
        "categories": [
            "math.OC",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.24155v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-28"
    },
    "http://arxiv.org/abs/2510.23993v2": {
        "title": "A GPU-based Compressible Combustion Solver for Applications Exhibiting Disparate Space and Time Scales",
        "link": "http://arxiv.org/abs/2510.23993v2",
        "abstract": "High-speed chemically active flows present significant computational challenges due to their disparate space and time scales, where stiff chemistry often dominates simulation time. While modern supercomputing scientific codes achieve exascale performance by leveraging graphics processing units (GPUs), existing GPU-based compressible combustion solvers face critical limitations in memory management, load balancing, and handling the highly localized nature of chemical reactions. To this end, we present a high-performance compressible reacting flow solver built on the AMReX framework and optimized for multi-GPU settings. Our approach addresses three GPU performance bottlenecks: memory access patterns through column-major storage optimization, computational workload variability via a bulk-sparse integration strategy for chemical kinetics, and multi-GPU load distribution for adaptive mesh refinement applications. The solver adapts existing matrix-based chemical kinetics formulations to multigrid contexts. Using representative combustion applications including hydrogen-air detonations and jet in supersonic crossflow configurations, we demonstrate $2-5\\times$ performance improvements over initial GPU implementations with near-ideal weak scaling across $1-96$ NVIDIA H100 GPUs. Roofline analysis reveals substantial improvements in arithmetic intensity for both convection ($\\sim 10 \\times$) and chemistry ($\\sim 4 \\times$) routines, confirming efficient utilization of GPU memory bandwidth and computational resources.",
        "authors": [
            "Anthony Carreon",
            "Jagmohan Singh",
            "Shivank Sharma",
            "Shuzhi Zhang",
            "Venkat Raman"
        ],
        "categories": [
            "cs.DC",
            "cs.CE"
        ],
        "id": "http://arxiv.org/abs/2510.23993v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-28"
    },
    "http://arxiv.org/abs/2511.00038v1": {
        "id": "http://arxiv.org/abs/2511.00038v1",
        "title": "AeroResQ: Edge-Accelerated UAV Framework for Scalable, Resilient and Collaborative Escape Route Planning in Wildfire Scenarios",
        "link": "http://arxiv.org/abs/2511.00038v1",
        "tags": [
            "edge",
            "training",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes AeroResQ, an edge-accelerated UAV framework for real-time escape route planning in wildfires. Uses multi-layer orchestration with service drones running DNNs on edge accelerators and coordinator drones for path planning. Achieves end-to-end latency <=500 ms and over 98% task reassignment success.",
        "abstract": "Drone fleets equipped with onboard cameras, computer vision, and Deep Neural Network (DNN) models present a powerful paradigm for real-time spatio-temporal decision-making. In wildfire response, such drones play a pivotal role in monitoring fire dynamics, supporting firefighter coordination, and facilitating safe evacuation. In this paper, we introduce AeroResQ, an edge-accelerated UAV framework designed for scalable, resilient, and collaborative escape route planning during wildfire scenarios. AeroResQ adopts a multi-layer orchestration architecture comprising service drones (SDs) and coordinator drones (CDs), each performing specialized roles. SDs survey fire-affected areas, detect stranded individuals using onboard edge accelerators running fire detection and human pose identification DNN models, and issue requests for assistance. CDs, equipped with lightweight data stores such as Apache IoTDB, dynamically generate optimal ground escape routes and monitor firefighter movements along these routes. The framework proposes a collaborative path-planning approach based on a weighted A* search algorithm, where CDs compute context-aware escape paths. AeroResQ further incorporates intelligent load-balancing and resilience mechanisms: CD failures trigger automated data redistribution across IoTDB replicas, while SD failures initiate geo-fenced re-partitioning and reassignment of spatial workloads to operational SDs. We evaluate AeroResQ using realistic wildfire emulated setup modeled on recent Southern California wildfires. Experimental results demonstrate that AeroResQ achieves a nominal end-to-end latency of <=500ms, much below the 2s request interval, while maintaining over 98% successful task reassignment and completion, underscoring its feasibility for real-time, on-field deployment in emergency response and firefighter safety operations.",
        "authors": [
            "Suman Raj",
            "Radhika Mittal",
            "Rajiv Mayani",
            "Pawel Zuk",
            "Anirban Mandal",
            "Michael Zink",
            "Yogesh Simmhan",
            "Ewa Deelman"
        ],
        "categories": [
            "cs.DC",
            "cs.RO"
        ],
        "submit_date": "2025-10-27"
    },
    "http://arxiv.org/abs/2510.23931v1": {
        "title": "Differential Privacy: Gradient Leakage Attacks in Federated Learning Environments",
        "link": "http://arxiv.org/abs/2510.23931v1",
        "abstract": "Federated Learning (FL) allows for the training of Machine Learning models in a collaborative manner without the need to share sensitive data. However, it remains vulnerable to Gradient Leakage Attacks (GLAs), which can reveal private information from the shared model updates. In this work, we investigate the effectiveness of Differential Privacy (DP) mechanisms - specifically, DP-SGD and a variant based on explicit regularization (PDP-SGD) - as defenses against GLAs. To this end, we evaluate the performance of several computer vision models trained under varying privacy levels on a simple classification task, and then analyze the quality of private data reconstructions obtained from the intercepted gradients in a simulated FL environment. Our results demonstrate that DP-SGD significantly mitigates the risk of gradient leakage attacks, albeit with a moderate trade-off in model utility. In contrast, PDP-SGD maintains strong classification performance but proves ineffective as a practical defense against reconstruction attacks. These findings highlight the importance of empirically evaluating privacy mechanisms beyond their theoretical guarantees, particularly in distributed learning scenarios where information leakage may represent an unassumable critical threat to data security and privacy.",
        "authors": [
            "Miguel Fernandez-de-Retana",
            "Unai Zulaika",
            "Rubén Sánchez-Corcuera",
            "Aitor Almeida"
        ],
        "categories": [
            "cs.LG",
            "cs.CR",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.23931v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-27"
    },
    "http://arxiv.org/abs/2510.23911v1": {
        "title": "The SAP Cloud Infrastructure Dataset: A Reality Check of Scheduling and Placement of VMs in Cloud Computing",
        "link": "http://arxiv.org/abs/2510.23911v1",
        "abstract": "Allocating resources in a distributed environment is a fundamental challenge. In this paper, we analyze the scheduling and placement of virtual machines (VMs) in the cloud platform of SAP, the world's largest enterprise resource planning software vendor. Based on data from roughly 1,800 hypervisors and 48,000 VMs within a 30-day observation period, we highlight potential improvements for workload management. The data was measured through observability tooling that tracks resource usage and performance metrics across the entire infrastructure. In contrast to existing datasets, ours uniquely offers fine-grained time-series telemetry data of fully virtualized enterprise-level workloads from both long-running and memory-intensive SAP S/4HANA and diverse, general-purpose applications. Our key findings include several suboptimal scheduling situations, such as CPU resource contention exceeding 40%, CPU ready times of up to 220 seconds, significantly imbalanced compute hosts with a maximum CPU~utilization on intra-building block hosts of up to 99%, and overprovisioned CPU and memory resources resulting into over 80% of VMs using less than 70% of the provided resources. Bolstered by these findings, we derive requirements for the design and implementation of novel placement and scheduling algorithms and provide guidance to optimize resource allocations. We make the full dataset used in this study publicly available to enable data-driven evaluations of scheduling approaches for large-scale cloud infrastructures in future research.",
        "authors": [
            "Arno Uhlig",
            "Iris Braun",
            "Matthias Wählisch"
        ],
        "categories": [
            "cs.DC",
            "cs.PF"
        ],
        "id": "http://arxiv.org/abs/2510.23911v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-27"
    },
    "http://arxiv.org/abs/2511.00037v1": {
        "title": "Benchmarking Federated Learning Frameworks for Medical Imaging Deployment: A Comparative Study of NVIDIA FLARE, Flower, and Owkin Substra",
        "link": "http://arxiv.org/abs/2511.00037v1",
        "abstract": "Federated Learning (FL) has emerged as a transformative paradigm in medical AI, enabling collaborative model training across institutions without direct data sharing. This study benchmarks three prominent FL frameworks NVIDIA FLARE, Flower, and Owkin Substra to evaluate their suitability for medical imaging applications in real-world settings. Using the PathMNIST dataset, we assess model performance, convergence efficiency, communication overhead, scalability, and developer experience. Results indicate that NVIDIA FLARE offers superior production scalability, Flower provides flexibility for prototyping and academic research, and Owkin Substra demonstrates exceptional privacy and compliance features. Each framework exhibits strengths optimized for distinct use cases, emphasizing their relevance to practical deployment in healthcare environments.",
        "authors": [
            "Riya Gupta",
            "Alexander Chowdhury",
            "Sahil Nalawade"
        ],
        "categories": [
            "cs.CV",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.00037v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-27"
    },
    "http://arxiv.org/abs/2510.23503v1": {
        "id": "http://arxiv.org/abs/2510.23503v1",
        "title": "Bayes-Split-Edge: Bayesian Optimization for Constrained Collaborative Inference in Wireless Edge Systems",
        "link": "http://arxiv.org/abs/2510.23503v1",
        "tags": [
            "edge",
            "offloading",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-10-27",
        "tldr": "Proposes Bayes-Split-Edge, a Bayesian optimization framework for jointly optimizing neural network split points and transmission power in edge-served inference to meet energy and latency constraints. Achieves 2.4x reduction in evaluation cost and near-linear convergence with as few as 20 function evaluations.",
        "abstract": "Mobile edge devices (e.g., AR/VR headsets) typically need to complete timely inference tasks while operating with limited on-board computing and energy resources. In this paper, we investigate the problem of collaborative inference in wireless edge networks, where energy-constrained edge devices aim to complete inference tasks within given deadlines. These tasks are carried out using neural networks, and the edge device seeks to optimize inference performance under energy and delay constraints. The inference process can be split between the edge device and an edge server, thereby achieving collaborative inference over wireless networks. We formulate an inference utility optimization problem subject to energy and delay constraints, and propose a novel solution called Bayes-Split-Edge, which leverages Bayesian optimization for collaborative split inference over wireless edge networks. Our solution jointly optimizes the transmission power and the neural network split point. The Bayes-Split-Edge framework incorporates a novel hybrid acquisition function that balances inference task utility, sample efficiency, and constraint violation penalties. We evaluate our approach using the VGG19 model on the ImageNet-Mini dataset, and Resnet101 on Tiny-ImageNet, and real-world mMobile wireless channel datasets. Numerical results demonstrate that Bayes-Split-Edge achieves up to 2.4x reduction in evaluation cost compared to standard Bayesian optimization and achieves near-linear convergence. It also outperforms several baselines, including CMA-ES, DIRECT, exhaustive search, and Proximal Policy Optimization (PPO), while matching exhaustive search performance under tight constraints. These results confirm that the proposed framework provides a sample-efficient solution requiring maximum 20 function evaluations and constraint-aware optimization for wireless split inference in edge computing systems.",
        "authors": [
            "Fatemeh Zahra Safaeipour",
            "Jacob Chakareski",
            "Morteza Hashemi"
        ],
        "categories": [
            "cs.DC",
            "cs.LG",
            "eess.SP"
        ],
        "submit_date": "2025-10-27"
    },
    "http://arxiv.org/abs/2510.23408v1": {
        "title": "AutoStreamPipe: LLM Assisted Automatic Generation of Data Stream Processing Pipelines",
        "link": "http://arxiv.org/abs/2510.23408v1",
        "abstract": "Data pipelines are essential in stream processing as they enable the efficient collection, processing, and delivery of real-time data, supporting rapid data analysis. In this paper, we present AutoStreamPipe, a novel framework that employs Large Language Models (LLMs) to automate the design, generation, and deployment of stream processing pipelines. AutoStreamPipe bridges the semantic gap between high-level user intent and platform-specific implementations across distributed stream processing systems for structured multi-agent reasoning by integrating a Hypergraph of Thoughts (HGoT) as an extended version of GoT. AutoStreamPipe combines resilient execution strategies, advanced query analysis, and HGoT to deliver pipelines with good accuracy. Experimental evaluations on diverse pipelines demonstrate that AutoStreamPipe significantly reduces development time (x6.3) and error rates (x5.19), as measured by a novel Error-Free Score (EFS), compared to LLM code-generation methods.",
        "authors": [
            "Abolfazl Younesi",
            "Zahra Najafabadi Samani",
            "Thomas Fahringer"
        ],
        "categories": [
            "cs.AI",
            "cs.DC",
            "cs.ET",
            "cs.LG",
            "cs.MA"
        ],
        "id": "http://arxiv.org/abs/2510.23408v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-27"
    },
    "http://arxiv.org/abs/2510.23330v1": {
        "title": "The First Star-by-star $N$-body/Hydrodynamics Simulation of Our Galaxy Coupling with a Surrogate Model",
        "link": "http://arxiv.org/abs/2510.23330v1",
        "abstract": "A major goal of computational astrophysics is to simulate the Milky Way Galaxy with sufficient resolution down to individual stars. However, the scaling fails due to some small-scale, short-timescale phenomena, such as supernova explosions. We have developed a novel integration scheme of $N$-body/hydrodynamics simulations working with machine learning. This approach bypasses the short timesteps caused by supernova explosions using a surrogate model, thereby improving scalability. With this method, we reached 300 billion particles using 148,900 nodes, equivalent to 7,147,200 CPU cores, breaking through the billion-particle barrier currently faced by state-of-the-art simulations. This resolution allows us to perform the first star-by-star galaxy simulation, which resolves individual stars in the Milky Way Galaxy. The performance scales over $10^4$ CPU cores, an upper limit in the current state-of-the-art simulations using both A64FX and X86-64 processors and NVIDIA CUDA GPUs.",
        "authors": [
            "Keiya Hirashima",
            "Michiko S. Fujii",
            "Takayuki R. Saitoh",
            "Naoto Harada",
            "Kentaro Nomura",
            "Kohji Yoshikawa",
            "Yutaka Hirai",
            "Tetsuro Asano",
            "Kana Moriwaki",
            "Masaki Iwasawa",
            "Takashi Okamoto",
            "Junichiro Makino"
        ],
        "categories": [
            "astro-ph.GA",
            "cs.DC",
            "cs.LG",
            "physics.comp-ph"
        ],
        "id": "http://arxiv.org/abs/2510.23330v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-27"
    },
    "http://arxiv.org/abs/2510.23679v1": {
        "title": "PanDelos-plus: A parallel algorithm for computing sequence homology in pangenomic analysis",
        "link": "http://arxiv.org/abs/2510.23679v1",
        "abstract": "The identification of homologous gene families across multiple genomes is a central task in bacterial pangenomics traditionally requiring computationally demanding all-against-all comparisons. PanDelos addresses this challenge with an alignment-free and parameter-free approach based on k-mer profiles, combining high speed, ease of use, and competitive accuracy with state-of-the-art methods. However, the increasing availability of genomic data requires tools that can scale efficiently to larger datasets. To address this need, we present PanDelos-plus, a fully parallel, gene-centric redesign of PanDelos. The algorithm parallelizes the most computationally intensive phases (Best Hit detection and Bidirectional Best Hit extraction) through data decomposition and a thread pool strategy, while employing lightweight data structures to reduce memory usage. Benchmarks on synthetic datasets show that PanDelos-plus achieves up to 14x faster execution and reduces memory usage by up to 96%, while maintaining accuracy. These improvements enable population-scale comparative genomics to be performed on standard multicore workstations, making large-scale bacterial pangenome analysis accessible for routine use in everyday research.",
        "authors": [
            "Simone Colli",
            "Emiliano Maresi",
            "Vincenzo Bonnici"
        ],
        "categories": [
            "q-bio.GN",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.23679v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-27"
    },
    "http://arxiv.org/abs/2510.23053v2": {
        "title": "AirFed: A Federated Graph-Enhanced Multi-Agent Reinforcement Learning Framework for Multi-UAV Cooperative Mobile Edge Computing",
        "link": "http://arxiv.org/abs/2510.23053v2",
        "abstract": "Multiple Unmanned Aerial Vehicles (UAVs) cooperative Mobile Edge Computing (MEC) systems face critical challenges in coordinating trajectory planning, task offloading, and resource allocation while ensuring Quality of Service (QoS) under dynamic and uncertain environments. Existing approaches suffer from limited scalability, slow convergence, and inefficient knowledge sharing among UAVs, particularly when handling large-scale IoT device deployments with stringent deadline constraints. This paper proposes AirFed, a novel federated graph-enhanced multi-agent reinforcement learning framework that addresses these challenges through three key innovations. First, we design dual-layer dynamic Graph Attention Networks (GATs) that explicitly model spatial-temporal dependencies among UAVs and IoT devices, capturing both service relationships and collaborative interactions within the network topology. Second, we develop a dual-Actor single-Critic architecture that jointly optimizes continuous trajectory control and discrete task offloading decisions. Third, we propose a reputation-based decentralized federated learning mechanism with gradient-sensitive adaptive quantization, enabling efficient and robust knowledge sharing across heterogeneous UAVs. Extensive experiments demonstrate that AirFed achieves 42.9% reduction in weighted cost compared to state-of-the-art baselines, attains over 99% deadline satisfaction and 94.2% IoT device coverage rate, and reduces communication overhead by 54.5%. Scalability analysis confirms robust performance across varying UAV numbers, IoT device densities, and system scales, validating AirFed's practical applicability for large-scale UAV-MEC deployments.",
        "authors": [
            "Zhiyu Wang",
            "Suman Raj",
            "Rajkumar Buyya"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.23053v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-27"
    },
    "http://arxiv.org/abs/2510.23019v1": {
        "title": "Sentinel: Dynamic Knowledge Distillation for Personalized Federated Intrusion Detection in Heterogeneous IoT Networks",
        "link": "http://arxiv.org/abs/2510.23019v1",
        "abstract": "Federated learning (FL) offers a privacy-preserving paradigm for machine learning, but its application in intrusion detection systems (IDS) within IoT networks is challenged by severe class imbalance, non-IID data, and high communication overhead.These challenges severely degrade the performance of conventional FL methods in real-world network traffic classification. To overcome these limitations, we propose Sentinel, a personalized federated IDS (pFed-IDS) framework that incorporates a dual-model architecture on each client, consisting of a personalized teacher and a lightweight shared student model. This design effectively balances deep local adaptation with efficient global model consensus while preserving client privacy by transmitting only the compact student model, thus reducing communication costs. Sentinel integrates three key mechanisms to ensure robust performance: bidirectional knowledge distillation with adaptive temperature scaling, multi-faceted feature alignment, and class-balanced loss functions. Furthermore, the server employs normalized gradient aggregation with equal client weighting to enhance fairness and mitigate client drift. Extensive experiments on the IoTID20 and 5GNIDD benchmark datasets demonstrate that Sentinel significantly outperforms state-of-the-art federated methods, establishing a new performance benchmark, especially under extreme data heterogeneity, while maintaining communication efficiency.",
        "authors": [
            "Gurpreet Singh",
            "Keshav Sood",
            "P. Rajalakshmi",
            "Yong Xiang"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.23019v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-27"
    },
    "http://arxiv.org/abs/2510.22986v1": {
        "title": "CodeAD: Synthesize Code of Rules for Log-based Anomaly Detection with LLMs",
        "link": "http://arxiv.org/abs/2510.22986v1",
        "abstract": "Log-based anomaly detection (LogAD) is critical for maintaining the reliability and availability of large-scale online service systems. While machine learning, deep learning, and large language models (LLMs)-based methods have advanced the LogAD, they often suffer from limited interpretability, high inference costs, and extensive preprocessing requirements, limiting their practicality for real-time, high-volume log analysis. In contrast, rule-based systems offer efficiency and transparency, but require significant manual effort and are difficult to scale across diverse and evolving environments. In this paper, We present CodeAD, a novel framework that automatically synthesizes lightweight Python rule functions for LogAD using LLMs. CodeAD introduces a hierarchical clustering and anchor-grounded sampling strategy to construct representative contrastive log windows, enabling LLMs to discern discriminative anomaly patterns. To ensure robustness and generalizability, CodeAD employs an agentic workflow that iteratively generates, tests, repairs, and refines the rules until it meets correctness and abstraction requirements. The synthesized rules are interpretable, lightweight, and directly executable on raw logs, supporting efficient and transparent online anomaly detection. Our comprehensive experiments on three public datasets (BGL, Hadoop, Thunderbird) demonstrate that CodeAD achieves an average absolute improvement of 3.6% F1 score over the state-of-the-art baselines, while processing large datasets up to 4x faster and at a fraction of the cost (total LLM invocation cost under 4 USD per dataset). These results highlight CodeAD as a practical and scalable solution for online monitoring systems, enabling interpretable, efficient, and automated LogAD in real-world environment.",
        "authors": [
            "Junjie Huang",
            "Minghua He",
            "Jinyang Liu",
            "Yintong Huo",
            "Domenico Bianculli",
            "Michael R. Lyu"
        ],
        "categories": [
            "cs.SE",
            "cs.DC",
            "cs.MA"
        ],
        "id": "http://arxiv.org/abs/2510.22986v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-27"
    },
    "http://arxiv.org/abs/2510.22909v1": {
        "id": "http://arxiv.org/abs/2510.22909v1",
        "title": "Rethinking Inference Placement for Deep Learning across Edge and Cloud Platforms: A Multi-Objective Optimization Perspective and Future Directions",
        "link": "http://arxiv.org/abs/2510.22909v1",
        "tags": [
            "offloading",
            "edge",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-10-27",
        "tldr": "Addresses optimal placement of DL model inference across edge and cloud to balance latency, cost, and privacy. Proposes a multi-objective optimization framework for partitioning and offloading, enabling tailored deployment for latency-sensitive applications like chatbots.",
        "abstract": "Edge intelligent applications like VR/AR and language model based chatbots have become widespread with the rapid expansion of IoT and mobile devices. However, constrained edge devices often cannot serve the increasingly large and complex deep learning (DL) models. To mitigate these challenges, researchers have proposed optimizing and offloading partitions of DL models among user devices, edge servers, and the cloud. In this setting, users can take advantage of different services to support their intelligent applications. For example, edge resources offer low response latency. In contrast, cloud platforms provide low monetary cost computation resources for computation-intensive workloads. However, communication between DL model partitions can introduce transmission bottlenecks and pose risks of data leakage. Recent research aims to balance accuracy, computation delay, transmission delay, and privacy concerns. They address these issues with model compression, model distillation, transmission compression, and model architecture adaptations, including internal classifiers. This survey contextualizes the state-of-the-art model offloading methods and model adaptation techniques by studying their implication to a multi-objective optimization comprising inference latency, data privacy, and resource monetary cost.",
        "authors": [
            "Zongshun Zhang",
            "Ibrahim Matta"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.PF"
        ],
        "submit_date": "2025-10-27"
    },
    "http://arxiv.org/abs/2510.22434v1": {
        "title": "Separation of Unconscious Robots with Obstructed Visibility",
        "link": "http://arxiv.org/abs/2510.22434v1",
        "abstract": "We study a recently introduced \\textit{unconscious} mobile robot model, where each robot is associated with a \\textit{color}, which is visible to other robots but not to itself. The robots are autonomous, anonymous, oblivious and silent, operating in the Euclidean plane under the conventional \\textit{Look-Compute-Move} cycle. A primary task in this model is the \\textit{separation problem}, where unconscious robots sharing the same color must separate from others, forming recognizable geometric shapes such as circles, points, or lines. All prior works model the robots as \\textit{transparent}, enabling each to know the positions and colors of all other robots. In contrast, we model the robots as \\textit{opaque}, where a robot can obstruct the visibility of two other robots, if it lies on the line segment between them. Under this obstructed visibility, we consider a variant of the separation problem in which robots, starting from any arbitrary initial configuration, are required to separate into concentric semicircles. We present a collision-free algorithm that solves the separation problem under a semi-synchronous scheduler in $O(n)$ epochs, where $n$ is the number of robots. The robots agree on one coordinate axis but have no knowledge of $n$.",
        "authors": [
            "Prajyot Pyati",
            "Navjot Kaur",
            "Saswata Jana",
            "Adri Bhattacharya",
            "Partha Sarathi Mandal"
        ],
        "categories": [
            "cs.DC",
            "cs.RO"
        ],
        "id": "http://arxiv.org/abs/2510.22434v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-25"
    },
    "http://arxiv.org/abs/2510.22309v1": {
        "title": "When Agents are Powerful: Black Hole Search in Time-Varying Graphs",
        "link": "http://arxiv.org/abs/2510.22309v1",
        "abstract": "A black hole is a harmful node in a graph that destroys any resource entering it, making its identification a critical task. In the \\emph{Black Hole Search (BHS)} problem, a team of agents operates on a graph $G$ with the objective that at least one agent must survive and correctly identify an edge incident to the black hole. Prior work has addressed BHS in arbitrary dynamic graphs under the restrictive \\emph{face-to-face} communication, where agents can exchange information only when co-located. This constraint significantly increases the number of agents required to solve the problem. In this work, we strengthen the capabilities of agents in two ways: (i) granting them \\emph{global communication}, enabling interaction regardless of location, and (ii) equipping them with \\emph{1-hop visibility}, allowing each agent to observe its immediate neighborhood. These enhancements lead to more efficient solutions for the BHS problem in dynamic graphs.",
        "authors": [
            "Tanvir Kaur",
            "Ashish Saxena"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.22309v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-25"
    },
    "http://arxiv.org/abs/2510.24776v1": {
        "title": "CFL-SparseMed: Communication-Efficient Federated Learning for Medical Imaging with Top-k Sparse Updates",
        "link": "http://arxiv.org/abs/2510.24776v1",
        "abstract": "Secure and reliable medical image classification is crucial for effective patient treatment, but centralized models face challenges due to data and privacy concerns. Federated Learning (FL) enables privacy-preserving collaborations but struggles with heterogeneous, non-IID data and high communication costs, especially in large networks. We propose \\textbf{CFL-SparseMed}, an FL approach that uses Top-k Sparsification to reduce communication overhead by transmitting only the top k gradients. This unified solution effectively addresses data heterogeneity while maintaining model accuracy. It enhances FL efficiency, preserves privacy, and improves diagnostic accuracy and patient care in non-IID medical imaging settings. The reproducibility source code is available on \\href{https://github.com/Aniket2241/APK_contruct}{Github}.",
        "authors": [
            "Gousia Habib",
            "Aniket Bhardwaj",
            "Ritvik Sharma",
            "Shoeib Amin Banday",
            "Ishfaq Ahmad Malik"
        ],
        "categories": [
            "eess.IV",
            "cs.CV",
            "cs.DC",
            "cs.LG"
        ],
        "id": "http://arxiv.org/abs/2510.24776v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-25"
    },
    "http://arxiv.org/abs/2511.01888v1": {
        "title": "Roadrunner: Accelerating Data Delivery to WebAssembly-Based Serverless Functions",
        "link": "http://arxiv.org/abs/2511.01888v1",
        "abstract": "Serverless computing provides infrastructure management and elastic auto-scaling, therefore reducing operational overhead. By design serverless functions are stateless, which means they typically leverage external remote services to store and exchange data. Transferring data over a network typically involves serialization and deserialization. These operations usually require multiple data copies and transitions between user and kernel space, resulting in overhead from context switching and memory allocation, contributing significantly to increased latency and resource consumption. To address these issues, we present Roadrunner, a sidecar shim that enables near-zero copy and serialization-free data transfer between WebAssembly-based serverless functions. Roadrunner reduces the multiple copies between user space and kernel space by mapping the function memory and moving the data along a dedicated virtual data hose, bypassing the costly processes of serialization and deserialization. This approach reduces data movement overhead and context switching, achieving near-native latency performance for WebAssembly-based serverless functions. Our experimental results demonstrate that Roadrunner significantly improves the inter-function communication latency from 44% up to 89%, reducing the serialization overhead in 97% of data transfer, and increasing throughput by 69 times compared to state-of-the-art WebAssembly-based serverless functions.",
        "authors": [
            "Cynthia Marcelino",
            "Thomas Pusztai",
            "Stefan Nastic"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.01888v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-25"
    },
    "http://arxiv.org/abs/2510.22149v1": {
        "title": "Power to the Clients: Federated Learning in a Dictatorship Setting",
        "link": "http://arxiv.org/abs/2510.22149v1",
        "abstract": "Federated learning (FL) has emerged as a promising paradigm for decentralized model training, enabling multiple clients to collaboratively learn a shared model without exchanging their local data. However, the decentralized nature of FL also introduces vulnerabilities, as malicious clients can compromise or manipulate the training process. In this work, we introduce dictator clients, a novel, well-defined, and analytically tractable class of malicious participants capable of entirely erasing the contributions of all other clients from the server model, while preserving their own. We propose concrete attack strategies that empower such clients and systematically analyze their effects on the learning process. Furthermore, we explore complex scenarios involving multiple dictator clients, including cases where they collaborate, act independently, or form an alliance in order to ultimately betray one another. For each of these settings, we provide a theoretical analysis of their impact on the global model's convergence. Our theoretical algorithms and findings about the complex scenarios including multiple dictator clients are further supported by empirical evaluations on both computer vision and natural language processing benchmarks.",
        "authors": [
            "Mohammadsajad Alipour",
            "Mohammad Mohammadi Amiri"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "cs.CR",
            "cs.CV",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.22149v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-25"
    },
    "http://arxiv.org/abs/2510.21950v1": {
        "title": "Heaven & Hell II: Scale Laws and Robustness in One-Step Heaven-Hell Consensus",
        "link": "http://arxiv.org/abs/2510.21950v1",
        "abstract": "We study Heaven-Hell dynamics, a model for network consensus. A known result establishes an exact one-step convergence threshold for systems with a single uniform hub: the per-node inbound hub weight W suffices if and only if W >= maxrest, the maximum non-hub inbound mass. We develop scale laws and operational refinements that make this threshold robust to tie-breaking policies, node-specific tolerances, targeted seeding, multiple hubs, and asynchronous updates. Our contributions include a conservation-law perspective, parameterized tie policies, tighter pointwise bounds improving on classical worst-case guarantees, one-pass fairness for asynchronous updates, and sufficient conditions for seeded convergence. All proofs are mechanized in Coq, with experiments on rings, grids, scale-free graphs, and heterogeneous weighted graphs validating tightness and gap closures",
        "authors": [
            "Nnamdi Daniel Aghanya",
            "Romain Leemans"
        ],
        "categories": [
            "cs.SI",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.21950v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-24"
    },
    "http://arxiv.org/abs/2510.21549v1": {
        "title": "Distributed $(Δ+1)$-Coloring in Graphs of Bounded Neighborhood Independence",
        "link": "http://arxiv.org/abs/2510.21549v1",
        "abstract": "The distributed coloring problem is arguably one of the key problems studied in the area of distributed graph algorithms. The most standard variant of the problem asks for a proper vertex coloring of a graph with $Δ+1$ colors, where $Δ$ is the maximum degree of the graph. Despite an immense amount of work on distributed coloring problems in the distributed setting, determining the deterministic complexity of $(Δ+1)$-coloring in the standard message passing model remains one of the most important open questions of the area. In this paper, we aim to improve our understanding of the deterministic complexity of $(Δ+1)$-coloring as a function of $Δ$ in a special family of graphs for which significantly faster algorithms are already known. The neighborhood independence $θ$ of a graph is the maximum number of pairwise non-adjacent neighbors of some node of the graph. In general, in graphs of neighborhood independence $θ=O(1)$ (e.g., line graphs), it is known that $(Δ+1)$-coloring can be solved in $2^{O(\\sqrt{\\logΔ})}+O(\\log^* n)$ rounds. In the present paper, we significantly improve this result, and we show that in graphs of neighborhood independence $θ$, a $(Δ+1)$-coloring can be computed in $(θ\\cdot\\logΔ)^{O(\\log\\logΔ/ \\log\\log\\logΔ)}+O(\\log^* n)$ rounds and thus in quasipolylogarithmic time in $Δ$ as long as $θ$ is at most polylogarithmic in $Δ$. We also show that the known approach that leads to a polylogarithmic in $Δ$ algorithm for $(2Δ-1)$-edge coloring already fails for edge colorings of hypergraphs of rank at least $3$.",
        "authors": [
            "Marc Fuchs",
            "Fabian Kuhn"
        ],
        "categories": [
            "cs.DC",
            "cs.CC"
        ],
        "id": "http://arxiv.org/abs/2510.21549v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-24"
    },
    "http://arxiv.org/abs/2510.21493v2": {
        "title": "On Reduction and Synthesis of Petri's Cycloids",
        "link": "http://arxiv.org/abs/2510.21493v2",
        "abstract": "Cycloids are particular Petri nets for modelling processes of actions and events, belonging to the fundaments of Petri's general systems theory. Defined by four parameters they provide an algebraic formalism to describe strongly synchronized sequential processes. To further investigate their structure, reduction systems of cycloids are defined in the style of rewriting systems and properties of irreducible cycloids are proved. In particular the synthesis of cycloid parameters from their Petri net structure is derived, leading to an efficient method for a decision procedure for cycloid isomorphism.",
        "authors": [
            "Rüdiger Valk",
            "Daniel Moldt"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.21493v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-24"
    },
    "http://arxiv.org/abs/2510.21491v1": {
        "title": "Benchmarking Catastrophic Forgetting Mitigation Methods in Federated Time Series Forecasting",
        "link": "http://arxiv.org/abs/2510.21491v1",
        "abstract": "Catastrophic forgetting (CF) poses a persistent challenge in continual learning (CL), especially within federated learning (FL) environments characterized by non-i.i.d. time series data. While existing research has largely focused on classification tasks in vision domains, the regression-based forecasting setting prevalent in IoT and edge applications remains underexplored. In this paper, we present the first benchmarking framework tailored to investigate CF in federated continual time series forecasting. Using the Beijing Multi-site Air Quality dataset across 12 decentralized clients, we systematically evaluate several CF mitigation strategies, including Replay, Elastic Weight Consolidation, Learning without Forgetting, and Synaptic Intelligence. Key contributions include: (i) introducing a new benchmark for CF in time series FL, (ii) conducting a comprehensive comparative analysis of state-of-the-art methods, and (iii) releasing a reproducible open-source framework. This work provides essential tools and insights for advancing continual learning in federated time-series forecasting systems.",
        "authors": [
            "Khaled Hallak",
            "Oudom Kem"
        ],
        "categories": [
            "cs.LG",
            "cs.DC",
            "stat.ML"
        ],
        "id": "http://arxiv.org/abs/2510.21491v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-24"
    },
    "http://arxiv.org/abs/2510.21419v1": {
        "id": "http://arxiv.org/abs/2510.21419v1",
        "title": "Learning to Schedule: A Supervised Learning Framework for Network-Aware Scheduling of Data-Intensive Workloads",
        "link": "http://arxiv.org/abs/2510.21419v1",
        "tags": [
            "networking",
            "scheduling",
            "storage"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Addresses network-aware job scheduling for data-intensive workloads via supervised learning. Collects real-time telemetry to predict job completion times per node and ranks placements. Achieves 34-54% higher accuracy in optimal node selection versus default Kubernetes scheduler.",
        "abstract": "Distributed cloud environments hosting data-intensive applications often experience slowdowns due to network congestion, asymmetric bandwidth, and inter-node data shuffling. These factors are typically not captured by traditional host-level metrics like CPU or memory. Scheduling without accounting for these conditions can lead to poor placement decisions, longer data transfers, and suboptimal job performance. We present a network-aware job scheduler that uses supervised learning to predict the completion time of candidate jobs. Our system introduces a prediction-and-ranking mechanism that collects real-time telemetry from all nodes, uses a trained supervised model to estimate job duration per node, and ranks them to select the best placement. We evaluate the scheduler on a geo-distributed Kubernetes cluster deployed on the FABRIC testbed by running network-intensive Spark workloads. Compared to the default Kubernetes scheduler, which makes placement decisions based on current resource availability alone, our proposed supervised scheduler achieved 34-54% higher accuracy in selecting optimal nodes for job placement. The novelty of our work lies in the demonstration of supervised learning for real-time, network-aware job scheduling on a multi-site cluster.",
        "authors": [
            "Sankalpa Timilsina",
            "Susmit Shannigrahi"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-10-24"
    },
    "http://arxiv.org/abs/2510.21373v1": {
        "title": "LIDC: A Location Independent Multi-Cluster Computing Framework for Data Intensive Science",
        "link": "http://arxiv.org/abs/2510.21373v1",
        "abstract": "Scientific communities are increasingly using geographically distributed computing platforms. The current methods of compute placement predominantly use logically centralized controllers such as Kubernetes (K8s) to match tasks to available resources. However, this centralized approach is unsuitable in multi-organizational collaborations. Furthermore, workflows often need to use manual configurations tailored for a single platform and cannot adapt to dynamic changes across infrastructure. Our work introduces a decentralized control plane for placing computations on geographically dispersed compute clusters using semantic names. We assign semantic names to computations to match requests with named Kubernetes (K8s) service endpoints. We show that this approach provides multiple benefits. First, it allows placement of computational jobs to be independent of location, enabling any cluster with sufficient resources to execute the computation. Second, it facilitates dynamic compute placement without requiring prior knowledge of cluster locations or predefined configurations.",
        "authors": [
            "Sankalpa Timilsina",
            "Susmit Shannigrahi"
        ],
        "categories": [
            "cs.DC",
            "cs.NI"
        ],
        "id": "http://arxiv.org/abs/2510.21373v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-24"
    },
    "http://arxiv.org/abs/2510.21348v1": {
        "title": "Parsley's Group Size Study",
        "link": "http://arxiv.org/abs/2510.21348v1",
        "abstract": "Parsley is a resilient group-based Distributed Hash Table that incorporates a preemptive peer relocation technique and a dynamic data sharding mechanism to enhance robustness and balance. In addition to the hard limits on group size, defined by minimum and maximum thresholds, Parsley introduces two soft limits that define a target interval for maintaining stable group sizes. These soft boundaries allow the overlay to take proactive measures to prevent violations of the hard limits, improving system stability under churn. This work provides an in-depth analysis of the rationale behind the parameter values adopted for Parsley's evaluation. Unlike related systems, which specify group size limits without justification, we conduct a systematic overlay characterization study to understand the effects of these parameters on performance and scalability. The study examines topology operations, the behavior of large groups, and the overall trade-offs observed, offering a grounded explanation for the chosen configuration values.",
        "authors": [
            "João A. Silva",
            "Hervé Paulino",
            "João M. Lourenço"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.21348v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-24"
    },
    "http://arxiv.org/abs/2510.21304v1": {
        "title": "Arbitration-Free Consistency is Available (and Vice Versa)",
        "link": "http://arxiv.org/abs/2510.21304v1",
        "abstract": "The fundamental tension between availability and consistency shapes the design of distributed storage systems. Classical results capture extreme points of this trade-off: the CAP theorem shows that strong models like linearizability preclude availability under partitions, while weak models like causal consistency remain implementable without coordination. These theorems apply to simple read-write interfaces, leaving open a precise explanation of the combinations of object semantics and consistency models that admit available implementations.   This paper develops a general semantic framework in which storage specifications combine operation semantics and consistency models. The framework encompasses a broad range of objects (key-value stores, counters, sets, CRDTs, and transactional databases) and consistency models (from causal consistency and sequential consistency to snapshot isolation and transactional and non-transactional SQL).   Within this framework, we prove the Arbitration-Free Consistency (AFC) theorem, showing that an object specification within a consistency model admits an available implementation if and only if it is arbitration-free, that is, it does not require a total arbitration order to resolve visibility or read dependencies.   The AFC theorem unifies and generalizes previous results, revealing arbitration-freedom as the fundamental property that delineates coordination-free consistency from inherently synchronized behavior.",
        "authors": [
            "Hagit Attiya",
            "Constantin Enea",
            "Enrique Román-Calvo"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.21304v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-24"
    },
    "http://arxiv.org/abs/2510.21183v1": {
        "title": "Generative Federated Learning for Smart Prediction and Recommendation Applications",
        "link": "http://arxiv.org/abs/2510.21183v1",
        "abstract": "This paper proposes a generative adversarial network and federated learning-based model to address various challenges of the smart prediction and recommendation applications, such as high response time, compromised data privacy, and data scarcity. The integration of the generative adversarial network and federated learning is referred to as Generative Federated Learning (GFL). As a case study of the proposed model, a heart health monitoring application is considered. The realistic synthetic datasets are generated using the generated adversarial network-based proposed algorithm for improving data diversity, data quality, and data augmentation, and remove the data scarcity and class imbalance issues. In this paper, we implement the centralized and decentralized federated learning approaches in an edge computing paradigm. In centralized federated learning, the edge nodes communicate with the central server to build the global and personalized local models in a collaborative manner. In the decentralized federated learning approach, the edge nodes communicate among themselves to exchange model updates for collaborative training. The comparative study shows that the proposed framework outperforms the existing heart health monitoring applications. The results show that using the proposed framework (i) the prediction accuracy is improved by 12% than the conventional framework, and (ii) the response time is reduced by 73% than the conventional cloud-only system.",
        "authors": [
            "Anwesha Mukherjee",
            "Rajkumar Buyya"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.21183v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-24"
    },
    "http://arxiv.org/abs/2510.21173v1": {
        "title": "From SLA to vendor-neutral metrics: An intelligent knowledge-based approach for multi-cloud SLA-based broker",
        "link": "http://arxiv.org/abs/2510.21173v1",
        "abstract": "Cloud computing has been consolidated as a support for the vast majority of current and emerging technologies. However, there are some barriers that prevent the exploitation of the full potential of this technology. First, the major cloud providers currently put the onus of implementing the mechanisms that ensure compliance with the desired service levels on cloud consumers. However, consumers do not have the required expertise. Since each cloud provider exports a different set of low-level metrics, the strategies defined to ensure compliance with the established service-level agreement (SLA) are bound to a particular cloud provider. This fosters provider lock-in and prevents consumers from benefiting from the advantages of multi-cloud environments. This paper presents a solution to the problem of automatically translating SLAs into objectives expressed as metrics that can be measured across multiple cloud providers. First, we propose an intelligent knowledge-based system capable of automatically translating high-level SLAs defined by cloud consumers into a set of conditions expressed as vendor-neutral metrics, providing feedback to cloud consumers (intelligent tutoring system). Secondly, we present the set of vendor-neutral metrics and explain how they can be measured for the different cloud providers. Finally, we report a validation based on two use cases (IaaS and PaaS) in a multi-cloud environment formed by leading cloud providers. This evaluation has demonstrated that, thanks to the complementarity of the two solutions, cloud consumers can automatically and transparently exploit the multi-cloud in many application domains, as endorsed by the cloud experts consulted in the course of this study.",
        "authors": [
            "Víctor Rampérez",
            "Javier Soriano",
            "David Lizcano",
            "Shadi Aljawarneh",
            "Juan A. Lara"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.21173v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-24"
    },
    "http://arxiv.org/abs/2510.21155v1": {
        "title": "Towards Straggler-Resilient Split Federated Learning: An Unbalanced Update Approach",
        "link": "http://arxiv.org/abs/2510.21155v1",
        "abstract": "Split Federated Learning (SFL) enables scalable training on edge devices by combining the parallelism of Federated Learning (FL) with the computational offloading of Split Learning (SL). Despite its great success, SFL suffers significantly from the well-known straggler issue in distributed learning systems. This problem is exacerbated by the dependency between Split Server and clients: the Split Server side model update relies on receiving activations from clients. Such synchronization requirement introduces significant time latency, making straggler a critical bottleneck to the scalability and efficiency of the system. To mitigate this problem, we propose MU-SplitFed, a straggler-resilient SFL algorithm in zeroth-order optimization that decouples training progress from straggler delays via a simple yet effective unbalanced update mechanism.   By enabling the server to perform $τ$ local updates per client round, MU-SplitFed achieves a convergence rate of $O(\\sqrt{d/(τT)})$ for non-convex objectives, demonstrating a linear speedup of $τ$ in communication rounds. Experiments demonstrate that MU-SplitFed consistently outperforms baseline methods with the presence of stragglers and effectively mitigates their impact through adaptive tuning of $τ$. The code for this project is available at https://github.com/Johnny-Zip/MU-SplitFed.",
        "authors": [
            "Dandan Liang",
            "Jianing Zhang",
            "Evan Chen",
            "Zhe Li",
            "Rui Li",
            "Haibo Yang"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.LG"
        ],
        "id": "http://arxiv.org/abs/2510.21155v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-24"
    },
    "http://arxiv.org/abs/2510.21103v1": {
        "title": "Sensing and Storing Less: A MARL-based Solution for Energy Saving in Edge Internet of Things",
        "link": "http://arxiv.org/abs/2510.21103v1",
        "abstract": "As the number of Internet of Things (IoT) devices continuously grows and application scenarios constantly enrich, the volume of sensor data experiences an explosive increase. However, substantial data demands considerable energy during computation and transmission. Redundant deployment or mobile assistance is essential to cover the target area reliably with fault-prone sensors. Consequently, the ``butterfly effect\" may appear during the IoT operation, since unreasonable data overlap could result in many duplicate data. To this end, we propose Senses, a novel online energy saving solution for edge IoT networks, with the insight of sensing and storing less at the network edge by adopting Muti-Agent Reinforcement Learning (MARL). Senses achieves data de-duplication by dynamically adjusting sensor coverage at the sensor level. For exceptional cases where sensor coverage cannot be altered, Senses conducts data partitioning and eliminates redundant data at the controller level. Furthermore, at the global level, considering the heterogeneity of IoT devices, Senses balances the operational duration among the devices to prolong the overall operational duration of edge IoT networks. We evaluate the performance of Senses through testbed experiments and simulations. The results show that Senses saves 11.37% of energy consumption on control devices and prolongs 20% overall operational duration of the IoT device network.",
        "authors": [
            "Zongyang Yuan",
            "Lailong Luo",
            "Qianzhen Zhang",
            "Bangbang Ren",
            "Deke Guo",
            "Richard T. B. Ma"
        ],
        "categories": [
            "cs.NI",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.21103v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-24"
    },
    "http://arxiv.org/abs/2510.21081v1": {
        "id": "http://arxiv.org/abs/2510.21081v1",
        "title": "Accelerating Mobile Inference through Fine-Grained CPU-GPU Co-Execution",
        "link": "http://arxiv.org/abs/2510.21081v1",
        "tags": [
            "edge",
            "kernel",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Reduces mobile inference latency via CPU-GPU co-execution. Introduces lightweight SVM-based synchronization and ML-based execution time prediction for task assignment. Achieves up to 1.89x speedup for linear layers on Pixel 5 smartphone.",
        "abstract": "Deploying deep neural networks on mobile devices is increasingly important but remains challenging due to limited computing resources. On the other hand, their unified memory architecture and narrower gap between CPU and GPU performance provide an opportunity to reduce inference latency by assigning tasks to both CPU and GPU. The main obstacles for such collaborative execution are the significant synchronization overhead required to combine partial results, and the difficulty of predicting execution times of tasks assigned to CPU and GPU (due to the dynamic selection of implementations and parallelism level). To overcome these obstacles, we propose both a lightweight synchronization mechanism based on OpenCL fine-grained shared virtual memory (SVM) and machine learning models to accurately predict execution times. Notably, these models capture the performance characteristics of GPU kernels and account for their dispatch times. A comprehensive evaluation on four mobile platforms shows that our approach can quickly select CPU-GPU co-execution strategies achieving up to 1.89x speedup for linear layers and 1.75x speedup for convolutional layers (close to the achievable maximum values of 2.01x and 1.87x, respectively, found by exhaustive grid search on a Pixel~5 smartphone).",
        "authors": [
            "Zhuojin Li",
            "Marco Paolieri",
            "Leana Golubchik"
        ],
        "categories": [
            "cs.LG",
            "cs.DC",
            "cs.PF"
        ],
        "submit_date": "2025-10-24"
    },
    "http://arxiv.org/abs/2510.21048v1": {
        "id": "http://arxiv.org/abs/2510.21048v1",
        "title": "xMem: A CPU-Based Approach for Accurate Estimation of GPU Memory in Deep Learning Training Workloads",
        "link": "http://arxiv.org/abs/2510.21048v1",
        "tags": [
            "training",
            "offloading",
            "storage"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes xMem, a CPU-only dynamic analysis framework for precise a priori estimation of peak GPU memory in deep learning training. Targets resource allocation optimizations without runtime GPU overhead. Achieves 91% lower median relative error and 75% reduction in OOM probability.",
        "abstract": "The global scarcity of GPUs necessitates more sophisticated strategies for Deep Learning jobs in shared cluster environments. Accurate estimation of how much GPU memory a job will require is fundamental to enabling advanced scheduling and GPU sharing, which helps prevent out-of-memory (OOM) errors and resource underutilization. However, existing estimation methods have limitations. Approaches relying on static analysis or historical data with machine learning often fail to accurately capture runtime dynamics. Furthermore, direct GPU analysis consumes scarce resources, and some techniques require intrusive code modifications. Thus, the key challenge lies in precisely estimating dynamic memory requirements, including memory allocator nuances, without consuming GPU resources and non-intrusive code changes. To address this challenge, we propose xMem, a novel framework that leverages CPU-only dynamic analysis to accurately estimate peak GPU memory requirements a priori. We conducted a thorough evaluation of xMem against state-of-the-art solutions using workloads from 25 different models, including architectures like Convolutional Neural Networks and Transformers. The analysis of 5209 runs, which includes ANOVA and Monte Carlo results, highlights xMem's benefits: it decreases the median relative error by 91% and significantly reduces the probability of estimation failure as safe OOM thresholds by 75%, meaning that the estimated value can often be used directly without causing OOM. Ultimately, these improvements lead to a 368% increase in memory conservation potential over current solutions.",
        "authors": [
            "Jiabo Shi",
            "Dimitrios Pezaros",
            "Yehia Elkhatib"
        ],
        "categories": [
            "cs.PF",
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-10-23"
    },
    "http://arxiv.org/abs/2511.01884v2": {
        "title": "CudaForge: An Agent Framework with Hardware Feedback for CUDA Kernel Optimization",
        "link": "http://arxiv.org/abs/2511.01884v2",
        "abstract": "Developing efficient CUDA kernels is increasingly critical for AI applications such as large-scale LLM training. However, manual kernel design is both costly and time-consuming, motivating automatic approaches that leverage LLMs for code generation. Existing methods for automatic kernel generation, however, often produce low-efficiency kernels, incur high computational overhead, and fail to generalize across settings. In this work, we propose CudaForge, a training-free multi-agent workflow for CUDA kernel generation and optimization. Our workflow is inspired by the iterative workflow of human experts, which contains steps such as developing initial kernels, testing correctness, analyzing hardware feedback, and iterative improvement. More specifically, CudaForge employs two LLM agents: a Coder and a Judge, that iteratively generate, correct, and optimize CUDA kernels, while integrating hardware feedback such as Nsight Compute (NCU) metrics. In extensive evaluations, we show that CudaForge, by leveraging base models like OpenAI-o3, achieves 97.6\\% correctness of generated kernels and an average 1.68$\\times$ speedup over PyTorch baselines, substantially surpassing state-of-the-art models including OpenAI-o3 and Kevin on KernelBench.Beyond accuracy and speed, CudaForge demonstrates strong generalization across GPUs (A100, RTX 6000, 4090, 3090) and base models (OpenAI-o3, GPT-5, gpt-oss-120B, Claude-Sonnet-4, QwQ-32B), while maintaining high efficiency. In particular, generating an optimized kernel takes about 26.5 minutes on one RTX6000 and incurs about \\$ 0.3 API cost, which is significantly cheaper than existing agentic work that costs 6 H100 hours and \\$ 5 API cost per kernel. Our results highlight that multi-agent, training-free workflows can enable cost-effective, generalizable, and high-performance CUDA kernel optimization. Code available at https://github.com/OptimAI-Lab/CudaForge",
        "authors": [
            "Zijian Zhang",
            "Rong Wang",
            "Shiyang Li",
            "Yuebo Luo",
            "Mingyi Hong",
            "Caiwen Ding"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.01884v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-23"
    },
    "http://arxiv.org/abs/2510.21024v1": {
        "title": "JSTprove: Pioneering Verifiable AI for a Trustless Future",
        "link": "http://arxiv.org/abs/2510.21024v1",
        "abstract": "The integration of machine learning (ML) systems into critical industries such as healthcare, finance, and cybersecurity has transformed decision-making processes, but it also brings new challenges around trust, security, and accountability. As AI systems become more ubiquitous, ensuring the transparency and correctness of AI-driven decisions is crucial, especially when they have direct consequences on privacy, security, or fairness. Verifiable AI, powered by Zero-Knowledge Machine Learning (zkML), offers a robust solution to these challenges. zkML enables the verification of AI model inferences without exposing sensitive data, providing an essential layer of trust and privacy. However, traditional zkML systems typically require deep cryptographic expertise, placing them beyond the reach of most ML engineers. In this paper, we introduce JSTprove, a specialized zkML toolkit, built on Polyhedra Network's Expander backend, to enable AI developers and ML engineers to generate and verify proofs of AI inference. JSTprove provides an end-to-end verifiable AI inference pipeline that hides cryptographic complexity behind a simple command-line interface while exposing auditable artifacts for reproducibility. We present the design, innovations, and real-world use cases of JSTprove as well as our blueprints and tooling to encourage community review and extension. JSTprove therefore serves both as a usable zkML product for current engineering needs and as a reproducible foundation for future research and production deployments of verifiable AI.",
        "authors": [
            "Jonathan Gold",
            "Tristan Freiberg",
            "Haruna Isah",
            "Shirin Shahabi"
        ],
        "categories": [
            "cs.CR",
            "cs.AI",
            "cs.DC",
            "cs.LG"
        ],
        "id": "http://arxiv.org/abs/2510.21024v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-23"
    },
    "http://arxiv.org/abs/2510.20931v1": {
        "id": "http://arxiv.org/abs/2510.20931v1",
        "title": "Lincoln AI Computing Survey (LAICS) and Trends",
        "link": "http://arxiv.org/abs/2510.20931v1",
        "tags": [
            "hardware",
            "training",
            "inference"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Surveys AI accelerators for GenAI training and inference, plotting peak performance vs. power. Updates previous data with new market segment analysis and architectural categorizations. Highlights trends and provides performance/power metrics for commercial systems.",
        "abstract": "In the past year, generative AI (GenAI) models have received a tremendous amount of attention, which in turn has increased attention to computing systems for training and inference for GenAI. Hence, an update to this survey is due. This paper is an update of the survey of AI accelerators and processors from past seven years, which is called the Lincoln AI Computing Survey -- LAICS (pronounced \"lace\"). This multi-year survey collects and summarizes the current commercial accelerators that have been publicly announced with peak performance and peak power consumption numbers. In the same tradition of past papers of this survey, the performance and power values are plotted on a scatter graph, and a number of dimensions and observations from the trends on this plot are again discussed and analyzed. Market segments are highlighted on the scatter plot, and zoomed plots of each segment are also included. A brief description of each of the new accelerators that have been added in the survey this year is included, and this update features a new categorization of computing architectures that implement each of the accelerators.",
        "authors": [
            "Albert Reuther",
            "Peter Michaleas",
            "Michael Jones",
            "Vijay Gadepally",
            "Jeremy Kepner"
        ],
        "categories": [
            "cs.DC",
            "cs.AR"
        ],
        "submit_date": "2025-10-23"
    },
    "http://arxiv.org/abs/2510.20645v1": {
        "title": "Decentralized Exchange that Mitigate a Bribery Attack",
        "link": "http://arxiv.org/abs/2510.20645v1",
        "abstract": "Despite the popularity of Hashed Time-Locked Contracts (HTLCs) because of their use in wide areas of applications such as payment channels, atomic swaps, etc, their use in exchange is still questionable. This is because of its incentive incompatibility and susceptibility to bribery attacks.   State-of-the-art solutions such as MAD-HTLC (Oakland'21) and He-HTLC (NDSS'23) address this by leveraging miners' profit-driven behaviour to mitigate such attacks. The former is the mitigation against passive miners; however, the latter works against both active and passive miners. However, they consider only two bribing scenarios where either of the parties involved in the transfer collude with the miner.   In this paper, we expose vulnerabilities in state-of-the-art solutions by presenting a miner-collusion bribery attack with implementation and game-theoretic analysis. Additionally, we propose a stronger attack on MAD-HTLC than He-HTLC, allowing the attacker to earn profits equivalent to attacking naive HTLC.   Leveraging our insights, we propose \\prot, a game-theoretically secure HTLC protocol resistant to all bribery scenarios. \\prot\\ employs a two-phase approach, preventing unauthorized token confiscation by third parties, such as miners. In Phase 1, parties commit to the transfer; in Phase 2, the transfer is executed without manipulation. We demonstrate \\prot's efficiency in transaction cost and latency via implementations on Bitcoin and Ethereum.",
        "authors": [
            "Nitin Awathare"
        ],
        "categories": [
            "cs.CR",
            "cs.CE",
            "cs.DC",
            "cs.GT"
        ],
        "id": "http://arxiv.org/abs/2510.20645v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-23"
    },
    "http://arxiv.org/abs/2510.20506v1": {
        "id": "http://arxiv.org/abs/2510.20506v1",
        "title": "Morpheus: Lightweight RTT Prediction for Performance-Aware Load Balancing",
        "link": "http://arxiv.org/abs/2510.20506v1",
        "tags": [
            "serving",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes lightweight RTT predictors using time-series cluster metrics for performance-aware load balancing in distributed systems. Achieves up to 95% prediction accuracy, reducing tail latency and resource waste in Kubernetes-managed GPU clusters via proactive routing optimization.",
        "abstract": "Distributed applications increasingly demand low end-to-end latency, especially in edge and cloud environments where co-located workloads contend for limited resources. Traditional load-balancing strategies are typically reactive and rely on outdated or coarse-grained metrics, often leading to suboptimal routing decisions and increased tail latencies. This paper investigates the use of round-trip time (RTT) predictors to enhance request routing by anticipating application latency. We develop lightweight and accurate RTT predictors that are trained on time-series monitoring data collected from a Kubernetes-managed GPU cluster. By leveraging a reduced set of highly correlated monitoring metrics, our approach maintains low overhead while remaining adaptable to diverse co-location scenarios and heterogeneous hardware. The predictors achieve up to 95% accuracy while keeping the prediction delay within 10% of the application RTT. In addition, we identify the minimum prediction accuracy threshold and key system-level factors required to ensure effective predictor deployment in resource-constrained clusters. Simulation-based evaluation demonstrates that performance-aware load balancing can significantly reduce application RTT and minimize resource waste. These results highlight the feasibility of integrating predictive load balancing into future production systems.",
        "authors": [
            "Panagiotis Giannakopoulos",
            "Bart van Knippenberg",
            "Kishor Chandra Joshi",
            "Nicola Calabretta",
            "George Exarchakos"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-10-23"
    },
    "http://arxiv.org/abs/2510.20499v2": {
        "title": "GPU-Accelerated Primal Heuristics for Mixed Integer Programming",
        "link": "http://arxiv.org/abs/2510.20499v2",
        "abstract": "We introduce a fusion of GPU accelerated primal heuristics for Mixed Integer Programming. Leveraging GPU acceleration enables exploration of larger search regions and faster iterations. A GPU-accelerated PDLP serves as an approximate LP solver, while a new probing cache facilitates rapid roundings and early infeasibility detection. Several state-of-the-art heuristics, including Feasibility Pump, Feasibility Jump, and Fix-and-Propagate, are further accelerated and enhanced. The combined approach of these GPU-driven algorithms yields significant improvements over existing methods, both in the number of feasible solutions and the quality of objectives by achieving 221 feasible solutions and 22% objective gap in the MIPLIB2017 benchmark on a presolved dataset.",
        "authors": [
            "Akif Çördük",
            "Piotr Sielski",
            "Alice Boucher",
            "Kumar Aatish"
        ],
        "categories": [
            "math.OC",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.20499v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-23"
    },
    "http://arxiv.org/abs/2510.20495v1": {
        "title": "Accurate Performance Predictors for Edge Computing Applications",
        "link": "http://arxiv.org/abs/2510.20495v1",
        "abstract": "Accurate prediction of application performance is critical for enabling effective scheduling and resource management in resource-constrained dynamic edge environments. However, achieving predictable performance in such environments remains challenging due to the co-location of multiple applications and the node heterogeneity. To address this, we propose a methodology that automatically builds and assesses various performance predictors. This approach prioritizes both accuracy and inference time to identify the most efficient model. Our predictors achieve up to 90% accuracy while maintaining an inference time of less than 1% of the Round Trip Time. These predictors are trained on the historical state of the most correlated monitoring metrics to application performance and evaluated across multiple servers in dynamic co-location scenarios. As usecase we consider electron microscopy (EM) workflows, which have stringent real-time demands and diverse resource requirements. Our findings emphasize the need for a systematic methodology that selects server-specific predictors by jointly optimizing accuracy and inference latency in dynamic co-location scenarios. Integrating such predictors into edge environments can improve resource utilization and result in predictable performance.",
        "authors": [
            "Panagiotis Giannakopoulos",
            "Bart van Knippenberg",
            "Kishor Chandra Joshi",
            "Nicola Calabretta",
            "George Exarchakos"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.20495v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-23"
    },
    "http://arxiv.org/abs/2510.20389v1": {
        "title": "Symmetry in Software Platforms as an Architectural Principle",
        "link": "http://arxiv.org/abs/2510.20389v1",
        "abstract": "Software platforms often act as structure preserving systems. They provide consistent interfaces and behaviors that remain stable under specific transformations that we denote as symmetries. This paper explores the idea that architectural robustness emerges from enforcing such structural regularities",
        "authors": [
            "Bjorn Remseth"
        ],
        "categories": [
            "cs.SE",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.20389v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-23"
    },
    "http://arxiv.org/abs/2510.20388v1": {
        "id": "http://arxiv.org/abs/2510.20388v1",
        "title": "FLAS: a combination of proactive and reactive auto-scaling architecture for distributed services",
        "link": "http://arxiv.org/abs/2510.20388v1",
        "tags": [
            "serving",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes FLAS, an auto-scaler combining proactive forecasting of SLA trends with reactive estimation from resource metrics, enabling adaptive resource scaling. Ensures SLA compliance for distributed services with >99% adherence during worst-case scenarios.",
        "abstract": "Cloud computing has established itself as the support for the vast majority of emerging technologies, mainly due to the characteristic of elasticity it offers. Auto-scalers are the systems that enable this elasticity by acquiring and releasing resources on demand to ensure an agreed service level. In this article we present FLAS (Forecasted Load Auto-Scaling), an auto-scaler for distributed services that combines the advantages of proactive and reactive approaches according to the situation to decide the optimal scaling actions in every moment. The main novelties introduced by FLAS are (i) a predictive model of the high-level metrics trend which allows to anticipate changes in the relevant SLA parameters (e.g. performance metrics such as response time or throughput) and (ii) a reactive contingency system based on the estimation of high-level metrics from resource use metrics, reducing the necessary instrumentation (less invasive) and allowing it to be adapted agnostically to different applications. We provide a FLAS implementation for the use case of a content-based publish-subscribe middleware (E-SilboPS) that is the cornerstone of an event-driven architecture. To the best of our knowledge, this is the first auto-scaling system for content-based publish-subscribe distributed systems (although it is generic enough to fit any distributed service). Through an evaluation based on several test cases recreating not only the expected contexts of use, but also the worst possible scenarios (following the Boundary-Value Analysis or BVA test methodology), we have validated our approach and demonstrated the effectiveness of our solution by ensuring compliance with performance requirements over 99% of the time.",
        "authors": [
            "Víctor Rampérez",
            "Javier Soriano",
            "David Lizcano",
            "Juan A. Lara"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-10-23"
    },
    "http://arxiv.org/abs/2510.20269v1": {
        "title": "In-DRAM True Random Number Generation Using Simultaneous Multiple-Row Activation: An Experimental Study of Real DRAM Chips",
        "link": "http://arxiv.org/abs/2510.20269v1",
        "abstract": "In this work, we experimentally demonstrate that it is possible to generate true random numbers at high throughput and low latency in commercial off-the-shelf (COTS) DRAM chips by leveraging simultaneous multiple-row activation (SiMRA) via an extensive characterization of 96 DDR4 DRAM chips. We rigorously analyze SiMRA's true random generation potential in terms of entropy, latency, and throughput for varying numbers of simultaneously activated DRAM rows (i.e., 2, 4, 8, 16, and 32), data patterns, temperature levels, and spatial variations. Among our 11 key experimental observations, we highlight four key results. First, we evaluate the quality of our TRNG designs using the commonly-used NIST statistical test suite for randomness and find that all SiMRA-based TRNG designs successfully pass each test. Second, 2-, 8-, 16-, and 32-row activation-based TRNG designs outperform the state-of-theart DRAM-based TRNG in throughput by up to 1.15x, 1.99x, 1.82x, and 1.39x, respectively. Third, SiMRA's entropy tends to increase with the number of simultaneously activated DRAM rows. Fourth, operational parameters and conditions (e.g., data pattern and temperature) significantly affect entropy. For example, for most of the tested modules, the average entropy of 32-row activation is 2.51x higher than that of 2-row activation. For example, increasing the temperature from 50°C to 90°C decreases SiMRA's entropy by 1.53x for 32-row activation. To aid future research and development, we open-source our infrastructure at https://github.com/CMU-SAFARI/SiMRA-TRNG.",
        "authors": [
            "Ismail Emir Yuksel",
            "Ataberk Olgun",
            "F. Nisa Bostanci",
            "Oguzhan Canpolat",
            "Geraldo F. Oliveira",
            "Mohammad Sadrosadati",
            "Abdullah Giray Yaglikci",
            "Onur Mutlu"
        ],
        "categories": [
            "cs.AR",
            "cs.CR",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.20269v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-23"
    },
    "http://arxiv.org/abs/2510.20243v1": {
        "title": "HHEML: Hybrid Homomorphic Encryption for Privacy-Preserving Machine Learning on Edge",
        "link": "http://arxiv.org/abs/2510.20243v1",
        "abstract": "Privacy-preserving machine learning (PPML) is an emerging topic to handle secure machine learning inference over sensitive data in untrusted environments. Fully homomorphic encryption (FHE) enables computation directly on encrypted data on the server side, making it a promising approach for PPML. However, it introduces significant communication and computation overhead on the client side, making it impractical for edge devices. Hybrid homomorphic encryption (HHE) addresses this limitation by combining symmetric encryption (SE) with FHE to reduce the computational cost on the client side, and combining with an FHE-friendly SE can also lessen the processing overhead on the server side, making it a more balanced and efficient alternative. Our work proposes a hardware-accelerated HHE architecture built around a lightweight symmetric cipher optimized for FHE compatibility and implemented as a dedicated hardware accelerator. To the best of our knowledge, this is the first design to integrate an end-to-end HHE framework with hardware acceleration. Beyond this, we also present several microarchitectural optimizations to achieve higher performance and energy efficiency. The proposed work is integrated into a full PPML pipeline, enabling secure inference with significantly lower latency and power consumption than software implementations. Our contributions validate the feasibility of low-power, hardware- accelerated HHE for edge deployment and provide a hardware- software co-design methodology for building scalable, secure machine learning systems in resource-constrained environments. Experiments on a PYNQ-Z2 platform with the MNIST dataset show over a 50x reduction in client-side encryption latency and nearly a 2x gain in hardware throughput compared to existing FPGA-based HHE accelerators.",
        "authors": [
            "Yu Hin Chan",
            "Hao Yang",
            "Shiyu Shen",
            "Xingyu Fan",
            "Shengzhe Lyu",
            "Patrick S. Y. Hung",
            "Ray C. C. Cheung"
        ],
        "categories": [
            "cs.CR",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.20243v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-23"
    },
    "http://arxiv.org/abs/2511.01881v1": {
        "title": "HGraphScale: Hierarchical Graph Learning for Autoscaling Microservice Applications in Container-based Cloud Computing",
        "link": "http://arxiv.org/abs/2511.01881v1",
        "abstract": "Microservice architecture has become a dominant paradigm in application development due to its advantages of being lightweight, flexible, and resilient. Deploying microservice applications in the container-based cloud enables fine-grained elastic resource allocation. Autoscaling is an effective approach to dynamically adjust the resource provisioned to containers. However, the intricate microservice dependencies and the deployment scheme of the container-based cloud bring extra challenges of resource scaling. This article proposes a novel autoscaling approach named HGraphScale. In particular, HGraphScale captures microservice dependencies and the deployment scheme by a newly designed hierarchical graph neural network, and makes effective scaling actions for rapidly changing user requests workloads. Extensive experiments based on real-world traces of user requests are conducted to evaluate the effectiveness of HGraphScale. The experiment results show that the HGraphScale outperforms existing state-of-the-art autoscaling approaches by reducing at most 80.16\\% of the average response time under a certain VM rental budget of application providers.",
        "authors": [
            "Zhengxin Fang",
            "Hui Ma",
            "Gang Chen",
            "Rajkumar Buyya"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.01881v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-23"
    },
    "http://arxiv.org/abs/2510.20171v3": {
        "id": "http://arxiv.org/abs/2510.20171v3",
        "title": "Collective Communication for 100k+ GPUs",
        "link": "http://arxiv.org/abs/2510.20171v3",
        "tags": [
            "training",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-10-23",
        "tldr": "Addresses communication bottlenecks in LLM training at extreme scale (100k+ GPUs). Proposes NCCLX, a collective communication framework optimized for high-throughput, low-latency data exchange across massive GPU clusters. Achieves substantial efficiency gains in Llama4 training.",
        "abstract": "The increasing scale of large language models (LLMs) necessitates highly efficient collective communication frameworks, particularly as training workloads extend to hundreds of thousands of GPUs. Traditional communication methods face significant throughput and latency limitations at this scale, hindering both the development and deployment of state-of-the-art models. This paper presents the NCCLX collective communication framework, developed at Meta, engineered to optimize performance across the full LLM lifecycle, from the synchronous demands of large-scale training to the low-latency requirements of inference. The framework is designed to support complex workloads on clusters exceeding 100,000 GPUs, ensuring reliable, high-throughput, and low-latency data exchange. Empirical evaluation on the Llama4 model demonstrates substantial improvements in communication efficiency. This research contributes a robust solution for enabling the next generation of LLMs to operate at unprecedented scales.",
        "authors": [
            "Min Si",
            "Pavan Balaji",
            "Yongzhou Chen",
            "Ching-Hsiang Chu",
            "Adi Gangidi",
            "Saif Hasan",
            "Subodh Iyengar",
            "Dan Johnson",
            "Bingzhe Liu",
            "Regina Ren",
            "Ashmitha Jeevaraj Shetty",
            "Greg Steinbrecher",
            "Yulun Wang",
            "Bruce Wu",
            "Xinfeng Xie",
            "Jingyi Yang",
            "Mingran Yang",
            "Kenny Yu",
            "Minlan Yu",
            "Cen Zhao",
            "Wes Bland",
            "Denis Boyda",
            "Suman Gumudavelli",
            "Prashanth Kannan",
            "Cristian Lumezanu",
            "Rui Miao",
            "Zhe Qu",
            "Venkat Ramesh",
            "Maxim Samoylov",
            "Jan Seidel",
            "Srikanth Sundaresan",
            "Feng Tian",
            "Qiye Tan",
            "Shuqiang Zhang",
            "Yimeng Zhao",
            "Shengbao Zheng",
            "Art Zhu",
            "Hongyi Zeng"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.NI"
        ],
        "submit_date": "2025-10-23"
    },
    "http://arxiv.org/abs/2510.20157v1": {
        "title": "ADP-VRSGP: Decentralized Learning with Adaptive Differential Privacy via Variance-Reduced Stochastic Gradient Push",
        "link": "http://arxiv.org/abs/2510.20157v1",
        "abstract": "Differential privacy is widely employed in decentralized learning to safeguard sensitive data by introducing noise into model updates. However, existing approaches that use fixed-variance noise often degrade model performance and reduce training efficiency. To address these limitations, we propose a novel approach called decentralized learning with adaptive differential privacy via variance-reduced stochastic gradient push (ADP-VRSGP). This method dynamically adjusts both the noise variance and the learning rate using a stepwise-decaying schedule, which accelerates training and enhances final model performance while providing node-level personalized privacy guarantees. To counteract the slowed convergence caused by large-variance noise in early iterations, we introduce a progressive gradient fusion strategy that leverages historical gradients. Furthermore, ADP-VRSGP incorporates decentralized push-sum and aggregation techniques, making it particularly suitable for time-varying communication topologies. Through rigorous theoretical analysis, we demonstrate that ADP-VRSGP achieves robust convergence with an appropriate learning rate, significantly improving training stability and speed. Experimental results validate that our method outperforms existing baselines across multiple scenarios, highlighting its efficacy in addressing the challenges of privacy-preserving decentralized learning.",
        "authors": [
            "Xiaoming Wu",
            "Teng Liu",
            "Xin Wang",
            "Ming Yang",
            "Jiguo Yu"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.20157v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-23"
    },
    "http://arxiv.org/abs/2510.20128v1": {
        "title": "A Full Stack Framework for High Performance Quantum-Classical Computing",
        "link": "http://arxiv.org/abs/2510.20128v1",
        "abstract": "To address the growing needs for scalable High Performance Computing (HPC) and Quantum Computing (QC) integration, we present our HPC-QC full stack framework and its hybrid workload development capability with modular hardware/device-agnostic software integration approach. The latest development in extensible interfaces for quantum programming, dispatching, and compilation within existing mature HPC programming environment are demonstrated. Our HPC-QC full stack enables high-level, portable invocation of quantum kernels from commercial quantum SDKs within HPC meta-program in compiled languages (C/C++ and Fortran) as well as Python through a quantum programming interface library extension. An adaptive circuit knitting hypervisor is being developed to partition large quantum circuits into sub-circuits that fit on smaller noisy quantum devices and classical simulators. At the lower-level, we leverage Cray LLVM-based compilation framework to transform and consume LLVM IR and Quantum IR (QIR) from commercial quantum software frontends in a retargetable fashion to different hardware architectures. Several hybrid HPC-QC multi-node multi-CPU and GPU workloads (including solving linear system of equations, quantum optimization, and simulating quantum phase transitions) have been demonstrated on HPE EX supercomputers to illustrate functionality and execution viability for all three components developed so far. This work provides the framework for a unified quantum-classical programming environment built upon classical HPC software stack (compilers, libraries, parallel runtime and process scheduling).",
        "authors": [
            "Xin Zhan",
            "K. Grace Johnson",
            "Aniello Esposito",
            "Barbara Chapman",
            "Marco Fiorentino",
            "Kirk M. Bresniker",
            "Raymond G. Beausoleil",
            "Masoud Mohseni"
        ],
        "categories": [
            "cs.DC",
            "quant-ph"
        ],
        "id": "http://arxiv.org/abs/2510.20128v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-23"
    },
    "http://arxiv.org/abs/2510.20111v1": {
        "id": "http://arxiv.org/abs/2510.20111v1",
        "title": "AsyncHZP: Hierarchical ZeRO Parallelism with Asynchronous Scheduling for Scalable LLM Training",
        "link": "http://arxiv.org/abs/2510.20111v1",
        "tags": [
            "training",
            "MoE",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-10-23",
        "tldr": "Proposes AsyncHZP, an asynchronous hierarchical ZeRO variant that reduces communication overhead and improves memory efficiency in large-scale LLM training. By adaptively resharding and overlapping communication with computation via multi-stream scheduling, it achieves superior scalability over ND parallelism on both dense and MoE models.",
        "abstract": "The training efficiency and scalability of language models on massive clusters currently remain a critical bottleneck. Mainstream approaches like ND parallelism are often cumbersome and complex, while flexible alternatives such as the Zero Redundancy Optimizer (ZeRO) are frequently hampered by communication overhead. In this paper, we propose Asynchronous Hierarchical Zero Parallelism (AsyncHZP), a novel asynchronous variant of ZeRO designed to achieve superior performance while maintaining simplicity and memory efficiency. Unlike traditional ZeRO, which employs over-fine-grained sharding that can lead to inefficient communication, AsyncHZP adaptively reshards parameters, gradients, and optimizer states across different replica groups. This strategy optimizes device memory utilization and significantly reduces communication overhead. In addition, we also design a multi-stream asynchronous scheduling method that executes parameter all-gather and gradient reduce-scatter operations in dedicated background threads, effectively overlapping communication with computation while incurring negligible memory fragmentation. Empirical evaluations on both Dense and Mixture-of-Experts (MoE) models confirm that AsyncHZP maintains robust stability at scale. It consistently outperforms classic ND parallelism, achieving state-of-the-art performance without complex strategic tuning, thereby simplifying the path to efficient large-scale training.",
        "authors": [
            "Huawei Bai",
            "Yifan Huang",
            "Wenqi Shi",
            "Ansheng You",
            "Feifan Shao",
            "Tengfei Han",
            "Minghui Yu"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-10-23"
    },
    "http://arxiv.org/abs/2510.20053v1": {
        "title": "Parallel Joinable B-Trees in the Fork-Join I/O Model",
        "link": "http://arxiv.org/abs/2510.20053v1",
        "abstract": "Balanced search trees are widely used in computer science to efficiently maintain dynamic ordered data. To support efficient set operations (e.g., union, intersection, difference) using trees, the join-based framework is widely studied. This framework has received particular attention in the parallel setting, and has been shown to be effective in enabling simple and theoretically efficient set operations on trees. Despite the widespread adoption of parallel join-based trees, a major drawback of previous work on such data structures is the inefficiency of their input/output (I/O) access patterns. Some recent work (e.g., C-trees and PaC-trees) focused on more I/O-friendly implementations of these algorithms. Surprisingly, however, there have been no results on bounding the I/O-costs for these algorithms. It remains open whether these algorithms can provide tight, provable guarantees in I/O-costs on trees.   This paper studies efficient parallel algorithms for set operations based on search tree algorithms using a join-based framework, with a special focus on achieving I/O efficiency in these algorithms. To better capture the I/O-efficiency in these algorithms in parallel, we introduce a new computational model, Fork-Join I/O Model, to measure the I/O costs in fork-join parallelism. This model measures the total block transfers (I/O work) and their critical path (I/O span). Under this model, we propose our new solution based on B-trees. Our parallel algorithm computes the union, intersection, and difference of two B-trees with $O(m \\log_B(n/m))$ I/O work and $O(\\log_B m \\cdot \\log_2 \\log_B n + \\log_B n)$ I/O span, where $n$ and $m \\leq n$ are the sizes of the two trees, and $B$ is the block size.",
        "authors": [
            "Michael Goodrich",
            "Yan Gu",
            "Ryuto Kitagawa",
            "Yihan Sun"
        ],
        "categories": [
            "cs.DS",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.20053v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-22"
    },
    "http://arxiv.org/abs/2510.19982v1": {
        "title": "QORE : Quantum Secure 5G/B5G Core",
        "link": "http://arxiv.org/abs/2510.19982v1",
        "abstract": "Quantum computing is reshaping the security landscape of modern telecommunications. The cryptographic foundations that secure todays 5G systems, including RSA, Elliptic Curve Cryptography (ECC), and Diffie-Hellman (DH), are all susceptible to attacks enabled by Shors algorithm. Protecting 5G networks against future quantum adversaries has therefore become an urgent engineering and research priority. In this paper we introduce QORE, a quantum-secure 5G and Beyond 5G (B5G) Core framework that provides a clear pathway for transitioning both the 5G Core Network Functions and User Equipment (UE) to Post-Quantum Cryptography (PQC). The framework uses the NIST-standardized lattice-based algorithms Module-Lattice Key Encapsulation Mechanism (ML-KEM) and Module-Lattice Digital Signature Algorithm (ML-DSA) and applies them across the 5G Service-Based Architecture (SBA). A Hybrid PQC (HPQC) configuration is also proposed, combining classical and quantum-safe primitives to maintain interoperability during migration. Experimental validation shows that ML-KEM achieves quantum security with minor performance overhead, meeting the low-latency and high-throughput requirements of carrier-grade 5G systems. The proposed roadmap aligns with ongoing 3GPP SA3 and SA5 study activities on the security and management of post-quantum networks as well as with NIST PQC standardization efforts, providing practical guidance for mitigating quantum-era risks while safeguarding long-term confidentiality and integrity of network data.",
        "authors": [
            "Vipin Rathi",
            "Lakshya Chopra",
            "Rudraksh Rawal",
            "Nitin Rajput",
            "Shiva Valia",
            "Madhav Aggarwal",
            "Aditya Gairola"
        ],
        "categories": [
            "cs.CR",
            "cs.DC",
            "cs.NI"
        ],
        "id": "http://arxiv.org/abs/2510.19982v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-22"
    },
    "http://arxiv.org/abs/2510.19972v2": {
        "title": "New Hardness Results for the LOCAL Model via a Simple Self-Reduction",
        "link": "http://arxiv.org/abs/2510.19972v2",
        "abstract": "Very recently, Khoury and Schild [FOCS 2025] showed that any randomized LOCAL algorithm that solves maximal matching requires $Ω(\\min\\{\\log Δ, \\log_Δn\\})$ rounds, where $n$ is the number of nodes in the graph and $Δ$ is the maximum degree. This result is shown through a new technique, called round elimination via self-reduction. The lower bound proof is beautiful and presents very nice ideas. However, it spans more than 25 pages of technical details, and hence it is hard to digest and generalize to other problems. Historically, the simplification of proofs and techniques has marked an important turning point in our understanding of the complexity of graph problems. Our paper makes a step forward towards this direction, and provides the following contributions.   1. We present a short and simplified version of the round elimination via self-reduction technique. The simplification of this technique enables us to obtain the following two hardness results.   2. We show that any randomized LOCAL algorithm that solves the maximal $b$-matching problem requires $Ω(\\min\\{\\log_{1+b}Δ, \\log_Δn\\})$ and $Ω(\\sqrt{\\log_{1+b} n})$ rounds. We recall that the $b$-matching problem is a generalization of the matching problem where each vertex can have up to $b$ incident edges in the matching. As a corollary, for $b=1$, we obtain a short proof for the maximal matching lower bound shown by Khoury and Schild.   3. Finally, we show that any randomized LOCAL algorithm that properly colors the edges of a graph with $Δ+ k$ colors requires $Ω(\\min\\{\\log Δ, \\log_Δn\\})$ and $Ω(\\sqrt{\\log n})$ rounds, for any $k\\le Δ^{1-\\varepsilon}$ and any constant $\\varepsilon > 0$.",
        "authors": [
            "Alkida Balliu",
            "Filippo Casagrande",
            "Francesco d'Amore",
            "Dennis Olivetti"
        ],
        "categories": [
            "cs.DC",
            "cs.CC"
        ],
        "id": "http://arxiv.org/abs/2510.19972v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-22"
    },
    "http://arxiv.org/abs/2510.19968v1": {
        "title": "Q-RAN: Quantum-Resilient O-RAN Architecture",
        "link": "http://arxiv.org/abs/2510.19968v1",
        "abstract": "The telecommunications industry faces a dual transformation: the architectural shift toward Open Radio Access Networks (O-RAN) and the emerging threat from quantum computing. O-RAN disaggregated, multi-vendor architecture creates a larger attack surface vulnerable to crypt-analytically relevant quantum computers(CRQCs) that will break current public key cryptography. The Harvest Now, Decrypt Later (HNDL) attack strategy makes this threat immediate, as adversaries can intercept encrypted data today for future decryption. This paper presents Q-RAN, a comprehensive quantum-resistant security framework for O-RAN networks using NIST-standardized Post-Quantum Cryptography (PQC). We detail the implementation of ML-KEM (FIPS 203) and ML-DSA (FIPS 204), integrated with Quantum Random Number Generators (QRNG) for cryptographic entropy. The solution deploys PQ-IPsec, PQ-DTLS, and PQ-mTLS protocols across all O-RAN interfaces, anchored by a centralized Post-Quantum Certificate Authority (PQ-CA) within the SMO framework. This work provides a complete roadmap for securing disaggregated O-RAN ecosystems against quantum adversaries.",
        "authors": [
            "Vipin Rathi",
            "Lakshya Chopra",
            "Madhav Agarwal",
            "Nitin Rajput",
            "Kriish Sharma",
            "Sushant Mundepi",
            "Shivam Gangwar",
            "Rudraksh Rawal",
            "Jishan"
        ],
        "categories": [
            "cs.CR",
            "cs.DC",
            "cs.NI"
        ],
        "id": "http://arxiv.org/abs/2510.19968v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-22"
    },
    "http://arxiv.org/abs/2510.19938v1": {
        "title": "Designing a Secure and Resilient Distributed Smartphone Participant Data Collection System",
        "link": "http://arxiv.org/abs/2510.19938v1",
        "abstract": "Real-world health studies require continuous and secure data collection from mobile and wearable devices. We introduce MotionPI, a smartphone-based system designed to collect behavioral and health data through sensors and surveys with minimal interaction from participants. The system integrates passive data collection (such as GPS and wristband motion data) with Ecological Momentary Assessment (EMA) surveys, which can be triggered randomly or based on physical activity. MotionPI is designed to work under real-life constraints, including limited battery life, weak or intermittent cellular connection, and minimal user supervision. It stores data both locally and on a secure cloud server, with encrypted transmission and storage. It integrates through Bluetooth Low Energy (BLE) into wristband devices that store raw data and communicate motion summaries and trigger events. MotionPI demonstrates a practical solution for secure and scalable mobile data collection in cyber-physical health studies.",
        "authors": [
            "Foad Namjoo",
            "Neng Wan",
            "Devan Mallory",
            "Yuyi Chang",
            "Nithin Sugavanam",
            "Long Yin Lee",
            "Ning Xiong",
            "Emre Ertin",
            "Jeff M. Phillips"
        ],
        "categories": [
            "cs.CR",
            "cs.DC",
            "cs.HC",
            "cs.SE"
        ],
        "id": "http://arxiv.org/abs/2510.19938v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-22"
    },
    "http://arxiv.org/abs/2510.19805v1": {
        "title": "Next Generation Cloud-native In-Memory Stores: From Redis to Valkey and Beyond",
        "link": "http://arxiv.org/abs/2510.19805v1",
        "abstract": "In-memory key-value datastores have become indispensable building blocks of modern cloud-native infrastructures, yet their evolution faces scalability, compatibility, and sustainability constraints. The current literature lacks an experimental evaluation of state-of-the-art tools in the domain. This study addressed this timely gap by benchmarking Redis alternatives and systematically evaluating Valkey, KeyDB, and Garnet under realistic workloads within Kubernetes deployments. The results demonstrate clear trade-offs among the benchmarked data systems. Our study presents a comprehensive performance and viability assessment of the emerging in-memory key-value stores. Metrics include throughput, tail latency, CPU and memory efficiency, and migration complexity. We highlight trade-offs between performance, compatibility, and long-term viability, including project maturity, community support, and sustained development.",
        "authors": [
            "Carl-Johan Fauvelle Munck af Rosensch\"old",
            "Feras M. Awaysheh",
            "Ahmad Awad"
        ],
        "categories": [
            "cs.DC",
            "cs.DB"
        ],
        "id": "http://arxiv.org/abs/2510.19805v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-22"
    },
    "http://arxiv.org/abs/2510.19725v1": {
        "title": "CommonSense: Efficient Set Intersection (SetX) Protocol Based on Compressed Sensing",
        "link": "http://arxiv.org/abs/2510.19725v1",
        "abstract": "In the set reconciliation (\\textsf{SetR}) problem, two parties Alice and Bob, holding sets $\\mathsf{A}$ and $\\mathsf{B}$, communicate to learn the symmetric difference $\\mathsf{A} Δ\\mathsf{B}$. In this work, we study a related but under-explored problem: set intersection (\\textsf{SetX})~\\cite{Ozisik2019}, where both parties learn $\\mathsf{A} \\cap \\mathsf{B}$ instead. However, existing solutions typically reuse \\textsf{SetR} protocols due to the absence of dedicated \\textsf{SetX} protocols and the misconception that \\textsf{SetR} and \\textsf{SetX} have comparable costs. Observing that \\textsf{SetX} is fundamentally cheaper than \\textsf{SetR}, we developed a multi-round \\textsf{SetX} protocol that outperforms the information-theoretic lower bound of \\textsf{SetR} problem. In our \\textsf{SetX} protocol, Alice sends Bob a compressed sensing (CS) sketch of $\\mathsf{A}$ to help Bob identify his unique elements (those in $\\mathsf{B \\setminus A}$). This solves the \\textsf{SetX} problem, if $\\mathsf{A} \\subseteq \\mathsf{B}$. Otherwise, Bob sends a CS sketch of the residue (a set of elements he cannot decode) back to Alice for her to decode her unique elements (those in $\\mathsf{A \\setminus B}$). As such, Alice and Bob communicate back and forth %with a set membership filter (SMF) of estimated $\\mathsf{B \\setminus A}$. Alice updates $\\mathsf{A}$ and communication repeats until both parties agrees on $\\mathsf{A} \\cap \\mathsf{B}$. On real world datasets, experiments show that our $\\mathsf{SetX}$ protocol reduces the communication cost by 8 to 10 times compared to the IBLT-based $\\mathsf{SetR}$ protocol.",
        "authors": [
            "Jingfan Meng",
            "Tianji Yang",
            "Jun Xu"
        ],
        "categories": [
            "cs.DC",
            "cs.NI"
        ],
        "id": "http://arxiv.org/abs/2510.19725v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-22"
    },
    "http://arxiv.org/abs/2510.19689v1": {
        "id": "http://arxiv.org/abs/2510.19689v1",
        "title": "Serverless GPU Architecture for Enterprise HR Analytics: A Production-Scale BDaaS Implementation",
        "link": "http://arxiv.org/abs/2510.19689v1",
        "tags": [
            "offline",
            "serving",
            "quantization"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes a serverless GPU architecture for low-latency, cost-efficient TabNet inference in regulated HR analytics. Integrates serverless GPU runtime with interpretability for compliance. Achieves up to 4.5× higher throughput and 98× lower latency vs. Spark at 90% lower cost per 1K inferences.",
        "abstract": "Industrial and government organizations increasingly depend on data-driven analytics for workforce, finance, and regulated decision processes, where timeliness, cost efficiency, and compliance are critical. Distributed frameworks such as Spark and Flink remain effective for massive-scale batch or streaming analytics but introduce coordination complexity and auditing overheads that misalign with moderate-scale, latency-sensitive inference. Meanwhile, cloud providers now offer serverless GPUs, and models such as TabNet enable interpretable tabular ML, motivating new deployment blueprints for regulated environments. In this paper, we present a production-oriented Big Data as a Service (BDaaS) blueprint that integrates a single-node serverless GPU runtime with TabNet. The design leverages GPU acceleration for throughput, serverless elasticity for cost reduction, and feature-mask interpretability for IL4/FIPS compliance. We conduct benchmarks on the HR, Adult, and BLS datasets, comparing our approach against Spark and CPU baselines. Our results show that GPU pipelines achieve up to 4.5x higher throughput, 98x lower latency, and 90% lower cost per 1K inferences compared to Spark baselines, while compliance mechanisms add only ~5.7 ms latency with p99 < 22 ms. Interpretability remains stable under peak load, ensuring reliable auditability. Taken together, these findings provide a compliance-aware benchmark, a reproducible Helm-packaged blueprint, and a decision framework that demonstrate the practicality of secure, interpretable, and cost-efficient serverless GPU analytics for regulated enterprise and government settings.",
        "authors": [
            "Guilin Zhang",
            "Wulan Guo",
            "Ziqi Tan",
            "Srinivas Vippagunta",
            "Suchitra Raman",
            "Shreeshankar Chatterjee",
            "Ju Lin",
            "Shang Liu",
            "Mary Schladenhauffen",
            "Jeffrey Luo",
            "Hailong Jiang"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.LG"
        ],
        "submit_date": "2025-10-22"
    },
    "http://arxiv.org/abs/2510.19617v1": {
        "title": "Propius: A Platform for Collaborative Machine Learning across the Edge and the Cloud",
        "link": "http://arxiv.org/abs/2510.19617v1",
        "abstract": "Collaborative Machine Learning is a paradigm in the field of distributed machine learning, designed to address the challenges of data privacy, communication overhead, and model heterogeneity. There have been significant advancements in optimization and communication algorithm design and ML hardware that enables fair, efficient and secure collaborative ML training. However, less emphasis is put on collaborative ML infrastructure development. Developers and researchers often build server-client systems for a specific collaborative ML use case, which is not scalable and reusable. As the scale of collaborative ML grows, the need for a scalable, efficient, and ideally multi-tenant resource management system becomes more pressing. We propose a novel system, Propius, that can adapt to the heterogeneity of client machines, and efficiently manage and control the computation flow between ML jobs and edge resources in a scalable fashion. Propius is comprised of a control plane and a data plane. The control plane enables efficient resource sharing among multiple collaborative ML jobs and supports various resource sharing policies, while the data plane improves the scalability of collaborative ML model sharing and result collection. Evaluations show that Propius outperforms existing resource management techniques and frameworks in terms of resource utilization (up to $1.88\\times$), throughput (up to $2.76$), and job completion time (up to $1.26\\times$).",
        "authors": [
            "Eric Ding"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.19617v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-22"
    },
    "http://arxiv.org/abs/2510.19470v1": {
        "id": "http://arxiv.org/abs/2510.19470v1",
        "title": "HybridEP: Scaling Expert Parallelism to Cross-Datacenter Scenario via Hybrid Expert/Data Transmission",
        "link": "http://arxiv.org/abs/2510.19470v1",
        "tags": [
            "MoE",
            "training",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-10-22",
        "tldr": "Addresses scalability limits of MoE training across datacenters due to low cross-DC bandwidth. Proposes HybridEP, a framework that dynamically optimizes expert and data communication patterns via modeling-guided hybrid transmission, achieving up to 5.6x speedup under constrained bandwidth.",
        "abstract": "Mixture-of-Experts (MoE) has become a popular architecture for scaling large models. However, the rapidly growing scale outpaces model training on a single DC, driving a shift toward a more flexible, cross-DC training paradigm. Under this, Expert Parallelism (EP) of MoE faces significant scalability issues due to the limited cross-DC bandwidth. Specifically, existing EP optimizations attempt to overlap data communication and computation, which has little benefit in low-bandwidth scenarios due to a much longer data communication time. Therefore, the trends of cross-DC EP scaling is fast becoming a critical roadblock to the continued growth of MoE models.   To address this, we propose HybridEP, a modeling-guided framework to optimize EP under constrained bandwidth. Our key idea is to dynamically transform the spatial placement of experts to reduce data communication traffic and frequency, thereby minimizing EP's communication overheads. However, it is non-trivial to find the optimal solution because it complicates the original communication pattern by mixing data and expert communication. We therefore build a stream-based model to determine the optimal transmission ratio. Guided by this, we incorporate two techniques: (1) domain-based partition to construct the mapping between hybrid patterns and specific communication topology at GPU level, and (2) parameter-efficient migration to further refine this topology by reducing expert transmission overhead and enlarging the domain size. Combining all these designs, HybridEP can be considered as a more general EP with better scalability. Experimental results show that HybridEP outperforms existing state-of-the-art MoE training systems by up to 5.6x under constrained bandwidth. We further compare HybridEP and EP on large-scale simulations. HybridEP achieves up to 1.45x speedup with 1k DCs under different bandwidths.",
        "authors": [
            "Weihao Yang",
            "Hao Huang",
            "Donglei Wu",
            "Ningke Li",
            "Yanqi Pan",
            "Qiyang Zheng",
            "Wen Xia",
            "Shiyi Li",
            "Qiang Wang"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.LG"
        ],
        "submit_date": "2025-10-22"
    },
    "http://arxiv.org/abs/2510.19322v1": {
        "id": "http://arxiv.org/abs/2510.19322v1",
        "title": "Enabling Reconfiguration-Communication Overlap for Collective Communication in Optical Networks",
        "link": "http://arxiv.org/abs/2510.19322v1",
        "tags": [
            "networking",
            "training"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Addresses inefficiency in optical networks for distributed ML training collectives. Proposes SWOT framework with intra-collective reconfiguration, overlapping switch reconfigurations with transmissions. Achieves performance improvements in simulations.",
        "abstract": "Collective communication (CC) is widely adopted for large-scale distributed machine learning (DML) training workloads. DML's predictable traffic pattern provides a great oppotunity for applying optical network technology. Existing optical interconnects-based CC schemes adopt ``one-shot network reconfiguration'', which provisions static high-capacity topologies for an entire collective operation -- sometimes for a full training iteration. However, this approach faces significant scalability limitations when supporting more complex and efficient CC algorithms required for modern workloads: the ``one-shot'' strategies either demand excessive resource overprovisioning or suffer performance degradation due to rigid resource allocation.   To address these challenges, we propose SWOT, a demand-aware optical network framework. SWOT employs ``intra-collective reconfiguration'' and can dynamically align network resources with CC traffic patterns. SWOT incorporates a novel scheduling technique that overlaps optical switch reconfigurations with ongoing transmissions, and improves communication efficiency. SWOT introduce a lightweight collective communication shim that enables coordinated optical network configuration and transmission scheduling while supporting seamless integration with existing CC libraries. Our simulation results demonstrate SWOT's significant performance improvements.",
        "authors": [
            "Changbo Wu",
            "Zhuolong Yu",
            "Gongming Zhao",
            "Hongli Xu"
        ],
        "categories": [
            "cs.NI",
            "cs.AI",
            "cs.DC"
        ],
        "submit_date": "2025-10-22"
    },
    "http://arxiv.org/abs/2510.19301v2": {
        "title": "FLASH Viterbi: Fast and Adaptive Viterbi Decoding for Modern Data Systems",
        "link": "http://arxiv.org/abs/2510.19301v2",
        "abstract": "The Viterbi algorithm is a key operator for structured sequence inference in modern data systems, with applications in trajectory analysis, online recommendation, and speech recognition. As these workloads increasingly migrate to resource-constrained edge platforms, standard Viterbi decoding remains memory-intensive and computationally inflexible. Existing methods typically trade decoding time for space efficiency, but often incur significant runtime overhead and lack adaptability to various system constraints. This paper presents FLASH Viterbi, a Fast, Lightweight, Adaptive, and Hardware-Friendly Viterbi decoding operator that enhances adaptability and resource efficiency. FLASH Viterbi combines a non-recursive divide-and-conquer strategy with pruning and parallelization techniques to enhance both time and memory efficiency, making it well-suited for resource-constrained data systems. To further decouple space complexity from the hidden state space size, we present FLASH-BS Viterbi, a dynamic beam search variant built on a memory-efficient data structure. Both proposed algorithms exhibit strong adaptivity to diverse deployment scenarios by dynamically tuning internal parameters. To ensure practical deployment on edge devices, we also develop FPGA-based hardware accelerators for both algorithms, demonstrating high throughput and low resource usage. Extensive experiments show that our algorithms consistently outperform existing baselines in both decoding time and memory efficiency, while preserving adaptability and hardware-friendly characteristics essential for modern data systems. All codes are publicly available at https://github.com/Dzh-16/FLASH-Viterbi.",
        "authors": [
            "Ziheng Deng",
            "Xue Liu",
            "Jiantong Jiang",
            "Yankai Li",
            "Qingxu Deng",
            "Xiaochun Yang"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.19301v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-22"
    },
    "http://arxiv.org/abs/2510.19262v2": {
        "id": "http://arxiv.org/abs/2510.19262v2",
        "title": "RailS: Load Balancing for All-to-All Communication in Distributed Mixture-of-Experts Training",
        "link": "http://arxiv.org/abs/2510.19262v2",
        "tags": [
            "training",
            "MoE",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-10-22",
        "tldr": "Addresses all-to-all communication bottlenecks in distributed MoE training by leveraging Rail topology symmetry to enable local, topology-aware load balancing. RailS uses multipath spraying and LPT scheduling to reduce iteration time by up to 40% and boost bus bandwidth by up to 78%.",
        "abstract": "Training Mixture-of-Experts (MoE) models introduces sparse and highly imbalanced all-to-all communication that dominates iteration time. Conventional load-balancing methods fail to exploit the deterministic topology of Rail architectures, leaving multi-NIC bandwidth underutilized. We present RailS, a distributed load-balancing framework that minimizes all-to-all completion time in MoE training. RailS leverages the Rail topology's symmetry to prove that uniform sending ensures uniform receiving, transforming global coordination into local scheduling. Each node independently executes a Longest Processing Time First (LPT) spraying scheduler to proactively balance traffic using local information. RailS activates N parallel rails for fine-grained, topology-aware multipath transmission. Across synthetic and real-world MoE workloads, RailS improves bus bandwidth by 20%--78% and reduces completion time by 17%--78%. For Mixtral workloads, it shortens iteration time by 18%--40% and achieves near-optimal load balance, fully exploiting architectural parallelism in distributed training.",
        "authors": [
            "Heng Xu",
            "Zhiwei Yu",
            "Chengze Du",
            "Ying Zhou",
            "Letian Li",
            "Haojie Wang",
            "Weiqiang Cheng",
            "Jialong Li"
        ],
        "categories": [
            "cs.DC",
            "cs.NI"
        ],
        "submit_date": "2025-10-22"
    },
    "http://arxiv.org/abs/2510.19225v2": {
        "id": "http://arxiv.org/abs/2510.19225v2",
        "title": "RLBoost: Harvesting Preemptible Resources for Cost-Efficient Reinforcement Learning on LLMs",
        "link": "http://arxiv.org/abs/2510.19225v2",
        "tags": [
            "RL",
            "offloading",
            "training"
        ],
        "relevant": true,
        "indexed_date": "2025-10-22",
        "tldr": "Improves cost efficiency of RL for LLMs by harvesting preemptible resources. Proposes RLBoost with adaptive offloading, pull-based weight transfer, and token-level migration for rollout. Achieves 28%-49% cost reduction and 1.51x-1.97x training throughput gain over on-demand usage.",
        "abstract": "Reinforcement learning (RL) has become essential for unlocking advanced reasoning capabilities in large language models (LLMs). RL workflows involve interleaving rollout and training stages with fundamentally different resource requirements. Rollout typically dominates overall execution time, yet scales efficiently through multiple independent instances. In contrast, training requires tightly-coupled GPUs with full-mesh communication. Existing RL frameworks fall into two categories: co-located and disaggregated architectures. Co-located ones fail to address this resource tension by forcing both stages to share the same GPUs. Disaggregated architectures, without modifications of well-established RL algorithms, suffer from resource under-utilization. Meanwhile, preemptible GPU resources, i.e., spot instances on public clouds and spare capacity in production clusters, present significant cost-saving opportunities for accelerating RL workflows, if efficiently harvested for rollout.   In this paper, we present RLBoost, a systematic solution for cost-efficient RL training that harvests preemptible GPU resources. Our key insight is that rollout's stateless and embarrassingly parallel nature aligns perfectly with preemptible and often fragmented resources. To efficiently utilize these resources despite frequent and unpredictable availability changes, RLBoost adopts a hybrid architecture with three key techniques: (1) adaptive rollout offload to dynamically adjust workloads on the reserved (on-demand) cluster, (2) pull-based weight transfer that quickly provisions newly available instances, and (3) token-level response collection and migration for efficient preemption handling and continuous load balancing. Extensive experiments show RLBoost increases training throughput by 1.51x-1.97x while improving cost efficiency by 28%-49% compared to using only on-demand GPU resources.",
        "authors": [
            "Yongji Wu",
            "Xueshen Liu",
            "Haizhong Zheng",
            "Juncheng Gu",
            "Beidi Chen",
            "Z. Morley Mao",
            "Arvind Krishnamurthy",
            "Ion Stoica"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-10-22"
    },
    "http://arxiv.org/abs/2510.19151v1": {
        "title": "On the Randomized Locality of Matching Problems in Regular Graphs",
        "link": "http://arxiv.org/abs/2510.19151v1",
        "abstract": "The main goal in distributed symmetry-breaking is to understand the locality of problems; i.e., the radius of the neighborhood that a node needs to explore in order to arrive at its part of a global solution. In this work, we study the locality of matching problems in the family of regular graphs, which is one of the main benchmarks for establishing lower bounds on the locality of symmetry-breaking problems, as well as for obtaining classification results. For approximate matching, we develop randomized algorithms to show that $(1 + ε)$-approximate matching in regular graphs is truly local; i.e., the locality depends only on $ε$ and is independent of all other graph parameters. Furthermore, as long as the degree $Δ$ is not very small (namely, as long as $Δ\\geq \\text{poly}(1/ε)$), this dependence is only logarithmic in $1/ε$. This stands in sharp contrast to maximal matching in regular graphs which requires some dependence on the number of nodes $n$ or the degree $Δ$. We show matching lower bounds for both results. For maximal matching, our techniques further allow us to establish a strong separation between the node-averaged complexity and worst-case complexity of maximal matching in regular graphs, by showing that the former is only $O(1)$. Central to our main technical contribution is a novel martingale-based analysis for the $\\approx 40$-year-old algorithm by Luby. In particular, our analysis shows that applying one round of Luby's algorithm on the line graph of a $Δ$-regular graph results in an almost $Δ/2$-regular graph.",
        "authors": [
            "Seri Khoury",
            "Manish Purohit",
            "Aaron Schild",
            "Joshua Wang"
        ],
        "categories": [
            "cs.DC",
            "cs.DS"
        ],
        "id": "http://arxiv.org/abs/2510.19151v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-22"
    },
    "http://arxiv.org/abs/2511.01872v1": {
        "id": "http://arxiv.org/abs/2511.01872v1",
        "title": "Learned Cost Model for Placement on Reconfigurable Dataflow Hardware",
        "link": "http://arxiv.org/abs/2511.01872v1",
        "tags": [
            "hardware",
            "training"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes a learned cost model to predict throughput for mapping ML dataflow graphs onto reconfigurable hardware, improving accuracy by 31-52% and achieving 5.6% faster compiled graphs compared to analytical models.",
        "abstract": "Mapping a dataflow-graph of an ML model onto a reconfigurable system is difficult, as different mappings have different throughputs and consume resource constraints differently. To solve this, a model to evaluate the throughput of mappings is necessary as measuring throughput completely is expensive. Many use a hand-designed analytical model, relying on proxy features or intuition, introducing error. We provide a Learned Approach that predicts throughput 31%-52% more accurately over a variety of graphs. In addition, our approach shows no accuracy degradation after removing performance annotations. We show that using this approach results in 5.6% faster compiled graphs.",
        "authors": [
            "Etash Guha",
            "Tianxiao Jiang",
            "Andrew Deng",
            "Jian Zhang",
            "Muthu Annamalai"
        ],
        "categories": [
            "cs.DC",
            "cs.LG",
            "cs.PL"
        ],
        "submit_date": "2025-10-21"
    },
    "http://arxiv.org/abs/2510.19012v1": {
        "title": "Comparative analysis of large data processing in Apache Spark using Java, Python and Scala",
        "link": "http://arxiv.org/abs/2510.19012v1",
        "abstract": "During the study, the results of a comparative analysis of the process of handling large datasets using the Apache Spark platform in Java, Python, and Scala programming languages were obtained. Although prior works have focused on individual stages, comprehensive comparisons of full ETL workflows across programming languages using Apache Iceberg remain limited. The analysis was performed by executing several operations, including downloading data from CSV files, transforming and loading it into an Apache Iceberg analytical table. It was found that the performance of the Spark algorithm varies significantly depending on the amount of data and the programming language used. When processing a 5-megabyte CSV file, the best result was achieved in Python: 6.71 seconds, which is superior to Scala's score of 9.13 seconds and Java's time of 9.62 seconds. For processing a large CSV file of 1.6 gigabytes, all programming languages demonstrated similar results: the fastest performance was showed in Python: 46.34 seconds, while Scala and Java showed results of 47.72 and 50.56 seconds, respectively. When performing a more complex operation that involved combining two CSV files into a single dataset for further loading into an Apache Iceberg table, Scala demonstrated the highest performance, at 374.42 seconds. Java processing was completed in 379.8 seconds, while Python was the least efficient, with a runtime of 398.32 seconds. It follows that the programming language significantly affects the efficiency of data processing by the Apache Spark algorithm, with Scala and Java being more productive for processing large amounts of data and complex operations, while Python demonstrates an advantage in working with small amounts of data. The results obtained can be useful for optimizing data handling processes depending on specific performance requirements and the amount of information being processed.",
        "authors": [
            "Ivan Borodii",
            "Illia Fedorovych",
            "Halyna Osukhivska",
            "Diana Velychko",
            "Roman Butsii"
        ],
        "categories": [
            "cs.DC",
            "cs.DB",
            "cs.PL",
            "cs.SE"
        ],
        "id": "http://arxiv.org/abs/2510.19012v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-21"
    },
    "http://arxiv.org/abs/2510.18838v1": {
        "title": "PCMS: Parallel Coupler For Multimodel Simulations",
        "link": "http://arxiv.org/abs/2510.18838v1",
        "abstract": "This paper presents the Parallel Coupler for Multimodel Simulations (PCMS), a new GPU accelerated generalized coupling framework for coupling simulation codes on leadership class supercomputers. PCMS includes distributed control and field mapping methods for up to five dimensions. For field mapping PCMS can utilize discretization and field information to accommodate physics constraints. PCMS is demonstrated with a coupling of the gyrokinetic microturbulence code XGC with a Monte Carlo neutral transport code DEGAS2 and with a 5D distribution function coupling of an energetic particle transport code (GNET) to a gyrokinetic microturbulence code (GTC). Weak scaling is also demonstrated on up to 2,080 GPUs of Frontier with a weak scaling efficiency of 85%.",
        "authors": [
            "Jacob S. Merson",
            "Cameron W. Smith",
            "Mark S. Shephard",
            "Fuad Hasan",
            "Abhiyan Paudel",
            "Angel Castillo-Crooke",
            "Joyal Mathew",
            "Mohammad Elahi"
        ],
        "categories": [
            "cs.DC",
            "physics.comp-ph",
            "physics.plasm-ph"
        ],
        "id": "http://arxiv.org/abs/2510.18838v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-21"
    },
    "http://arxiv.org/abs/2510.18830v1": {
        "id": "http://arxiv.org/abs/2510.18830v1",
        "title": "MTraining: Distributed Dynamic Sparse Attention for Efficient Ultra-Long Context Training",
        "link": "http://arxiv.org/abs/2510.18830v1",
        "tags": [
            "training",
            "sparse"
        ],
        "relevant": true,
        "indexed_date": "2025-10-21",
        "tldr": "Addresses inefficient training of LLMs with ultra-long contexts due to computational imbalance. Proposes MTraining, a distributed method with dynamic sparse attention and balanced ring attention, achieving 6× higher training throughput while scaling context from 32K to 512K tokens on 32 A100 GPUs.",
        "abstract": "The adoption of long context windows has become a standard feature in Large Language Models (LLMs), as extended contexts significantly enhance their capacity for complex reasoning and broaden their applicability across diverse scenarios. Dynamic sparse attention is a promising approach for reducing the computational cost of long-context. However, efficiently training LLMs with dynamic sparse attention on ultra-long contexts-especially in distributed settings-remains a significant challenge, due in large part to worker- and step-level imbalance. This paper introduces MTraining, a novel distributed methodology leveraging dynamic sparse attention to enable efficient training for LLMs with ultra-long contexts. Specifically, MTraining integrates three key components: a dynamic sparse training pattern, balanced sparse ring attention, and hierarchical sparse ring attention. These components are designed to synergistically address the computational imbalance and communication overheads inherent in dynamic sparse attention mechanisms during the training of models with extensive context lengths. We demonstrate the efficacy of MTraining by training Qwen2.5-3B, successfully expanding its context window from 32K to 512K tokens on a cluster of 32 A100 GPUs. Our evaluations on a comprehensive suite of downstream tasks, including RULER, PG-19, InfiniteBench, and Needle In A Haystack, reveal that MTraining achieves up to a 6x higher training throughput while preserving model accuracy. Our code is available at https://github.com/microsoft/MInference/tree/main/MTraining.",
        "authors": [
            "Wenxuan Li",
            "Chengruidong Zhang",
            "Huiqiang Jiang",
            "Yucheng Li",
            "Yuqing Yang",
            "Lili Qiu"
        ],
        "categories": [
            "cs.CL",
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-10-21"
    },
    "http://arxiv.org/abs/2510.18756v1": {
        "title": "sNVMe-oF: Secure and Efficient Disaggregated Storage",
        "link": "http://arxiv.org/abs/2510.18756v1",
        "abstract": "Disaggregated storage with NVMe-over-Fabrics (NVMe-oF) has emerged as the standard solution in modern data centers, achieving superior performance, resource utilization, and power efficiency. Simultaneously, confidential computing (CC) is becoming the de facto security paradigm, enforcing stronger isolation and protection for sensitive workloads. However, securing state-of-the-art storage with traditional CC methods struggles to scale and compromises performance or security. To address these issues, we introduce sNVMe-oF, a storage management system extending the NVMe-oF protocol and adhering to the CC threat model by providing confidentiality, integrity, and freshness guarantees. sNVMe-oF offers an appropriate control path and novel concepts such as counter-leasing. sNVMe-oF also optimizes data path performance by leveraging NVMe metadata, introducing a new disaggregated Hazel Merkle Tree (HMT), and avoiding redundant IPSec protections. We achieve this without modifying the NVMe-oF protocol. To prevent excessive resource usage while delivering line rate, sNVMe-oF also uses accelerators of CC-capable smart NICs. We prototype sNVMe-oF on an NVIDIA BlueField-3 and demonstrate how it can achieve as little as 2% performance degradation for synthetic patterns and AI training.",
        "authors": [
            "Marcin Chrapek",
            "Meni Orenbach",
            "Ahmad Atamli",
            "Marcin Copik",
            "Fritz Alder",
            "Torsten Hoefler"
        ],
        "categories": [
            "cs.CR",
            "cs.AR",
            "cs.DC",
            "cs.NI",
            "cs.OS"
        ],
        "id": "http://arxiv.org/abs/2510.18756v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-21"
    },
    "http://arxiv.org/abs/2510.18640v1": {
        "title": "Towards an Optimized Benchmarking Platform for CI/CD Pipelines",
        "link": "http://arxiv.org/abs/2510.18640v1",
        "abstract": "Performance regressions in large-scale software systems can lead to substantial resource inefficiencies, making their early detection critical. Frequent benchmarking is essential for identifying these regressions and maintaining service-level agreements (SLAs). Performance benchmarks, however, are resource-intensive and time-consuming, which is a major challenge for integration into Continuous Integration / Continuous Deployment (CI/CD) pipelines. Although numerous benchmark optimization techniques have been proposed to accelerate benchmark execution, there is currently no practical system that integrates these optimizations seamlessly into real-world CI/CD pipelines. In this vision paper, we argue that the field of benchmark optimization remains under-explored in key areas that hinder its broader adoption. We identify three central challenges to enabling frequent and efficient benchmarking: (a) the composability of benchmark optimization strategies, (b) automated evaluation of benchmarking results, and (c) the usability and complexity of applying these strategies as part of CI/CD systems in practice. We also introduce a conceptual cloud-based benchmarking framework handling these challenges transparently. By presenting these open problems, we aim to stimulate research toward making performance regression detection in CI/CD systems more practical and effective.",
        "authors": [
            "Nils Japke",
            "Sebastian Koch",
            "Helmut Lukasczyk",
            "David Bermbach"
        ],
        "categories": [
            "cs.DC",
            "cs.SE"
        ],
        "id": "http://arxiv.org/abs/2510.18640v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-21"
    },
    "http://arxiv.org/abs/2510.18592v1": {
        "title": "Distributed Interactive Proofs for Planarity with Log-Star Communication",
        "link": "http://arxiv.org/abs/2510.18592v1",
        "abstract": "We provide new communication-efficient distributed interactive proofs for planarity. The notion of a \\emph{distributed interactive proof (DIP)} was introduced by Kol, Oshman, and Saxena (PODC 2018). In a DIP, the \\emph{prover} is a single centralized entity whose goal is to prove a certain claim regarding an input graph $G$. To do so, the prover communicates with a distributed \\emph{verifier} that operates concurrently on all $n$ nodes of $G$. A DIP is measured by the amount of prover-verifier communication it requires. Namely, the goal is to design a DIP with a small number of interaction rounds and a small \\emph{proof size}, i.e., a small amount of communication per round. Our main result is an $O(\\log ^{*}n)$-round DIP protocol for embedded planarity and planarity with a proof size of $O(1)$ and $O(\\lceil\\log Δ/\\log ^{*}n\\rceil)$, respectively. In fact, this result can be generalized as follows. For any $1\\leq r\\leq \\log^{*}n$, there exists an $O(r)$-round protocol for embedded planarity and planarity with a proof size of $O(\\log ^{(r)}n)$ and $O(\\log ^{(r)}n+\\log Δ/r)$, respectively.",
        "authors": [
            "Yuval Gil",
            "Merav Parter"
        ],
        "categories": [
            "cs.DC",
            "cs.DS"
        ],
        "id": "http://arxiv.org/abs/2510.18592v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-21"
    },
    "http://arxiv.org/abs/2510.18586v2": {
        "id": "http://arxiv.org/abs/2510.18586v2",
        "title": "Tokencake: A KV-Cache-centric Serving Framework for LLM-based Multi-Agent Applications",
        "link": "http://arxiv.org/abs/2510.18586v2",
        "tags": [
            "serving",
            "offloading",
            "RL"
        ],
        "relevant": true,
        "indexed_date": "2025-10-21",
        "tldr": "Addresses KV cache inefficiencies in multi-agent LLM serving with frequent function calls. Introduces Tokencake framework with space scheduling via dynamic partitioning and time scheduling via proactive offload/predictive upload. Reduces end-to-end latency by 47.06% and improves GPU memory utilization by 16.9% over vLLM.",
        "abstract": "Large Language Models (LLMs) are increasingly deployed in complex multi-agent applications that use external function calls. This workload creates severe performance challenges for the KV Cache: space contention leads to the eviction of critical agents' caches and time underutilization leaves the cache of agents stalled on long-running tool calls idling in GPU memory. We present Tokencake, a KV-Cache-centric serving framework that co-optimizes scheduling and memory management with an agent-aware design. Tokencake's Space Scheduler uses dynamic memory partitioning to shield critical agents from contention, while its Time Scheduler employs a proactive offload and predictive upload mechanism to repurpose GPU memory during function call stalls. Our evaluation on representative multi-agent benchmarks shows that Tokencake can reduce end-to-end latency by over 47.06%, improve effective GPU memory utilization by up to 16.9% compared to vLLM.",
        "authors": [
            "Zhuohang Bian",
            "Feiyang Wu",
            "Teng Ma",
            "Youwei Zhuo"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-10-21"
    },
    "http://arxiv.org/abs/2511.01871v1": {
        "title": "Structural Analysis of Multi-Core Processor and Reliability Evaluation Model",
        "link": "http://arxiv.org/abs/2511.01871v1",
        "abstract": "In the present paper, the models of structural analysis and evaluation of efficiency indicators (reliability, fault tolerance, viability, and flexibility) of a multi core processor with variable structure, equipped with multi functional cores, are considered. Using logical probabilistic methods, the following has been developed: models for evaluating the reliability and fault tolerance of processor cores as multi functional elements; logical probabilistic models of the shortest paths, flexibility, and performance conditions for successful operation of multi core processors based on multi functional cores; and models for estimating the reliability, fault tolerance, and lifetime of multi core processors considering all possible states of performance. The results of the structural analysis of two core and four core processors and the trends of increasing the efficiency indicators of multi core processors are presented.",
        "authors": [
            "S. Tsiramua",
            "H. Meladze",
            "T. Davitashvili",
            "J. M. Sanchez",
            "F. Criado-Aldeanueva"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.01871v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-21"
    },
    "http://arxiv.org/abs/2510.18544v3": {
        "id": "http://arxiv.org/abs/2510.18544v3",
        "title": "SLICE: SLO-Driven Scheduling for LLM Inference on Edge Computing Devices",
        "link": "http://arxiv.org/abs/2510.18544v3",
        "tags": [
            "serving",
            "edge"
        ],
        "relevant": true,
        "indexed_date": "2025-10-21",
        "tldr": "Develops SLICE, an SLO-driven scheduler for LLM inference on edge devices, combining utility-maximizing request scheduling with dynamic iterative generation rate control. Achieves up to 35x higher SLO attainment than Orca and FastServe.",
        "abstract": "Large Language Models (LLMs), as the foundational architecture for next-generation interactive AI applications, not only power intelligent dialogue systems but also drive the evolution of embodied intelligence on edge devices, including humanoid robots, smart vehicles, and other scenarios. The applications running on these edge devices impose differentiated Service Level Objectives (SLO) requirements on LLM services, specifically manifested as distinct constraints on Time to First Token (TTFT) and Time Per Output Token (TPOT) as well as end-to-end latency. Notably, edge devices typically handle real-time tasks that are extremely sensitive to latency, such as machine control and navigation planning. However, existing scheduling service systems still prioritize maximizing output token throughput as the sole optimization objective, failing to adequately address the diversity of SLO requirements. This ultimately results in persistently high violation rates for end-to-end latency or TPOT related SLOs.   This paper proposes SLICE, an innovative scheduling solution designed for edge computing scenarios with differentiated SLO requirements. By combining a utility-maximizing request scheduling algorithm with a dynamic iterative control mechanism for generation rates, SLICE significantly improves LLM inference service SLO attainment. Experimental results demonstrate that compared to state-of-the-art solutions Orca and FastServe, SLICE achieves up to 35x higher SLO attainment and 3.4x advantage in task completion time than the other two solutions. This version is temporarily hosted anonymously for double-blind review.",
        "authors": [
            "Will Chow"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-10-21"
    },
    "http://arxiv.org/abs/2510.18477v2": {
        "title": "LAFA: Agentic LLM-Driven Federated Analytics over Decentralized Data Sources",
        "link": "http://arxiv.org/abs/2510.18477v2",
        "abstract": "Large Language Models (LLMs) have shown great promise in automating data analytics tasks by interpreting natural language queries and generating multi-operation execution plans. However, existing LLM-agent-based analytics frameworks operate under the assumption of centralized data access, offering little to no privacy protection. In contrast, federated analytics (FA) enables privacy-preserving computation across distributed data sources, but lacks support for natural language input and requires structured, machine-readable queries. In this work, we present LAFA, the first system that integrates LLM-agent-based data analytics with FA. LAFA introduces a hierarchical multi-agent architecture that accepts natural language queries and transforms them into optimized, executable FA workflows. A coarse-grained planner first decomposes complex queries into sub-queries, while a fine-grained planner maps each subquery into a Directed Acyclic Graph of FA operations using prior structural knowledge. To improve execution efficiency, an optimizer agent rewrites and merges multiple DAGs, eliminating redundant operations and minimizing computational and communicational overhead. Our experiments demonstrate that LAFA consistently outperforms baseline prompting strategies by achieving higher execution plan success rates and reducing resource-intensive FA operations by a substantial margin. This work establishes a practical foundation for privacy-preserving, LLM-driven analytics that supports natural language input in the FA setting.",
        "authors": [
            "Haichao Ji",
            "Zibo Wang",
            "Cheng Pan",
            "Meng Han",
            "Yifei Zhu",
            "Dan Wang",
            "Zhu Han"
        ],
        "categories": [
            "cs.AI",
            "cs.CR",
            "cs.DC",
            "cs.MA"
        ],
        "id": "http://arxiv.org/abs/2510.18477v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-21"
    },
    "http://arxiv.org/abs/2510.18300v1": {
        "title": "A Distributed Framework for Causal Modeling of Performance Variability in GPU Traces",
        "link": "http://arxiv.org/abs/2510.18300v1",
        "abstract": "Large-scale GPU traces play a critical role in identifying performance bottlenecks within heterogeneous High-Performance Computing (HPC) architectures. However, the sheer volume and complexity of a single trace of data make performance analysis both computationally expensive and time-consuming. To address this challenge, we present an end-to-end parallel performance analysis framework designed to handle multiple large-scale GPU traces efficiently. Our proposed framework partitions and processes trace data concurrently and employs causal graph methods and parallel coordinating chart to expose performance variability and dependencies across execution flows. Experimental results demonstrate a 67% improvement in terms of scalability, highlighting the effectiveness of our pipeline for analyzing multiple traces independently.",
        "authors": [
            "Ankur Lahiry",
            "Ayush Pokharel",
            "Banooqa Banday",
            "Seth Ockerman",
            "Amal Gueroudji",
            "Mohammad Zaeed",
            "Tanzima Z. Islam",
            "Line Pouchard"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "id": "http://arxiv.org/abs/2510.18300v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-21"
    },
    "http://arxiv.org/abs/2511.01866v1": {
        "id": "http://arxiv.org/abs/2511.01866v1",
        "title": "EdgeReasoning: Characterizing Reasoning LLM Deployment on Edge GPUs",
        "link": "http://arxiv.org/abs/2511.01866v1",
        "tags": [
            "edge",
            "serving",
            "thinking"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Characterizes deployment of reasoning LLMs on edge GPUs under latency constraints. Evaluates model sizes, token reduction techniques, and parallelism strategies to balance accuracy-latency. Achieves optimized configurations mapping Pareto frontiers, improving edge deployment efficiency by up to 4x.",
        "abstract": "Edge intelligence paradigm is increasingly demanded by the emerging autonomous systems, such as robotics. Beyond ensuring privacy-preserving operation and resilience in connectivity-limited environments, edge deployment offers significant energy and cost advantages over cloud-based solutions. However, deploying large language models (LLMs) for reasoning tasks on edge GPUs faces critical challenges from strict latency constraints and limited computational resources. To navigate these constraints, developers must balance multiple design factors - choosing reasoning versus non-reasoning architectures, selecting appropriate model sizes, allocating token budgets, and applying test-time scaling strategies - to meet target latency and optimize accuracy. Yet guidance on optimal combinations of these variables remains scarce. In this work, we present EdgeReasoning, a comprehensive study characterizing the deployment of reasoning LLMs on edge GPUs. We systematically quantify latency-accuracy tradeoffs across various LLM architectures and model sizes. We systematically evaluate prompt-based and model-tuning-based techniques for reducing reasoning token length while maintaining performance quality. We further profile test-time scaling methods with varying degrees of parallelism to maximize accuracy under strict latency budgets. Through these analyses, EdgeReasoning maps the Pareto frontier of achievable accuracy-latency configurations, offering systematic guidance for optimal edge deployment of reasoning LLMs.",
        "authors": [
            "Benjamin Kubwimana",
            "Qijing Huang"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.AR"
        ],
        "submit_date": "2025-10-21"
    },
    "http://arxiv.org/abs/2510.18273v1": {
        "title": "Distributed Allocation and Resource Scheduling Algorithms Resilient to Link Failure",
        "link": "http://arxiv.org/abs/2510.18273v1",
        "abstract": "Distributed resource allocation (DRA) is fundamental to modern networked systems, spanning applications from economic dispatch in smart grids to CPU scheduling in data centers. Conventional DRA approaches require reliable communication, yet real-world networks frequently suffer from link failures, packet drops, and communication delays due to environmental conditions, network congestion, and security threats.   We introduce a novel resilient DRA algorithm that addresses these critical challenges, and our main contributions are as follows: (1) guaranteed constraint feasibility at all times, ensuring resource-demand balance even during algorithm termination or network disruption; (2) robust convergence despite sector-bound nonlinearities at nodes/links, accommodating practical constraints like quantization and saturation; and (3) optimal performance under merely uniformly-connected networks, eliminating the need for continuous connectivity.   Unlike existing approaches that require persistent network connectivity and provide only asymptotic feasibility, our graph-theoretic solution leverages network percolation theory to maintain performance during intermittent disconnections. This makes it particularly valuable for mobile multi-agent systems where nodes frequently move out of communication range. Theoretical analysis and simulations demonstrate that our algorithm converges to optimal solutions despite heterogeneous time delays and substantial link failures, significantly advancing the reliability of distributed resource allocation in practical network environments.",
        "authors": [
            "Mohammadreza Doostmohammadian",
            "Sergio Pequito"
        ],
        "categories": [
            "eess.SY",
            "cs.DC",
            "cs.MA",
            "eess.SP",
            "math.OC"
        ],
        "id": "http://arxiv.org/abs/2510.18273v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-21"
    },
    "http://arxiv.org/abs/2510.18152v1": {
        "title": "Efficient Multi-Worker Selection based Distributed Swarm Learning via Analog Aggregation",
        "link": "http://arxiv.org/abs/2510.18152v1",
        "abstract": "Recent advances in distributed learning systems have introduced effective solutions for implementing collaborative artificial intelligence techniques in wireless communication networks. Federated learning approaches provide a model-aggregation mechanism among edge devices to achieve collaborative training, while ensuring data security, communication efficiency, and sharing computational overheads. On the other hand, limited transmission resources and complex communication environments remain significant bottlenecks to the efficient collaborations among edge devices, particularly within large-scale networks. To address such issues, this paper proposes an over-the-air (OTA) analog aggregation method designed for the distributed swarm learning (DSL), termed DSL-OTA, aiming to enhance communication efficiency, enable effective cooperation, and ensure privacy preserving. Incorporating multi-worker selection strategy with over-the-air aggregation not only makes the standard DSL based on single best worker contributing to global model update to become more federated, but also secures the aggregation from potential risks of data leakage. Our theoretical analyses verify the advantages of the proposed DSL-OTA algorithm in terms of fast convergence rate and low communication costs. Simulation results reveal that our DSL-OTA outperforms the other existing methods by achieving better learning performance under both homogeneous and heterogeneous dataset settings.",
        "authors": [
            "Zhuoyu Yao",
            "Yue Wang",
            "Songyang Zhang",
            "Yingshu Li",
            "Zhipeng Cai",
            "Zhi Tian"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.18152v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-20"
    },
    "http://arxiv.org/abs/2510.18121v1": {
        "id": "http://arxiv.org/abs/2510.18121v1",
        "title": "Efficient Long-context Language Model Training by Core Attention Disaggregation",
        "link": "http://arxiv.org/abs/2510.18121v1",
        "tags": [
            "training",
            "kernel",
            "sparse"
        ],
        "relevant": true,
        "indexed_date": "2025-10-20",
        "tldr": "Proposes Core Attention Disaggregation (CAD) to eliminate load imbalance in long-context LLM training by offloading attention computation to dedicated devices. Leverages stateless, composable attention kernels with dynamic rebatching, achieving up to 1.35x training throughput on 512k context lengths.",
        "abstract": "We present core attention disaggregation (CAD), a technique that improves long-context large language model training by decoupling the core attention computation, softmax(QK^T)V, from the rest of the model and executing it on a separate pool of devices. In existing systems, core attention is colocated with other layers; at long context lengths, its quadratic compute growth compared to the near-linear growth of other components causes load imbalance and stragglers across data and pipeline parallel groups. CAD is enabled by two observations. First, core attention is stateless: it has no trainable parameters and only minimal transient data, so balancing reduces to scheduling compute-bound tasks. Second, it is composable: modern attention kernels retain high efficiency when processing fused batches of token-level shards with arbitrary lengths. CAD partitions core attention into token-level tasks and dispatches them to dedicated attention servers, which dynamically rebatch tasks to equalize compute without sacrificing kernel efficiency. We implement CAD in a system called DistCA, which uses a ping-pong execution scheme to fully overlap communication with computation and in-place execution on attention servers to reduce memory use. On 512 H200 GPUs and context lengths up to 512k tokens, DistCA improves end-to-end training throughput by up to 1.35x, eliminates data and pipeline parallel stragglers, and achieves near-perfect compute and memory balance.",
        "authors": [
            "Yonghao Zhuang",
            "Junda Chen",
            "Bo Pang",
            "Yi Gu",
            "Yibo Zhu",
            "Yimin Jiang",
            "Ion Stoica",
            "Eric Xing",
            "Hao Zhang"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "submit_date": "2025-10-20"
    },
    "http://arxiv.org/abs/2510.18058v1": {
        "title": "A New Broadcast Model for Several Network Topologies",
        "link": "http://arxiv.org/abs/2510.18058v1",
        "abstract": "We present Broadcast by Balanced Saturation (BBS), a general broadcast algorithm designed to optimize communication efficiency across diverse network topologies. BBS maximizes node utilization, addressing challenges in broadcast operations such as topology constraints, bandwidth limitations, and synchronization overhead, particularly in large-scale systems like supercomputers. The algorithm ensures sustained activity with nodes throughout the broadcast, thereby enhancing data propagation and significantly reducing latency. Through a precise communication cycle, BBS provides a repeatable, streamlined, stepwise broadcasting framework. Simulation results across various topologies demonstrate that the BBS algorithm consistently outperforms common general broadcast algorithms, often by a substantial margin. These findings suggest that BBS is a versatile and robust framework with the potential to redefine broadcast strategies across network topologies.",
        "authors": [
            "Hongbo Lu",
            "Junsung Hwang",
            "Bernard Tenreiro",
            "Nabila Jaman Tripti",
            "Darren Hamilton",
            "Yuefan Deng"
        ],
        "categories": [
            "cs.NI",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.18058v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-20"
    },
    "http://arxiv.org/abs/2510.18897v1": {
        "id": "http://arxiv.org/abs/2510.18897v1",
        "title": "AI for Distributed Systems Design: Scalable Cloud Optimization Through Repeated LLMs Sampling And Simulators",
        "link": "http://arxiv.org/abs/2510.18897v1",
        "tags": [
            "serving",
            "storage",
            "training"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Develops AI-driven distributed scheduler design via LLM-generated policies verified in a domain-specific simulator. Proposes iterative generate-verify loop using simulator (Eudoxia) and feedback to optimize policies. Achieves throughput improvements across models in Function-as-a-Service runtime.",
        "abstract": "We explore AI-driven distributed-systems policy design by combining stochastic code generation from large language models (LLMs) with deterministic verification in a domain-specific simulator. Using a Function-as-a-Service runtime (Bauplan) and its open-source simulator (Eudoxia) as a case study, we frame scheduler design as an iterative generate-and-verify loop: an LLM proposes a Python policy, the simulator evaluates it on standardized traces, and structured feedback steers subsequent generations. This setup preserves interpretability while enabling targeted search over a large design space. We detail the system architecture and report preliminary results on throughput improvements across multiple models. Beyond early gains, we discuss the limits of the current setup and outline next steps; in particular, we conjecture that AI will be crucial for scaling this methodology by helping to bootstrap new simulators.",
        "authors": [
            "Jacopo Tagliabue"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.DB",
            "cs.SE"
        ],
        "submit_date": "2025-10-20"
    },
    "http://arxiv.org/abs/2510.17642v1": {
        "title": "Quantum Federated Learning: Architectural Elements and Future Directions",
        "link": "http://arxiv.org/abs/2510.17642v1",
        "abstract": "Federated learning (FL) focuses on collaborative model training without the need to move the private data silos to a central server. Despite its several benefits, the classical FL is plagued with several limitations, such as high computational power required for model training(which is critical for low-resource clients), privacy risks, large update traffic, and non-IID heterogeneity. This chapter surveys a hybrid paradigm - Quantum Federated Learning (QFL), which introduces quantum computation, that addresses multiple challenges of classical FL and offers rapid computing capability while keeping the classical orchestration intact. Firstly, we motivate QFL with a concrete presentation on pain points of classical FL, followed by a discussion on a general architecture of QFL frameworks specifying the roles of client and server, communication primitives and the quantum model placement. We classify the existing QFL systems based on four criteria - quantum architecture (pure QFL, hybrid QFL), data processing method (quantum data encoding, quantum feature mapping, and quantum feature selection & dimensionality reduction), network topology (centralized, hierarchial, decentralized), and quantum security mechanisms (quantum key distribution, quantum homomorphic encryption, quantum differential privacy, blind quantum computing). We then describe applications of QFL in healthcare, vehicular networks, wireless networks, and network security, clearly highlighting where QFL improves communication efficiency, security, and performance compared to classical FL. We close with multiple challenges and future works in QFL, including extension of QFL beyond classification tasks, adversarial attacks, realistic hardware deployment, quantum communication protocols deployment, aggregation of different quantum models, and quantum split learning as an alternative to QFL.",
        "authors": [
            "Siva Sai",
            "Abhishek Sawaika",
            "Prabhjot Singh",
            "Rajkumar Buyya"
        ],
        "categories": [
            "quant-ph",
            "cs.DC",
            "cs.LG"
        ],
        "id": "http://arxiv.org/abs/2510.17642v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-20"
    },
    "http://arxiv.org/abs/2510.17639v2": {
        "title": "On the Universality of Round Elimination Fixed Points",
        "link": "http://arxiv.org/abs/2510.17639v2",
        "abstract": "Recent work on distributed graph algorithms [e.g. STOC 2022, ITCS 2022, PODC 2020] has drawn attention to the following open question: are round elimination fixed points a universal technique for proving lower bounds? That is, given a locally checkable problem $Π$ that requires at least $Ω(\\log n)$ rounds in the deterministic LOCAL model, can we always find a relaxation $Π'$ of $Π$ that is a nontrivial fixed point for the round elimination technique [see STOC 2016, PODC 2019]? If yes, then a key part of distributed computational complexity would be also decidable.   The key obstacle so far has been a certain family of homomorphism problems [ITCS 2022], which require $Ω(\\log n)$ rounds, but the only known proof is based on Marks' technique [J. AMS 2016].   We develop a new technique for constructing round elimination lower bounds systematically. Using so-called tripotent inputs we show that the aforementioned homomorphism problems indeed admit a lower bound proof that is based on round elimination fixed points. Hence we eliminate the only known obstacle for the universality of round elimination.   Yet we also present a new obstacle: we show that there are some problems with inputs that require $Ω(\\log n)$ rounds, yet there is no proof that is based on relaxations to nontrivial round elimination fixed points. Hence round elimination cannot be a universal technique for problems with inputs (but it might be universal for problems without inputs).   We also prove the first fully general lower bound theorem that is applicable to any problem, with or without inputs, that is a fixed point in round elimination. Prior results of this form were only able to handle certain very restricted inputs.",
        "authors": [
            "Alkida Balliu",
            "Sebastian Brandt",
            "Ole Gabsdil",
            "Dennis Olivetti",
            "Jukka Suomela"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.17639v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-20"
    },
    "http://arxiv.org/abs/2511.07427v1": {
        "id": "http://arxiv.org/abs/2511.07427v1",
        "title": "DynaKV: Enabling Accurate and Efficient Long-Sequence LLM Decoding on Smartphones",
        "link": "http://arxiv.org/abs/2511.07427v1",
        "tags": [
            "offloading",
            "edge",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes DynaKV, an adaptive key-value cache management system for efficient long-sequence LLM decoding on smartphones. Uses migration-free cluster adaptation, continuity-centric flash layout, and virtualized cache to address KVCache distribution shifts. Achieves 1.47x speedups and 1.38x accuracy gains over state-of-the-art.",
        "abstract": "As the demand for human-like reasoning, multi-turn dialogues, and long-form responses grows, large language models (LLMs) are increasingly expected to support efficient and effective long-sequence decoding. However, due to limited DRAM capacity, long-seuqence LLM decoding on smartphones is constrained by the key-value cache (KVCache), whose memory footprint increases linearly with sequence length. Retrieval-based methods mitigate DRAM pressure by offloading KVCache to flash and retrieving query-relevant entries through cluster-based indexing. Unfortunately, as decoding progresses, KVCache distribution shifts render static or local cluster updates progressively misaligned, excluding essential entries or fetching redundant ones. These issues are further exacerbated by smartphone-specific limitations in bandwidth, IOPS, and memory capacity.   We propose DynaKV, the first adaptive KVCache management approach that jointly addresses accuracy and efficiency for long-sequence decoding on smartphones. DynaKV integrates three key techniques: (1) Migration-Free Cluster Adaptation, which adaptively splits clusters during retrieval without incurring additional transfers; (2) Continuity-Centric Flash Management, which co-locates correlated entries and clusters and employs a dual-head layout for efficient updates; and (3) Memory-Efficient Cache Design, which virtualizes cache space across DRAM and flash and extends replacement policies to align with cluster-level access patterns. Evaluations demonstrate that DynaKV improves retrieval accuracy and reduces end-to-end latency compared to state-of-the-art solutions, achieving average gains of $1.38\\times$ in accuracy and $1.47\\times$ speedups. Furthermore, the insights of DynaKV naturally extend to other long-context workloads and multi-tier memory hierarchies, underscoring its broader applicability.",
        "authors": [
            "Tuowei Wang",
            "Minxing Huang",
            "Fengzu Li",
            "Ligeng Chen",
            "Jinrui Zhang",
            "Ju Ren"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-10-20"
    },
    "http://arxiv.org/abs/2511.07426v1": {
        "id": "http://arxiv.org/abs/2511.07426v1",
        "title": "Network and Systems Performance Characterization of MCP-Enabled LLM Agents",
        "link": "http://arxiv.org/abs/2511.07426v1",
        "tags": [
            "agentic",
            "RL",
            "offline"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Characterizes performance and cost of MCP-enabled LLM agents. Measures token efficiency, cost, time and success rate across different models and MCP configurations. Achieves optimizations via parallel tool calls and task abort for cost-effective workflows.",
        "abstract": "Model Context Protocol (MCP) has recently gained increased attention within the AI community for providing a standardized way for large language models (LLMs) to interact with external tools and services, significantly enhancing their capabilities. However, the inclusion of extensive contextual information, including system prompts, MCP tool definitions, and context histories, in MCP-enabled LLM interactions, dramatically inflates token usage. Given that LLM providers charge based on tokens, these expanded contexts can quickly escalate monetary costs and increase the computational load on LLM services. This paper presents a comprehensive measurement-based analysis of MCP-enabled interactions with LLMs, revealing trade-offs between capability, performance, and cost. We explore how different LLM models and MCP configurations impact key performance metrics such as token efficiency, monetary cost, task completion times, and task success rates, and suggest potential optimizations, including enabling parallel tool calls and implementing robust task abort mechanisms. These findings provide useful insights for developing more efficient, robust, and cost-effective MCP-enabled workflows.",
        "authors": [
            "Zihao Ding",
            "Mufeng Zhu",
            "Yao Liu"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.CL",
            "cs.NI",
            "cs.SE"
        ],
        "submit_date": "2025-10-20"
    },
    "http://arxiv.org/abs/2510.17158v1": {
        "id": "http://arxiv.org/abs/2510.17158v1",
        "title": "Integrating Performance Tools in Model Reasoning for GPU Kernel Optimization",
        "link": "http://arxiv.org/abs/2510.17158v1",
        "tags": [
            "training",
            "kernel",
            "thinking"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes training LLMs to interact with performance tools during reasoning for GPU kernel optimization. Introduces a method integrating hardware/environment data via chain-of-thought reasoning. Achieves state-of-the-art results in optimizing GPU kernels, though specific metrics not detailed in abstract.",
        "abstract": "Language models are now prevalent in software engineering with many developers using them to automate tasks and accelerate their development. While language models have been tremendous at accomplishing complex software engineering tasks, there are still many areas where they fail to deliver desirable results, for instance code performance related tasks. Tasks like optimization depend on many complex data from the environment, hardware, etc. that are not directly represented in source code. Recent efforts have seen large improvements in general code modeling tasks using chain-of-thought style reasoning, but these models still fail to comprehend how the environment interacts with code performance. In this paper we propose a methodology to train language models that can interact with performance tools during their reasoning process. We then demonstrate how this methodology can be used to train a state-of-the-art GPU kernel optimization model.",
        "authors": [
            "Daniel Nichols",
            "Konstantinos Parasyris",
            "Charles Jekel",
            "Abhinav Bhatele",
            "Harshitha Menon"
        ],
        "categories": [
            "cs.DC",
            "cs.PF",
            "cs.SE"
        ],
        "submit_date": "2025-10-20"
    },
    "http://arxiv.org/abs/2511.07425v1": {
        "id": "http://arxiv.org/abs/2511.07425v1",
        "title": "An Evaluation of LLMs Inference on Popular Single-board Computers",
        "link": "http://arxiv.org/abs/2511.07425v1",
        "tags": [
            "edge",
            "quantization",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Evaluates LLM inference performance on single-board computers (SBCs) for edge deployment. Benchmarks 25 quantized LLMs using Ollama/Llamafile runtimes on three SBCs, measuring throughput, memory, and power. Achieves 4x higher throughput and 30-40% lower power usage with Llamafile.",
        "abstract": "The growing demand for on-device large language model (LLM) inference is driving interest in deploying lightweight, cost-effective AI solutions on edge hardware. Single-board computers (SBCs) such as the Raspberry Pi and Orange Pi offer a promising platform for localized, privacy-preserving inference-but remain underexplored in the context of LLM workloads. In this work, we benchmark the performance of 25 quantized open-source LLMs across three SBCs-Raspberry Pi 4, Raspberry Pi 5, and Orange Pi 5 Pro-using two inference runtimes: Ollama and Llamafile. We evaluate generation throughput, memory usage, and power consumption under varying CPU configurations, using multiple prompt types to simulate realistic workloads. Our results show that SBCs can reliably support models up to 1.5B parameters, with Llamafile achieving up to 4x higher throughput and 30-40% lower power usage than Ollama. We identify architecture-specific bottlenecks, highlight runtime-level trade-offs, and provide practical deployment recommendations. This study offers the first broad evaluation of LLM inference on SBCs, bridging the gap between high-performance language models and affordable edge computing.",
        "authors": [
            "Tung",
            "Nguyen",
            "Tuyen Nguyen"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-10-20"
    },
    "http://arxiv.org/abs/2510.17015v1": {
        "id": "http://arxiv.org/abs/2510.17015v1",
        "title": "Justitia: Fair and Efficient Scheduling for LLM Applications",
        "link": "http://arxiv.org/abs/2510.17015v1",
        "tags": [
            "serving",
            "offloading",
            "thinking"
        ],
        "relevant": true,
        "indexed_date": "2025-10-19",
        "tldr": "Addresses unfair and inefficient scheduling of LLM applications in shared GPU environments. Proposes Justitia, a memory-centric scheduler using neural demand prediction and virtual-time fair queuing to balance efficiency and worst-case guarantees, reducing completion time by up to 3.2× while ensuring fairness compared to vLLM.",
        "abstract": "In the era of Large Language Models (LLMs), it has been popular to launch a series of LLM inferences -- we call an LLM application -- to better solve real-world problems. When serving those applications in shared GPU servers, the schedulers are expected to attain fast application completions with guaranteed worst-case performance. However, mainstream LLM schedulers fail to behave well for LLM applications -- due to head-of-line blocking or over-constrained resource allocation. In this paper, we propose to serve LLM applications in a fair and also efficient manner. To this end, we design Justitia, a novel scheduler with three key techniques. First, given that memory is prevalently a bottleneck for mainstream inference frameworks like vLLM, Justitia models the service cost of LLM applications in a memory-centric manner. Meanwhile, it uses a simple neural network model to conduct light-weight and also accurate demand prediction. Moreover, Justitia adopts a virtual-time based fair queuing algorithm to reduce the overall performance with guaranteed worst-case delay. We have implemented Justitia atop vLLM, and experimental results involving diverse LLM applications show that it can substantially enhance the scheduling efficiency with fairness preserved.",
        "authors": [
            "Mingyan Yang",
            "Guanjie Wang",
            "Manqi Luo",
            "Yifei Liu",
            "Chen Chen",
            "Han Zhao",
            "Yu Feng",
            "Quan Chen",
            "Minyi Guo"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC"
        ],
        "submit_date": "2025-10-19"
    },
    "http://arxiv.org/abs/2510.16946v1": {
        "id": "http://arxiv.org/abs/2510.16946v1",
        "title": "Host-Side Telemetry for Performance Diagnosis in Cloud and HPC GPU Infrastructure",
        "link": "http://arxiv.org/abs/2510.16946v1",
        "tags": [
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Presents an eBPF-based telemetry system for diagnosing GPU tail latency in cloud/HPC. Correlates host-side metrics with GPU events for unified observability, achieving 81-88% diagnostic accuracy and detecting spikes in under 5 seconds with 1.21% CPU overhead at 100Hz.",
        "abstract": "Diagnosing GPU tail latency spikes in cloud and HPC infrastructure is critical for maintaining performance predictability and resource utilization, yet existing monitoring tools lack the granularity for root cause analysis in shared computing environments. We introduce an eBPF-based telemetry system that provides unified host-side monitoring of GPU workloads, correlating eBPF-derived host metrics with GPU-internal events for holistic system observability. The system achieves 81--88\\% diagnostic accuracy, detects spikes within 5 seconds, and completes root cause analysis in 6--8 seconds, operating with 1.21\\% CPU overhead at 100Hz sampling. Evaluated on distributed learning workloads, the system identifies root causes including NIC contention, PCIe pressure, and CPU interference, enabling operational debugging for multi-tenant GPU infrastructure without requiring cluster-wide instrumentation.",
        "authors": [
            "Erfan Darzi",
            "Aldo Pareja",
            "Shreeanant Bharadwaj"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-10-19"
    },
    "http://arxiv.org/abs/2510.16933v1": {
        "title": "Tutoring LLM into a Better CUDA Optimizer",
        "link": "http://arxiv.org/abs/2510.16933v1",
        "abstract": "Recent leaps in large language models (LLMs) caused a revolution in programming tools (like GitHub Copilot) that can help with code generation, debugging, and even performance optimization. In this paper, we focus on the capabilities of the most recent reasoning models to generate optimized CUDA code for predefined, well-known tasks. Our objective is to determine which types of code optimizations and parallel patterns the LLMs can perform by themselves and whether they can be improved by tutoring (providing more detailed hints and guidelines in the prompt). The generated solutions were evaluated both automatically (for correctness and speedup) and manually (code reviews) to provide a more detailed perspective. We also tried an interactive approach where the LLM can fix its previous mistakes within a session. The results indicate that LLMs are quite skilled coders; however, they require tutoring to reach optimized solutions provided by parallel computing experts.",
        "authors": [
            "Matyáš Brabec",
            "Jiří Klepl",
            "Michal Töpfer",
            "Martin Kruliš"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "id": "http://arxiv.org/abs/2510.16933v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-19"
    },
    "http://arxiv.org/abs/2510.16896v2": {
        "title": "FTI-TMR: A Fault Tolerance and Isolation Algorithm for Interconnected Multicore Systems",
        "link": "http://arxiv.org/abs/2510.16896v2",
        "abstract": "Two-Phase TMR conserves energy by partitioning redundancy operations into two stages and making the execution of the third task copy optional, yet it remains susceptible to permanent faults. Reactive-TMR (R-TMR) counters this by isolating faulty cores, handling both transient and permanent faults. However, the lightweight hardware required by R-TMR not only increases complexity but also becomes a single point of failure itself. To bypass isolated node constraints, this paper proposes a Fault Tolerance and Isolation TMR (FTI-TMR) algorithm for interconnected multicore systems. By constructing a stability metric to identify the most reliable nodes in the system, which then perform periodic diagnostics to isolate permanent faults. Experimental results show that FTI-TMR reduces task workload by approximately 30% compared with baseline TMR while achieving higher permanent fault coverage.",
        "authors": [
            "Yiming Hu"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.16896v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-19"
    },
    "http://arxiv.org/abs/2510.16890v1": {
        "title": "Layout-Agnostic MPI Abstraction for Distributed Computing in Modern C++",
        "link": "http://arxiv.org/abs/2510.16890v1",
        "abstract": "Message Passing Interface (MPI) has been a well-established technology in the domain of distributed high-performance computing for several decades. However, one of its greatest drawbacks is a rather ancient pure-C interface. It lacks many useful features of modern languages (namely C++), like basic type-checking or support for generic code design. In this paper, we propose a novel abstraction for MPI, which we implemented as an extension of the C++ Noarr library. It follows Noarr paradigms (first-class layout and traversal abstraction) and offers layout-agnostic design of MPI applications. We also implemented a layout-agnostic distributed GEMM kernel as a case study to demonstrate the usability and syntax of the proposed abstraction. We show that the abstraction achieves performance comparable to the state-of-the-art MPI C++ bindings while allowing for a more flexible design of distributed applications.",
        "authors": [
            "Jiří Klepl",
            "Martin Kruliš",
            "Matyáš Brabec"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.16890v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-19"
    },
    "http://arxiv.org/abs/2510.16850v1": {
        "title": "DiRAC - Distributed Robot Awareness and Consensus",
        "link": "http://arxiv.org/abs/2510.16850v1",
        "abstract": "DiRAC is a scalable, distributed framework designed to enable efficient task assignment and path planning in very large robotic swarms. It introduces a novel zone-partitioned architecture with dynamically elected leaders and a tick-synchronized consensus protocol that yields strong consistency and deterministic outcomes. For path planning, DiRAC uses a novel algorithm, a force-based decentralized planner for real-time collision resolution. Validated within ROS 2 middleware through preliminary simulation, DiRAC demonstrates architectural scalability and modular efficiency in simulated warehouse environments, laying the groundwork for real-world deployment in large-scale industrial and logistics domains.",
        "authors": [
            "Uday Gopan",
            "Manjari Kulkarni",
            "Lakshasri S",
            "Kashish Mittal",
            "Sriram Radhakrishna",
            "Aditya Naskar",
            "Rameshwar DL"
        ],
        "categories": [
            "cs.MA",
            "cs.DC",
            "cs.RO"
        ],
        "id": "http://arxiv.org/abs/2510.16850v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-19"
    },
    "http://arxiv.org/abs/2510.17901v1": {
        "title": "The Sherpa.ai Blind Vertical Federated Learning Paradigm to Minimize the Number of Communications",
        "link": "http://arxiv.org/abs/2510.17901v1",
        "abstract": "Federated Learning (FL) enables collaborative decentralized training across multiple parties (nodes) while keeping raw data private. There are two main paradigms in FL: Horizontal FL (HFL), where all participant nodes share the same feature space but hold different samples, and Vertical FL (VFL), where participants hold complementary features for the same samples. While HFL is widely adopted, VFL is employed in domains where nodes hold complementary features about the same samples. Still, VFL presents a significant limitation: the vast number of communications required during training. This compromises privacy and security, and can lead to high energy consumption, and in some cases, make model training unfeasible due to the high number of communications.   In this paper, we introduce Sherpa.ai Blind Vertical Federated Learning (SBVFL), a novel paradigm that leverages a distributed training mechanism enhanced for privacy and security. Decoupling the vast majority of node updates from the server dramatically reduces node-server communication. Experiments show that SBVFL reduces communication by ~99% compared to standard VFL while maintaining accuracy and robustness. Therefore, SBVFL enables practical, privacy-preserving VFL across sensitive domains, including healthcare, finance, manufacturing, aerospace, cybersecurity, and the defense industry.",
        "authors": [
            "Alex Acero",
            "Daniel M. Jimenez-Gutierrez",
            "Dario Pighin",
            "Enrique Zuazua",
            "Joaquin Del Rio",
            "Xabi Uribe-Etxebarria"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.17901v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-19"
    },
    "http://arxiv.org/abs/2510.16736v1": {
        "id": "http://arxiv.org/abs/2510.16736v1",
        "title": "Exact Nearest-Neighbor Search on Energy-Efficient FPGA Devices",
        "link": "http://arxiv.org/abs/2510.16736v1",
        "tags": [
            "serving",
            "edge",
            "hardware"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Investigates energy-efficient exact kNN search for high-dimensional latent spaces using FPGA configurations. Proposes two FPGA solutions: one maximizing throughput via batched stream processing, another minimizing latency with in-memory processing. Achieves 16.6× higher throughput and 11.9× energy savings versus CPUs.",
        "abstract": "This paper investigates the usage of FPGA devices for energy-efficient exact kNN search in high-dimension latent spaces. This work intercepts a relevant trend that tries to support the increasing popularity of learned representations based on neural encoder models by making their large-scale adoption greener and more inclusive. The paper proposes two different energy-efficient solutions adopting the same FPGA low-level configuration. The first solution maximizes system throughput by processing the queries of a batch in parallel over a streamed dataset not fitting into the FPGA memory. The second minimizes latency by processing each kNN incoming query in parallel over an in-memory dataset. Reproducible experiments on publicly available image and text datasets show that our solution outperforms state-of-the-art CPU-based competitors regarding throughput, latency, and energy consumption. Specifically, experiments show that the proposed FPGA solutions achieve the best throughput in terms of queries per second and the best-observed latency with scale-up factors of up to 16.6X. Similar considerations can be made regarding energy efficiency, where results show that our solutions can achieve up to 11.9X energy saving w.r.t. strong CPU-based competitors.",
        "authors": [
            "Patrizio Dazzi",
            "William Guglielmo",
            "Franco Maria Nardini",
            "Raffaele Perego",
            "Salvatore Trani"
        ],
        "categories": [
            "cs.IR",
            "cs.DC"
        ],
        "submit_date": "2025-10-19"
    },
    "http://arxiv.org/abs/2510.16694v1": {
        "title": "CLIP: Client-Side Invariant Pruning for Mitigating Stragglers in Secure Federated Learning",
        "link": "http://arxiv.org/abs/2510.16694v1",
        "abstract": "Secure federated learning (FL) preserves data privacy during distributed model training. However, deploying such frameworks across heterogeneous devices results in performance bottlenecks, due to straggler clients with limited computational or network capabilities, slowing training for all participating clients. This paper introduces the first straggler mitigation technique for secure aggregation with deep neural networks. We propose CLIP, a client-side invariant neuron pruning technique coupled with network-aware pruning, that addresses compute and network bottlenecks due to stragglers during training with minimal accuracy loss. Our technique accelerates secure FL training by 13% to 34% across multiple datasets (CIFAR10, Shakespeare, FEMNIST) with an accuracy impact of between 1.3% improvement to 2.6% reduction.",
        "authors": [
            "Anthony DiMaggio",
            "Raghav Sharma",
            "Gururaj Saileshwar"
        ],
        "categories": [
            "cs.LG",
            "cs.CR",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.16694v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-19"
    },
    "http://arxiv.org/abs/2510.18893v1": {
        "id": "http://arxiv.org/abs/2510.18893v1",
        "title": "CodeCRDT: Observation-Driven Coordination for Multi-Agent LLM Code Generation",
        "link": "http://arxiv.org/abs/2510.18893v1",
        "tags": [
            "serving",
            "offline"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes CodeCRDT, an observation-driven coordination using CRDTs for multi-agent LLM code generation to avoid explicit messaging. Enables lock-free concurrent code generation with 100% convergence, achieving up to 21.1% speedup but also up to 39.4% slowdown depending on task structure.",
        "abstract": "Multi-agent LLM systems fail to realize parallel speedups due to costly coordination. We present CodeCRDT, an observation-driven coordination pattern where agents coordinate by monitoring a shared state with observable updates and deterministic convergence, rather than explicit message passing. Using Conflict-Free Replicated Data Types (CRDTs), CodeCRDT enables lock-free, conflict-free concurrent code generation with strong eventual consistency. Evaluation across 600 trials (6 tasks, 50 runs per mode) shows both benefits and trade-offs: up to 21.1% speedup on some tasks, up to 39.4% slowdown on others, and 100% convergence with zero merge failures. The study formalizes observation-driven coordination for stochastic LLM agents, revealing semantic conflict rates (5-10%) and quality-performance tradeoffs, and provides empirical characterization of when parallel coordination succeeds versus fails based on task structure.",
        "authors": [
            "Sergey Pugachev"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.SE"
        ],
        "submit_date": "2025-10-18"
    },
    "http://arxiv.org/abs/2510.16606v1": {
        "id": "http://arxiv.org/abs/2510.16606v1",
        "title": "Reimagining RDMA Through the Lens of ML",
        "link": "http://arxiv.org/abs/2510.16606v1",
        "tags": [
            "networking",
            "training"
        ],
        "relevant": true,
        "indexed_date": "2025-10-18",
        "tldr": "Celeris redesigns RDMA transport for distributed ML training by eliminating strict reliability and in-order delivery, leveraging ML's fault tolerance to reduce tail latency. By removing retransmissions and using software-level recovery (e.g., Hadamard Transform), it cuts 99th-percentile latency by 2.3x and reduces BRAM usage by 67%.",
        "abstract": "As distributed machine learning (ML) workloads scale to thousands of GPUs connected by ultra-high-speed inter-connects, tail latency in collective communication has emerged as a primary bottleneck. Prior RDMA designs, like RoCE, IRN, and SRNIC, enforce strict reliability and in-order delivery, relying on retransmissions and packet sequencing to ensure correctness. While effective for general-purpose workloads, these mechanisms introduce complexity and latency that scale poorly, where even rare packet losses or delays can consistently degrade system performance. We introduce Celeris, a domain-specific RDMA transport that revisits traditional reliability guarantees based on ML's tolerance for lost or partial data. Celeris removes retransmissions and in-order delivery from the RDMA NIC, enabling best-effort transport that exploits the robustness of ML workloads. It retains congestion control (e.g., DCQCN) and manages communication with software-level mechanisms such as adaptive timeouts and data prioritization, while shifting loss recovery to the ML pipeline (e.g., using the Hadamard Transform). Early results show that Celeris reduces 99th-percentile latency by up to 2.3x, cuts BRAM usage by 67%, and nearly doubles NIC resilience to faults -- delivering a resilient, scalable transport tailored for ML at cluster scale.",
        "authors": [
            "Ertza Warraich",
            "Ali Imran",
            "Annus Zulfiqar",
            "Shay Vargaftik",
            "Sonia Fahmy",
            "Muhammad Shahbaz"
        ],
        "categories": [
            "cs.DC",
            "cs.NI"
        ],
        "submit_date": "2025-10-18"
    },
    "http://arxiv.org/abs/2510.16497v1": {
        "id": "http://arxiv.org/abs/2510.16497v1",
        "title": "Edge-Based Speech Transcription and Synthesis for Kinyarwanda and Swahili Languages",
        "link": "http://arxiv.org/abs/2510.16497v1",
        "tags": [
            "edge",
            "serving",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes an edge-cloud cascading framework to optimize speech transcription/synthesis for under-resourced languages using Whisper and SpeechT5. Distributes inference workload between edge and cloud to reduce latency and memory usage. Achieves 9.5-14% model compression on edge devices with memory usage capped at 149MB.",
        "abstract": "This paper presents a novel framework for speech transcription and synthesis, leveraging edge-cloud parallelism to enhance processing speed and accessibility for Kinyarwanda and Swahili speakers. It addresses the scarcity of powerful language processing tools for these widely spoken languages in East African countries with limited technological infrastructure. The framework utilizes the Whisper and SpeechT5 pre-trained models to enable speech-to-text (STT) and text-to-speech (TTS) translation. The architecture uses a cascading mechanism that distributes the model inference workload between the edge device and the cloud, thereby reducing latency and resource usage, benefiting both ends. On the edge device, our approach achieves a memory usage compression of 9.5% for the SpeechT5 model and 14% for the Whisper model, with a maximum memory usage of 149 MB. Experimental results indicate that on a 1.7 GHz CPU edge device with a 1 MB/s network bandwidth, the system can process a 270-character text in less than a minute for both speech-to-text and text-to-speech transcription. Using real-world survey data from Kenya, it is shown that the cascaded edge-cloud architecture proposed could easily serve as an excellent platform for STT and TTS transcription with good accuracy and response time.",
        "authors": [
            "Pacome Simon Mbonimpa",
            "Diane Tuyizere",
            "Azizuddin Ahmed Biyabani",
            "Ozan K. Tonguz"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-10-18"
    },
    "http://arxiv.org/abs/2510.16418v1": {
        "id": "http://arxiv.org/abs/2510.16418v1",
        "title": "FourierCompress: Layer-Aware Spectral Activation Compression for Efficient and Accurate Collaborative LLM Inference",
        "link": "http://arxiv.org/abs/2510.16418v1",
        "tags": [
            "serving",
            "offloading",
            "quantization"
        ],
        "relevant": true,
        "indexed_date": "2025-10-18",
        "tldr": "Addresses communication bottlenecks in collaborative LLM inference by compressing intermediate activations across edge-client and server boundaries. Proposes FourierCompress, a layer-aware FFT-based method retaining low-frequency coefficients, achieving 7.6x compression with <0.3% accuracy loss and 32x faster compression than Top-k.",
        "abstract": "Collaborative large language model (LLM) inference enables real-time, privacy-preserving AI services on resource-constrained edge devices by partitioning computational workloads between client devices and edge servers. However, this paradigm is severely hindered by communication bottlenecks caused by the transmission of high-dimensional intermediate activations, exacerbated by the autoregressive decoding structure of LLMs, where bandwidth consumption scales linearly with output length. Existing activation compression methods struggle to simultaneously achieve high compression ratios, low reconstruction error, and computational efficiency. This paper proposes FourierCompress, a novel, layer-aware activation compression framework that exploits the frequency-domain sparsity of LLM activations. We rigorously demonstrate that activations from the first Transformer layer exhibit strong smoothness and energy concentration in the low-frequency domain, making them highly amenable to near-lossless compression via the Fast Fourier Transform (FFT). FourierCompress transforms activations into the frequency domain, retains only a compact block of low-frequency coefficients, and reconstructs the signal at the server using conjugate symmetry, enabling seamless hardware acceleration on DSPs and FPGAs. Extensive experiments on Llama 3 and Qwen2.5 models across 10 commonsense reasoning datasets demonstrate that FourierCompress preserves performance remarkably close to the uncompressed baseline, outperforming Top-k, QR, and SVD. FourierCompress bridges the gap between communication efficiency (an average 7.6x reduction in activation size), near-lossless inference (less than 0.3% average accuracy loss), and significantly faster compression (achieving over 32x reduction in compression time compared to Top-k via hardware acceleration) for edge-device LLM inference.",
        "authors": [
            "Jian Ma",
            "Xinchen Lyu",
            "Jun Jiang",
            "Longhao Zou",
            "Chenshan Ren",
            "Qimei Cui",
            "Xiaofeng Tao"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-10-18"
    },
    "http://arxiv.org/abs/2510.16415v1": {
        "id": "http://arxiv.org/abs/2510.16415v1",
        "title": "MeCeFO: Enhancing LLM Training Robustness via Fault-Tolerant Optimization",
        "link": "http://arxiv.org/abs/2510.16415v1",
        "tags": [
            "training",
            "offloading",
            "sparse"
        ],
        "relevant": true,
        "indexed_date": "2025-10-18",
        "tldr": "Proposes MeCeFO, a fault-tolerant LLM training system that transfers failed node workloads to neighbors with minimal overhead using skip-connections, recomputation, and low-rank gradient approximation. Achieves 5.0–6.7× higher resilience than SOTA with only 4.18% throughput drop under failures.",
        "abstract": "As distributed optimization scales to meet the demands of Large Language Model (LLM) training, hardware failures become increasingly non-negligible. Existing fault-tolerant training methods often introduce significant computational or memory overhead, demanding additional resources. To address this challenge, we propose Memory- and Computation-efficient Fault-tolerant Optimization (MeCeFO), a novel algorithm that ensures robust training with minimal overhead. When a computing node fails, MeCeFO seamlessly transfers its training task to a neighboring node while employing memory- and computation-efficient algorithmic optimizations to minimize the extra workload imposed on the neighboring node handling both tasks. MeCeFO leverages three key algorithmic designs: (i) Skip-connection, which drops the multi-head attention (MHA) module during backpropagation for memory- and computation-efficient approximation; (ii) Recomputation, which reduces activation memory in feedforward networks (FFNs); and (iii) Low-rank gradient approximation, enabling efficient estimation of FFN weight matrix gradients. Theoretically, MeCeFO matches the convergence rate of conventional distributed training, with a rate of $\\mathcal{O}(1/\\sqrt{nT})$, where n is the data parallelism size and T is the number of iterations. Empirically, MeCeFO maintains robust performance under high failure rates, incurring only a 4.18% drop in throughput, demonstrating 5.0$\\times$ to 6.7$\\times$ greater resilience than previous SOTA approaches. Codes are available at https://github.com/pkumelon/MeCeFO.",
        "authors": [
            "Rizhen Hu",
            "Yutong He",
            "Ran Yan",
            "Mou Sun",
            "Binghang Yuan",
            "Kun Yuan"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-10-18"
    },
    "http://arxiv.org/abs/2510.16284v1": {
        "title": "Communication-Efficient and Memory-Aware Parallel Bootstrapping using MPI",
        "link": "http://arxiv.org/abs/2510.16284v1",
        "abstract": "Bootstrapping is a powerful statistical resampling technique for estimating the sampling distribution of an estimator. However, its computational cost becomes prohibitive for large datasets or a high number of resamples. This paper presents a theoretical analysis and design of parallel bootstrapping algorithms using the Message Passing Interface (MPI). We address two key challenges: high communication overhead and memory constraints in distributed environments. We propose two novel strategies: 1) Local Statistic Aggregation, which drastically reduces communication by transmitting sufficient statistics instead of full resampled datasets, and 2) Synchronized Pseudo-Random Number Generation, which enables distributed resampling when the entire dataset cannot be stored on a single process. We develop analytical models for communication and computation complexity, comparing our methods against naive baseline approaches. Our analysis demonstrates that the proposed methods offer significant reductions in communication volume and memory usage, facilitating scalable parallel bootstrapping on large-scale systems.",
        "authors": [
            "Di Zhang"
        ],
        "categories": [
            "cs.DC",
            "cs.MS",
            "math.NA",
            "stat.CO"
        ],
        "id": "http://arxiv.org/abs/2510.16284v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-18"
    },
    "http://arxiv.org/abs/2511.07424v1": {
        "id": "http://arxiv.org/abs/2511.07424v1",
        "title": "Enhancing reliability in AI inference services: An empirical study on real production incidents",
        "link": "http://arxiv.org/abs/2511.07424v1",
        "tags": [
            "serving",
            "offline",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Presents empirical analysis of production incidents in hyperscale LLM inference services. Develops taxonomy and methodology using real incident data, identifying failure modes and mitigation strategies (e.g., GPU-aware routing). Achieves high labeling consistency (Cohen's K ~0.89) and reduces incident impact.",
        "abstract": "Hyperscale large language model (LLM) inference places extraordinary demands on cloud systems, where even brief failures can translate into significant user and business impact. To better understand and mitigate these risks, we present one of the first provider-internal, practice-based analysis of LLM inference incidents. We developed a taxonomy and methodology grounded in a year of operational experience, validating it on 156 high-severity incidents, and conducted a focused quantitative study of Apr-Jun 2025 to ensure recency and relevance. Our approach achieves high labeling consistency (Cohen's K ~0.89), identifies dominant failure modes (in our dataset ~60% inference engine failures, within that category ~40% timeouts), and surfaces mitigation levers (~74% auto-detected; ~28% required hotfix). Beyond hotfixes, many incidents were mitigated via traffic routing, node rebalancing, or capacity increase policies, indicating further automation opportunities. We also show how the taxonomy guided targeted strategies such as connection liveness, GPU capacity-aware routing, and per-endpoint isolation and reduced incident impact and accelerated recovery. Finally, we contribute a practitioner-oriented adoption checklist that enables others to replicate our taxonomy, analysis, and automation opportunities in their own systems. This study demonstrates how systematic, empirically grounded analysis of inference operations can drive more reliable and cost-efficient LLM serving at scale.",
        "authors": [
            "Bhala Ranganathan",
            "Mickey Zhang",
            "Kai Wu"
        ],
        "categories": [
            "cs.DC",
            "cs.CY"
        ],
        "submit_date": "2025-10-17"
    },
    "http://arxiv.org/abs/2510.16087v1": {
        "title": "Towards a Blockchain-Based CI/CD Framework to Enhance Security in Cloud Environments",
        "link": "http://arxiv.org/abs/2510.16087v1",
        "abstract": "Security is becoming a pivotal point in cloud platforms. Several divisions, such as business organisations, health care, government, etc., have experienced cyber-attacks on their infrastructures. This research focuses on security issues within Continuous Integration and Deployment (CI/CD) pipelines in a cloud platform as a reaction to recent cyber breaches. This research proposes a blockchain-based solution to enhance CI/CD pipeline security. This research aims to develop a framework that leverages blockchain's distributed ledger technology and tamper-resistant features to improve CI/CD pipeline security. The goal is to emphasise secure software deployment by integrating threat modelling frameworks and adherence to coding standards. It also aims to employ tools to automate security testing to detect publicly disclosed vulnerabilities and flaws, such as an outdated version of Java Spring Framework, a JavaScript library from an unverified source, or a database library that allows SQL injection attacks in the deployed software through the framework.",
        "authors": [
            "Sabbir M Saleh",
            "Nazim Madhavji",
            "John Steinbacher"
        ],
        "categories": [
            "cs.CR",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.16087v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-17"
    },
    "http://arxiv.org/abs/2510.15755v1": {
        "id": "http://arxiv.org/abs/2510.15755v1",
        "title": "Funky: Cloud-Native FPGA Virtualization and Orchestration",
        "link": "http://arxiv.org/abs/2510.15755v1",
        "tags": [
            "hardware",
            "training",
            "offline"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Addresses FPGA virtualization and orchestration challenges in cloud-native environments. Proposes Funky, a full-stack FPGA-aware orchestration engine with virtualization, state management, and standard-compliant components. Achieves 7.4% performance overhead vs native with strong isolation and 28.7x smaller images.",
        "abstract": "The adoption of FPGAs in cloud-native environments is facing impediments due to FPGA limitations and CPU-oriented design of orchestrators, as they lack virtualization, isolation, and preemption support for FPGAs. Consequently, cloud providers offer no orchestration services for FPGAs, leading to low scalability, flexibility, and resiliency.   This paper presents Funky, a full-stack FPGA-aware orchestration engine for cloud-native applications. Funky offers primary orchestration services for FPGA workloads to achieve high performance, utilization, scalability, and fault tolerance, accomplished by three contributions: (1) FPGA virtualization for lightweight sandboxes, (2) FPGA state management enabling task preemption and checkpointing, and (3) FPGA-aware orchestration components following the industry-standard CRI/OCI specifications.   We implement and evaluate Funky using four x86 servers with Alveo U50 FPGA cards. Our evaluation highlights that Funky allows us to port 23 OpenCL applications from the Xilinx Vitis and Rosetta benchmark suites by modifying 3.4% of the source code while keeping the OCI image sizes 28.7 times smaller than AMD's FPGA-accessible Docker containers. In addition, Funky incurs only 7.4% performance overheads compared to native execution, while providing virtualization support with strong hypervisor-enforced isolation and cloud-native orchestration for a set of distributed FPGAs. Lastly, we evaluate Funky's orchestration services in a large-scale cluster using Google production traces, showing its scalability, fault tolerance, and scheduling efficiency.",
        "authors": [
            "Atsushi Koshiba",
            "Charalampos Mainas",
            "Pramod Bhatotia"
        ],
        "categories": [
            "cs.DC",
            "cs.OS"
        ],
        "submit_date": "2025-10-17"
    },
    "http://arxiv.org/abs/2510.15747v1": {
        "title": "Grassroots Logic Programs: A Secure, Multiagent, Concurrent, Logic Programming Language",
        "link": "http://arxiv.org/abs/2510.15747v1",
        "abstract": "Grassroots platforms are distributed applications run by\\linebreak cryptographically-identified people on their networked personal devices, where multiple disjoint platform instances emerge independently and coalesce when they interoperate. Their foundation is the grassroots social graph, upon which grassroots social networks, grassroots cryptocurrencies, and grassroots democratic federations can be built.   Grassroots platforms have yet to be implemented, the key challenge being faulty and malicious participants: without secure programming support, correct participants cannot reliably identify each other, establish secure communication, or verify each other's code integrity.   We present Grassroots Logic Programs (GLP), a secure, multiagent, concurrent, logic programming language for implementing grassroots platforms. GLP extends logic programs with paired single-reader/single-writer (SRSW) logic variables, providing secure communication channels among cryptographically-identified people through encrypted, signed and attested messages, which enable identity and code integrity verification. We present GLP progressively: logic programs, concurrent GLP, multiagent GLP, augmenting it with cryptographic security, and providing smartphone implementation-ready specifications. We prove safety properties including that GLP computations are deductions, SRSW preservation, acyclicity, and monotonicity. We prove multiagent GLP is grassroots and that GLP streams achieve blockchain security properties. We present a grassroots social graph protocol establishing authenticated peer-to-peer connections and demonstrate secure grassroots social networking applications.",
        "authors": [
            "Ehud Shapiro"
        ],
        "categories": [
            "cs.PL",
            "cs.CR",
            "cs.DC",
            "cs.LO",
            "cs.MA"
        ],
        "id": "http://arxiv.org/abs/2510.15747v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-17"
    },
    "http://arxiv.org/abs/2510.15698v1": {
        "title": "A Post-Quantum Lower Bound for the Distributed Lovász Local Lemma",
        "link": "http://arxiv.org/abs/2510.15698v1",
        "abstract": "In this work, we study the Lovász local lemma (LLL) problem in the area of distributed quantum computing, which has been the focus of attention of recent advances in quantum computing [STOC'24, STOC'25, STOC'25]. We prove a lower bound of $2^{Ω(\\log^* n)}$ for the complexity of the distributed LLL in the quantum-LOCAL model. More specifically, we obtain our lower bound already for a very well-studied special case of the LLL, called sinkless orientation, in a stronger model than quantum-LOCAL, called the randomized online-LOCAL model. As a consequence, we obtain the same lower bounds for sinkless orientation and the distributed LLL also in a variety of other models studied across different research communities.   Our work provides the first superconstant lower bound for sinkless orientation and the distributed LLL in all of these models, addressing recently stated open questions. Moreover, to obtain our results, we develop an entirely new lower bound technique that we believe has the potential to become the first generic technique for proving post-quantum lower bounds for many of the most important problems studied in the context of locality.",
        "authors": [
            "Sebastian Brandt",
            "Tim Göttlicher"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.15698v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-17"
    },
    "http://arxiv.org/abs/2510.15652v1": {
        "id": "http://arxiv.org/abs/2510.15652v1",
        "title": "GOGH: Correlation-Guided Orchestration of GPUs in Heterogeneous Clusters",
        "link": "http://arxiv.org/abs/2510.15652v1",
        "tags": [
            "training",
            "serving",
            "hardware"
        ],
        "relevant": true,
        "indexed_date": "2025-10-17",
        "tldr": "Proposes GOGH, a learning-based system for adaptive GPU resource orchestration in heterogeneous clusters to minimize energy and meet performance targets. Uses two neural networks to predict model-hardware compatibility and co-location effects, improving allocation over time. Reduces energy consumption by up to 27% while maintaining SLOs.",
        "abstract": "The growing demand for computational resources in machine learning has made efficient resource allocation a critical challenge, especially in heterogeneous hardware clusters where devices vary in capability, age, and energy efficiency. Upgrading to the latest hardware is often infeasible, making sustainable use of existing, mixed-generation resources essential. In this paper, we propose a learning-based architecture for managing machine learning workloads in heterogeneous clusters. The system operates online, allocating resources to incoming training or inference requests while minimizing energy consumption and meeting performance requirements. It uses two neural networks: the first provides initial estimates of how well a new model will utilize different hardware types and how it will affect co-located models. An optimizer then allocates resources based on these estimates. After deployment, the system monitors real performance and uses this data to refine its predictions via a second neural network. This updated model improves estimates not only for the current hardware but also for hardware not initially allocated and for co-location scenarios not yet observed. The result is an adaptive, iterative approach that learns over time to make more effective resource allocation decisions in heterogeneous deep learning clusters.",
        "authors": [
            "Ahmad Raeisi",
            "Mahdi Dolati",
            "Sina Darabi",
            "Sadegh Talebi",
            "Patrick Eugster",
            "Ahmad Khonsari"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-10-17"
    },
    "http://arxiv.org/abs/2510.15596v1": {
        "id": "http://arxiv.org/abs/2510.15596v1",
        "title": "PRISM: Probabilistic Runtime Insights and Scalable Performance Modeling for Large-Scale Distributed Training",
        "link": "http://arxiv.org/abs/2510.15596v1",
        "tags": [
            "training",
            "kernel",
            "sparse"
        ],
        "relevant": true,
        "indexed_date": "2025-10-17",
        "tldr": "PRISM models stochastic performance variability in large-scale distributed training, offering probabilistic guarantees on training time. It identifies communication kernels (e.g., AllGather, ReduceScatter) as primary variability sources and enables 1.26x performance gains via placement-aware optimization, with 20.8% KS distance in prediction accuracy.",
        "abstract": "Large model training beyond tens of thousands of GPUs is an uncharted territory. At such scales, disruptions to the training process are not a matter of if, but a matter of when -- a stochastic process degrading training productivity. Dynamic runtime variation will become increasingly more frequent as training scales up and GPUs are operated in increasingly power-limited and thermally-stressed environments. At the 64k GPU scale, we already observed 9% GPU time variability for frontier foundation model training. To understand potential causes of variability, we analyze GPU microbenchmarks at scale across a variety of platforms, showing up to 14% variation in GPU performance on GEMM workloads depending on training hardware and deployed environment.   Motivated by our analysis and the large design space around performance variability, we present PRISM -- a performance modeling framework that considers the stochastic nature of the large-scale distributed training. The core of PRISM is the statistical method that provides a quantifiable measure for probabilistic guarantees on training time. Using PRISM, we explore the design and optimization space of distributed training, from parallelization methods to next-generation training systems. PRISM is validated with real-system measurement, showing training time prediction accuracy with 20.8% Kolmogorov-Smirnov distance. Using PRISM, we demonstrate that, depending on computation node placement, up to 1.26x performance improvement potential is available if we factor in sensitivities of parallelization strategies to variation. In addition, we use PRISM to identify kernels to optimize for reducing performance variability and predict probability of slow-down for large-scale jobs where variation is magnified. We find optimizing communication kernels, such as AllGather and ReduceScatter, contribute most to minimizing variability in training step time.",
        "authors": [
            "Alicia Golden",
            "Michael Kuchnik",
            "Samuel Hsia",
            "Zachary DeVito",
            "Gu-Yeon Wei",
            "David Brooks",
            "Carole-Jean Wu"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-10-17"
    },
    "http://arxiv.org/abs/2510.15490v1": {
        "title": "Retrofitting Service Dependency Discovery in Distributed Systems",
        "link": "http://arxiv.org/abs/2510.15490v1",
        "abstract": "Modern distributed systems rely on complex networks of interconnected services, creating direct or indirect dependencies that can propagate faults and cause cascading failures. To localize the root cause of performance degradation in these environments, constructing a service dependency graph is highly beneficial. However, building an accurate service dependency graph is impaired by complex routing techniques, such as Network Address Translation (NAT), an essential mechanism for connecting services across networks. NAT obfuscates the actual hosts running the services, causing existing run-time approaches that passively observe network metadata to fail in accurately inferring service dependencies. To this end, this paper introduces XXXX, a novel run-time system for constructing process-level service dependency graphs. It operates without source code instrumentation and remains resilient under complex network routing mechanisms, including NAT. XXXX implements a non-disruptive method of injecting metadata onto a TCP packet's header that maintains protocol correctness across host boundaries. In other words, if no receiving agent is present, the instrumentation leaves existing TCP connections unaffected, ensuring non-disruptive operation when it is partially deployed across hosts. We evaluated XXXX extensively against three state-of-the-art systems across nine scenarios, involving three network configurations (NAT-free, internal-NAT, external-NAT) and three microservice benchmarks. XXXX was the only approach that performed consistently across networking configurations. With regards to correctness, it performed on par with, or better than, the state-of-the-art with precision and recall values of 100% in the majority of the scenarios.",
        "authors": [
            "Diogo Landau",
            "Gijs Blanken",
            "Jorge Barbosa",
            "Nishant Saurabh"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.15490v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-17"
    },
    "http://arxiv.org/abs/2510.15485v1": {
        "title": "Balancing Fairness and Performance in Multi-User Spark Workloads with Dynamic Scheduling (extended version)",
        "link": "http://arxiv.org/abs/2510.15485v1",
        "abstract": "Apache Spark is a widely adopted framework for large-scale data processing. However, in industrial analytics environments, Spark's built-in schedulers, such as FIFO and fair scheduling, struggle to maintain both user-level fairness and low mean response time, particularly in long-running shared applications. Existing solutions typically focus on job-level fairness which unintentionally favors users who submit more jobs. Although Spark offers a built-in fair scheduler, it lacks adaptability to dynamic user workloads and may degrade overall job performance. We present the User Weighted Fair Queuing (UWFQ) scheduler, designed to minimize job response times while ensuring equitable resource distribution across users and their respective jobs. UWFQ simulates a virtual fair queuing system and schedules jobs based on their estimated finish times under a bounded fairness model. To further address task skew and reduce priority inversions, which are common in Spark workloads, we introduce runtime partitioning, a method that dynamically refines task granularity based on expected runtime. We implement UWFQ within the Spark framework and evaluate its performance using multi-user synthetic workloads and Google cluster traces. We show that UWFQ reduces the average response time of small jobs by up to 74% compared to existing built-in Spark schedulers and to state-of-the-art fair scheduling algorithms.",
        "authors": [
            "Dāvis Kažemaks",
            "Laurens Versluis",
            "Burcu Kulahcioglu Ozkan",
            "Jérémie Decouchant"
        ],
        "categories": [
            "cs.DC",
            "cs.DB",
            "eess.SY"
        ],
        "id": "http://arxiv.org/abs/2510.15485v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-17"
    },
    "http://arxiv.org/abs/2510.15473v1": {
        "title": "(Almost) Perfect Discrete Iterative Load Balancing",
        "link": "http://arxiv.org/abs/2510.15473v1",
        "abstract": "We consider discrete, iterative load balancing via matchings on arbitrary graphs. Initially each node holds a certain number of tokens, defining the load of the node, and the objective is to redistribute the tokens such that eventually each node has approximately the same number of tokens. We present results for a general class of simple local balancing schemes where the tokens are balanced via matchings. In each round the process averages the tokens of any two matched nodes. If the sum of their tokens is odd, the node to receive the one excess token is selected at random. Our class covers three popular models: in the matching model a new matching is generated randomly in each round, in the balancing circuit model a fixed sequence of matchings is applied periodically, and in the asynchronous model the load is balanced over a randomly chosen edge.   We measure the quality of a load vector by its discrepancy, defined as the difference between the maximum and minimum load across all nodes. As our main result we show that with high probability our discrete balancing scheme reaches a discrepancy of $3$ in a number of rounds which asymptotically matches the spectral bound for continuous load balancing with fractional load.   This result improves and tightens a long line of previous works, by not only achieving a small constant discrepancy (instead of a non-explicit, large constant) but also holding for arbitrary instead of regular graphs. The result also demonstrates that in the general model we consider, discrete load balancing is no harder than continuous load balancing.",
        "authors": [
            "Petra Berenbrink",
            "Robert Elsässer",
            "Tom Friedetzky",
            "Hamed Hosseinpour",
            "Dominik Kaaser",
            "Peter Kling",
            "Thomas Sauerwald"
        ],
        "categories": [
            "cs.DC",
            "math.PR"
        ],
        "id": "http://arxiv.org/abs/2510.15473v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-17"
    },
    "http://arxiv.org/abs/2510.15355v1": {
        "title": "Cloud-Enabled Virtual Prototypes",
        "link": "http://arxiv.org/abs/2510.15355v1",
        "abstract": "The rapid evolution of embedded systems, along with the growing variety and complexity of AI algorithms, necessitates a powerful hardware/software co-design methodology based on virtual prototyping technologies. The market offers a diverse range of simulation solutions, each with its unique technological approach and therefore strengths and weaknesses. Additionally, with the increasing availability of remote on-demand computing resources and their adaptation throughout the industry, the choice of the host infrastructure for execution opens even more new possibilities for operational strategies. This work explores the dichotomy between local and cloud-based simulation environments, focusing on the trade-offs between scalability and privacy. We discuss how the setup of the compute infrastructure impacts the performance of the execution and security of data involved in the process. Furthermore, we highlight the development workflow associated with embedded AI and the critical role of efficient simulations in optimizing these algorithms. With the proposed solution, we aim to sustainably improve trust in remote simulations and facilitate the adoption of virtual prototyping practices.",
        "authors": [
            "Tim Kraus",
            "Axel Sauer",
            "Ingo Feldner"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.15355v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-17"
    },
    "http://arxiv.org/abs/2510.15330v1": {
        "id": "http://arxiv.org/abs/2510.15330v1",
        "title": "BeLLMan: Controlling LLM Congestion",
        "link": "http://arxiv.org/abs/2510.15330v1",
        "tags": [
            "serving",
            "thinking"
        ],
        "relevant": true,
        "indexed_date": "2025-10-17",
        "tldr": "Addresses LLM inference congestion due to uncontrolled autoregressive token generation. Proposes beLLMan, a feedback controller that dynamically adjusts output length based on system load, reducing end-to-end latency by up to 8× and energy use by 25% while increasing request throughput by 19%.",
        "abstract": "Large language model (LLM) applications are blindfolded to the infrastructure underneath and generate tokens autoregressively, indifferent to the system load, thus risking inferencing latency inflation and poor user experience. Our first-cut controller, named beLLMan, enables the LLM infrastructure to actively and progressively signal the first-party LLM application to adjust the output length in response to changing system load. On a real testbed with H100 GPUs, beLLMan helps keep inferencing latency under control (upto 8X lower end-to-end latency) and reduces energy consumption by 25% (while serving 19% more requests) during periods of congestion for a summarization workload.",
        "authors": [
            "Tella Rajashekhar Reddy",
            "Atharva Deshmukh",
            "Karan Tandon",
            "Rohan Gandhi",
            "Anjaly Parayil",
            "Debopam Bhattacherjee"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.CL",
            "cs.NI"
        ],
        "submit_date": "2025-10-17"
    },
    "http://arxiv.org/abs/2511.07423v1": {
        "id": "http://arxiv.org/abs/2511.07423v1",
        "title": "Synera: Synergistic LLM Serving across Device and Cloud at Scale",
        "link": "http://arxiv.org/abs/2511.07423v1",
        "tags": [
            "serving",
            "offloading",
            "edge"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes Synera, a device-cloud synergistic LLM serving system with selective offloading, parallel inference, and scalable batching to balance quality and latency. Achieves 1.20-5.47x better generation quality than baselines with on-par latency and 8.2-16.5% lower cloud costs.",
        "abstract": "Large Language Models (LLMs) are becoming key components in various mobile operating systems, driving smart applications like interactive chatbots and personal assistants. While bringing enhanced intelligence to mobile ends, their deployment suffers from a set of performance challenges, especially the generation quality degradation and prolonged latency. Prior works have mainly relied on solutions of cloud offloading or on-device Small Language Models (SLMs). However, the former is usually limited by the communication bottleneck, and the latter sacrifices generation quality due to resource constraints. To mitigate these limitations, this paper proposes Synera, a device-cloud synergistic LLM serving system that applies an efficient SLM-LLM synergistic mechanism. Through empirical studies on LLM's unique computing characteristics, Synera identifies a set of underexplored optimization opportunities in device-cloud synergistic LLM inference, including offloading decisions, pipeline stalls, and batching bottlenecks. To translate them into enhanced performance, Synera introduces tailored designs of communication-efficient selective offloading, stall-free parallel inference, and scalable cloud batching. Extensive evaluations with real-world testbeds show that Synera enables 1.20-5.47x better generation quality against competitive baselines with on-par latency performance. Compared with existing cloud serving, Synera achieves 8.2-16.5% lower cloud serving cost on various benchmarks.",
        "authors": [
            "Genglin Wang",
            "Liekang Zeng",
            "Bufang Yang",
            "Kaiwei Liu",
            "Guoliang Xing",
            "Chumin Sun",
            "Li Zhou",
            "Jie Sun",
            "Zhenyu Yan"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.LG"
        ],
        "submit_date": "2025-10-17"
    },
    "http://arxiv.org/abs/2510.16067v1": {
        "title": "A Multi-Cloud Framework for Zero-Trust Workload Authentication",
        "link": "http://arxiv.org/abs/2510.16067v1",
        "abstract": "Static, long-lived credentials for workload authentication create untenable security risks that violate Zero-Trust principles. This paper presents a multi-cloud framework using Workload Identity Federation (WIF) and OpenID Connect (OIDC) for secretless authentication. Our approach uses cryptographically-verified, ephemeral tokens, allowing workloads to authenticate without persistent private keys and mitigating credential theft. We validate this framework in an enterprise-scale Kubernetes environment, which significantly reduces the attack surface. The model offers a unified solution to manage workload identities across disparate clouds, enabling future implementation of robust, attribute-based access control.",
        "authors": [
            "Saurabh Deochake",
            "Ryan Murphy",
            "Jeremiah Gearheart"
        ],
        "categories": [
            "cs.CR",
            "cs.DC",
            "cs.NI"
        ],
        "id": "http://arxiv.org/abs/2510.16067v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-17"
    },
    "http://arxiv.org/abs/2510.15215v1": {
        "title": "Spatiotemporal Traffic Prediction in Distributed Backend Systems via Graph Neural Networks",
        "link": "http://arxiv.org/abs/2510.15215v1",
        "abstract": "This paper addresses the problem of traffic prediction in distributed backend systems and proposes a graph neural network based modeling approach to overcome the limitations of traditional models in capturing complex dependencies and dynamic features. The system is abstracted as a graph with nodes and edges, where node features represent traffic and resource states, and adjacency relations describe service interactions. A graph convolution mechanism enables multi order propagation and aggregation of node features, while a gated recurrent structure models historical sequences dynamically, thus integrating spatial structures with temporal evolution. A spatiotemporal joint modeling module further fuses graph representation with temporal dependency, and a decoder generates future traffic predictions. The model is trained with mean squared error to minimize deviations from actual values. Experiments based on public distributed system logs construct combined inputs of node features, topology, and sequences, and compare the proposed method with mainstream baselines using MSE, RMSE, MAE, and MAPE. Results show that the proposed method achieves stable performance and low error across different prediction horizons and model depths, significantly improving the accuracy and robustness of traffic forecasting in distributed backend systems and verifying the potential of graph neural networks in complex system modeling.",
        "authors": [
            "Zhimin Qiu",
            "Feng Liu",
            "Yuxiao Wang",
            "Chenrui Hu",
            "Ziyu Cheng",
            "Di Wu"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.15215v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-17"
    },
    "http://arxiv.org/abs/2510.15147v1": {
        "title": "An Elastic Job Scheduler for HPC Applications on the Cloud",
        "link": "http://arxiv.org/abs/2510.15147v1",
        "abstract": "The last few years have seen an increase in adoption of the cloud for running HPC applications. The pay-as-you-go cost model of these cloud resources has necessitated the development of specialized programming models and schedulers for HPC jobs for efficient utilization of cloud resources. A key aspect of efficient utilization is the ability to rescale applications on the fly to maximize the utilization of cloud resources. Most commonly used parallel programming models like MPI have traditionally not supported autoscaling either in a cloud environment or on supercomputers. While more recent work has been done to implement this functionality in MPI, it is still nascent and requires additional programmer effort. Charm++ is a parallel programming model that natively supports dynamic rescaling through its migratable objects paradigm. In this paper, we present a Kubernetes operator to run Charm++ applications on a Kubernetes cluster. We then present a priority-based elastic job scheduler that can dynamically rescale jobs based on the state of a Kubernetes cluster to maximize cluster utilization while minimizing response time for high-priority jobs. We show that our elastic scheduler, with the ability to rescale HPC jobs with minimal overhead, demonstrates significant performance improvements over traditional static schedulers.",
        "authors": [
            "Aditya Bhosale",
            "Kavitha Chandrasekar",
            "Laxmikant Kale",
            "Sara Kokkila-Schumacher"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.15147v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-16"
    },
    "http://arxiv.org/abs/2510.15122v1": {
        "title": "NEMO: Faster Parallel Execution for Highly Contended Blockchain Workloads (Full version)",
        "link": "http://arxiv.org/abs/2510.15122v1",
        "abstract": "Following the design of more efficient blockchain consensus algorithms, the execution layer has emerged as the new performance bottleneck of blockchains, especially under high contention. Current parallel execution frameworks either rely on optimistic concurrency control (OCC) or on pessimistic concurrency control (PCC), both of which see their performance decrease when workloads are highly contended, albeit for different reasons. In this work, we present NEMO, a new blockchain execution engine that combines OCC with the object data model to address this challenge. NEMO introduces four core innovations: (i) a greedy commit rule for transactions using only owned objects; (ii) refined handling of dependencies to reduce re-executions; (iii) the use of incomplete but statically derivable read/write hints to guide execution; and (iv) a priority-based scheduler that favors transactions that unblock others. Through simulated execution experiments, we demonstrate that NEMO significantly reduces redundant computation and achieves higher throughput than representative approaches. For example, with 16 workers NEMO's throughput is up to 42% higher than the one of Block-STM, the state-of-the-art OCC approach, and 61% higher than the pessimistic concurrency control baseline used.",
        "authors": [
            "François Ezard",
            "Can Umut Ileri",
            "Jérémie Decouchant"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.15122v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-16"
    },
    "http://arxiv.org/abs/2510.15109v1": {
        "title": "Targeted Attacks and Defenses for Distributed Federated Learning in Vehicular Networks",
        "link": "http://arxiv.org/abs/2510.15109v1",
        "abstract": "In emerging networked systems, mobile edge devices such as ground vehicles and unmanned aerial system (UAS) swarms collectively aggregate vast amounts of data to make machine learning decisions such as threat detection in remote, dynamic, and infrastructure-constrained environments where power and bandwidth are scarce. Federated learning (FL) addresses these constraints and privacy concerns by enabling nodes to share local model weights for deep neural networks instead of raw data, facilitating more reliable decision-making than individual learning. However, conventional FL relies on a central server to coordinate model updates in each learning round, which imposes significant computational burdens on the central node and may not be feasible due to the connectivity constraints. By eliminating dependence on a central server, distributed federated learning (DFL) offers scalability, resilience to node failures, learning robustness, and more effective defense strategies. Despite these advantages, DFL remains vulnerable to increasingly advanced and stealthy cyberattacks. In this paper, we design sophisticated targeted training data poisoning and backdoor (Trojan) attacks, and characterize the emerging vulnerabilities in a vehicular network. We analyze how DFL provides resilience against such attacks compared to individual learning and present effective defense mechanisms to further strengthen DFL against the emerging cyber threats.",
        "authors": [
            "Utku Demir",
            "Tugba Erpek",
            "Yalin E. Sagduyu",
            "Sastry Kompella",
            "Mengran Xue"
        ],
        "categories": [
            "cs.NI",
            "cs.AI",
            "cs.DC",
            "cs.LG",
            "eess.SP"
        ],
        "id": "http://arxiv.org/abs/2510.15109v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-16"
    },
    "http://arxiv.org/abs/2510.15095v1": {
        "id": "http://arxiv.org/abs/2510.15095v1",
        "title": "Hive Hash Table: A Warp-Cooperative, Dynamically Resizable Hash Table for GPUs",
        "link": "http://arxiv.org/abs/2510.15095v1",
        "tags": [
            "kernel",
            "training",
            "offline"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Introduces Hive, a high-performance GPU hash table with warp-cooperative protocols and dynamic resizing for concurrent updates. Proposes cache-aligned buckets, warp-synchronous concurrency, and load-aware resizing. Achieves 3.5 billion updates/s and 2x higher throughput than state-of-the-art GPU hash tables.",
        "abstract": "Hash tables are essential building blocks in data-intensive applications, yet existing GPU implementations often struggle with concurrent updates, high load factors, and irregular memory access patterns. We present Hive hash table, a high-performance, warp-cooperative and dynamically resizable GPU hash table that adapts to varying workloads without global rehashing.   Hive hash table makes three key contributions. First, a cache-aligned packed bucket layout stores key-value pairs as 64-bit words, enabling coalesced memory access and atomic updates via single-CAS operations. Second, warp-synchronous concurrency protocols - Warp-Aggregated-Bitmask-Claim (WABC) and Warp-Cooperative Match-and-Elect (WCME) - reduce contention to one atomic operation per warp while ensuring lock-free progress. Third, a load-factor-aware dynamic resizing strategy expands or contracts capacity in warp-parallel K-bucket batches using linear hashing, maintaining balanced occupancy. To handle insertions under heavy contention, Hive hash table employs a four-step strategy: replace, claim-and-commit, bounded cuckoo eviction, and overflow-stash fallback. This design provides lock-free fast paths and bounded recovery cost under contention determined by a fixed eviction depth, while eliminating ABA hazards during concurrent updates.   Experimental evaluation on an NVIDIA RTX 4090 shows Hive hash table sustains load factors up to 95% while delivering 1.5-2x higher throughput than state-of-the-art GPU hash tables (Slab-Hash, DyCuckoo, WarpCore) under mixed insert-delete-lookup workloads. On balanced workload, Hive hash table reaches 3.5 billion updates/s and nearly 4 billion lookups/s, demonstrating scalability and efficiency for GPU-accelerated data processing.",
        "authors": [
            "Md Sabbir Hossain Polak",
            "David Troendle",
            "Byunghyun Jang"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-10-16"
    },
    "http://arxiv.org/abs/2510.14862v1": {
        "id": "http://arxiv.org/abs/2510.14862v1",
        "title": "Multi-modal video data-pipelines for machine learning with minimal human supervision",
        "link": "http://arxiv.org/abs/2510.14862v1",
        "tags": [
            "video",
            "multi-modal",
            "edge"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes a multi-modal video data-pipeline requiring minimal human supervision, using pre-trained experts and PHG-MAE for efficient modeling. Deploys a distilled model (<1M params) for real-time semantic segmentation on commodity hardware, achieving competitive results against larger models.",
        "abstract": "The real-world is inherently multi-modal at its core. Our tools observe and take snapshots of it, in digital form, such as videos or sounds, however much of it is lost. Similarly for actions and information passing between humans, languages are used as a written form of communication. Traditionally, Machine Learning models have been unimodal (i.e. rgb -> semantic or text -> sentiment_class). Recent trends go towards bi-modality, where images and text are learned together, however, in order to truly understand the world, we need to integrate all these independent modalities. In this work we try to combine as many visual modalities as we can using little to no human supervision. In order to do this, we use pre-trained experts and procedural combinations between them on top of raw videos using a fully autonomous data-pipeline, which we also open-source. We then make use of PHG-MAE, a model specifically designed to leverage multi-modal data. We show that this model which was efficiently distilled into a low-parameter (<1M) can have competitive results compared to models of ~300M parameters. We deploy this model and analyze the use-case of real-time semantic segmentation from handheld devices or webcams on commodity hardware. Finally, we deploy other off-the-shelf models using the same framework, such as DPT for near real-time depth estimation.",
        "authors": [
            "Mihai-Cristian Pîrvu",
            "Marius Leordeanu"
        ],
        "categories": [
            "cs.CV",
            "cs.DC"
        ],
        "submit_date": "2025-10-16"
    },
    "http://arxiv.org/abs/2510.14798v1": {
        "title": "Balls and Bins and the Infinite Process with Random Deletions",
        "link": "http://arxiv.org/abs/2510.14798v1",
        "abstract": "We consider an infinite balls-into-bins process with deletions where in each discrete step $t$ a coin is tossed as to whether, with probability $β(t) \\in (0,1)$, a new ball is allocated using the Greedy[2] strategy (which places the ball in the lower loaded of two bins sampled uniformly at random) or, with remaining probability $1-β(t)$, a ball is deleted from a non-empty bin chosen uniformly at random. Let $n$ be the number of bins and $m(t)$ the total load at time $t$. We are interested in bounding the discrepancy $x_{\\max}(t) - m(t)/n$ (current maximum load relative to current average) and the overload $x_{\\max}(t) - m_{\\max}(t)/n$ (current maximum load relative to highest average observed so far).   We prove that at an arbitrarily chosen time $t$ the total number of balls above the average is $O(n)$ and that the discrepancy is $ O(\\log(n))$. For the discrepancy, we provide a matching lower bound. Furthermore we prove that at an arbitrarily chosen time $t$ the overload is $\\log\\log(n)+O(1)$. For \"good\" insertion probability sequences (in which the average load of time intervals with polynomial length increases in expectation) we show that even the discrepancy is bounded by $\\log\\log(n)+O(1)$.   One of our main analytical tools is a layered induction, as per [ABKU99]. Since our model allows for rather more general scenarios than what was previously considered, the formal analysis requires some extra ingredients as well, in particular a detailed potential analysis. Furthermore, we simplify the setup by applying probabilistic couplings to obtain certain \"recovery\" properties, which eliminate much of the need for intricate and careful conditioning elsewhere in the analysis.",
        "authors": [
            "Petra Berenbrink",
            "Tom Friedetzky",
            "Peter Kling",
            "Lars Nagel"
        ],
        "categories": [
            "cs.DC",
            "cs.DS",
            "math.PR"
        ],
        "id": "http://arxiv.org/abs/2510.14798v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-16"
    },
    "http://arxiv.org/abs/2510.14730v1": {
        "title": "Deadlock-free routing for Full-mesh networks without using Virtual Channels",
        "link": "http://arxiv.org/abs/2510.14730v1",
        "abstract": "High-radix, low-diameter networks like HyperX and Dragonfly use a Full-mesh core, and rely on multiple virtual channels (VCs) to avoid packet deadlocks in adaptive routing. However, VCs introduce significant overhead in the switch in terms of area, power, and design complexity, limiting the switch scalability. This paper starts by revisiting VC-less routing through link ordering schemes in Full-mesh networks, which offer implementation simplicity but suffer from performance degradation under adversarial traffic. Thus, to overcome these challenges, we propose TERA (Topology-Embedded Routing Algorithm), a novel routing algorithm which employs an embedded physical subnetwork to provide deadlock-free non-minimal paths without using VCs.   In a Full-mesh network, TERA outperforms link ordering routing algorithms by 80% when dealing with adversarial traffic, and up to 100% in application kernels. Furthermore, compared to other VC-based approaches, it reduces buffer requirements by 50%, while maintaining comparable latency and throughput. Lastly, early results from a 2D-HyperX evaluation show that TERA outperforms state-of-the-art algorithms that use the same number of VCs, achieving performance improvements of up to 32%.",
        "authors": [
            "Alejandro Cano",
            "Cristóbal Camarero",
            "Carmen Martínez",
            "Ramón Beivide"
        ],
        "categories": [
            "cs.DC",
            "cs.AR"
        ],
        "id": "http://arxiv.org/abs/2510.14730v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-16"
    },
    "http://arxiv.org/abs/2510.14686v1": {
        "id": "http://arxiv.org/abs/2510.14686v1",
        "title": "xLLM Technical Report",
        "link": "http://arxiv.org/abs/2510.14686v1",
        "tags": [
            "serving",
            "multi-modal",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-10-16",
        "tldr": "Designs xLLM, a high-performance LLM serving framework with decoupled service-engine architecture for multimodal inference, featuring dynamic PD/EPD disaggregation and global KV cache management. Achieves up to 2.2x higher throughput than vLLM-Ascend under identical TPOT constraints on Qwen models.",
        "abstract": "We introduce xLLM, an intelligent and efficient Large Language Model (LLM) inference framework designed for high-performance, large-scale enterprise-grade serving, with deep optimizations for diverse AI accelerators. To address these challenges, xLLM builds a novel decoupled service-engine architecture. At the service layer, xLLM-Service features an intelligent scheduling module that efficiently processes multimodal requests and co-locates online and offline tasks through unified elastic scheduling to maximize cluster utilization. This module also relies on a workload-adaptive dynamic Prefill-Decode (PD) disaggregation policy and a novel Encode-Prefill-Decode (EPD) disaggregation policy designed for multimodal inputs. Furthermore, it incorporates a distributed architecture to provide global KV Cache management and robust fault-tolerant capabilities for high availability. At the engine layer, xLLM-Engine co-optimizes system and algorithm designs to fully saturate computing resources. This is achieved through comprehensive multi-layer execution pipeline optimizations, an adaptive graph mode and an xTensor memory management. xLLM-Engine also further integrates algorithmic enhancements such as optimized speculative decoding and dynamic EPLB, collectively serving to substantially boost throughput and inference efficiency. Extensive evaluations demonstrate that xLLM delivers significantly superior performance and resource efficiency. Under identical TPOT constraints, xLLM achieves throughput up to 1.7x that of MindIE and 2.2x that of vLLM-Ascend with Qwen-series models, while maintaining an average throughput of 1.7x that of MindIE with Deepseek-series models. xLLM framework is publicly available at https://github.com/jd-opensource/xllm and https://github.com/jd-opensource/xllm-service.",
        "authors": [
            "Tongxuan Liu",
            "Tao Peng",
            "Peijun Yang",
            "Xiaoyang Zhao",
            "Xiusheng Lu",
            "Weizhe Huang",
            "Zirui Liu",
            "Xiaoyu Chen",
            "Zhiwei Liang",
            "Jun Xiong",
            "Donghe Jin",
            "Minchao Zhang",
            "Jinrong Guo",
            "Yingxu Deng",
            "Xu Zhang",
            "Xianzhe Dong",
            "Siqi Wang",
            "Siyu Wu",
            "Yu Wu",
            "Zihan Tang",
            "Yuting Zeng",
            "Yanshu Wang",
            "Jinguang Liu",
            "Meng Kang",
            "Menxin Li",
            "Yunlong Wang",
            "Yiming Liu",
            "Xiaolong Ma",
            "Yifan Wang",
            "Yichen Zhang",
            "Jinrun Yin",
            "Keyang Zheng",
            "Jiawei Yin",
            "Jun Zhang",
            "Ziyue Wang",
            "Xiaobo Lin",
            "Liangyu Liu",
            "Liwei Lan",
            "Yang Liu",
            "Chunhua Peng",
            "Han Liu",
            "Songcheng Ren",
            "Xuezhu Wang",
            "Yunheng Shen",
            "Yi Wang",
            "Guyue Liu",
            "Hui Chen",
            "Tong Yang",
            "Hailong Yang",
            "Jing Li",
            "Guiguang Ding",
            "Ke Zhang"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-10-16"
    },
    "http://arxiv.org/abs/2510.14642v1": {
        "title": "The Bidding Games: Reinforcement Learning for MEV Extraction on Polygon Blockchain",
        "link": "http://arxiv.org/abs/2510.14642v1",
        "abstract": "In blockchain networks, the strategic ordering of transactions within blocks has emerged as a significant source of profit extraction, known as Maximal Extractable Value (MEV). The transition from spam-based Priority Gas Auctions to structured auction mechanisms like Polygon Atlas has transformed MEV extraction from public bidding wars into sealed-bid competitions under extreme time constraints. While this shift reduces network congestion, it introduces complex strategic challenges where searchers must make optimal bidding decisions within a sub-second window without knowledge of competitor behavior or presence. Traditional game-theoretic approaches struggle in this high-frequency, partially observable environment due to their reliance on complete information and static equilibrium assumptions. We present a reinforcement learning framework for MEV extraction on Polygon Atlas and make three contributions: (1) A novel simulation environment that accurately models the stochastic arrival of arbitrage opportunities and probabilistic competition in Atlas auctions; (2) A PPO-based bidding agent optimized for real-time constraints, capable of adaptive strategy formulation in continuous action spaces while maintaining production-ready inference speeds; (3) Empirical validation demonstrating our history-conditioned agent captures 49\\% of available profits when deployed alongside existing searchers and 81\\% when replacing the market leader, significantly outperforming static bidding strategies. Our work establishes that reinforcement learning provides a critical advantage in high-frequency MEV environments where traditional optimization methods fail, offering immediate value for industrial participants and protocol designers alike.",
        "authors": [
            "Andrei Seoev",
            "Leonid Gremyachikh",
            "Anastasiia Smirnova",
            "Yash Madhwal",
            "Alisa Kalacheva",
            "Dmitry Belousov",
            "Ilia Zubov",
            "Aleksei Smirnov",
            "Denis Fedyanin",
            "Vladimir Gorgadze",
            "Yury Yanovich"
        ],
        "categories": [
            "cs.GT",
            "cs.AI",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.14642v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-16"
    },
    "http://arxiv.org/abs/2510.14622v1": {
        "id": "http://arxiv.org/abs/2510.14622v1",
        "title": "MPI-over-CXL: Enhancing Communication Efficiency in Distributed HPC Systems",
        "link": "http://arxiv.org/abs/2510.14622v1",
        "tags": [
            "hardware",
            "training",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes MPI-over-CXL, a communication paradigm using CXL's shared memory for direct pointer-based access in MPI, eliminating data copying. Reduces latency and bandwidth usage in distributed HPC systems. Achieves substantial performance gains over traditional MPI implementations.",
        "abstract": "MPI implementations commonly rely on explicit memory-copy operations, incurring overhead from redundant data movement and buffer management. This overhead notably impacts HPC workloads involving intensive inter-processor communication. In response, we introduce MPI-over-CXL, a novel MPI communication paradigm leveraging CXL, which provides cache-coherent shared memory across multiple hosts. MPI-over-CXL replaces traditional data-copy methods with direct shared memory access, significantly reducing communication latency and memory bandwidth usage. By mapping shared memory regions directly into the virtual address spaces of MPI processes, our design enables efficient pointer-based communication, eliminating redundant copying operations. To validate this approach, we implement a comprehensive hardware and software environment, including a custom CXL 3.2 controller, FPGA-based multi-host emulation, and dedicated software stack. Our evaluations using representative benchmarks demonstrate substantial performance improvements over conventional MPI systems, underscoring MPI-over-CXL's potential to enhance efficiency and scalability in large-scale HPC environments.",
        "authors": [
            "Miryeong Kwon",
            "Donghyun Gouk",
            "Hyein Woo",
            "Junhee Kim",
            "Jinwoo Baek",
            "Kyungkuk Nam",
            "Sangyoon Ji",
            "Jiseon Kim",
            "Hanyeoreum Bae",
            "Junhyeok Jang",
            "Hyunwoo You",
            "Junseok Moon",
            "Myoungsoo Jung"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-10-16"
    },
    "http://arxiv.org/abs/2510.14599v1": {
        "title": "JASDA: Introducing Job-Aware Scheduling in Scheduler-Driven Job Atomization",
        "link": "http://arxiv.org/abs/2510.14599v1",
        "abstract": "The increasing complexity and temporal variability of workloads on MIG-enabled GPUs challenge the scalability of traditional centralized scheduling. Building upon the SJA concept, this paper introduces JASDA-a novel paradigm that extends SJA from a largely centralized scheduling model toward a fully decentralized negotiation process. In JASDA, jobs actively generate and score feasible subjobs in response to scheduler-announced execution windows, while the scheduler performs policy-driven clearing that balances utilization, fairness, and temporal responsiveness. This bidirectional, iterative interaction embeds feedback, calibration, and probabilistic safety directly into the scheduling loop, enabling adaptive and transparent decision-making. By coupling principles from auction theory and online optimization with the temporal granularity of GPU workloads, JASDA provides a scalable foundation for market-aware and fairness-driven resource management-bridging theoretical scheduling models with practical deployment in modern MIG-enabled environments relevant to Artificial Intelligence and Agriculture 4.0.",
        "authors": [
            "Michal Konopa",
            "Jan Fesl",
            "Ladislav Ber ánek"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.14599v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-16"
    },
    "http://arxiv.org/abs/2510.14580v1": {
        "id": "http://arxiv.org/abs/2510.14580v1",
        "title": "ScalePool: Hybrid XLink-CXL Fabric for Composable Resource Disaggregation in Unified Scale-up Domains",
        "link": "http://arxiv.org/abs/2510.14580v1",
        "tags": [
            "training",
            "offloading",
            "hardware"
        ],
        "relevant": true,
        "indexed_date": "2025-10-16",
        "tldr": "Proposes ScalePool, a hybrid XLink-CXL fabric for disaggregated memory and accelerator interconnection in LLM training clusters. Achieves 1.22x average speedup and 4.5x lower latency for memory-intensive workloads by enabling coherent, tiered memory pooling beyond RDMA.",
        "abstract": "This paper proposes ScalePool, a novel cluster architecture designed to interconnect numerous accelerators using unified hardware interconnects rather than traditional long-distance networking. ScalePool integrates Accelerator-Centric Links (XLink) and Compute Express Link (CXL) into a unified XLink-CXL hybrid fabric. Specifically, ScalePool employs XLink for intra-cluster, low-latency accelerator communication, while using hierarchical CXL-based switching fabrics for scalable and coherent inter-cluster memory sharing. By abstracting interfaces through CXL, ScalePool structurally resolves interoperability constraints, enabling heterogeneous cluster operation and composable resource disaggregation. In addition, ScalePool introduces explicit memory tiering: the latency-critical tier-1 combines accelerator-local memory with coherence-centric CXL and XLink, whereas the highcapacity tier-2 employs dedicated memory nodes interconnected by a CXL-based fabric, achieving scalable and efficient memory pooling. Evaluation results show that ScalePool accelerates LLM training by 1.22x on average and up to 1.84x compared to conventional RDMA-based environments. Furthermore, the proposed tier-2 memory disaggregation strategy reduces latency by up to 4.5x for memory-intensive workloads.",
        "authors": [
            "Hyein Woo",
            "Miryeong Kwon",
            "Jiseon Kim",
            "Eunjee Na",
            "Hanjin Choi",
            "Seonghyeon Jang",
            "Myoungsoo Jung"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-10-16"
    },
    "http://arxiv.org/abs/2510.14392v1": {
        "id": "http://arxiv.org/abs/2510.14392v1",
        "title": "FairBatching: Fairness-Aware Batch Formation for LLM Inference",
        "link": "http://arxiv.org/abs/2510.14392v1",
        "tags": [
            "serving",
            "offline"
        ],
        "relevant": true,
        "indexed_date": "2025-10-16",
        "tldr": "Addresses computational unfairness in LLM inference batching by redesigning batch formation to balance prefill and decode tasks. Introduces adaptive budgeting and dynamic scheduling to reduce TTFT tail latency by up to 2.29x while improving single-node and cluster capacity by 20.0% and 54.3%, respectively.",
        "abstract": "Large language model (LLM) inference systems face a fundamental tension between minimizing Time-to-First-Token (TTFT) latency for new requests and maintaining a high, steady token generation rate (low Time-Per-Output-Token, or TPOT) for ongoing requests. Existing stall-free batching schedulers proposed by Sarathi, while effective at preventing decode stalls, introduce significant computational unfairness. They prioritize decode tasks excessively, simultaneously leading to underutilized decode slack and unnecessary prefill queuing delays, which collectively degrade the system's overall quality of service (QoS).   This work identifies the root cause of this unfairness: the non-monotonic nature of Time-Between-Tokens (TBT) as a scheduling metric and the rigid decode-prioritizing policy that fails to adapt to dynamic workload bursts. We therefore propose FairBatching, a novel LLM inference scheduler that enforces fair resource allocation between prefill and decode tasks. It features an adaptive batch capacity determination mechanism, which dynamically adjusts the computational budget to improve the GPU utilization without triggering SLO violations. Its fair and dynamic batch formation algorithm breaks away from the decode-prioritizing paradigm, allowing computation resources to be reclaimed from bursting decode tasks to serve prefill surges, achieving global fairness. Furthermore, FairBatching provides a novel load estimation method, enabling more effective coordination with upper-level schedulers. Implemented and evaluated on realistic traces, FairBatching significantly reduces TTFT tail latency by up to 2.29x while robustly maintaining TPOT SLOs, achieving overall 20.0% improvement in single-node capacity and 54.3% improvement in cluster-level capacity.",
        "authors": [
            "Hongtao Lyu",
            "Boyue Liu",
            "Mingyu Wu",
            "Haibo Chen"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-10-16"
    },
    "http://arxiv.org/abs/2511.07422v1": {
        "id": "http://arxiv.org/abs/2511.07422v1",
        "title": "From Attention to Disaggregation: Tracing the Evolution of LLM Inference",
        "link": "http://arxiv.org/abs/2511.07422v1",
        "tags": [
            "serving",
            "offloading",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes disaggregated inference architecture for LLMs, decoupling prefill and decode phases as independently scalable components to overcome GPU cluster limitations. Mitigates resource contention, optimizing Time to First Token and Inter Token Latency while reducing cost. Achieves significant improvements in latency and throughput.",
        "abstract": "The evolution of Large Language Models from the Transformer architecture to models with trillions of parameters has shifted the primary bottleneck from model training to real time inference. Deploying these massive models is a complex distributed systems challenge constrained by memory bandwidth, computational throughput, and latency requirements. LLM inference fundamentally requires solving a multi objective optimization problem to minimize latency, maximize throughput, and reduce cost. This paper explores the necessary architectural shift towards disaggregated inference, which applies distributed systems principles such as service decomposition, resource disaggregation, and workload partitioning to overcome the limitations of traditional monolithic GPU clusters. By decoupling the compute intensive prefill phase from the memory intensive decode phase into independently scalable components, this paradigm mitigates resource contention and enables independent optimization of key metrics like Time to First Token and Inter Token Latency.",
        "authors": [
            "Madabattula Rajesh Kumar",
            "Srinivasa Rao Aravilli",
            "Mustafa Saify",
            "Shashank Srivastava"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-10-16"
    },
    "http://arxiv.org/abs/2510.14208v2": {
        "title": "Incentive-Based Federated Learning: Architectural Elements and Future Directions",
        "link": "http://arxiv.org/abs/2510.14208v2",
        "abstract": "Federated learning promises to revolutionize machine learning by enabling collaborative model training without compromising data privacy. However, practical adaptability can be limited by critical factors, such as the participation dilemma. Participating entities are often unwilling to contribute to a learning system unless they receive some benefits, or they may pretend to participate and free-ride on others. This chapter identifies the fundamental challenges in designing incentive mechanisms for federated learning systems. It examines how foundational concepts from economics and game theory can be applied to federated learning, alongside technology-driven solutions such as blockchain and deep reinforcement learning. This work presents a comprehensive taxonomy that thoroughly covers both centralized and decentralized architectures based on the aforementioned theoretical concepts. Furthermore, the concepts described are presented from an application perspective, covering emerging industrial applications, including healthcare, smart infrastructure, vehicular networks, and blockchain-based decentralized systems. Through this exploration, this chapter demonstrates that well-designed incentive mechanisms are not merely optional features but essential components for the practical success of federated learning. This analysis reveals both the promising solutions that have emerged and the significant challenges that remain in building truly sustainable, fair, and robust federated learning ecosystems.",
        "authors": [
            "Chanuka A. S. Hewa Kaluannakkage",
            "Rajkumar Buyya"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.14208v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-16"
    },
    "http://arxiv.org/abs/2510.14186v1": {
        "title": "Proof-Carrying Fair Ordering: Asymmetric Verification for BFT via Incremental Graphs",
        "link": "http://arxiv.org/abs/2510.14186v1",
        "abstract": "Byzantine Fault-Tolerant (BFT) consensus protocols ensure agreement on transaction ordering despite malicious actors, but unconstrained ordering power enables sophisticated value extraction attacks like front running and sandwich attacks - a critical threat to blockchain systems. Order-fair consensus curbs adversarial value extraction by constraining how leaders may order transactions. While state-of-the-art protocols such as Themis attain strong guarantees through graph-based ordering, they ask every replica to re-run the leader's expensive ordering computation for validation - an inherently symmetric and redundant paradigm. We present AUTIG, a high-performance, pluggable order-fairness service that breaks this symmetry. Our key insight is that verifying a fair order does not require re-computing it. Instead, verification can be reduced to a stateless audit of succinct, verifiable assertions about the ordering graph's properties. AUTIG realizes this via an asymmetric architecture: the leader maintains a persistent Unconfirmed-Transaction Incremental Graph (UTIG) to amortize graph construction across rounds and emits a structured proof of fairness with each proposal; followers validate the proof without maintaining historical state. AUTIG introduces three critical innovations: (i) incremental graph maintenance driven by threshold-crossing events and state changes; (ii) a decoupled pipeline that overlaps leader-side collection/update/extraction with follower-side stateless verification; and (iii) a proof design covering all internal pairs in the finalized prefix plus a frontier completeness check to rule out hidden external dependencies. We implement AUTIG and evaluate it against symmetric graph-based baselines under partial synchrony. Experiments show higher throughput and lower end-to-end latency while preserving gamma-batch-order-fairness.",
        "authors": [
            "Pengkun Ren",
            "Hai Dong",
            "Nasrin Sohrabi",
            "Zahir Tari",
            "Pengcheng Zhang"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.14186v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-16"
    },
    "http://arxiv.org/abs/2510.14151v1": {
        "title": "Privacy-Preserving and Incentive-Driven Relay-Based Framework for Cross-Domain Blockchain Interoperability",
        "link": "http://arxiv.org/abs/2510.14151v1",
        "abstract": "Interoperability is essential for transforming blockchains from isolated networks into collaborative ecosystems, unlocking their full potential. While significant progress has been made in public blockchain interoperability, bridging permissioned and permissionless blockchains poses unique challenges due to differences in access control, architectures, and security requirements. This paper introduces a blockchain-agnostic framework to enable interoperability between permissioned and permissionless networks. Leveraging cryptographic techniques, the framework ensures secure data exchanges. Its lightweight architectural design simplifies implementation and maintenance, while the integration of Clover and Dandelion++ protocols enhances transaction anonymity. Performance evaluations demonstrate the framework's effectiveness in achieving secure and efficient interoperability by measuring the forwarding time, the throughput, the availability, and their collusion impact of the system across heterogeneous blockchain ecosystems.",
        "authors": [
            "Saeed Moradi",
            "Koosha Esmaeilzadeh Khorasani",
            "Sara Rouhani"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.14151v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-15"
    },
    "http://arxiv.org/abs/2510.14147v1": {
        "title": "Distributed-Memory Parallel Algorithms for Fixed-Radius Near Neighbor Graph Construction",
        "link": "http://arxiv.org/abs/2510.14147v1",
        "abstract": "Computing fixed-radius near-neighbor graphs is an important first step for many data analysis algorithms. Near-neighbor graphs connect points that are close under some metric, endowing point clouds with a combinatorial structure. As computing power and data acquisition methods advance, diverse sources of large scientific datasets would greatly benefit from scalable solutions to this common subroutine for downstream analysis. Prior work on parallel nearest neighbors has made great progress in problems like k-nearest and approximate nearest neighbor search problems, with particular attention on Euclidean spaces. Yet many applications need exact solutions and non-Euclidean metrics. This paper presents a scalable sparsity-aware distributed memory algorithm using cover trees to compute near-neighbor graphs in general metric spaces. We provide a shared-memory algorithm for cover tree construction and demonstrate its competitiveness with state-of-the-art fixed-radius search data structures. We then introduce two distributed-memory algorithms for the near-neighbor graph problem, a simple point-partitioning strategy and a spatial-partitioning strategy, which leverage the cover tree algorithm on each node. Our algorithms exhibit parallel scaling across a variety of real and synthetic datasets for both traditional and non-traditional metrics. On real world high dimensional datasets with one million points, we achieve speedups up to 678.34x over the state-of-the-art using 1024 cores for graphs with 70 neighbors per vertex (on average), and up to 1590.99x using 4096 cores for graphs with 500 neighbors per vertex (on average).",
        "authors": [
            "Gabriel Raulet",
            "Dmitriy Morozov",
            "Aydin Buluc",
            "Katherine Yelick"
        ],
        "categories": [
            "cs.DC",
            "cs.CG",
            "cs.DS"
        ],
        "id": "http://arxiv.org/abs/2510.14147v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-15"
    },
    "http://arxiv.org/abs/2510.14126v1": {
        "id": "http://arxiv.org/abs/2510.14126v1",
        "title": "Cortex: Workflow-Aware Resource Pooling and Scheduling for Agentic Serving",
        "link": "http://arxiv.org/abs/2510.14126v1",
        "tags": [
            "serving",
            "agentic"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Investigates resource pooling and scheduling for agentic workflows. Proposes Cortex with stage isolation, dedicating resource pools per workflow stage to mitigate interference and improve KV cache utilization. Increases throughput by 4.2× and reduces SLO violations by 48% compared to baseline.",
        "abstract": "We introduce Cortex, a prototype workflow-aware serving platform designed for agentic workloads. The core principle of Cortex is stage isolation: it provisions dedicated resource pools for each distinct stage of an agentic workflow. This simple yet powerful strategy mitigates inter-stage interference in compute and memory, leading to better KV cache utilization, higher throughput, and more predictable performance. By customizing resource allocation and scheduling within each distinct stage of agentic workflows, Cortex lays the groundwork for more advanced, agent-native serving paradigms, including malleable resource management, speculative execution of workflow branches, and a shared, multi-tiered cache for \"agentic state.\"",
        "authors": [
            "Nikos Pagonas",
            "Yeounoh Chung",
            "Kostis Kaffes",
            "Arvind Krishnamurthy"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-10-15"
    },
    "http://arxiv.org/abs/2510.14054v1": {
        "id": "http://arxiv.org/abs/2510.14054v1",
        "title": "FedHFT: Efficient Federated Finetuning with Heterogeneous Edge Clients",
        "link": "http://arxiv.org/abs/2510.14054v1",
        "tags": [
            "training",
            "RL",
            "edge"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes FedHFT for efficient federated fine-tuning of LLMs across heterogeneous edge clients. Uses masked adapters for resource heterogeneity and bi-level optimization with client clustering for non-iid data. Achieves up to 15% accuracy gain with 60% less communication overhead.",
        "abstract": "Fine-tuning pre-trained large language models (LLMs) has become a common practice for personalized natural language understanding (NLU) applications on downstream tasks and domain-specific datasets. However, there are two main challenges: (i) limited and/or heterogeneous data for fine-tuning due to proprietary data confidentiality or privacy requirements, and (ii) varying computation resources available across participating clients such as edge devices. This paper presents FedHFT - an efficient and personalized federated fine-tuning framework to address both challenges. First, we introduce a mixture of masked adapters to handle resource heterogeneity across participating clients, enabling high-performance collaborative fine-tuning of pre-trained language model(s) across multiple clients in a distributed setting, while keeping proprietary data local. Second, we introduce a bi-level optimization approach to handle non-iid data distribution based on masked personalization and client clustering. Extensive experiments demonstrate significant performance and efficiency improvements over various natural language understanding tasks under data and resource heterogeneity compared to representative heterogeneous federated learning methods.",
        "authors": [
            "Fatih Ilhan",
            "Selim Furkan Tekin",
            "Tiansheng Huang",
            "Gaowen Liu",
            "Ramana Kompella",
            "Greg Eisenhauer",
            "Yingyan Celine Lin",
            "Calton Pu",
            "Ling Liu"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "submit_date": "2025-10-15"
    },
    "http://arxiv.org/abs/2510.14050v1": {
        "title": "Anonymized Network Sensing using C++26 std::execution on GPUs",
        "link": "http://arxiv.org/abs/2510.14050v1",
        "abstract": "Large-scale network sensing plays a vital role in network traffic analysis and characterization. As network packet data grows increasingly large, parallel methods have become mainstream for network analytics. While effective, GPU-based implementations still face start-up challenges in host-device memory management and porting complex workloads on devices, among others. To mitigate these challenges, composable frameworks have emerged using modern C++ programming language, for efficiently deploying analytics tasks on GPUs. Specifically, the recent C++26 Senders model of asynchronous data operation chaining provides a simple interface for bulk pushing tasks to varied device execution contexts.   Considering the prominence of contemporary dense-GPU platforms and vendor-leveraged software libraries, such a programming model consider GPUs as first-class execution resources (compared to traditional host-centric programming models), allowing convenient development of multi-GPU application workloads via expressive and standardized asynchronous semantics. In this paper, we discuss practical aspects of developing the Anonymized Network Sensing Graph Challenge on dense-GPU systems using the recently proposed C++26 Senders model. Adopting a generic and productive programming model does not necessarily impact the critical-path performance (as compared to low-level proprietary vendor-based programming models): our commodity library-based implementation achieves up to 55x performance improvements on 8x NVIDIA A100 GPUs as compared to the reference serial GraphBLAS baseline.",
        "authors": [
            "Michael Mandulak",
            "Sayan Ghosh",
            "S M Ferdous",
            "Mahantesh Halappanavar",
            "George Slota"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.14050v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-15"
    },
    "http://arxiv.org/abs/2510.14024v1": {
        "id": "http://arxiv.org/abs/2510.14024v1",
        "title": "Efficiently Executing High-throughput Lightweight LLM Inference Applications on Heterogeneous Opportunistic GPU Clusters with Pervasive Context Management",
        "link": "http://arxiv.org/abs/2510.14024v1",
        "tags": [
            "serving",
            "offloading",
            "edge"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Addresses high LLM startup costs and queue delays in HPC clusters with lightweight LLM inference. Proposes pervasive context management to decouple initialization from inference, retaining context on GPUs opportunistically. Achieves 72.1% execution time reduction (from 3h to 48min) and further scales to 13min using idle GPUs.",
        "abstract": "The rise of Generative AI introduces a new class of HPC workloads that integrates lightweight LLMs with traditional high-throughput applications to accelerate scientific discovery. The current design of HPC clusters is inadequate to support this new class however, either incurring long wait times on static batch queues or repeatedly paying expensive LLM startup costs upon resource preemption. To circumvent both the long queues and high startup costs, we propose to \"decouple\" the LLM initialization context from the actual LLM inferences, and retain the context in GPUs until it is no longer needed, a technique we term \"Pervasive Context Management\". We transform a fact verification application to enable this technique, allowing it to reduce its execution time by 72.1% (from 3 hours to 48 minutes) using the same amount of GPUs, and scale opportunistically on 32.8% of all GPUs in the cluster and further reduce the execution time to 13 minutes.",
        "authors": [
            "Thanh Son Phung",
            "Douglas Thain"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-10-15"
    },
    "http://arxiv.org/abs/2510.16024v1": {
        "title": "On-Chain Decentralized Learning and Cost-Effective Inference for DeFi Attack Mitigation",
        "link": "http://arxiv.org/abs/2510.16024v1",
        "abstract": "Billions of dollars are lost every year in DeFi platforms by transactions exploiting business logic or accounting vulnerabilities. Existing defenses focus on static code analysis, public mempool screening, attacker contract detection, or trusted off-chain monitors, none of which prevents exploits submitted through private relays or malicious contracts that execute within the same block. We present the first decentralized, fully on-chain learning framework that: (i) performs gas-prohibitive computation on Layer-2 to reduce cost, (ii) propagates verified model updates to Layer-1, and (iii) enables gas-bounded, low-latency inference inside smart contracts. A novel Proof-of-Improvement (PoIm) protocol governs the training process and verifies each decentralized micro update as a self-verifying training transaction. Updates are accepted by \\textit{PoIm} only if they demonstrably improve at least one core metric (e.g., accuracy, F1-score, precision, or recall) on a public benchmark without degrading any of the other core metrics, while adversarial proposals get financially penalized through an adaptable test set for evolving threats. We develop quantization and loop-unrolling techniques that enable inference for logistic regression, SVM, MLPs, CNNs, and gated RNNs (with support for formally verified decision tree inference) within the Ethereum block gas limit, while remaining bit-exact to their off-chain counterparts, formally proven in Z3. We curate 298 unique real-world exploits (2020 - 2025) with 402 exploit transactions across eight EVM chains, collectively responsible for \\$3.74 B in losses.",
        "authors": [
            "Abdulrahman Alhaidari",
            "Balaji Palanisamy",
            "Prashant Krishnamurthy"
        ],
        "categories": [
            "cs.CR",
            "cs.AI",
            "cs.DC",
            "cs.LG"
        ],
        "id": "http://arxiv.org/abs/2510.16024v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-15"
    },
    "http://arxiv.org/abs/2510.13755v1": {
        "title": "Tight Conditions for Binary-Output Tasks under Crashes",
        "link": "http://arxiv.org/abs/2510.13755v1",
        "abstract": "This paper explores necessary and sufficient system conditions to solve distributed tasks with binary outputs (\\textit{i.e.}, tasks with output values in $\\{0,1\\}$). We focus on the distinct output sets of values a task can produce (intentionally disregarding validity and value multiplicity), considering that some processes may output no value. In a distributed system with $n$ processes, of which up to $t \\leq n$ can crash, we provide a complete characterization of the tight conditions on $n$ and $t$ under which every class of tasks with binary outputs is solvable, for both synchronous and asynchronous systems. This output-set approach yields highly general results: it unifies multiple distributed computing problems, such as binary consensus and symmetry breaking, and it produces impossibility proofs that hold for stronger task formulations, including those that consider validity, account for value multiplicity, or move beyond binary outputs.",
        "authors": [
            "Timothé Albouy",
            "Antonio Fernández Anta",
            "Chryssis Georgiou",
            "Nicolas Nicolaou",
            "Junlang Wang"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.13755v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-15"
    },
    "http://arxiv.org/abs/2510.13724v1": {
        "id": "http://arxiv.org/abs/2510.13724v1",
        "title": "FIRST: Federated Inference Resource Scheduling Toolkit for Scientific AI Model Access",
        "link": "http://arxiv.org/abs/2510.13724v1",
        "tags": [
            "serving",
            "offloading",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-10-15",
        "tldr": "FIRST enables federated LLM inference across distributed HPC clusters using a cloud-like API, auto-scaling resources and maintaining hot nodes for low-latency serving. It achieves scalable, on-premises generation of billions of tokens daily without commercial cloud reliance.",
        "abstract": "We present the Federated Inference Resource Scheduling Toolkit (FIRST), a framework enabling Inference-as-a-Service across distributed High-Performance Computing (HPC) clusters. FIRST provides cloud-like access to diverse AI models, like Large Language Models (LLMs), on existing HPC infrastructure. Leveraging Globus Auth and Globus Compute, the system allows researchers to run parallel inference workloads via an OpenAI-compliant API on private, secure environments. This cluster-agnostic API allows requests to be distributed across federated clusters, targeting numerous hosted models. FIRST supports multiple inference backends (e.g., vLLM), auto-scales resources, maintains \"hot\" nodes for low-latency execution, and offers both high-throughput batch and interactive modes. The framework addresses the growing demand for private, secure, and scalable AI inference in scientific workflows, allowing researchers to generate billions of tokens daily on-premises without relying on commercial cloud infrastructure.",
        "authors": [
            "Aditya Tanikanti",
            "Benoit Côté",
            "Yanfei Guo",
            "Le Chen",
            "Nickolaus Saint",
            "Ryan Chard",
            "Ken Raffenetti",
            "Rajeev Thakur",
            "Thomas Uram",
            "Ian Foster",
            "Michael E. Papka",
            "Venkatram Vishwanath"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.SE"
        ],
        "submit_date": "2025-10-15"
    },
    "http://arxiv.org/abs/2510.13668v1": {
        "id": "http://arxiv.org/abs/2510.13668v1",
        "title": "Adaptive Rescheduling in Prefill-Decode Disaggregated LLM Inference",
        "link": "http://arxiv.org/abs/2510.13668v1",
        "tags": [
            "serving",
            "offloading",
            "thinking"
        ],
        "relevant": true,
        "indexed_date": "2025-10-15",
        "tldr": "Addresses workload imbalance in disaggregated LLM inference due to unpredictable decode lengths. Proposes ARES, a system using LLM-internal state to predict output length and dynamically reschedule decode tasks, reducing P99 TPOT by 74.77% and improving goodput by 2.24×.",
        "abstract": "Large Language Model (LLM) inference has emerged as a fundamental paradigm. In real-world scenarios, variations in output length cause severe workload imbalance in the decode phase, particularly for long-output reasoning tasks. Existing systems, such as PD disaggregation architectures, rely on static prefill-to-decode scheduling, which often results in SLO violations and OOM failures under evolving decode workloads.   In this paper, we propose ARES, an adaptive decoding rescheduling system powered by length prediction to anticipate future workloads. Our core contributions include: (1) A lightweight and continuous LLM-native prediction method that leverages LLM hidden state to model remaining generation length with high precision (reducing MAE by 49.42%) and low overhead (cutting predictor parameters by 93.28%); (2) A rescheduling solution in decode phase with : A dynamic balancing mechanism that integrates current and predicted workloads, reducing P99 TPOT by 74.77% and achieving up to 2.24 times higher goodput.",
        "authors": [
            "Zhibin Wang",
            "Zetao Hong",
            "Xue Li",
            "Zibo Wang",
            "Shipeng Li",
            "Qingkai Meng",
            "Qing Wang",
            "Chengying Huan",
            "Rong Gu",
            "Sheng Zhong",
            "Chen Tian"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-10-15"
    },
    "http://arxiv.org/abs/2510.13447v1": {
        "title": "Service-Level Energy Modeling and Experimentation for Cloud-Native Microservices",
        "link": "http://arxiv.org/abs/2510.13447v1",
        "abstract": "Microservice architectures have become the dominant paradigm for cloud-native systems, offering flexibility and scalability. However, this shift has also led to increased demand for cloud resources, contributing to higher energy consumption and carbon emissions. While existing research has focused on measuring fine-grained energy usage of CPU and memory at the container level, or on system-wide assessments, these approaches often overlook the energy impact of cross-container service interactions, especially those involving network and storage for auxiliary services such as observability and system monitoring. To address this gap, we introduce a service-level energy model that captures the distributed nature of microservice execution across containers. Our model is supported by an experimentation tool that accounts for energy consumption not just in CPU and memory, but also in network and storage components. We validate our approach through extensive experimentation with diverse experiment configurations of auxiliary services for a popular open-source cloud-native microservice application. Results show that omitting network and storage can lead to an underestimation of auxiliary service energy use by up to 63%, highlighting the need for more comprehensive energy assessments in the design of energy-efficient microservice architectures.",
        "authors": [
            "Julian Legler",
            "Sebastian Werner",
            "Maria C. Borges",
            "Stefan Tai"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.13447v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-15"
    },
    "http://arxiv.org/abs/2510.13427v1": {
        "title": "Verification Challenges in Sparse Matrix Vector Multiplication in High Performance Computing: Part I",
        "link": "http://arxiv.org/abs/2510.13427v1",
        "abstract": "Sparse matrix vector multiplication (SpMV) is a fundamental kernel in scientific codes that rely on iterative solvers. In this first part of our work, we present both a sequential and a basic MPI parallel implementations of SpMV, aiming to provide a challenge problem for the scientific software verification community. The implementations are described in the context of the PETSc library.",
        "authors": [
            "Junchao Zhang"
        ],
        "categories": [
            "cs.LO",
            "cs.DC",
            "cs.MS"
        ],
        "id": "http://arxiv.org/abs/2510.13427v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-15"
    },
    "http://arxiv.org/abs/2510.13413v1": {
        "title": "VSS Challenge Problem: Verifying the Correctness of AllReduce Algorithms in the MPICH Implementation of MPI",
        "link": "http://arxiv.org/abs/2510.13413v1",
        "abstract": "We describe a challenge problem for verification based on the MPICH implementation of MPI. The MPICH implementation includes several algorithms for allreduce, all of which should be functionally equivalent to reduce followed by broadcast. We created standalone versions of three algorithms and verified two of them using CIVL.",
        "authors": [
            "Paul D. Hovland"
        ],
        "categories": [
            "cs.LO",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.13413v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-15"
    },
    "http://arxiv.org/abs/2510.13401v1": {
        "id": "http://arxiv.org/abs/2510.13401v1",
        "title": "F-BFQ: Flexible Block Floating-Point Quantization Accelerator for LLMs",
        "link": "http://arxiv.org/abs/2510.13401v1",
        "tags": [
            "quantization",
            "hardware",
            "edge"
        ],
        "relevant": true,
        "indexed_date": "2025-10-15",
        "tldr": "Designs a hardware accelerator (F-BFQ) to efficiently execute mixed block-floating-point quantized LLMs on edge devices, dynamically switching between BFP variants without reconfiguration. Achieves 1.4x faster inference than Arm NEON CPU and 5.2 tokens/s on AMD Kria.",
        "abstract": "Large Language Models (LLMs) have become increasingly prominent for daily tasks, from improving sound-totext translation to generating additional frames for the latest video games. With the help of LLM inference frameworks, such as llama.cpp, which support optimizations such as KV-caching and quantization, it is now easier than ever to deploy LLMs on edge devices. Quantization is fundamental to enable LLMs on resource-constrained edge devices, and llama.cpp utilizes block floating point (BFP) quantization to drastically reduce the bit width of weights and input tensors, the memory footprint, and the computational power required to run LLMs. LLMs are typically quantized with mixed BFP quantization across the model layers to reduce the loss of model accuracy due to quantization. Therefore, to efficiently accelerate across the layers of BFP-quantized LLMs, specialized accelerators need to support different BFP variants without reconfiguration. To address this issue, we propose a Flexible Block FloatingPoint Quantization (F-BFQ) accelerator, which can dynamically switch between two BFP quantization variants and perform matrix multiplication (MatMul) operations. Our initial F-BFQ accelerator design, deployed on the AMD Kria board, reduces inference time by 1.4x on average over the Arm NEON-based CPU execution across three BFP quantized LLMs while achieving 5.2 tokens per second (~3.9 words per second).",
        "authors": [
            "Jude Haris",
            "José Cano"
        ],
        "categories": [
            "cs.AR",
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-10-15"
    },
    "http://arxiv.org/abs/2511.11586v1": {
        "id": "http://arxiv.org/abs/2511.11586v1",
        "title": "ACE-GNN: Adaptive GNN Co-Inference with System-Aware Scheduling in Dynamic Edge Environments",
        "link": "http://arxiv.org/abs/2511.11586v1",
        "tags": [
            "edge",
            "serving",
            "offline"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes ACE-GNN, an adaptive device-edge co-inference framework for GNNs in dynamic edge environments. Uses system-level abstraction and prediction for runtime optimization, combining pipeline and data parallelism. Achieves up to 12.7x speedup and 82.3% energy savings over prior methods.",
        "abstract": "The device-edge co-inference paradigm effectively bridges the gap between the high resource demands of Graph Neural Networks (GNNs) and limited device resources, making it a promising solution for advancing edge GNN applications. Existing research enhances GNN co-inference by leveraging offline model splitting and pipeline parallelism (PP), which enables more efficient computation and resource utilization during inference. However, the performance of these static deployment methods is significantly affected by environmental dynamics such as network fluctuations and multi-device access, which remain unaddressed. We present ACE-GNN, the first Adaptive GNN Co-inference framework tailored for dynamic Edge environments, to boost system performance and stability. ACE-GNN achieves performance awareness for complex multi-device access edge systems via system-level abstraction and two novel prediction methods, enabling rapid runtime scheme optimization. Moreover, we introduce a data parallelism (DP) mechanism in the runtime optimization space, enabling adaptive scheduling between PP and DP to leverage their distinct advantages and maintain stable system performance. Also, an efficient batch inference strategy and specialized communication middleware are implemented to further improve performance. Extensive experiments across diverse applications and edge settings demonstrate that ACE-GNN achieves a speedup of up to 12.7x and an energy savings of 82.3% compared to GCoDE, as well as 11.7 better energy efficiency than Fograph.",
        "authors": [
            "Ao Zhou",
            "Jianlei Yang",
            "Tong Qiao",
            "Yingjie Qi",
            "Xinming Wei",
            "Cenlin Duan",
            "Weisheng Zhao",
            "Chunming Hu"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-10-15"
    },
    "http://arxiv.org/abs/2510.13306v1": {
        "title": "Distributed Reductions for the Maximum Weight Independent Set Problem",
        "link": "http://arxiv.org/abs/2510.13306v1",
        "abstract": "Finding maximum-weight independent sets in graphs is an important NP-hard optimization problem. Given a vertex-weighted graph $G$, the task is to find a subset of pairwise non-adjacent vertices of $G$ with maximum weight. Most recently published practical exact algorithms and heuristics for this problem use a variety of data-reduction rules to compute (near-)optimal solutions. Applying these rules results in an equivalent instance of reduced size. An optimal solution to the reduced instance can be easily used to construct an optimal solution for the original input.   In this work, we present the first distributed-memory parallel reduction algorithms for this problem, targeting graphs beyond the scale of previous sequential approaches. Furthermore, we propose the first distributed reduce-and-greedy and reduce-and-peel algorithms for finding a maximum weight independent set heuristically.   In our practical evaluation, our experiments on up to $1024$ processors demonstrate good scalability of our distributed reduce algorithms while maintaining good reduction impact. Our asynchronous reduce-and-peel approach achieves an average speedup of $33\\times$ over a sequential state-of-the-art reduce-and-peel approach on 36 real-world graphs with a solution quality close to the sequential algorithm. Our reduce-and-greedy algorithms even achieve average speedups of up to $50\\times$ at the cost of a lower solution quality. Moreover, our distributed approach allows us to consider graphs with more than one billion vertices and 17 billion edges.",
        "authors": [
            "Jannick Borowitz",
            "Ernestine Großmann",
            "Mattthias Schimek"
        ],
        "categories": [
            "cs.DC",
            "cs.DS"
        ],
        "id": "http://arxiv.org/abs/2510.13306v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-15"
    },
    "http://arxiv.org/abs/2510.13223v1": {
        "id": "http://arxiv.org/abs/2510.13223v1",
        "title": "BanaServe: Unified KV Cache and Dynamic Module Migration for Balancing Disaggregated LLM Serving in AI Infrastructure",
        "link": "http://arxiv.org/abs/2510.13223v1",
        "tags": [
            "serving",
            "offloading",
            "RAG"
        ],
        "relevant": true,
        "indexed_date": "2025-10-15",
        "tldr": "Addresses load imbalance in disaggregated LLM serving by dynamically migrating KV cache and model layers between prefill and decode nodes. Enables cache-agnostic scheduling with overlapped transmission, achieving 1.2x-3.9x higher throughput and up to 78.4% lower latency than vLLM.",
        "abstract": "Large language models (LLMs) are increasingly deployed in AI infrastructure, driving the need for high throughput, resource efficient serving systems. Disaggregated LLM serving, which separates prompt prefill from auto-regressive decode, has emerged as a promising architecture by isolating their heterogeneous compute and memory demands. However, current disaggregated systems face three key limitations: (i) static resource allocation cannot adapt to highly dynamic workloads, causing over-provisioning that wastes resources or under-provisioning that violates service level objectives (SLOs); (ii) inherent load imbalance between prefill and decode stages, where prefill is compute-bound and decode is memory-bound, causes under-utilization in one tier while the other becomes a bottleneck; and (iii) prefix cache aware routing skews load distribution, as high cache hit rate prefill nodes attract disproportionately more requests, further degrading balance and efficiency. To address these issues, we present BanaServe, a dynamic orchestration framework that continuously rebalances computational and memory resources across prefill and decode instances while eliminating hotspots induced by cache. BanaServe introduces layer level weight migration, attention level Key Value Cache (KV Cache) migration, and Global KV Cache Store sharing with layer wise overlapped transmission, enabling both coarse grained (layer level) and fine grained (attention level) load redistribution with minimal latency overhead. These mechanisms allow routers to perform purely load aware scheduling, unconstrained by cache placement. Compared to vLLM, BanaServe achieves 1.2x-3.9x higher throughput with 3.9%-78.4% lower total processing time, and outperforms DistServe by 1.1x-2.8x in throughput with 1.4%-70.1% latency reduction.",
        "authors": [
            "Yiyuan He",
            "Minxian Xu",
            "Jingfeng Wu",
            "Jianmin Hu",
            "Chong Ma",
            "Min Shen",
            "Le Chen",
            "Chengzhong Xu",
            "Lin Qu",
            "Kejiang Ye"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-10-15"
    },
    "http://arxiv.org/abs/2510.13203v1": {
        "title": "Scrutiny new framework in integrated distributed reliable systems",
        "link": "http://arxiv.org/abs/2510.13203v1",
        "abstract": "In this paper we represent a new framework for integrated distributed systems. In the proposed framework we have used three parts to increase Satisfaction and Performance of this framework. At first we analyse integrated systems and their evolution process and also ERPSD and ERPDRT framework briefly then we explain the new FDIRS framework. Finally we compare the results of simulation of the new framework with presented frameworks. Result showed In FIDRS framework, the technique of heterogeneous distributed data base is used to improve Performance and speed in responding to users. Finally by using FDIRS framework we succeeded to increase Efficiency, Performance and reliability of integrated systems and remove some of previous frameworks problems.",
        "authors": [
            "Mehdi Zekriyapanah Gashti"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.13203v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-15"
    },
    "http://arxiv.org/abs/2511.07421v1": {
        "title": "Towards Affordable, Adaptive and Automatic GNN Training on CPU-GPU Heterogeneous Platforms",
        "link": "http://arxiv.org/abs/2511.07421v1",
        "abstract": "Graph Neural Networks (GNNs) have been widely adopted due to their strong performance. However, GNN training often relies on expensive, high-performance computing platforms, limiting accessibility for many tasks. Profiling of representative GNN workloads indicates that substantial efficiency gains are possible on resource-constrained devices by fully exploiting available resources. This paper introduces A3GNN, a framework for affordable, adaptive, and automatic GNN training on heterogeneous CPU-GPU platforms. It improves resource usage through locality-aware sampling and fine-grained parallelism scheduling. Moreover, it leverages reinforcement learning to explore the design space and achieve pareto-optimal trade-offs among throughput, memory footprint, and accuracy. Experiments show that A3GNN can bridge the performance gap, allowing seven Nvidia 2080Ti GPUs to outperform two A100 GPUs by up to 1.8X in throughput with minimal accuracy loss.",
        "authors": [
            "Tong Qiao",
            "Ao Zhou",
            "Yingjie Qi",
            "Yiou Wang",
            "Han Wan",
            "Jianlei Yang",
            "Chunming Hu"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.07421v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-15"
    },
    "http://arxiv.org/abs/2510.12889v1": {
        "title": "Dodoor: Efficient Randomized Decentralized Scheduling with Load Caching for Heterogeneous Tasks and Clusters",
        "link": "http://arxiv.org/abs/2510.12889v1",
        "abstract": "This paper introduces Dodoor, an efficient randomized decentralized scheduler designed for task scheduling in modern data centers. Dodoor leverages advanced research on the weighted balls-into-bins model with b-batched setting. Unlike other decentralized schedulers that rely on real-time probing of remote servers, Dodoor makes scheduling decisions based on cached server information, which is updated in batches, to reduce communication overheads. To schedule tasks with dynamic, multidimensional resource requirements in heterogeneous cluster, Dodoor uses a novel load score to measure servers' loads for each scheduled task. This score captures the anti-affinity between servers and tasks in contrast to the commonly used heuristic of counting pending tasks to balance load. On a 101-node heterogeneous cluster, Dodoor is evaluated using two workloads: (i) simulated Azure virtual machines placements and (ii) real serverless Python functions executions in Docker. The evaluation shows that Dodoor reduces scheduling messages by 55--66% on both workloads. Dodoor can also increase throughput by up to 33.2% and 21.5%, reduce mean makespan latency by 12.1% and 7.2%, and improve tail latency by 21.9% and 24.6% across the two workloads.",
        "authors": [
            "Wei Da",
            "Evangelia Kalyvianaki"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.12889v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-14"
    },
    "http://arxiv.org/abs/2510.12741v1": {
        "title": "Personalized Federated Fine-Tuning of Vision Foundation Models for Healthcare",
        "link": "http://arxiv.org/abs/2510.12741v1",
        "abstract": "Foundation models open up new possibilities for the use of AI in healthcare. However, even when pre-trained on health data, they still need to be fine-tuned for specific downstream tasks. Furthermore, although foundation models reduce the amount of training data required to achieve good performance, obtaining sufficient data is still a challenge. This is due, in part, to restrictions on sharing and aggregating data from different sources to protect patients' privacy. One possible solution to this is to fine-tune foundation models via federated learning across multiple participating clients (i.e., hospitals, clinics, etc.). In this work, we propose a new personalized federated fine-tuning method that learns orthogonal LoRA adapters to disentangle general and client-specific knowledge, enabling each client to fully exploit both their own data and the data of others. Our preliminary results on real-world federated medical imaging tasks demonstrate that our approach is competitive against current federated fine-tuning methods.",
        "authors": [
            "Adam Tupper",
            "Christian Gagné"
        ],
        "categories": [
            "cs.CV",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.12741v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-14"
    },
    "http://arxiv.org/abs/2510.12727v1": {
        "title": "Hierarchical Federated Learning for Crop Yield Prediction in Smart Agricultural Production Systems",
        "link": "http://arxiv.org/abs/2510.12727v1",
        "abstract": "In this paper, we presents a novel hierarchical federated learning architecture specifically designed for smart agricultural production systems and crop yield prediction. Our approach introduces a seasonal subscription mechanism where farms join crop-specific clusters at the beginning of each agricultural season. The proposed three-layer architecture consists of individual smart farms at the client level, crop-specific aggregators at the middle layer, and a global model aggregator at the top level. Within each crop cluster, clients collaboratively train specialized models tailored to specific crop types, which are then aggregated to produce a higher-level global model that integrates knowledge across multiple crops. This hierarchical design enables both local specialization for individual crop types and global generalization across diverse agricultural contexts while preserving data privacy and reducing communication overhead. Experiments demonstrate the effectiveness of the proposed system, showing that local and crop-layer models closely follow actual yield patterns with consistent alignment, significantly outperforming standard machine learning models. The results validate the advantages of hierarchical federated learning in the agricultural context, particularly for scenarios involving heterogeneous farming environments and privacy-sensitive agricultural data.",
        "authors": [
            "Anas Abouaomar",
            "Mohammed El hanjri",
            "Abdellatif Kobbane",
            "Anis Laouiti",
            "Khalid Nafil"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.12727v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-14"
    },
    "http://arxiv.org/abs/2510.12705v1": {
        "id": "http://arxiv.org/abs/2510.12705v1",
        "title": "A GPU-resident Memory-Aware Algorithm for Accelerating Bidiagonalization of Banded Matrices",
        "link": "http://arxiv.org/abs/2510.12705v1",
        "tags": [
            "kernel",
            "training",
            "hardware"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Presents a GPU-optimized algorithm for bidiagonal reduction of banded matrices, crucial for SVD in scientific computing. Utilizes hardware-agnostic implementation via Julia's Array abstractions and KernelAbstractions. Achieves up to 100x speedup over CPU libraries for 32k x 32k matrices and linear performance scaling with bandwidth size.",
        "abstract": "The reduction of a banded matrix to a bidiagonal form is a crucial step in the Singular Value Decomposition (SVD), a cornerstone of scientific computing and AI. Despite being a highly parallel algorithm, it was previously believed to be unsuitable for GPU computation because it is memory bandwidth-bound. Recent developments in GPU hardware, including larger L1 memory per Streaming Multiprocessor/Compute Unit, have changed that. We present the first GPU algorithm for reducing a banded matrix to bidiagonal form as part of the NextLA$.$jl open-source software package. Our algorithm is based on previous CPU-based multicore parallel cache-efficient bulge chasing algorithms and adapted to optimize for GPU throughput. We leverage Julia Language's Array abstractions and KernelAbstractions to implement a single hardware- and data precision-agnostic function on NVIDIA, AMD, Intel, and Apple Metal GPUs for half, single, and double precision, and examine performance optimization across hardware architectures and data precision. We also develop a hardware-aware performance model and identify key hyperparameters, such as inner tilewidth and block concurrency, that govern optimal GPU execution for bandwidth-bound workloads. We demonstrate highly parallel bandwidth-bound algorithm on the GPU can outperform CPU-based implementations: the GPU algorithm outperforms multithreaded CPU High-Performance libraries PLASMA and SLATE as of matrix size 1024 x 1024 and by a factor over 100 for matrices of 32k x 32k. In addition, the performance of the algorithm increases linearly with matrix bandwidth size, making faster reduction of larger matrix bandwidths now also possible. With this work, we break memory bandwidth barriers, as well as matrix bandwidth barriers, resulting in orders-of-magnitude faster algorithms for the reduction of banded matrices to bidiagonal form on the GPU.",
        "authors": [
            "Evelyne Ringoot",
            "Rabab Alomairy",
            "Alan Edelman"
        ],
        "categories": [
            "cs.DC",
            "cs.MS"
        ],
        "submit_date": "2025-10-14"
    },
    "http://arxiv.org/abs/2510.12633v1": {
        "id": "http://arxiv.org/abs/2510.12633v1",
        "title": "Laminar: A Scalable Asynchronous RL Post-Training Framework",
        "link": "http://arxiv.org/abs/2510.12633v1",
        "tags": [
            "RL",
            "training",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-10-14",
        "tldr": "Addresses scalability bottlenecks in RL post-training of LLMs caused by trajectory latency skew. Proposes Laminar, a fully decoupled architecture with tiered relay workers for asynchronous parameter updates and dynamic trajectory repackaging. Achieves up to 5.48× training throughput speedup on a 1024-GPU cluster.",
        "abstract": "Reinforcement learning (RL) post-training for Large Language Models (LLMs) is now scaling to large clusters and running for extended durations to enhance model reasoning performance. However, the scalability of existing RL frameworks is limited, as extreme long-tail skewness in RL trajectory generation causes severe GPU underutilization. Current asynchronous RL systems attempt to mitigate this, but they rely on global weight synchronization between the actor and all rollouts, which creates a rigid model update schedule. This global synchronization is ill-suited for the highly skewed and evolving distribution of trajectory generation latency in RL training, crippling training efficiency. Our key insight is that efficient scaling requires breaking this lockstep through trajectory-level asynchrony, which generates and consumes each trajectory independently. We propose Laminar, a scalable and robust RL post-training system built on a fully decoupled architecture. First, we replace global updates with a tier of relay workers acting as a distributed parameter service. This enables asynchronous and fine-grained weight synchronization, allowing rollouts to pull the latest weight anytime without stalling the actor's training loop. Second, a dynamic repack mechanism consolidates long-tail trajectories onto a few dedicated rollouts, maximizing generation throughput. The fully decoupled design also isolates failures, ensuring robustness for long-running jobs. Our evaluation on a 1024-GPU cluster shows that Laminar achieves up to 5.48$\\times$ training throughput speedup over state-of-the-art systems, while reducing model convergence time.",
        "authors": [
            "Guangming Sheng",
            "Yuxuan Tong",
            "Borui Wan",
            "Wang Zhang",
            "Chaobo Jia",
            "Xibin Wu",
            "Yuqi Wu",
            "Xiang Li",
            "Chi Zhang",
            "Yanghua Peng",
            "Haibin Lin",
            "Xin Liu",
            "Chuan Wu"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC"
        ],
        "submit_date": "2025-10-14"
    },
    "http://arxiv.org/abs/2510.12597v1": {
        "title": "Low Latency, High Bandwidth Streaming of Experimental Data with EJFAT",
        "link": "http://arxiv.org/abs/2510.12597v1",
        "abstract": "Thomas Jefferson National Accelerator Facility (JLab) has partnered with Energy Sciences Network (ESnet) to define and implement an edge to compute cluster computational load balancing acceleration architecture. The ESnet-JLab FPGA Accelerated Transport (EJFAT) architecture focuses on FPGA acceleration to address compression, fragmentation, UDP packet destination redirection (Network Address Translation (NAT)) and decompression and reassembly.   EJFAT seamlessly integrates edge and cluster computing to support direct processing of streamed experimental data. This will directly benefit the JLab science program as well as data centers of the future that require high throughput and low latency for both time-critical data acquisition systems and data center workflows.   The EJFAT project will be presented along with how it is synergistic with other DOE activities such as an Integrated Research Infrastructure (IRI), and recent results using data sources at JLab, an EJFAT LB at ESnet, and computational cluster resources at Lawrence Berkeley National Laboratory (LBNL).",
        "authors": [
            "Ilya Baldin",
            "Michael Goodrich",
            "Vardan Gyurjyan",
            "Graham Heyes",
            "Derek Howard",
            "Yatish Kumar",
            "David Lawrence",
            "Brad Sawatzky",
            "Stacey Sheldon",
            "Carl Timmer"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.12597v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-14"
    },
    "http://arxiv.org/abs/2510.12494v1": {
        "title": "PubSub-VFL: Towards Efficient Two-Party Split Learning in Heterogeneous Environments via Publisher/Subscriber Architecture",
        "link": "http://arxiv.org/abs/2510.12494v1",
        "abstract": "With the rapid advancement of the digital economy, data collaboration between organizations has become a well-established business model, driving the growth of various industries. However, privacy concerns make direct data sharing impractical. To address this, Two-Party Split Learning (a.k.a. Vertical Federated Learning (VFL)) has emerged as a promising solution for secure collaborative learning. Despite its advantages, this architecture still suffers from low computational resource utilization and training efficiency. Specifically, its synchronous dependency design increases training latency, while resource and data heterogeneity among participants further hinder efficient computation. To overcome these challenges, we propose PubSub-VFL, a novel VFL paradigm with a Publisher/Subscriber architecture optimized for two-party collaborative learning with high computational efficiency. PubSub-VFL leverages the decoupling capabilities of the Pub/Sub architecture and the data parallelism of the parameter server architecture to design a hierarchical asynchronous mechanism, reducing training latency and improving system efficiency. Additionally, to mitigate the training imbalance caused by resource and data heterogeneity, we formalize an optimization problem based on participants' system profiles, enabling the selection of optimal hyperparameters while preserving privacy. We conduct a theoretical analysis to demonstrate that PubSub-VFL achieves stable convergence and is compatible with security protocols such as differential privacy. Extensive case studies on five benchmark datasets further validate its effectiveness, showing that, compared to state-of-the-art baselines, PubSub-VFL not only accelerates training by $2 \\sim 7\\times$ without compromising accuracy, but also achieves a computational resource utilization rate of up to 91.07%.",
        "authors": [
            "Yi Liu",
            "Yang Liu",
            "Leqian Zheng",
            "Jue Hong",
            "Junjie Shi",
            "Qingyou Yang",
            "Ye Wu",
            "Cong Wang"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.12494v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-14"
    },
    "http://arxiv.org/abs/2510.12469v1": {
        "title": "Proof of Cloud: Data Center Execution Assurance for Confidential VMs",
        "link": "http://arxiv.org/abs/2510.12469v1",
        "abstract": "Confidential Virtual Machines (CVMs) protect data in use by running workloads inside hardware-isolated environments. In doing so, they also inherit the limitations of the underlying hardware. Trusted Execution Environments (TEEs), which enforce this isolation, explicitly exclude adversaries with physical access from their threat model. Commercial TEEs, e.g., Intel TDX, thus assume infrastructure providers do not physically exploit hardware and serve as safeguards instead. This creates a tension: tenants must trust provider integrity at the hardware layer, yet existing remote attestation offers no way to verify that CVMs actually run on physically trusted platforms, leaving today's CVM deployments unable to demonstrate that their guarantees align with the TEE vendor's threat model.   We bridge this confidence gap with Data Center Execution Assurance (DCEA), a design generating \"Proofs of Cloud\". DCEA binds a CVM to its underlying platform using vTPM-anchored measurements, ensuring CVM launch evidence and TPM quotes refer to the same physical chassis.   This takes advantage of the fact that data centers are often identifiable via TPMs. Our approach applies to CVMs accessing vTPMs and running on top of software stacks fully controlled by the cloud provider, as well as single-tenant bare-metal deployments with discrete TPMs. We trust providers for integrity (certificate issuance), but not for the confidentiality of CVM-visible state. DCEA enables remote verification of a CVM's platform origin and integrity, mitigating attacks like replay and attestation proxying. We include a candidate implementation on Google Cloud and Intel TDX that leverages Intel TXT for trusted launch. Our design refines CVMs' threat model and provides a practical path for deploying high-assurance, confidential workloads in minimally trusted environments.",
        "authors": [
            "Filip Rezabek",
            "Moe Mahhouk",
            "Andrew Miller",
            "Stefan Genchev",
            "Quintus Kilbourn",
            "Georg Carle",
            "Jonathan Passerat-Palmbach"
        ],
        "categories": [
            "cs.CR",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.12469v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-14"
    },
    "http://arxiv.org/abs/2510.12436v1": {
        "title": "TALP-Pages: An easy-to-integrate continuous performance monitoring framework",
        "link": "http://arxiv.org/abs/2510.12436v1",
        "abstract": "Ensuring good performance is a key aspect in the development of codes that target HPC machines. As these codes are under active development, the necessity to detect performance degradation early in the development process becomes apparent. In addition, having meaningful insight into application scaling behavior tightly coupled to the development workflow is helpful. In this paper, we introduce TALP-Pages, an easy-to-integrate framework that enables developers to get fast and in-repository feedback about their code performance using established fundamental performance and scaling factors. The framework relies on TALP, which enables the on-the-fly collection of these metrics. Based on a folder structure suited for CI which contains the files generated by TALP, TALP-Pages generates an HTML report with visualizations of the performance factor regression as well as scaling-efficiency tables. We compare TALP-Pages to tracing-based tools in terms of overhead and post-processing requirements and find that TALP-Pages can produce the scaling-efficiency tables faster and under tighter resource constraints. To showcase the ease of use and effectiveness of this approach, we extend the current CI setup of GENE-X with only minimal changes required and showcase the ability to detect and explain a performance improvement.",
        "authors": [
            "Valentin Seitz",
            "Jordy Trilaksono",
            "Marta Garcia-Gasulla"
        ],
        "categories": [
            "cs.DC",
            "cs.PF"
        ],
        "id": "http://arxiv.org/abs/2510.12436v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-14"
    },
    "http://arxiv.org/abs/2510.12397v1": {
        "title": "Should I Run My Cloud Benchmark on Black Friday?",
        "link": "http://arxiv.org/abs/2510.12397v1",
        "abstract": "Benchmarks and performance experiments are frequently conducted in cloud environments. However, their results are often treated with caution, as the presumed high variability of performance in the cloud raises concerns about reproducibility and credibility. In a recent study, we empirically quantified the impact of this variability on benchmarking results by repeatedly executing a stream processing application benchmark at different times of the day over several months. Our analysis confirms that performance variability is indeed observable at the application level, although it is less pronounced than often assumed. The larger scale of our study compared to related work allowed us to identify subtle daily and weekly performance patterns. We now extend this investigation by examining whether a major global event, such as Black Friday, affects the outcomes of performance benchmarks.",
        "authors": [
            "Sören Henning",
            "Adriano Vogel",
            "Esteban Perez-Wohlfeil",
            "Otmar Ertl",
            "Rick Rabiser"
        ],
        "categories": [
            "cs.SE",
            "cs.DC",
            "cs.PF"
        ],
        "id": "http://arxiv.org/abs/2510.12397v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-14"
    },
    "http://arxiv.org/abs/2510.12354v1": {
        "title": "A Non-Intrusive Framework for Deferred Integration of Cloud Patterns in Energy-Efficient Data-Sharing Pipelines",
        "link": "http://arxiv.org/abs/2510.12354v1",
        "abstract": "As data mesh architectures gain traction in federated environments, organizations are increasingly building consumer-specific data-sharing pipelines using modular, cloud-native transformation services. Prior work has shown that structuring these pipelines with reusable transformation stages enhances both scalability and energy efficiency. However, integrating traditional cloud design patterns into such pipelines poses a challenge: predefining and embedding patterns can compromise modularity, reduce reusability, and conflict with the pipelines dynamic, consumer-driven nature. To address this, we introduce a Kubernetes-based tool that enables the deferred and non-intrusive application of selected cloud design patterns without requiring changes to service source code. The tool supports automated pattern injection and collects energy consumption metrics, allowing developers to make energy-aware decisions while preserving the flexible, composable structure of reusable data-sharing pipelines.",
        "authors": [
            "Sepideh Masoudi",
            "Mark Edward Michael Daly",
            "Jannis Kiesel",
            "Stefan Tai"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.12354v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-14"
    },
    "http://arxiv.org/abs/2510.12274v1": {
        "id": "http://arxiv.org/abs/2510.12274v1",
        "title": "Metronome: Efficient Scheduling for Periodic Traffic Jobs with Network and Priority Awareness",
        "link": "http://arxiv.org/abs/2510.12274v1",
        "tags": [
            "training",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Addresses scheduling inefficiencies for distributed training jobs with periodic traffic. Proposes Metronome, a network-aware scheduler with time-division multiplexing and priority-based optimization. Reduces job completion time by up to 19.50% and improves bandwidth utilization by 23.20%.",
        "abstract": "With the rapid growth in computing power demand, cloud native networks have emerged as a promising solution to address the challenges of efficient resource coordination, particularly in coping with the dynamic fluctuations of network bandwidth in clusters. We propose Metronome, a network-aware and priority-aware scheduling mechanism for cloud native networks. This mechanism is designed to support jobs that exhibit periodic traffic patterns and dynamic bandwidth demands, particularly in the context of distributed training. Specifically, Metronome employs a time-division multiplexing approach that leverages job traffic characteristics to construct an elastic network resource allocation model, enabling efficient bandwidth sharing across multiple jobs. In addition, it incorporates a multi-objective optimization strategy, jointly considering latency and job priorities to achieve globally optimal as well as dynamic resource allocation. Finally, Metronome adapts to the dynamic environment by monitoring the cluster and performing reconfiguration operations. Extensive experiments with 13 common machine learning models demonstrate that Metronome can enhance cluster resource utilization while guaranteeing service performance. Compared with the existing Kubernetes scheduling mechanisms across multiple scenarios, Metronome reduces job completion time by up to 19.50% while improving average bandwidth utilization by up to 23.20%.",
        "authors": [
            "Hao Jiang",
            "Meng Qin",
            "Ruijie Kuai",
            "Dandan Liang"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-10-14"
    },
    "http://arxiv.org/abs/2510.12196v1": {
        "title": "GPU-Accelerated Algorithms for Process Mapping",
        "link": "http://arxiv.org/abs/2510.12196v1",
        "abstract": "Process mapping asks to assign vertices of a task graph to processing elements of a supercomputer such that the computational workload is balanced while the communication cost is minimized. Motivated by the recent success of GPU-based graph partitioners, we propose two GPU-accelerated algorithms for this optimization problem. The first algorithm employs hierarchical multisection, which partitions the task graph alongside the hierarchy of the supercomputer. The method utilizes GPU-based graph partitioners to accelerate the mapping process. The second algorithm integrates process mapping directly into the modern multilevel graph partitioning pipeline. Vital phases like coarsening and refinement are accelerated by exploiting the parallelism of GPUs. In our experiments, both methods achieve speedups exceeding 300 when compared to state-of-the-art CPU-based algorithms. The first algorithm has, on average, about 10 percent greater communication costs and thus remains competitive to CPU algorithms. The second approach is much faster, with a geometric mean speedup of 77.6 and peak speedup of 598 at the cost of lower solution quality. To our knowledge, these are the first GPU-based algorithms for process mapping.",
        "authors": [
            "Petr Samoldekin",
            "Christian Schulz",
            "Henning Woydt"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.12196v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-14"
    },
    "http://arxiv.org/abs/2510.12166v1": {
        "title": "Comparing Cross-Platform Performance via Node-to-Node Scaling Studies",
        "link": "http://arxiv.org/abs/2510.12166v1",
        "abstract": "Due to the increasing diversity of high-performance computing architectures, researchers and practitioners are increasingly interested in comparing a code's performance and scalability across different platforms. However, there is a lack of available guidance on how to actually set up and analyze such cross-platform studies. In this paper, we contend that the natural base unit of computing for such studies is a single compute node on each platform and offer guidance in setting up, running, and analyzing node-to-node scaling studies. We propose templates for presenting scaling results of these studies and provide several case studies highlighting the benefits of this approach.",
        "authors": [
            "Kenneth Weiss",
            "Thomas M. Stitt",
            "Daryl Hawkins",
            "Olga Pearce",
            "Stephanie Brink",
            "Robert N. Rieben"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.12166v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-14"
    },
    "http://arxiv.org/abs/2510.12128v1": {
        "title": "nuGPR: GPU-Accelerated Gaussian Process Regression with Iterative Algorithms and Low-Rank Approximations",
        "link": "http://arxiv.org/abs/2510.12128v1",
        "abstract": "Gaussian Process Regression (GPR) is an important type of supervised machine learning model with inherent uncertainty measure in its predictions. We propose a new framework, nuGPR, to address the well-known challenge of high computation cost associated with GPR training. Our framework includes several ideas from numerical linear algebra to reduce the amount of computation in key steps of GPR, and we combine them to establish an end-to-end training algorithm. Specifically, we leverage the preconditioned conjugate gradient method to accelerate the convergence of the linear solves required in GPR. We exploit clustering in the input data to identify block-diagonal structure of the covariance matrix and subsequently construct low-rank approximations of the off-diagonal blocks. These enhancements significantly reduce the time and space complexity of our computations. In addition, unlike other frameworks that rely on exact differentiation, we employ numerical gradients to optimize the hyperparameters of our GPR model, further reducing the training cost by eliminating the need for backpropagation. Lastly, we leverage the CUDA Toolkit to efficiently parallelize the training procedure on NVIDIA GPUs. As a result, nuGPR reduces total training time by up to 2x and peak memory consumption by up to 12x on various synthetic and real-world datasets when compared to the best existing GPU-based GPR implementation.",
        "authors": [
            "Ziqi Zhao",
            "Vivek Sarin"
        ],
        "categories": [
            "cs.LG",
            "cs.DC",
            "math.NA"
        ],
        "id": "http://arxiv.org/abs/2510.12128v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-14"
    },
    "http://arxiv.org/abs/2510.17852v1": {
        "id": "http://arxiv.org/abs/2510.17852v1",
        "title": "Deploying Atmospheric and Oceanic AI Models on Chinese Hardware and Framework: Migration Strategies, Performance Optimization and Analysis",
        "link": "http://arxiv.org/abs/2510.17852v1",
        "tags": [
            "training",
            "hardware",
            "quantization"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Develops migration framework for porting atmospheric/oceanic AI models (e.g., FourCastNet) to Chinese chips/MindSpore. Optimizes via software-hardware adaptation, memory optimization, and parallelism. Achieves preserved accuracy with improved operational efficiency and reduced dependencies.",
        "abstract": "With the growing role of artificial intelligence in climate and weather research, efficient model training and inference are in high demand. Current models like FourCastNet and AI-GOMS depend heavily on GPUs, limiting hardware independence, especially for Chinese domestic hardware and frameworks. To address this issue, we present a framework for migrating large-scale atmospheric and oceanic models from PyTorch to MindSpore and optimizing for Chinese chips, and evaluating their performance against GPUs. The framework focuses on software-hardware adaptation, memory optimization, and parallelism. Furthermore, the model's performance is evaluated across multiple metrics, including training speed, inference speed, model accuracy, and energy efficiency, with comparisons against GPU-based implementations. Experimental results demonstrate that the migration and optimization process preserves the models' original accuracy while significantly reducing system dependencies and improving operational efficiency by leveraging Chinese chips as a viable alternative for scientific computing. This work provides valuable insights and practical guidance for leveraging Chinese domestic chips and frameworks in atmospheric and oceanic AI model development, offering a pathway toward greater technological independence.",
        "authors": [
            "Yuze Sun",
            "Wentao Luo",
            "Yanfei Xiang",
            "Jiancheng Pan",
            "Jiahao Li",
            "Quan Zhang",
            "Xiaomeng Huang"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.LG"
        ],
        "submit_date": "2025-10-14"
    },
    "http://arxiv.org/abs/2510.11938v1": {
        "id": "http://arxiv.org/abs/2510.11938v1",
        "title": "FlexPipe: Adapting Dynamic LLM Serving Through Inflight Pipeline Refactoring in Fragmented Serverless Clusters",
        "link": "http://arxiv.org/abs/2510.11938v1",
        "tags": [
            "serving",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Addresses efficient LLM serving in serverless clusters with dynamic pipeline adaptations. Introduces FlexPipe, with fine-grained model partitioning, live pipeline refactoring, and topology-aware resource placement. Achieves 8.5× better resource efficiency and 38.3% lower latency while reducing GPU reservations to 30%.",
        "abstract": "Serving Large Language Models (LLMs) in production faces significant challenges from highly variable request patterns and severe resource fragmentation in serverless clusters. Current systems rely on static pipeline configurations that struggle to adapt to dynamic workload conditions, leading to substantial inefficiencies. We present FlexPipe, a novel system that dynamically reconfigures pipeline architectures during runtime to address these fundamental limitations. FlexPipe decomposes models into fine-grained stages and intelligently adjusts pipeline granularity based on real-time request pattern analysis, implementing three key innovations: fine-grained model partitioning with preserved computational graph constraints, inflight pipeline refactoring with consistent cache transitions, and topology-aware resource allocation that navigates GPU fragmentation. Comprehensive evaluation on an 82-GPU cluster demonstrates that FlexPipe achieves up to 8.5x better resource efficiency while maintaining 38.3% lower latency compared to state-of-the-art systems, reducing GPU reservation requirements from 75% to 30% of peak capacity.",
        "authors": [
            "Yanying Lin",
            "Shijie Peng",
            "Chengzhi Lu",
            "Chengzhong Xu",
            "Kejiang Ye"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-10-13"
    },
    "http://arxiv.org/abs/2510.11866v1": {
        "title": "Rationally Analyzing Shelby: Proving Incentive Compatibility in a Decentralized Storage Network",
        "link": "http://arxiv.org/abs/2510.11866v1",
        "abstract": "Decentralized storage is one of the most natural applications built on blockchains and a central component of the Web3 ecosystem. Yet despite a decade of active development -- from IPFS and Filecoin to more recent entrants -- most of these storage protocols have received limited formal analysis of their incentive properties. Claims of incentive compatibility are sometimes made, but rarely proven. This gap matters: without well-designed incentives, a system may distribute storage but fail to truly decentralize it.   We analyze Shelby -- a storage network protocol recently proposed by Aptos Labs and Jump Crypto -- and provide the first formal proof of its incentive properties. Our game-theoretic model shows that while off-chain audits alone collapse to universal shirking, Shelby's combination of peer audits with occasional on-chain verification yields incentive compatibility under natural parameter settings. We also examine coalition behavior and outline a simple modification that strengthens the protocol's collusion-resilience.",
        "authors": [
            "Michael Crystal",
            "Guy Goren",
            "Scott Duke Kominers"
        ],
        "categories": [
            "cs.GT",
            "cs.DC",
            "cs.MA"
        ],
        "id": "http://arxiv.org/abs/2510.11866v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-13"
    },
    "http://arxiv.org/abs/2510.11697v1": {
        "title": "A Fast-Converging Decentralized Approach to the Weighted Minimum Vertex Cover Problem",
        "link": "http://arxiv.org/abs/2510.11697v1",
        "abstract": "We address the problem of computing a Minimum Weighted Vertex Cover (MWVC) in a decentralized network. MWVC, a classical NP-hard problem, is foundational in applications such as network monitoring and resource placement. We propose a fully decentralized protocol where each node makes decisions using only local knowledge and communicates with its neighbors. The method is adaptive, communication-efficient, and avoids centralized coordination. We evaluate the protocol on real-world and synthetic graphs, comparing it to both centralized and decentralized baselines. Our results demonstrate competitive solution quality with reduced communication overhead, highlighting the feasibility of MWVC computation in decentralized environments.",
        "authors": [
            "Matteo Mordacchini",
            "Emanuele Carlini",
            "Patrizio Dazzi"
        ],
        "categories": [
            "cs.DC",
            "cs.DS"
        ],
        "id": "http://arxiv.org/abs/2510.11697v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-13"
    },
    "http://arxiv.org/abs/2510.11513v1": {
        "title": "An Asynchronous Many-Task Algorithm for Unstructured $S_{N}$ Transport on Shared Memory Systems",
        "link": "http://arxiv.org/abs/2510.11513v1",
        "abstract": "Discrete ordinates $S_N$ transport solvers on unstructured meshes pose a challenge to scale due to complex data dependencies, memory access patterns and a high-dimensional domain. In this paper, we review the performance bottlenecks within the shared memory parallelization scheme of an existing transport solver on modern many-core architectures with high core counts. With this analysis, we then survey the performance of this solver across a variety of compute hardware. We then present a new Asynchronous Many-Task (AMT) algorithm for shared memory parallelism, present results showing an increase in computational performance over the existing method, and evaluate why performance is improved.",
        "authors": [
            "Alex Elwood",
            "Tom Deakin",
            "Justin Lovegrove",
            "Chris Nelson"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.11513v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-13"
    },
    "http://arxiv.org/abs/2510.11211v1": {
        "id": "http://arxiv.org/abs/2510.11211v1",
        "title": "An Explorative Study on Distributed Computing Techniques in Training and Inference of Large Language Models",
        "link": "http://arxiv.org/abs/2510.11211v1",
        "tags": [
            "training",
            "serving",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-10-13",
        "tldr": "Explores distributed computing techniques for training and serving LLMs, including system modifications to enable consumer-grade deployment and a comparative analysis of three serving frameworks. Implements a metaheuristic-based offloading method that reduces memory usage by up to 40% on consumer hardware.",
        "abstract": "Large language models (LLM) are advanced AI systems trained on extensive textual data, leveraging deep learning techniques to understand and generate human-like language. Today's LLMs with billions of parameters are so huge that hardly any single computing node can train, fine-tune, or infer from them. Therefore, several distributed computing techniques are being introduced in the literature to properly utilize LLMs. We have explored the application of distributed computing techniques in LLMs from two angles.   \\begin{itemize}   \\item We study the techniques that democratize the LLM, that is, how large models can be run on consumer-grade computers. Here, we also implement a novel metaheuristics-based modification to an existing system.   \\item We perform a comparative study on three state-of-the-art LLM serving techniques. \\end{itemize}",
        "authors": [
            "Sheikh Azizul Hakim",
            "Saem Hasan"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-10-13"
    },
    "http://arxiv.org/abs/2510.11189v1": {
        "id": "http://arxiv.org/abs/2510.11189v1",
        "title": "A Decentralized Microservice Scheduling Approach Using Service Mesh in Cloud-Edge Systems",
        "link": "http://arxiv.org/abs/2510.11189v1",
        "tags": [
            "edge",
            "serving",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes a decentralized microservice scheduling approach using service mesh sidecar proxies for cloud-edge systems. Embeds lightweight autonomous schedulers in sidecars to avoid centralized control, leveraging service mesh for distributed traffic management. Initial results show improved scalability in response time and latency under varying request rates.",
        "abstract": "As microservice-based systems scale across the cloud-edge continuum, traditional centralized scheduling mechanisms increasingly struggle with latency, coordination overhead, and fault tolerance. This paper presents a new architectural direction: leveraging service mesh sidecar proxies as decentralized, in-situ schedulers to enable scalable, low-latency coordination in large-scale, cloud-native environments. We propose embedding lightweight, autonomous scheduling logic into each sidecar, allowing scheduling decisions to be made locally without centralized control. This approach leverages the growing maturity of service mesh infrastructures, which support programmable distributed traffic management. We describe the design of such an architecture and present initial results demonstrating its scalability potential in terms of response time and latency under varying request rates. Rather than delivering a finalized scheduling algorithm, this paper presents a system-level architectural direction and preliminary evidence to support its scalability potential.",
        "authors": [
            "Yangyang Wen",
            "Paul Townend",
            "Per-Olov Östberg",
            "Abel Souza",
            "Clément Courageux-Sudan"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-10-13"
    },
    "http://arxiv.org/abs/2510.11119v1": {
        "id": "http://arxiv.org/abs/2510.11119v1",
        "title": "Improving AI Efficiency in Data Centres by Power Dynamic Response",
        "link": "http://arxiv.org/abs/2510.11119v1",
        "tags": [
            "training",
            "hardware",
            "storage"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes dynamic power management for AI data centres to improve efficiency. Introduces active/passive solutions where input power adapts to computing demand. Quantifies 12-15% energy savings and capital expenditure reduction in global hyperscalers.",
        "abstract": "The steady growth of artificial intelligence (AI) has accelerated in the recent years, facilitated by the development of sophisticated models such as large language models and foundation models. Ensuring robust and reliable power infrastructures is fundamental to take advantage of the full potential of AI. However, AI data centres are extremely hungry for power, putting the problem of their power management in the spotlight, especially with respect to their impact on environment and sustainable development. In this work, we investigate the capacity and limits of solutions based on an innovative approach for the power management of AI data centres, i.e., making part of the input power as dynamic as the power used for data-computing functions. The performance of passive and active devices are quantified and compared in terms of computational gain, energy efficiency, reduction of capital expenditure, and management costs by analysing power trends from multiple data platforms worldwide. This strategy, which identifies a paradigm shift in the AI data centre power management, has the potential to strongly improve the sustainability of AI hyperscalers, enhancing their footprint on environmental, financial, and societal fields.",
        "authors": [
            "Andrea Marinoni",
            "Sai Shivareddy",
            "Pietro Lio'",
            "Weisi Lin",
            "Erik Cambria",
            "Clare Grey"
        ],
        "categories": [
            "cs.AI",
            "cs.AR",
            "cs.DC"
        ],
        "submit_date": "2025-10-13"
    },
    "http://arxiv.org/abs/2510.10833v1": {
        "title": "FIDRS: A Novel Framework for Integrated Distributed Reliable Systems",
        "link": "http://arxiv.org/abs/2510.10833v1",
        "abstract": "In this paper we represent a new framework for integrated distributed and reliable systems. In the proposed framework we have used three parts to increase Satisfaction and Performance of this framework. At first we analyze previous frameworks related to integrated systems, then represent new proposed framework in order to improving previous framework, and we discuss its different phases. Finally we compare the results of simulation of the new framework with previous ones. In FIDRS framework, the technique of heterogeneous distributed data base is used to improve Performance and speed in responding to users and in this way we can improve dependability and reliability of framework simultaneously. In extraction phase of the new framework we have used RMSD algorithm that decreases responding time in big database. Finally by using FDIRS framework we succeeded to increase Efficiency, Performance and reliability of integrated systems and remove some of previous frameworks problems.",
        "authors": [
            "Mehdi Zekriyapanah Gashti"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.10833v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-12"
    },
    "http://arxiv.org/abs/2510.10818v1": {
        "title": "Fair Kernel-Lock-Free Claim/Release Protocol for Shared Object Access in Cooperatively Scheduled Runtimes",
        "link": "http://arxiv.org/abs/2510.10818v1",
        "abstract": "We present the first spin-free, kernel-lock-free mutex that cooperates with user-mode schedulers and is formally proven FIFO-fair and linearizable using CSP/FDR. Our fairness oracle and stability-based proof method are reusable across coroutine runtime designs. We designed the claim/release protocol for a process-oriented language -- ProcessJ -- to manage the race for claiming shared inter-process communication channels. Internally, we use a lock-free queue to park waiting processes for gaining access to a shared object, such as exclusive access to a shared channel to read from or write to. The queue ensures control and fairness for processes wishing to access a shared resource, as the protocol handles claim requests in the order they are inserted into the queue. We produce CSP models of our protocol and a mutex specification, demonstrating with FDR that our protocol behaves as a locking mutex.",
        "authors": [
            "Kevin Chalmers",
            "Jan Bækgaard Pedersen"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.10818v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-12"
    },
    "http://arxiv.org/abs/2511.01863v1": {
        "title": "SPHERE: Spherical partitioning for large-scale routing optimization",
        "link": "http://arxiv.org/abs/2511.01863v1",
        "abstract": "We study shortest-path routing in large weighted, undirected graphs, where expanding search frontiers raise time and memory costs for exact solvers. We propose \\emph{SPHERE}, a source-target-aware heuristic that identifies an $s$-$t$ overlap: vertices that are close to both $s$ and $t$ in hop count. Selecting an anchor $a$ in this overlap partitions the task into two subproblems with unchanged problem-topology, $s\\to a$ and $a\\to t$; if either remains large, the procedure recurses on its induced subgraph. Because the cut lies inside the overlap, concatenating the resulting subpaths yields a valid $s\\to t$ route without boundary repair. SPHERE is independent of the downstream solver (e.g., Dijkstra) and exposes parallelism across subproblems. On large networks, it achieves faster runtimes and smaller optimality gaps than Louvain-based routing and a METIS-based pipeline, even on graphs with more than a million nodes and edges, while also outperforming Dijkstra in runtime.",
        "authors": [
            "Robert Fabian Lindermann",
            "Paul-Niklas Ken Kandora",
            "Simon Caspar Zeller",
            "Adrian Asmund Fessler",
            "Steffen Rebennack"
        ],
        "categories": [
            "cs.DC",
            "cs.DM"
        ],
        "id": "http://arxiv.org/abs/2511.01863v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-12"
    },
    "http://arxiv.org/abs/2510.10747v1": {
        "title": "CPU-Limits kill Performance: Time to rethink Resource Control",
        "link": "http://arxiv.org/abs/2510.10747v1",
        "abstract": "Research in compute resource management for cloud-native applications is dominated by the problem of setting optimal CPU limits -- a fundamental OS mechanism that strictly restricts a container's CPU usage to its specified CPU-limits . Rightsizing and autoscaling works have innovated on allocation/scaling policies assuming the ubiquity and necessity of CPU-limits . We question this. Practical experiences of cloud users indicate that CPU-limits harms application performance and costs more than it helps. These observations are in contradiction to the conventional wisdom presented in both academic research and industry best practices. We argue that this indiscriminate adoption of CPU-limits is driven by erroneous beliefs that CPU-limits is essential for operational and safety purposes. We provide empirical evidence making a case for eschewing CPU-limits completely from latency-sensitive applications. This prompts a fundamental rethinking of auto-scaling and billing paradigms and opens new research avenues. Finally, we highlight specific scenarios where CPU-limits can be beneficial if used in a well-reasoned way (e.g. background jobs).",
        "authors": [
            "Chirag Shetty",
            "Sarthak Chakraborty",
            "Hubertus Franke",
            "Larisa Shwartz",
            "Chandra Narayanaswami",
            "Indranil Gupta",
            "Saurabh Jha"
        ],
        "categories": [
            "cs.DC",
            "cs.OS",
            "cs.PF"
        ],
        "id": "http://arxiv.org/abs/2510.10747v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-12"
    },
    "http://arxiv.org/abs/2510.10620v1": {
        "id": "http://arxiv.org/abs/2510.10620v1",
        "title": "DCP: Addressing Input Dynamism In Long-Context Training via Dynamic Context Parallelism",
        "link": "http://arxiv.org/abs/2510.10620v1",
        "tags": [
            "training",
            "sparse",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-10-12",
        "tldr": "Addresses dynamic sequence length and attention pattern variability in long-context LLM training. Proposes DCP, a fine-grained dynamic context parallelism framework that adapts data and computation blocks to device resources, achieving up to 1.46x end-to-end training speed-up over static methods.",
        "abstract": "Context parallelism has emerged as a key technique to support long-context training, a growing trend in generative AI for modern large models. However, existing context parallel methods rely on static parallelization configurations that overlook the dynamic nature of training data, specifically, the variability in sequence lengths and token relationships (i.e., attention patterns) across samples. As a result, these methods often suffer from unnecessary communication overhead and imbalanced computation. In this paper, we present DCP, a dynamic context parallel training framework that introduces fine-grained blockwise partitioning of both data and computation. By enabling flexible mapping of data and computation blocks to devices, DCP can adapt to varying sequence characteristics, effectively reducing communication and improving memory and computation balance. Micro-benchmarks demonstrate that DCP accelerates attention by 1.19x~2.45x under causal masks and 2.15x~3.77x under sparse attention patterns. Additionally, we observe up to 0.94x~1.16x end-to-end training speed-up for causal masks, and 1.00x~1.46x for sparse masks.",
        "authors": [
            "Chenyu Jiang",
            "Zhenkun Cai",
            "Ye Tian",
            "Zhen Jia",
            "Yida Wang",
            "Chuan Wu"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-10-12"
    },
    "http://arxiv.org/abs/2510.10570v1": {
        "title": "Multitask Learning with Learned Task Relationships",
        "link": "http://arxiv.org/abs/2510.10570v1",
        "abstract": "Classical consensus-based strategies for federated and decentralized learning are statistically suboptimal in the presence of heterogeneous local data or task distributions. As a result, in recent years, there has been growing interest in multitask or personalized strategies, which allow individual agents to benefit from one another in pursuing locally optimal models without enforcing consensus. Existing strategies require either precise prior knowledge of the underlying task relationships or are fully non-parametric and instead rely on meta-learning or proximal constructions. In this work, we introduce an algorithmic framework that strikes a balance between these extremes. By modeling task relationships through a Gaussian Markov Random Field with an unknown precision matrix, we develop a strategy that jointly learns both the task relationships and the local models, allowing agents to self-organize in a way consistent with their individual data distributions. Our theoretical analysis quantifies the quality of the learned relationship, and our numerical experiments demonstrate its practical effectiveness.",
        "authors": [
            "Zirui Wan",
            "Stefan Vlaski"
        ],
        "categories": [
            "cs.LG",
            "cs.DC",
            "cs.MA"
        ],
        "id": "http://arxiv.org/abs/2510.10570v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-12"
    },
    "http://arxiv.org/abs/2510.10531v1": {
        "id": "http://arxiv.org/abs/2510.10531v1",
        "title": "A Verified High-Performance Composable Object Library for Remote Direct Memory Access (Extended Version)",
        "link": "http://arxiv.org/abs/2510.10531v1",
        "tags": [
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Introduces LOCO, a formally verified composable object library for RDMA to simplify distributed programming. Achieves performance comparable to custom RDMA systems with verified correctness via the Mowgli verification framework. Reduces complexity while matching custom system performance.",
        "abstract": "Remote Direct Memory Access (RDMA) is a memory technology that allows remote devices to directly write to and read from each other's memory, bypassing components such as the CPU and operating system. This enables low-latency high-throughput networking, as required for many modern data centres, HPC applications and AI/ML workloads. However, baseline RDMA comprises a highly permissive weak memory model that is difficult to use in practice and has only recently been formalised. In this paper, we introduce the Library of Composable Objects (LOCO), a formally verified library for building multi-node objects on RDMA, filling the gap between shared memory and distributed system programming. LOCO objects are well-encapsulated and take advantage of the strong locality and the weak consistency characteristics of RDMA. They have performance comparable to custom RDMA systems (e.g. distributed maps), but with a far simpler programming model amenable to formal proofs of correctness. To support verification, we develop a novel modular declarative verification framework, called Mowgli, that is flexible enough to model multinode objects and is independent of a memory consistency model. We instantiate Mowgli with the RDMA memory model, and use it to verify correctness of LOCO libraries.",
        "authors": [
            "Guillaume Ambal",
            "George Hodgkins",
            "Mark Madler",
            "Gregory Chockler",
            "Brijesh Dongol",
            "Joseph Izraelevitz",
            "Azalea Raad",
            "Viktor Vafeiadis"
        ],
        "categories": [
            "cs.PL",
            "cs.DC",
            "cs.LO",
            "eess.SY"
        ],
        "submit_date": "2025-10-12"
    },
    "http://arxiv.org/abs/2510.10380v1": {
        "title": "FLAMMABLE: A Multi-Model Federated Learning Framework with Multi-Model Engagement and Adaptive Batch Sizes",
        "link": "http://arxiv.org/abs/2510.10380v1",
        "abstract": "Multi-Model Federated Learning (MMFL) is an emerging direction in Federated Learning (FL) where multiple models are trained in parallel, generally on various datasets. Optimizing the models' accuracies and training times in the MMFL setting requires adapting to data and system heterogeneity across clients as in single-model FL; these challenges are amplified in the MMFL setting due to additional heterogeneity across models. Neither existing solutions nor naïve extensions of single-model FL frameworks efficiently address these challenges. To bridge this gap, we propose FLAMMABLE, a comprehensive MMFL training framework. FLAMMABLE optimizes model training by intelligently adapting client batch sizes while engaging them to train multiple carefully chosen models, depending on their system capabilities, in each training round. To evaluate FLAMMABLE, we develop the first benchmark platform for the MMFL setting, which may enable future reproducible MMFL research. Extensive evaluations on multiple datasets and models show that FLAMMABLE boosts the MMFL time-to-accuracy performance by 1.1$\\sim$10.0$\\times$ while improving the final model accuracy by 1.3$\\sim$5.4\\% compared to several known baselines.",
        "authors": [
            "Shouxu Lin",
            "Zimeng Pan",
            "Yuhang Yao",
            "Haeyoung Noh",
            "Pei Zhang",
            "Carlee Joe-Wong"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "id": "http://arxiv.org/abs/2510.10380v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-12"
    },
    "http://arxiv.org/abs/2510.10302v2": {
        "id": "http://arxiv.org/abs/2510.10302v2",
        "title": "SP-MoE: Speculative Decoding and Prefetching for Accelerating MoE-based Model Inference",
        "link": "http://arxiv.org/abs/2510.10302v2",
        "tags": [
            "MoE",
            "offloading",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-10-11",
        "tldr": "Addresses GPU memory and bandwidth bottlenecks in speculative decoding for MoE-based LLMs. Proposes SP-MoE with speculative expert prefetching, cutoff-layer policy, and pipelined runtime for compute-communication overlap. Achieves 1.07-3.5× TPOT speedup over baselines across models and datasets.",
        "abstract": "The Mixture-of-Experts (MoE) architecture has been widely adopted in large language models (LLMs) to reduce computation cost through model sparsity. Employing speculative decoding (SD) can further accelerate MoE inference by drafting multiple tokens per step and verifying them in parallel. However, combining MoE with SD inflates GPU memory and aggravates CPU-GPU bandwidth contention during multi-token verification. Existing MoE offloading systems are SD-agnostic and do not address this bottleneck. We present SP-MoE, the first SD-aware expert-offloading and compute-communication pipelining framework. SP-MoE introduces: (1) speculative expert prefetching that exploits structural correspondence between the draft and target models to prefetch likely experts ahead of verification; (2) a cutoff-layer policy that bounds per-layer prefetch depth based on empirical profiles and an analytical latency model, guaranteeing just-in-time availability without overfetch; and (3) a pipelined runtime with asynchronous prefetch threads and batched I/O to hide loading latency. Extensive experiments demonstrate that SP-MoE achieves a 1.07-3.5 times TPOT speedup over state-of-the-art methods across diverse datasets, environments, and MoE-based models.",
        "authors": [
            "Liangkun Chen",
            "Zijian Wen",
            "Tian Wu",
            "Xiaoxi Zhang",
            "Chuan Wu"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-10-11"
    },
    "http://arxiv.org/abs/2511.01862v1": {
        "title": "Possible Futures for Cloud Cost Models",
        "link": "http://arxiv.org/abs/2511.01862v1",
        "abstract": "Cloud is now the leading software and computing hardware innovator, and is changing the landscape of compute to one that is optimized for artificial intelligence and machine learning (AI/ML). Computing innovation was initially driven to meet the needs of scientific computing. As industry and consumer usage of computing proliferated, there was a shift to satisfy a multipolar customer base. Demand for AI/ML now dominates modern computing and innovation has centralized on cloud. As a result, cost and resource models designed to serve AI/ML use cases are not currently well suited for science. If resource contention resulting from a unipole consumer makes access to contended resources harder for scientific users, a likely future is running scientific workloads where they were not intended. In this article, we discuss the past, current, and possible futures of cloud cost models for the continued support of discovery and science.",
        "authors": [
            "Vanessa Sochat",
            "Daniel Milroy"
        ],
        "categories": [
            "cs.DC",
            "cs.CY"
        ],
        "id": "http://arxiv.org/abs/2511.01862v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-11"
    },
    "http://arxiv.org/abs/2510.10166v1": {
        "title": "Proactive and Reactive Autoscaling Techniques for Edge Computing",
        "link": "http://arxiv.org/abs/2510.10166v1",
        "abstract": "Edge computing allows for the decentralization of computing resources. This decentralization is achieved through implementing microservice architectures, which require low latencies to meet stringent service level agreements (SLA) such as performance, reliability, and availability metrics. While cloud computing offers the large data storage and computation resources necessary to handle peak demands, a hybrid cloud and edge environment is required to ensure SLA compliance. Several auto-scaling algorithms have been proposed to try to achieve these compliance challenges, but they suffer from performance issues and configuration complexity. This chapter provides a brief overview of edge computing architecture, its uses, benefits, and challenges for resource scaling. We then introduce Service Level Agreements, and existing research on devising algorithms used in edge computing environments to meet these agreements, along with their benefits and drawbacks.",
        "authors": [
            "Suhrid Gupta",
            "Muhammed Tawfiqul Islam",
            "Rajkumar Buyya"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.10166v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-11"
    },
    "http://arxiv.org/abs/2511.11585v1": {
        "id": "http://arxiv.org/abs/2511.11585v1",
        "title": "Parameter-Efficient and Personalized Federated Training of Generative Models at the Edge",
        "link": "http://arxiv.org/abs/2511.11585v1",
        "tags": [
            "training",
            "edge",
            "quantization"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes FedGen-Edge, a federated learning framework using LoRA adapters to train generative models efficiently at the edge. Reduces uplink traffic by >99% vs full FedAvg, achieves lower perplexity/FID on PTB/CIFAR-10 while personalizing client models.",
        "abstract": "Large generative models (for example, language and diffusion models) enable high-quality text and image synthesis but are hard to train or adapt in cross-device federated settings due to heavy computation and communication and statistical/system heterogeneity. We propose FedGen-Edge, a framework that decouples a frozen, pre-trained global backbone from lightweight client-side adapters and federates only the adapters. Using Low-Rank Adaptation (LoRA) constrains client updates to a compact subspace, which reduces uplink traffic by more than 99 percent versus full-model FedAvg, stabilizes aggregation under non-IID data, and naturally supports personalization because each client can keep a locally tuned adapter. On language modeling (PTB) and image generation (CIFAR-10), FedGen-Edge achieves lower perplexity/FID and faster convergence than strong baselines while retaining a simple FedAvg-style server. A brief ablation shows diminishing returns beyond moderate LoRA rank and a trade-off between local epochs and client drift. FedGen-Edge offers a practical path toward privacy-preserving, resource-aware, and personalized generative AI on heterogeneous edge devices.",
        "authors": [
            "Kabir Khan",
            "Manju Sarkar",
            "Anita Kar",
            "Suresh Ghosh"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "submit_date": "2025-10-11"
    },
    "http://arxiv.org/abs/2510.10126v1": {
        "title": "FedMon: Federated eBPF Monitoring for Distributed Anomaly Detection in Multi-Cluster Cloud Environments",
        "link": "http://arxiv.org/abs/2510.10126v1",
        "abstract": "Kubernetes multi-cluster deployments demand scalable and privacy-preserving anomaly detection. Existing eBPF-based monitors provide low-overhead system and network visibility but are limited to single clusters, while centralized approaches incur bandwidth, privacy, and heterogeneity challenges. We propose FedMon, a federated eBPF framework that unifies kernel-level telemetry with federated learning (FL) for cross-cluster anomaly detection. Lightweight eBPF agents capture syscalls and network events, extract local statistical and sequence features, and share only model updates with a global server. A hybrid detection engine combining Variational Autoencoders (VAEs) with Isolation Forests enables both temporal pattern modeling and outlier detection. Deployed across three Kubernetes clusters, FedMon achieves 94% precision, 91% recall, and an F1-score of 0.92, while cutting bandwidth usage by 60% relative to centralized baselines. Results demonstrate that FedMon enhances accuracy, scalability, and privacy, providing an effective defense for large-scale, multi-tenant cloud-native environments.",
        "authors": [
            "Sehar Zehra",
            "Hassan Jamil Syed",
            "Ummay Faseeha"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.10126v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-11"
    },
    "http://arxiv.org/abs/2510.10028v1": {
        "id": "http://arxiv.org/abs/2510.10028v1",
        "title": "Efficient Onboard Vision-Language Inference in UAV-Enabled Low-Altitude Economy Networks via LLM-Enhanced Optimization",
        "link": "http://arxiv.org/abs/2510.10028v1",
        "tags": [
            "edge",
            "serving",
            "RL"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Addresses efficient vision-language inference on resource-constrained UAVs. Proposes combined resource allocation and LLM-enhanced reinforcement learning for trajectory optimization. Achieves reduced task latency and power consumption under accuracy constraints in dynamic networks.",
        "abstract": "The rapid advancement of Low-Altitude Economy Networks (LAENets) has enabled a variety of applications, including aerial surveillance, environmental sensing, and semantic data collection. To support these scenarios, unmanned aerial vehicles (UAVs) equipped with onboard vision-language models (VLMs) offer a promising solution for real-time multimodal inference. However, ensuring both inference accuracy and communication efficiency remains a significant challenge due to limited onboard resources and dynamic network conditions. In this paper, we first propose a UAV-enabled LAENet system model that jointly captures UAV mobility, user-UAV communication, and the onboard visual question answering (VQA) pipeline. Based on this model, we formulate a mixed-integer non-convex optimization problem to minimize task latency and power consumption under user-specific accuracy constraints. To solve the problem, we design a hierarchical optimization framework composed of two parts: (i) an Alternating Resolution and Power Optimization (ARPO) algorithm for resource allocation under accuracy constraints, and (ii) a Large Language Model-augmented Reinforcement Learning Approach (LLaRA) for adaptive UAV trajectory optimization. The large language model (LLM) serves as an expert in refining reward design of reinforcement learning in an offline fashion, introducing no additional latency in real-time decision-making. Numerical results demonstrate the efficacy of our proposed framework in improving inference performance and communication efficiency under dynamic LAENet conditions.",
        "authors": [
            "Yang Li",
            "Ruichen Zhang",
            "Yinqiu Liu",
            "Guangyuan Liu",
            "Dusit Niyato",
            "Abbas Jamalipour",
            "Xianbin Wang",
            "Dong In Kim"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC"
        ],
        "submit_date": "2025-10-11"
    },
    "http://arxiv.org/abs/2510.09851v1": {
        "title": "QONNECT: A QoS-Aware Orchestration System for Distributed Kubernetes Clusters",
        "link": "http://arxiv.org/abs/2510.09851v1",
        "abstract": "Modern applications increasingly span across cloud, fog, and edge environments, demanding orchestration systems that can adapt to diverse deployment contexts while meeting Quality-of-Service (QoS) requirements. Standard Kubernetes schedulers do not account for user-defined objectives such as energy efficiency, cost optimization, and global performance, often leaving operators to make manual, cluster-by-cluster placement decisions. To address this need, we present QONNECT, a vendor-agnostic orchestration framework that enables declarative, QoS-driven application deployment across heterogeneous Kubernetes and K3s clusters. QONNECT introduces a distributed architecture composed of a central Knowledge Base, Raft-replicated Resource Lead Agents, and lightweight Resource Agents in each cluster. Through a minimal YAML-based interface, users specify high-level QoS goals, which the system translates into concrete placement and migration actions. Our implementation is evaluated on a federated testbed of up to nine cloud-fog-edge clusters using the Istio Bookinfo microservice application. The system demonstrates dynamic, policy-driven microservice placement, automated failover, QoS-compliant rescheduling, and leader re-election after node failure, all without manual intervention. By bridging the gap between declarative deployment models and operational QoS goals, QONNECT transforms the cloud-edge continuum into a unified, self-optimizing platform.",
        "authors": [
            "Haci Ismail Aslan",
            "Syed Muhammad Mahmudul Haque",
            "Joel Witzke",
            "Odej Kao"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.09851v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-10"
    },
    "http://arxiv.org/abs/2510.09847v1": {
        "title": "THEAS: Efficient Power Management in Multi-Core CPUs via Cache-Aware Resource Scheduling",
        "link": "http://arxiv.org/abs/2510.09847v1",
        "abstract": "The dynamic adaptation of resource levels enables the system to enhance energy efficiency while maintaining the necessary computational resources, particularly in scenarios where workloads fluctuate significantly over time. The proposed approach can play a crucial role in heterogeneous systems where workload characteristics are not uniformly distributed, such as non-pinning tasks. The deployed THEAS algorithm in this research work ensures a balance between performance and power consumption, making it suitable for a wide range of real-time applications. A comparative analysis of the proposed THEAS algorithm with well-known scheduling techniques such as Completely Fair Scheduler (CFS), Energy-Aware Scheduling (EAS), Heterogeneous Scheduling (HeteroSched), and Utility-Based Scheduling is presented in Table III. Each scheme is compared based on adaptability, core selection criteria, performance scaling, cache awareness, overhead, and real-time suitability.",
        "authors": [
            "Said Muhammad",
            "Lahlou Laaziz",
            "Nadjia Kara",
            "Phat Tan Nguyen",
            "Timothy Murphy"
        ],
        "categories": [
            "cs.DC",
            "cs.PF"
        ],
        "id": "http://arxiv.org/abs/2510.09847v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-10"
    },
    "http://arxiv.org/abs/2510.09799v1": {
        "title": "Distributed clustering in partially overlapping feature spaces",
        "link": "http://arxiv.org/abs/2510.09799v1",
        "abstract": "We introduce and address a novel distributed clustering problem where each participant has a private dataset containing only a subset of all available features, and some features are included in multiple datasets. This scenario occurs in many real-world applications, such as in healthcare, where different institutions have complementary data on similar patients. We propose two different algorithms suitable for solving distributed clustering problems that exhibit this type of feature space heterogeneity. The first is a federated algorithm in which participants collaboratively update a set of global centroids. The second is a one-shot algorithm in which participants share a statistical parametrization of their local clusters with the central server, who generates and merges synthetic proxy datasets. In both cases, participants perform local clustering using algorithms of their choice, which provides flexibility and personalized computational costs. Pretending that local datasets result from splitting and masking an initial centralized dataset, we identify some conditions under which the proposed algorithms are expected to converge to the optimal centralized solution. Finally, we test the practical performance of the algorithms on three public datasets.",
        "authors": [
            "Alessio Maritan",
            "Luca Schenato"
        ],
        "categories": [
            "cs.DS",
            "cs.DC",
            "cs.LG"
        ],
        "id": "http://arxiv.org/abs/2510.09799v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-10"
    },
    "http://arxiv.org/abs/2510.09163v1": {
        "id": "http://arxiv.org/abs/2510.09163v1",
        "title": "Co-designing a Programmable RISC-V Accelerator for MPC-based Energy and Thermal Management of Many-Core HPC Processors",
        "link": "http://arxiv.org/abs/2510.09163v1",
        "tags": [
            "hardware",
            "training",
            "edge"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes a hardware-software codesign for an MPC-based energy/thermal management accelerator for many-core processors. Uses a pruned operator-splitting quadratic programming solver and scheduled parallel execution on RISC-V cores. Achieves sub-millisecond latency controlling 144 PEs with 33× lower latency than baseline.",
        "abstract": "Managing energy and thermal profiles is critical for many-core HPC processors with hundreds of application-class processing elements (PEs). Advanced model predictive control (MPC) delivers state-of-the-art performance but requires solving an online optimization problem over a thousand times per second (1 kHz control bandwidth), with computational and memory demands scaling with PE count. Traditional MPC approaches execute the controller on the PEs, but operating system overheads create jitter and limit control bandwidth. Running MPC on dedicated on-chip controllers enables fast, deterministic control but raises concerns about area and power overhead. In this work, we tackle these challenges by proposing a hardware-software codesign of a lightweight MPC controller, based on an operator-splitting quadratic programming solver and an embedded multi-core RISC-V controller. Key innovations include pruning weak thermal couplings to reduce model memory and ahead-of-time scheduling for efficient parallel execution of sparse triangular systems arising from the optimization problem. The proposed controller achieves sub-millisecond latency when controlling 144 PEs at 500 MHz, delivering 33x lower latency and 7.9x higher energy efficiency than a single-core baseline. Operating within a compact less than 1 MiB memory footprint, it consumes as little as 325 mW while occupying less than 1.5% of a typical HPC processor's die area.",
        "authors": [
            "Alessandro Ottaviano",
            "Andrino Meli",
            "Paul Scheffler",
            "Giovanni Bambini",
            "Robert Balas",
            "Davide Rossi",
            "Andrea Bartolini",
            "Luca Benini"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-10-10"
    },
    "http://arxiv.org/abs/2510.09143v1": {
        "title": "Multiparty equality in the local broadcast model",
        "link": "http://arxiv.org/abs/2510.09143v1",
        "abstract": "In this paper we consider the multiparty equality problem in graphs, where every vertex of a graph $G$ is given an input, and the goal of the vertices is to decide whether all inputs are equal. We study this problem in the local broadcast model, where a message sent by a vertex is received by all its neighbors and the total cost of a protocol is the sum of the lengths of the messages sent by the vertices. This setting was studied by Khan and Vaidya, who gave in 2021 a protocol achieving a 4-approximation in the general case.   We study this multiparty communication problem through the lens of network topology. We design a new protocol for 2-connected graphs, whose efficiency relies on the notion of total vertex cover in graph theory. This protocol outperforms the aforementioned 4-approximation in a number of cases. To demonstrate its applicability, we apply it to obtain optimal or asymptotically optimal protocols for several natural network topologies such as cycles, hypercubes, and grids. On the way we also provide new bounds of independent interest on the size of total vertex covers in regular graphs.",
        "authors": [
            "Louis Esperet",
            "Jean-Florent Raymond"
        ],
        "categories": [
            "math.CO",
            "cs.CC",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.09143v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-10"
    },
    "http://arxiv.org/abs/2510.08976v1": {
        "id": "http://arxiv.org/abs/2510.08976v1",
        "title": "Hierarchical Scheduling for Multi-Vector Image Retrieval",
        "link": "http://arxiv.org/abs/2510.08976v1",
        "tags": [
            "RAG",
            "multi-modal",
            "sparse"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes HiMIR, a hierarchical scheduling framework for multi-vector image retrieval to improve accuracy and efficiency in MLLM RAG systems. Uses multiple granularities and cross-hierarchy similarity with sparsity to reduce redundant matching. Reduces computation by 3.5× while improving accuracy.",
        "abstract": "To effectively leverage user-specific data, retrieval augmented generation (RAG) is employed in multimodal large language model (MLLM) applications. However, conventional retrieval approaches often suffer from limited retrieval accuracy. Recent advances in multi-vector retrieval (MVR) improve accuracy by decomposing queries and matching against segmented images. They still suffer from sub-optimal accuracy and efficiency, overlooking alignment between the query and varying image objects and redundant fine-grained image segments. In this work, we present an efficient scheduling framework for image retrieval - HiMIR. First, we introduce a novel hierarchical paradigm, employing multiple intermediate granularities for varying image objects to enhance alignment. Second, we minimize redundancy in retrieval by leveraging cross-hierarchy similarity consistency and hierarchy sparsity to minimize unnecessary matching computation. Furthermore, we configure parameters for each dataset automatically for practicality across diverse scenarios. Our empirical study shows that, HiMIR not only achieves substantial accuracy improvements but also reduces computation by up to 3.5 times over the existing MVR system.",
        "authors": [
            "Maoliang Li",
            "Ke Li",
            "Yaoyang Liu",
            "Jiayu Chen",
            "Zihao Zheng",
            "Yinjun Wu",
            "Xiang Chen"
        ],
        "categories": [
            "cs.CV",
            "cs.DC",
            "cs.IR"
        ],
        "submit_date": "2025-10-10"
    },
    "http://arxiv.org/abs/2510.08874v1": {
        "id": "http://arxiv.org/abs/2510.08874v1",
        "title": "Slicing Is All You Need: Towards A Universal One-Sided Algorithm for Distributed Matrix Multiplication",
        "link": "http://arxiv.org/abs/2510.08874v1",
        "tags": [
            "training",
            "storage",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes a universal one-sided algorithm for distributed matrix multiplication to avoid operand redistribution across partitionings. Uses slicing to compute overlapping tiles and lowers to optimized IR. Achieves competitive performance with PyTorch DTensor for varied partitionings and replication factors.",
        "abstract": "Many important applications across science, data analytics, and AI workloads depend on distributed matrix multiplication. Prior work has developed a large array of algorithms suitable for different problem sizes and partitionings including 1D, 2D, 1.5D, and 2.5D algorithms. A limitation of current work is that existing algorithms are limited to a subset of partitionings. Multiple algorithm implementations are required to support the full space of possible partitionings. If no algorithm implementation is available for a particular set of partitionings, one or more operands must be redistributed, increasing communication costs. This paper presents a universal one-sided algorithm for distributed matrix multiplication that supports all combinations of partitionings and replication factors. Our algorithm uses slicing (index arithmetic) to compute the sets of overlapping tiles that must be multiplied together. This list of local matrix multiplies can then either be executed directly, or reordered and lowered to an optimized IR to maximize overlap. We implement our algorithm using a high-level C++-based PGAS programming framework that performs direct GPU-to-GPU communication using intra-node interconnects. We evaluate performance for a wide variety of partitionings and replication factors, finding that our work is competitive with PyTorch DTensor, a highly optimized distributed tensor library targeting AI models.",
        "authors": [
            "Benjamin Brock",
            "Renato Golin"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-10-10"
    },
    "http://arxiv.org/abs/2510.08863v1": {
        "title": "Comparative Performance Analysis of Modern NoSQL Data Technologies: Redis, Aerospike, and Dragonfly",
        "link": "http://arxiv.org/abs/2510.08863v1",
        "abstract": "The rise of distributed applications and cloud computing has created a demand for scalable, high-performance key-value storage systems. This paper presents a performance evaluation of three prominent NoSQL key-value stores: Redis, Aerospike, and Dragonfly, using the Yahoo! Cloud Serving Benchmark (YCSB) framework. We conducted extensive experiments across three distinct workload patterns (read-heavy, write-heavy), and balanced while systematically varying client concurrency from 1 to 32 clients. Our evaluation methodology captures both latency, throughput, and memory characteristics under realistic operational conditions, providing insights into the performance trade-offs and scalability behaviour of each system",
        "authors": [
            "Deep Bodra",
            "Sushil Khairnar"
        ],
        "categories": [
            "cs.DB",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.08863v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-09"
    },
    "http://arxiv.org/abs/2510.08842v2": {
        "id": "http://arxiv.org/abs/2510.08842v2",
        "title": "Maple: A Multi-agent System for Portable Deep Learning across Clusters",
        "link": "http://arxiv.org/abs/2510.08842v2",
        "tags": [
            "training",
            "offline",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes Maple, a multi-agent system for generating correct command lines for distributed DL training across heterogeneous GPU clusters using natural language. Agents handle extraction, template retrieval, verification, and correction, achieving 92.0% accuracy across 567 test cases.",
        "abstract": "Training deep learning (DL) models across Graphics Processing Unit (GPU) clusters is technically challenging. One aspect is that users have to compose command lines to adapt to the heterogeneous launchers, schedulers, affinity options, DL framework arguments, and environment variables. Composing correct command lines is error-prone and can easily frustrate users, impeding research or wasting resources. In this work, we present Maple, a multi-agent system that generates correct DL command lines with users' natural language input. Maple consists of four agents with the functionalities of information extraction, template retrieval, command line verification, and error correction. We evaluate Maple on nine GPU clusters across national computing centers in the U.S., five representative deep learning model families, and four commonly used parallel DL training paradigms. Our experiments also cover schedulers of SLURM and PBS and heterogeneous architectures, such as NVIDIA A100/H200 GPUs and Intel Max series GPUs. Maple achieves 92.0% accuracy in generating command lines across the 567 test cases. Leverage multiple language models with an aggregated size of 10B parameters, Maple delivers comparable performance to the state-of-the-art models of GPT-5, Claude, and Gemini. Together, these results highlight Maple's practical value in enabling portable and scalable distributed DL across heterogeneous HPC environments.",
        "authors": [
            "Molang Wu",
            "Zhao Zhang"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-10-09"
    },
    "http://arxiv.org/abs/2510.08839v1": {
        "title": "Reinforcement Learning-Driven Edge Management for Reliable Multi-view 3D Reconstruction",
        "link": "http://arxiv.org/abs/2510.08839v1",
        "abstract": "Real-time multi-view 3D reconstruction is a mission-critical application for key edge-native use cases, such as fire rescue, where timely and accurate 3D scene modeling enables situational awareness and informed decision-making. However, the dynamic and unpredictable nature of edge resource availability introduces disruptions, such as degraded image quality, unstable network links, and fluctuating server loads, which challenge the reliability of the reconstruction pipeline. In this work, we present a reinforcement learning (RL)-based edge resource management framework for reliable 3D reconstruction to ensure high quality reconstruction within a reasonable amount of time, despite the system operating under a resource-constrained and disruption-prone environment. In particular, the framework adopts two cooperative Q-learning agents, one for camera selection and one for server selection, both of which operate entirely online, learning policies through interactions with the edge environment. To support learning under realistic constraints and evaluate system performance, we implement a distributed testbed comprising lab-hosted end devices and FABRIC infrastructure-hosted edge servers to emulate smart city edge infrastructure under realistic disruption scenarios. Results show that the proposed framework improves application reliability by effectively balancing end-to-end latency and reconstruction quality in dynamic environments.",
        "authors": [
            "Motahare Mounesan",
            "Sourya Saha",
            "Houchao Gan",
            "Md. Nurul Absur",
            "Saptarshi Debroy"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "cs.DC",
            "cs.GR",
            "cs.MM"
        ],
        "id": "http://arxiv.org/abs/2510.08839v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-09"
    },
    "http://arxiv.org/abs/2510.08803v1": {
        "title": "Man-Made Heuristics Are Dead. Long Live Code Generators!",
        "link": "http://arxiv.org/abs/2510.08803v1",
        "abstract": "Policy design for various systems controllers has conventionally been a manual process, with domain experts carefully tailoring heuristics for the specific instance in which the policy will be deployed. In this paper, we re-imagine policy design via a novel automated search technique fueled by recent advances in generative models, specifically Large Language Model (LLM)-driven code generation. We outline the design and implementation of PolicySmith, a framework that applies LLMs to synthesize instance-optimal heuristics. We apply PolicySmith to two long-standing systems policies - web caching and congestion control, highlighting the opportunities unraveled by this LLM-driven heuristic search. For caching, PolicySmith discovers heuristics that outperform established baselines on standard open-source traces. For congestion control, we show that PolicySmith can generate safe policies that integrate directly into the Linux kernel.",
        "authors": [
            "Rohit Dwivedula",
            "Divyanshu Saxena",
            "Aditya Akella",
            "Swarat Chaudhuri",
            "Daehyeok Kim"
        ],
        "categories": [
            "cs.OS",
            "cs.DC",
            "cs.LG",
            "cs.NE"
        ],
        "id": "http://arxiv.org/abs/2510.08803v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-09"
    },
    "http://arxiv.org/abs/2510.08700v1": {
        "title": "Are Voters Willing to Collectively Secure Elections? Unraveling a Practical Blockchain Voting System",
        "link": "http://arxiv.org/abs/2510.08700v1",
        "abstract": "Ensuring ballot secrecy is critical for fair and trustworthy electronic voting systems, yet achieving strong secrecy guarantees in decentralized, large-scale elections remains challenging. This paper proposes the concept of collectively secure voting, in which voters themselves can opt in as secret holders to protect ballot secrecy. A practical blockchain-based collectively secure voting system is designed and implemented. Our design strikes a balance between strong confidentiality guarantees and real-world applicability. The proposed system combines threshold cryptography and smart contracts to ensure ballots remain confidential during voting, while all protocol steps remain transparent and verifiable. Voters can use the system without prior blockchain knowledge through an intuitive user interface that hides underlying complexity. To evaluate this approach, a user testing is conducted. Results show a high willingness to act as secret holders, reliable participation in share release, and high security confidence in the proposed system. The findings demonstrate that voters can collectively maintain secrecy and that such a practical deployment is feasible.",
        "authors": [
            "Zhuolun Li",
            "Haluk Sonmezler",
            "Faiza Shirazi",
            "Febin Shaji",
            "Tymoteusz Mroczkowski",
            "Dexter Lardner",
            "Matthew Alain Camus",
            "Evangelos Pournaras"
        ],
        "categories": [
            "cs.CR",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.08700v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-09"
    },
    "http://arxiv.org/abs/2510.08544v1": {
        "id": "http://arxiv.org/abs/2510.08544v1",
        "title": "SPAD: Specialized Prefill and Decode Hardware for Disaggregated LLM Inference",
        "link": "http://arxiv.org/abs/2510.08544v1",
        "tags": [
            "disaggregation",
            "hardware",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-10-10",
        "tldr": "Proposes SPAD, specialized hardware for disaggregated LLM inference with dedicated prefill and decode chips. Prefill Chips optimize compute with GDDR memory, while Decode Chips focus on bandwidth. Reduces hardware cost by 19-41% and TDP by 2-17% versus H100 clusters at same performance.",
        "abstract": "Large Language Models (LLMs) have gained popularity in recent years, driving up the demand for inference. LLM inference is composed of two phases with distinct characteristics: a compute-bound prefill phase followed by a memory-bound decode phase. To efficiently serve LLMs, prior work proposes prefill-decode disaggregation to run each phase on separate hardware. However, existing hardware poorly matches the different requirements of each phase. Current datacenter GPUs and TPUs follow a more-is-better design philosophy that maximizes compute and memory resources, causing memory bandwidth underutilization in the prefill phase and compute underutilization in the decode phase. Such underutilization directly translates into increased serving costs.   This paper proposes SPAD (Specialized Prefill and Decode hardware), adopting a less-is-more methodology to design specialized chips tailored to the distinct characteristics of prefill and decode phases. The proposed Prefill Chips have larger systolic arrays and use cost-effective GDDR memory, whereas the proposed Decode Chips retain high memory bandwidth but reduce compute capacity. Compared to modeled H100s, simulations show that the proposed Prefill Chips deliver 8% higher prefill performance on average at 52% lower hardware cost, while the proposed Decode Chips achieve 97% of the decode performance with 28% lower TDP.   End-to-end simulations on production traces show that SPAD reduces hardware cost by 19%-41% and TDP by 2%-17% compared to modeled baseline clusters while offering the same performance. Even when models and workloads change, SPAD can reallocate either type of chip to run either phase and still achieve 11%-43% lower hardware costs, demonstrating the longevity of the SPAD design.",
        "authors": [
            "Hengrui Zhang",
            "Pratyush Patel",
            "August Ning",
            "David Wentzlaff"
        ],
        "categories": [
            "cs.AR",
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-10-09"
    },
    "http://arxiv.org/abs/2510.08536v1": {
        "title": "Investigating Matrix Repartitioning to Address the Over- and Undersubscription Challenge for a GPU-based CFD Solver",
        "link": "http://arxiv.org/abs/2510.08536v1",
        "abstract": "Modern high-performance computing (HPC) increasingly relies on GPUs, but integrating GPU acceleration into complex scientific frameworks like OpenFOAM remains a challenge. Existing approaches either fully refactor the codebase or use plugin-based GPU solvers, each facing trade-offs between performance and development effort. In this work, we address the limitations of plugin-based GPU acceleration in OpenFOAM by proposing a repartitioning strategy that better balances CPU matrix assembly and GPU-based linear solves. We present a detailed computational model, describe a novel matrix repartitioning and update procedure, and evaluate its performance on large-scale CFD simulations. Our results show that the proposed method significantly mitigates oversubscription issues, improving solver performance and resource utilization in heterogeneous CPU-GPU environments.",
        "authors": [
            "Gregor Olenik",
            "Marcel Koch",
            "Hartwig Anzt"
        ],
        "categories": [
            "cs.DC",
            "cs.SE"
        ],
        "id": "http://arxiv.org/abs/2510.08536v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-09"
    },
    "http://arxiv.org/abs/2510.08522v1": {
        "id": "http://arxiv.org/abs/2510.08522v1",
        "title": "DYNAMIX: RL-based Adaptive Batch Size Optimization in Distributed Machine Learning Systems",
        "link": "http://arxiv.org/abs/2510.08522v1",
        "tags": [
            "training",
            "RL"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes DYNAMIX, an RL-based framework for adaptive batch size optimization in distributed ML training. Uses PPO with multi-dimensional system state to dynamically adjust batch sizes without explicit modeling. Achieves 46% training time reduction and 6.3% higher accuracy.",
        "abstract": "Existing batch size selection approaches in distributed machine learning rely on static allocation or simplistic heuristics that fail to adapt to heterogeneous, dynamic computing environments. We present DYNAMIX, a reinforcement learning framework that formulates batch size optimization as a sequential decision-making problem using Proximal Policy Optimization (PPO). Our approach employs a multi-dimensional state representation encompassing network-level metrics, system-level resource utilization, and training statistical efficiency indicators to enable informed decision-making across diverse computational resources. Our approach eliminates the need for explicit system modeling while integrating seamlessly with existing distributed training frameworks. Through evaluations across diverse workloads, hardware configurations, and network conditions, DYNAMIX achieves up to 6.3% improvement in the final model accuracy and 46% reduction in the total training time. Our scalability experiments demonstrate that DYNAMIX maintains the best performance as cluster size increases to 32 nodes, while policy transfer experiments show that learned policies generalize effectively across related model architectures.",
        "authors": [
            "Yuanjun Dai",
            "Keqiang He",
            "An Wang"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "submit_date": "2025-10-09"
    },
    "http://arxiv.org/abs/2511.01861v1": {
        "title": "Conceptual Design Report for FAIR Computing",
        "link": "http://arxiv.org/abs/2511.01861v1",
        "abstract": "This Conceptual Design Report (CDR) presents the plans of the computing infrastructure for research at FAIR, Darmstadt, Germany. It presents the computing requirements of the various research groups, the policies for the computing and storage infrastructure, the foreseen FAIR computing model including the open data, software and services policies and architecture for the periods starting in 2028 with the \"first science (plus)\" phase to the modularized start version of FAIR. The overall ambition is to create a federated and centrally-orchestrated infrastructure serving the large diversity of the research lines present with sufficient scalability and flexibility to cope with future data challenges that will be present at FAIR.",
        "authors": [
            "Johan Messchendorp",
            "Mohammad Al-Turany",
            "Volker Friese",
            "Thorsten Kollegger",
            "Bastian Loeher",
            "Jochen Markert",
            "Andrew Mistry",
            "Thomas Neff",
            "Adrian Oeftiger",
            "Michael Papenbrock",
            "Stephane Pietri",
            "Shahab Sanjari",
            "Tobias Stockmanns"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2511.01861v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-09"
    },
    "http://arxiv.org/abs/2510.08244v1": {
        "title": "Energy-Efficient Maximal Independent Sets in Radio Networks",
        "link": "http://arxiv.org/abs/2510.08244v1",
        "abstract": "The maximal independent set (MIS) is one of the most fundamental problems in distributed computing, and it has been studied intensively for over four decades. This paper focuses on the MIS problem in the Radio Network model, a standard model widely used to model wireless networks, particularly ad hoc wireless and sensor networks. Energy is a premium resource in these networks, which are typically battery-powered. Hence, designing distributed algorithms that use as little energy as possible is crucial. We use the well-established energy model where a node can be sleeping or awake in a round, and only the awake rounds (when it can send or listen) determine the energy complexity of the algorithm, which we want to minimize.   We present new, more energy-efficient MIS algorithms in radio networks with arbitrary and unknown graph topology. We present algorithms for two popular variants of the radio model -- with collision detection (CD) and without collision detection (no-CD). Specifically, we obtain the following results:   1. CD model: We present a randomized distributed MIS algorithm with energy complexity $O(\\log n)$, round complexity $O(\\log^2 n)$, and failure probability $1 / poly(n)$, where $n$ is the network size. We show that our energy complexity is optimal by showing a matching $Ω(\\log n)$ lower bound.   2. no-CD model: In the more challenging no-CD model, we present a randomized distributed MIS algorithm with energy complexity $O(\\log^2n \\log \\log n)$, round complexity $O(\\log^3 n \\log Δ)$, and failure probability $1 / poly(n)$. The energy complexity of our algorithm is significantly lower than the round (and energy) complexity of $O(\\log^3 n)$ of the best known distributed MIS algorithm of Davies [PODC 2023] for arbitrary graph topology.",
        "authors": [
            "Dominick Banasik",
            "Varsha Dani",
            "Fabien Dufoulon",
            "Aayush Gupta",
            "Thomas P. Hayes",
            "Gopal Pandurangan"
        ],
        "categories": [
            "cs.DC",
            "cs.DS"
        ],
        "id": "http://arxiv.org/abs/2510.08244v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-09"
    },
    "http://arxiv.org/abs/2510.08230v1": {
        "title": "pyGinkgo: A Sparse Linear Algebra Operator Framework for Python",
        "link": "http://arxiv.org/abs/2510.08230v1",
        "abstract": "Sparse linear algebra is a cornerstone of many scientific computing and machine learning applications. Python has become a popular choice for these applications due to its simplicity and ease of use. Yet high performance sparse kernels in Python remain limited in functionality, especially on modern CPU and GPU architectures. We present pyGinkgo, a lightweight and Pythonic interface to the Ginkgo library, offering high-performance sparse linear algebra support with platform portability across CUDA, HIP, and OpenMP backends. pyGinkgo bridges the gap between high-performance C++ backends and Python usability by exposing Ginkgo's capabilities via Pybind11 and a NumPy and PyTorch compatible interface. We benchmark pyGinkgo's performance against state-of-the-art Python libraries including SciPy, CuPy, PyTorch, and TensorFlow. Results across hardware from different vendors demonstrate that pyGinkgo consistently outperforms existing Python tools in both sparse matrix vector (SpMV) product and iterative solver performance, while maintaining performance parity with native Ginkgo C++ code. Our work positions pyGinkgo as a compelling backend for sparse machine learning models and scientific workflows.",
        "authors": [
            "Keshvi Tuteja",
            "Gregor Olenik",
            "Roman Mishchuk",
            "Yu-Hsiang Tsai",
            "Markus Götz",
            "Achim Streit",
            "Hartwig Anzt",
            "Charlotte Debus"
        ],
        "categories": [
            "cs.MS",
            "cs.DC",
            "cs.PF",
            "cs.SE"
        ],
        "id": "http://arxiv.org/abs/2510.08230v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-09"
    },
    "http://arxiv.org/abs/2510.08228v1": {
        "id": "http://arxiv.org/abs/2510.08228v1",
        "title": "Distributed Resource Selection for Self-Organising Cloud-Edge Systems",
        "link": "http://arxiv.org/abs/2510.08228v1",
        "tags": [
            "edge",
            "training",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Addresses efficient resource allocation in cloud-edge systems for distributed applications. Proposes a distributed consensus-based mechanism using local knowledge and inter-agent collaboration for dynamic resource selection. Achieves up to 30 times faster allocation than centralized heuristics without compromising optimality.",
        "abstract": "This paper presents a distributed resource selection mechanism for diverse cloud-edge environments, enabling dynamic and context-aware allocation of resources to meet the demands of complex distributed applications. By distributing the decision-making process, our approach ensures efficiency, scalability, and resilience in highly dynamic cloud-edge environments where centralised coordination becomes a bottleneck. The proposed mechanism aims to function as a core component of a broader, distributed, and self-organising orchestration system that facilitates the intelligent placement and adaptation of applications in real-time. This work leverages a consensus-based mechanism utilising local knowledge and inter-agent collaboration to achieve efficient results without relying on a central controller, thus paving the way for distributed orchestration. Our results indicate that computation time is the key factor influencing allocation decisions. Our approach consistently delivers rapid allocations without compromising optimality or incurring additional cost, achieving timely results at scale where exhaustive search is infeasible and centralised heuristics run up to 30 times slower.",
        "authors": [
            "Quentin Renau",
            "Amjad Ullah",
            "Emma Hart"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-10-09"
    },
    "http://arxiv.org/abs/2510.08180v1": {
        "id": "http://arxiv.org/abs/2510.08180v1",
        "title": "Towards Energy-Efficient Serverless Computing with Hardware Isolation",
        "link": "http://arxiv.org/abs/2510.08180v1",
        "tags": [
            "hardware",
            "serving",
            "edge"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes hardware isolation for serverless computing by assigning individual processors per function to avoid software overhead and idle servers. Achieves 90.63% reduction in energy consumption overheads in preliminary evaluation.",
        "abstract": "Serverless computing provides just-in-time infrastructure provisioning with rapid elasticity and a finely-grained pricing model. As full control of resource allocation is in the hands of the cloud provider and applications only consume resources when they actually perform work, we believe that serverless computing is uniquely positioned to maximize energy efficiency.   However, the focus of current serverless platforms is to run hundreds or thousands of serverless functions from different tenants on traditional server hardware, requiring expensive software isolation mechanisms and a high degree of overprovisioning, i.e., idle servers, to anticipate load spikes. With shared caches, high clock frequencies, and many-core architectures, servers today are optimized for large, singular workloads but not to run thousands of isolated functions.   We propose rethinking the serverless hardware architecture to align it with the requirements of serverless software. Specifically, we propose using hardware isolation with individual processors per function instead of software isolation resulting in a serverless hardware stack that consumes energy only when an application actually performs work. In preliminary evaluation with real hardware and a typical serverless workload we find that this could reduce energy consumption overheads by 90.63% or an average 70.8MW.",
        "authors": [
            "Natalie Carl",
            "Tobias Pfandzelter",
            "David Bermbach"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-10-09"
    },
    "http://arxiv.org/abs/2510.08164v2": {
        "title": "A Multi-Simulation Bridge for IoT Digital Twins",
        "link": "http://arxiv.org/abs/2510.08164v2",
        "abstract": "The increasing capabilities of Digital Twins (DTs) in the context of the Internet of Things (IoT) and Industrial IoT (IIoT) call for seamless integration with simulation platforms to support system design, validation, and real-time operation. This paper introduces the concept, design, and experimental evaluation of the DT Simulation Bridge - a software framework that enables diverse interaction patterns between active DTs and simulation environments. The framework supports both the DT development lifecycle and the incorporation of simulations during active operation. Through bidirectional data exchange, simulations can update DT models dynamically, while DTs provide real-time feedback to adapt simulation parameters. We describe the architectural design and core software components that ensure flexible interoperability and scalable deployment. Experimental results show that the DT Simulation Bridge enhances design agility, facilitates virtual commissioning, and supports live behavioral analysis under realistic conditions, demonstrating its effectiveness across a range of industrial scenarios.",
        "authors": [
            "Marco Picone",
            "Samuele Burattini",
            "Marco Melloni",
            "Prasad Talasila",
            "Davide Ziglioli",
            "Matteo Martinelli",
            "Nicola Bicocchi",
            "Alessandro Ricci",
            "Peter Gorm Larsen"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.08164v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-09"
    },
    "http://arxiv.org/abs/2510.08139v1": {
        "title": "BlockSDN: Towards a High-Performance Blockchain via Software-Defined Cross Networking optimization",
        "link": "http://arxiv.org/abs/2510.08139v1",
        "abstract": "The scalability of blockchain systems is constrained by inefficient P2P broadcasting, as most existing optimizations focus only on the logical layer without considering physical network conditions. To address this, we propose BlockSDN, the first SDN-based integrated architecture for blockchain. BlockSDN employs a distributed control plane for a global network view, a graph engine for hierarchical clustering, and a hybrid macro-micro neighbor selection with hierarchical broadcasting. A dedicated simulation platform shows that BlockSDN reduces global block synchronization time by 65% and 55% compared to Gossip and Mercury, respectively.These results highlight the potential of SDN-enabled cross-layer coordination to significantly enhance blockchain scalability and performance.",
        "authors": [
            "Wenyang Jia",
            "Jingjing Wang",
            "Ziwei Yan",
            "Xiangli Peng",
            "Guohui Yuan"
        ],
        "categories": [
            "cs.NI",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.08139v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-09"
    },
    "http://arxiv.org/abs/2510.09690v1": {
        "title": "A Semantic Model for Audit of Cloud Engines based on ISO/IEC TR 3445:2022",
        "link": "http://arxiv.org/abs/2510.09690v1",
        "abstract": "Cloud computing has become the foundation of modern digital infrastructure, yet the absence of a unified architectural and compliance framework impedes interoperability, auditability, and robust security. This paper introduces a formal, machine-readable semantic model for Cloud Engines, integrating the architectural taxonomy of ISO/IEC 22123 (Cloud Reference Architecture) with the security and compliance controls of ISO/IEC 27001:2022 and ISO/IEC TR 3445:2022. The model decomposes cloud systems into four canonical interfaces--Control, Business, Audit, and Data--and extends them with a security ontology that maps mechanisms such as authentication, authorization, and encryption to specific compliance controls. Expressed in RDF/Turtle, the model enables semantic reasoning, automated compliance validation, and vendor-neutral architecture design. We demonstrate its practical utility through OpenStack and AWS case studies, and provide reproducible validation workflows using SPARQL and SHACL. This work advances the state of cloud security modeling by bridging architectural and compliance standards in a unified framework, with a particular emphasis on auditability.",
        "authors": [
            "Morteza Sargolzaei Javan"
        ],
        "categories": [
            "cs.CR",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.09690v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-09"
    },
    "http://arxiv.org/abs/2510.08072v1": {
        "id": "http://arxiv.org/abs/2510.08072v1",
        "title": "When Light Bends to the Collective Will: A Theory and Vision for Adaptive Photonic Scale-up Domains",
        "link": "http://arxiv.org/abs/2510.08072v1",
        "tags": [
            "networking",
            "training"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes adaptive photonic interconnects to optimize collective communications in scale-up systems. Uses dynamic reconfiguration guided by operation structure and theoretical framework balancing reconfiguration delay and performance gains. Improves maximum concurrent flow for collectives (e.g., AllReduce) via BvN decomposition.",
        "abstract": "As chip-to-chip silicon photonics gain traction for their bandwidth and energy efficiency, collective communication has emerged as a critical bottleneck in scale-up systems. Programmable photonic interconnects offer a promising path forward: by dynamically reconfiguring the fabric, they can establish direct, high-bandwidth optical paths between communicating endpoints -- \\emph{synchronously and guided by the structure of collective operations} (e.g., AllReduce). However, realizing this vision -- \\emph{when light bends to the collective will} -- requires navigating a fundamental trade-off between reconfiguration delay and the performance gains of adaptive topologies.   In this paper, we present a simple theoretical framework for adaptive photonic scale-up domains that makes this trade-off explicit and clarifies when reconfiguration is worthwhile. Along the way, we highlight a connection -- not surprising but still powerful -- between the Birkhoff--von Neumann (BvN) decomposition, maximum concurrent flow (a classic measure of network throughput), and the well-known $α$-$β$ cost model for collectives. Finally, we outline a research agenda in algorithm design and systems integration that can build on this foundation.",
        "authors": [
            "Vamsi Addanki"
        ],
        "categories": [
            "cs.NI",
            "cs.DC"
        ],
        "submit_date": "2025-10-09"
    },
    "http://arxiv.org/abs/2510.08055v1": {
        "id": "http://arxiv.org/abs/2510.08055v1",
        "title": "From Tokens to Layers: Redefining Stall-Free Scheduling for LLM Serving with Layered Prefill",
        "link": "http://arxiv.org/abs/2510.08055v1",
        "tags": [
            "serving",
            "MoE",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-10-09",
        "tldr": "Redefines scheduling for LLM serving to minimize MoE weight reloads. Introduces layered prefill, partitioning model into contiguous layer groups and interleaving prefill/decode across groups. Reduces TTFT by up to 70% and per-token energy by 22% compared to chunked prefill.",
        "abstract": "Large Language Model (LLM) inference in production must meet stringent service-level objectives for both time-to-first-token (TTFT) and time-between-token (TBT) while maximizing throughput under fixed compute, memory, and interconnect budgets. Modern serving systems adopt stall-free scheduling techniques such as chunked prefill, which splits long prompt processing along the token dimension and interleaves prefill with ongoing decode iterations. While effective at stabilizing TBT, chunked prefill incurs substantial overhead in Mixture-of-Experts (MoE) models: redundant expert weight loads increase memory traffic by up to 39% and inflate energy consumption. We propose layered prefill, a new scheduling paradigm that treats transformer layer groups as the primary scheduling unit. By vertically partitioning the model into contiguous layer groups and interleaving prefill and decode across the groups, layered prefill sustains stall-free decoding while eliminating chunk-induced MoE weight reloads. It reduces off-chip bandwidth demand, lowering TTFT by up to 70%, End-to-End latency by 41% and per-token energy by up to 22%. Evaluations show that layered prefill consistently improves the TTFT--TBT Pareto frontier over chunked prefill, reducing expert-load traffic and energy cost while maintaining stall-free decoding. Overall, shifting the scheduling axis from tokens to layers unlocks a new operating regime for high-efficiency, energy-aware LLM serving in co-located environments.",
        "authors": [
            "Gunjun Lee",
            "Jiwon Kim",
            "Jaiyoung Park",
            "Younjoo Lee",
            "Jung Ho Ahn"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "submit_date": "2025-10-09"
    },
    "http://arxiv.org/abs/2510.07922v2": {
        "title": "SketchGuard: Scaling Byzantine-Robust Decentralized Federated Learning via Sketch-Based Screening",
        "link": "http://arxiv.org/abs/2510.07922v2",
        "abstract": "Decentralized Federated Learning (DFL) enables privacy-preserving collaborative training without centralized servers, but remains vulnerable to Byzantine attacks where malicious clients submit corrupted model updates. Existing Byzantine-robust DFL defenses rely on similarity-based neighbor screening that requires every client to exchange and compare complete high-dimensional model vectors with all neighbors in each training round, creating prohibitive communication and computational costs that prevent deployment at web scale. We propose SketchGuard, a general framework that decouples Byzantine filtering from model aggregation through sketch-based neighbor screening. SketchGuard compresses $d$-dimensional models to $k$-dimensional sketches ($k \\ll d$) using Count Sketch for similarity comparisons, then selectively fetches full models only from accepted neighbors, reducing per-round communication complexity from $O(d|N_i|)$ to $O(k|N_i| + d|S_i|)$, where $|N_i|$ is the neighbor count and $|S_i| \\le |N_i|$ is the accepted neighbor count. We establish rigorous convergence guarantees in both strongly convex and non-convex settings, proving that Count Sketch compression preserves Byzantine resilience with controlled degradation bounds where approximation errors introduce only a $(1+O(ε))$ factor in the effective threshold parameter. Comprehensive experiments across multiple datasets, network topologies, and attack scenarios demonstrate that SketchGuard maintains identical robustness to state-of-the-art methods while reducing computation time by up to 82% and communication overhead by 50-70% depending on filtering effectiveness, with benefits scaling multiplicatively with model dimensionality and network connectivity. These results establish the viability of sketch-based compression as a fundamental enabler of robust DFL at web scale.",
        "authors": [
            "Murtaza Rangwala",
            "Farag Azzedin",
            "Richard O. Sinnott",
            "Rajkumar Buyya"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.07922v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-09"
    },
    "http://arxiv.org/abs/2510.07901v1": {
        "title": "Decentralised Blockchain Management Through Digital Twins",
        "link": "http://arxiv.org/abs/2510.07901v1",
        "abstract": "The necessity of blockchain systems to remain decentralised limits current solutions to blockchain governance and dynamic management, forcing a trade-off between control and decentralisation. In light of the above, this work proposes a dynamic and decentralised blockchain management mechanism based on digital twins. To ensure decentralisation, the proposed mechanism utilises multiple digital twins that the system's stakeholders control. To facilitate decentralised decision-making, the twins are organised in a secondary blockchain system that orchestrates agreement on, and propagation of decisions to the managed blockchain. This enables the management of blockchain systems without centralised control. A preliminary evaluation of the performance and impact of the overheads introduced by the proposed mechanism is conducted through simulation. The results demonstrate the proposed mechanism's ability to reach consensus on decisions quickly and reconfigure the primary blockchain with minimal overhead.",
        "authors": [
            "Georgios Diamantopoulos",
            "Nikos Tziritas",
            "Rami Bahsoon",
            "Georgios Theodoropoulos"
        ],
        "categories": [
            "cs.CR",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.07901v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-09"
    },
    "http://arxiv.org/abs/2510.07811v1": {
        "title": "Adaptive Execution Scheduler for DataDios SmartDiff",
        "link": "http://arxiv.org/abs/2510.07811v1",
        "abstract": "We present an adaptive scheduler for a single differencing engine (SmartDiff) with two execution modes: (i) in-memory threads and (ii) Dask based parallelism. The scheduler continuously tunes batch size and worker/thread count within fixed CPU and memory budgets to minimize p95 latency. A lightweight preflight profiler estimates bytes/row and I/O rate; an online cost/memory model prunes unsafe actions; and a guarded hill-climb policy favors lower latency with backpressure and straggler mitigation. Backend selection is gated by a conservative working-set estimate so that in-memory execution is chosen when safe, otherwise Dask is used. Across synthetic and public tabular benchmarks, the scheduler reduces p95 latency by 23 to 28 percent versus a tuned warm-up heuristic (and by 35 to 40 percent versus fixed grid baselines), while lowering peak memory by 16 to 22 percent (25 to 32 percent vs. fixed) with zero OOMs and comparable throughput.",
        "authors": [
            "Aryan Poduri"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "id": "http://arxiv.org/abs/2510.07811v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-09"
    },
    "http://arxiv.org/abs/2510.07664v2": {
        "title": "FedQS: Optimizing Gradient and Model Aggregation for Semi-Asynchronous Federated Learning",
        "link": "http://arxiv.org/abs/2510.07664v2",
        "abstract": "Federated learning (FL) enables collaborative model training across multiple parties without sharing raw data, with semi-asynchronous FL (SAFL) emerging as a balanced approach between synchronous and asynchronous FL. However, SAFL faces significant challenges in optimizing both gradient-based (e.g., FedSGD) and model-based (e.g., FedAvg) aggregation strategies, which exhibit distinct trade-offs in accuracy, convergence speed, and stability. While gradient aggregation achieves faster convergence and higher accuracy, it suffers from pronounced fluctuations, whereas model aggregation offers greater stability but slower convergence and suboptimal accuracy. This paper presents FedQS, the first framework to theoretically analyze and address these disparities in SAFL. FedQS introduces a divide-and-conquer strategy to handle client heterogeneity by classifying clients into four distinct types and adaptively optimizing their local training based on data distribution characteristics and available computational resources. Extensive experiments on computer vision, natural language processing, and real-world tasks demonstrate that FedQS achieves the highest accuracy, attains the lowest loss, and ranks among the fastest in convergence speed, outperforming state-of-the-art baselines. Our work bridges the gap between aggregation strategies in SAFL, offering a unified solution for stable, accurate, and efficient federated learning. The code and datasets are available at https://github.com/bkjod/FedQS_.",
        "authors": [
            "Yunbo Li",
            "Jiaping Gui",
            "Zhihang Deng",
            "Fanchao Meng",
            "Yue Wu"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.07664v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-09"
    },
    "http://arxiv.org/abs/2510.07132v1": {
        "title": "DPMM-CFL: Clustered Federated Learning via Dirichlet Process Mixture Model Nonparametric Clustering",
        "link": "http://arxiv.org/abs/2510.07132v1",
        "abstract": "Clustered Federated Learning (CFL) improves performance under non-IID client heterogeneity by clustering clients and training one model per cluster, thereby balancing between a global model and fully personalized models. However, most CFL methods require the number of clusters K to be fixed a priori, which is impractical when the latent structure is unknown. We propose DPMM-CFL, a CFL algorithm that places a Dirichlet Process (DP) prior over the distribution of cluster parameters. This enables nonparametric Bayesian inference to jointly infer both the number of clusters and client assignments, while optimizing per-cluster federated objectives. This results in a method where, at each round, federated updates and cluster inferences are coupled, as presented in this paper. The algorithm is validated on benchmark datasets under Dirichlet and class-split non-IID partitions.",
        "authors": [
            "Mariona Jaramillo-Civill",
            "Peng Wu",
            "Pau Closas"
        ],
        "categories": [
            "cs.LG",
            "cs.DC",
            "stat.ML"
        ],
        "id": "http://arxiv.org/abs/2510.07132v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-08"
    },
    "http://arxiv.org/abs/2510.07126v1": {
        "title": "Validation of Various Normalization Methods for Brain Tumor Segmentation: Can Federated Learning Overcome This Heterogeneity?",
        "link": "http://arxiv.org/abs/2510.07126v1",
        "abstract": "Deep learning (DL) has been increasingly applied in medical imaging, however, it requires large amounts of data, which raises many challenges related to data privacy, storage, and transfer. Federated learning (FL) is a training paradigm that overcomes these issues, though its effectiveness may be reduced when dealing with non-independent and identically distributed (non-IID) data. This study simulates non-IID conditions by applying different MRI intensity normalization techniques to separate data subsets, reflecting a common cause of heterogeneity. These subsets are then used for training and testing models for brain tumor segmentation. The findings provide insights into the influence of the MRI intensity normalization methods on segmentation models, both training and inference. Notably, the FL methods demonstrated resilience to inconsistently normalized data across clients, achieving the 3D Dice score of 92%, which is comparable to a centralized model (trained using all data). These results indicate that FL is a solution to effectively train high-performing models without violating data privacy, a crucial concern in medical applications. The code is available at: https://github.com/SanoScience/fl-varying-normalization.",
        "authors": [
            "Jan Fiszer",
            "Dominika Ciupek",
            "Maciej Malawski"
        ],
        "categories": [
            "cs.CV",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.07126v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-08"
    },
    "http://arxiv.org/abs/2510.06998v1": {
        "id": "http://arxiv.org/abs/2510.06998v1",
        "title": "Evaluating Rapid Makespan Predictions for Heterogeneous Systems with Programmable Logic",
        "link": "http://arxiv.org/abs/2510.06998v1",
        "tags": [
            "serving",
            "networking",
            "hardware"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Addresses rapid makespan prediction for task scheduling on heterogeneous accelerators (CPUs, GPUs, FPGAs). Proposes an evaluation framework using task graphs to validate analytical models against real-world data transfers and congestion. Achieves practical prediction methods for mapping efficiency.",
        "abstract": "Heterogeneous computing systems, which combine general-purpose processors with specialized accelerators, are increasingly important for optimizing the performance of modern applications. A central challenge is to decide which parts of an application should be executed on which accelerator or, more generally, how to map the tasks of an application to available devices. Predicting the impact of a change in a task mapping on the overall makespan is non-trivial. While there are very capable simulators, these generally require a full implementation of the tasks in question, which is particularly time-intensive for programmable logic. A promising alternative is to use a purely analytical function, which allows for very fast predictions, but abstracts significantly from reality. Bridging the gap between theory and practice poses a significant challenge to algorithm developers. This paper aims to aid in the development of rapid makespan prediction algorithms by providing a highly flexible evaluation framework for heterogeneous systems consisting of CPUs, GPUs and FPGAs, which is capable of collecting real-world makespan results based on abstract task graph descriptions. We analyze to what extent actual makespans can be predicted by existing analytical approaches. Furthermore, we present common challenges that arise from high-level characteristics such as data transfer overhead and device congestion in heterogeneous systems.",
        "authors": [
            "Martin Wilhelm",
            "Franz Freitag",
            "Max Tzschoppe",
            "Thilo Pionteck"
        ],
        "categories": [
            "cs.DC",
            "cs.AR"
        ],
        "submit_date": "2025-10-08"
    },
    "http://arxiv.org/abs/2510.06902v1": {
        "title": "GROMACS Unplugged: How Power Capping and Frequency Shapes Performance on GPUs",
        "link": "http://arxiv.org/abs/2510.06902v1",
        "abstract": "Molecular dynamics simulations are essential tools in computational biophysics, but their performance depend heavily on hardware choices and configuration. In this work, we presents a comprehensive performance analysis of four NVIDIA GPU accelerators -- A40, A100, L4, and L40 -- using six representative GROMACS biomolecular workloads alongside two synthetic benchmarks: Pi Solver (compute bound) and STREAM Triad (memory bound). We investigate how performance scales with GPU graphics clock frequency and how workloads respond to power capping. The two synthetic benchmarks define the extremes of frequency scaling: Pi Solver shows ideal compute scalability, while STREAM Triad reveals memory bandwidth limits -- framing GROMACS's performance in context. Our results reveal distinct frequency scaling behaviors: Smaller GROMACS systems exhibit strong frequency sensitivity, while larger systems saturate quickly, becoming increasingly memory bound. Under power capping, performance remains stable until architecture- and workload-specific thresholds are reached, with high-end GPUs like the A100 maintaining near-maximum performance even under reduced power budgets. Our findings provide practical guidance for selecting GPU hardware and optimizing GROMACS performance for large-scale MD workflows under power constraints.",
        "authors": [
            "Ayesha Afzal",
            "Anna Kahler",
            "Georg Hager",
            "Gerhard Wellein"
        ],
        "categories": [
            "cs.DC",
            "cs.PF"
        ],
        "id": "http://arxiv.org/abs/2510.06902v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-08"
    },
    "http://arxiv.org/abs/2510.06882v1": {
        "id": "http://arxiv.org/abs/2510.06882v1",
        "title": "Multi-Dimensional Autoscaling of Stream Processing Services on Edge Devices",
        "link": "http://arxiv.org/abs/2510.06882v1",
        "tags": [
            "edge",
            "serving",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes MUDAP for multi-dimensional autoscaling of stream processing on resource-constrained edge devices. Combines service-level (e.g., data quality, model size) and resource-level vertical scaling with a RASK agent for regression-based action optimization. Achieves 28% fewer SLO violations than Kubernetes VPA and RL baselines.",
        "abstract": "Edge devices have limited resources, which inevitably leads to situations where stream processing services cannot satisfy their needs. While existing autoscaling mechanisms focus entirely on resource scaling, Edge devices require alternative ways to sustain the Service Level Objectives (SLOs) of competing services. To address these issues, we introduce a Multi-dimensional Autoscaling Platform (MUDAP) that supports fine-grained vertical scaling across both service- and resource-level dimensions. MUDAP supports service-specific scaling tailored to available parameters, e.g., scale data quality or model size for a particular service. To optimize the execution across services, we present a scaling agent based on Regression Analysis of Structural Knowledge (RASK). The RASK agent efficiently explores the solution space and learns a continuous regression model of the processing environment for inferring optimal scaling actions. We compared our approach with two autoscalers, the Kubernetes VPA and a reinforcement learning agent, for scaling up to 9 services on a single Edge device. Our results showed that RASK can infer an accurate regression model in merely 20 iterations (i.e., observe 200s of processing). By increasingly adding elasticity dimensions, RASK sustained the highest request load with 28% less SLO violations, compared to baselines.",
        "authors": [
            "Boris Sedlak",
            "Philipp Raith",
            "Andrea Morichetta",
            "Víctor Casamayor Pujol",
            "Schahram Dustdar"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.LG",
            "cs.PF"
        ],
        "submit_date": "2025-10-08"
    },
    "http://arxiv.org/abs/2510.06834v1": {
        "id": "http://arxiv.org/abs/2510.06834v1",
        "title": "Vectorized FlashAttention with Low-cost Exponential Computation in RISC-V Vector Processors",
        "link": "http://arxiv.org/abs/2510.06834v1",
        "tags": [
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-10-08",
        "tldr": "Vectorizes FlashAttention on RISC-V vector processors using low-cost exponential approximations and tiling to optimize attention kernels. Achieves significant performance gains without custom ISA extensions.",
        "abstract": "Attention is a core operation in numerous machine learning and artificial intelligence models. This work focuses on the acceleration of attention kernel using FlashAttention algorithm, in vector processors, particularly those based on the RISC-V instruction set architecture (ISA). This work represents the first effort to vectorize FlashAttention, minimizing scalar code and simplifying the computational complexity of evaluating exponentials needed by softmax used in attention. By utilizing a low-cost approximation for exponentials in floating-point arithmetic, we reduce the cost of computing the exponential function without the need to extend baseline vector ISA with new custom instructions. Also, appropriate tiling strategies are explored with the goal to improve memory locality. Experimental results highlight the scalability of our approach, demonstrating significant performance gains with the vectorized implementations when processing attention layers in practical applications.",
        "authors": [
            "Vasileios Titopoulos",
            "Kosmas Alexandridis",
            "Giorgos Dimitrakopoulos"
        ],
        "categories": [
            "cs.LG",
            "cs.DC",
            "cs.PF"
        ],
        "submit_date": "2025-10-08"
    },
    "http://arxiv.org/abs/2510.06675v1": {
        "title": "REACH: Reinforcement Learning for Adaptive Microservice Rescheduling in the Cloud-Edge Continuum",
        "link": "http://arxiv.org/abs/2510.06675v1",
        "abstract": "Cloud computing, despite its advantages in scalability, may not always fully satisfy the low-latency demands of emerging latency-sensitive pervasive applications. The cloud-edge continuum addresses this by integrating the responsiveness of edge resources with cloud scalability. Microservice Architecture (MSA) characterized by modular, loosely coupled services, aligns effectively with this continuum. However, the heterogeneous and dynamic computing resource poses significant challenges to the optimal placement of microservices. We propose REACH, a novel rescheduling algorithm that dynamically adapts microservice placement in real time using reinforcement learning to react to fluctuating resource availability, and performance variations across distributed infrastructures. Extensive experiments on a real-world testbed demonstrate that REACH reduces average end-to-end latency by 7.9%, 10%, and 8% across three benchmark MSA applications, while effectively mitigating latency fluctuations and spikes.",
        "authors": [
            "Xu Bai",
            "Muhammed Tawfiqul Islam",
            "Rajkumar Buyya",
            "Adel N. Toosi"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.06675v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-08"
    },
    "http://arxiv.org/abs/2510.06513v1": {
        "id": "http://arxiv.org/abs/2510.06513v1",
        "title": "On-Package Memory with Universal Chiplet Interconnect Express (UCIe): A Low Power, High Bandwidth, Low Latency and Low Cost Approach",
        "link": "http://arxiv.org/abs/2510.06513v1",
        "tags": [
            "hardware",
            "serving",
            "training"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes enhancing UCIe with memory semantics for on-package memory to address AI memory walls. Approaches include reusing LPDDR6/HBM via logic die and native UCIe DRAM, yielding up to 10x bandwidth density, 3x lower latency/power, and reduced cost vs. HBM4/LPDDR.",
        "abstract": "Emerging computing applications such as Artificial Intelligence (AI) are facing a memory wall with existing on-package memory solutions that are unable to meet the power-efficient bandwidth demands. We propose to enhance UCIe with memory semantics to deliver power-efficient bandwidth and cost-effective on-package memory solutions applicable across the entire computing continuum. We propose approaches by reusing existing LPDDR6 and HBM memory through a logic die that connects to the SoC using UCIe. We also propose an approach where the DRAM die natively supports UCIe instead of the LPDDR6 bus interface. Our approaches result in significantly higher bandwidth density (up to 10x), lower latency (up to 3x), lower power (up to 3x), and lower cost compared to existing HBM4 and LPDDR on-package memory solutions.",
        "authors": [
            "Debendra Das Sharma",
            "Swadesh Choudhary",
            "Peter Onufryk",
            "Rob Pelt"
        ],
        "categories": [
            "cs.AR",
            "cs.DC"
        ],
        "submit_date": "2025-10-07"
    },
    "http://arxiv.org/abs/2510.06444v1": {
        "title": "Context-Aware Inference via Performance Forecasting in Decentralized Learning Networks",
        "link": "http://arxiv.org/abs/2510.06444v1",
        "abstract": "In decentralized learning networks, predictions from many participants are combined to generate a network inference. While many studies have demonstrated performance benefits of combining multiple model predictions, existing strategies using linear pooling methods (ranging from simple averaging to dynamic weight updates) face a key limitation. Dynamic prediction combinations that rely on historical performance to update weights are necessarily reactive. Due to the need to average over a reasonable number of epochs (with moving averages or exponential weighting), they tend to be slow to adjust to changing circumstances (phase or regime changes). In this work, we develop a model that uses machine learning to forecast the performance of predictions by models at each epoch in a time series. This enables `context-awareness' by assigning higher weight to models that are likely to be more accurate at a given time. We show that adding a performance forecasting worker in a decentralized learning network, following a design similar to the Allora network, can improve the accuracy of network inferences. Specifically, we find forecasting models that predict regret (performance relative to the network inference) or regret z-score (performance relative to other workers) show greater improvement than models predicting losses, which often do not outperform the naive network inference (historically weighted average of all inferences). Through a series of optimization tests, we show that the performance of the forecasting model can be sensitive to choices in the feature set and number of training epochs. These properties may depend on the exact problem and should be tailored to each domain. Although initially designed for a decentralized learning network, using performance forecasting for prediction combination may be useful in any situation where predictive rather than reactive model weighting is needed.",
        "authors": [
            "Joel Pfeffer",
            "J. M. Diederik Kruijssen",
            "Clément Gossart",
            "Mélanie Chevance",
            "Diego Campo Millan",
            "Florian Stecker",
            "Steven N. Longmore"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.06444v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-07"
    },
    "http://arxiv.org/abs/2510.06404v1": {
        "title": "MuFASA -- Asynchronous Checkpoint for Weakly Consistent Fully Replicated Databases",
        "link": "http://arxiv.org/abs/2510.06404v1",
        "abstract": "We focus on the problem of checkpointing in fully replicated weakly consistent distributed databases, which we refer to as Distributed Transaction Consistent Snapshot (DTCS). A typical example of such a system is a main-memory database that provides strong eventual consistency. This problem is important and challenging for several reasons: (1) eventual consistency often creates anomalies that the users do not anticipate. Hence, frequent checkpoints to ascertain desired invariants is highly beneficial in their use, and (2) traditional checkpoints lead to significant overhead and/or inconsistencies. By showing that the traditional checkpoint leads to inconsistencies or excessive overhead, we define the notion of size-minimal checkpointing for fully replicated databases. We present an algorithm for checkpointing with minimal checkpointing overhead (only O(n) new messages and addition of a single counter for existing messages). It also provides a significant benefit over existing checkpointing algorithms for distributed systems and main-memory databases.   A key benefit of DTCS is that it summarizes the computation by a sequence of snapshots that are strongly consistent even though the underlying computation is weakly consistent. In essence, when anomalies arise in an eventually consistent system, DTCS enables one to concentrate solely on the snapshots surrounding the time point of the anomaly.",
        "authors": [
            "Raaghav Ravishankar",
            "Sandeep Kulkarni",
            "Nitin H Vaidya"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.06404v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-07"
    },
    "http://arxiv.org/abs/2510.06396v1": {
        "id": "http://arxiv.org/abs/2510.06396v1",
        "title": "Adaptive Protein Design Protocols and Middleware",
        "link": "http://arxiv.org/abs/2510.06396v1",
        "tags": [
            "training",
            "offline"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes IMPRESS, an adaptive protein design system with middleware for coupling AI/ML to HPC. Implements dynamic resource allocation and asynchronous workload execution to improve throughput. Achieves increased consistency and enhanced throughput in computational protein design tasks.",
        "abstract": "Computational protein design is experiencing a transformation driven by AI/ML. However, the range of potential protein sequences and structures is astronomically vast, even for moderately sized proteins. Hence, achieving convergence between generated and predicted structures demands substantial computational resources for sampling. The Integrated Machine-learning for Protein Structures at Scale (IMPRESS) offers methods and advanced computing systems for coupling AI to high-performance computing tasks, enabling the ability to evaluate the effectiveness of protein designs as they are developed, as well as the models and simulations used to generate data and train models. This paper introduces IMPRESS and demonstrates the development and implementation of an adaptive protein design protocol and its supporting computing infrastructure. This leads to increased consistency in the quality of protein design and enhanced throughput of protein design due to dynamic resource allocation and asynchronous workload execution.",
        "authors": [
            "Aymen Alsaadi",
            "Jonathan Ash",
            "Mikhail Titov",
            "Matteo Turilli",
            "Andre Merzky",
            "Shantenu Jha",
            "Sagar Khare"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.PF",
            "cs.SE"
        ],
        "submit_date": "2025-10-07"
    },
    "http://arxiv.org/abs/2510.06387v1": {
        "title": "DiLi: A Lock-Free Asynchronously Distributable Linked List",
        "link": "http://arxiv.org/abs/2510.06387v1",
        "abstract": "Modern databases use dynamic search structures that store a huge amount of data, and often serve them using multi-threaded algorithms to support the ever-increasing throughput needs. When this throughput need exceeds the capacity of the machine hosting the structure, one either needs to replace the underlying hardware (an option that is typically not viable and introduces a long down time) or make the data structure distributed. Static partitioning of the data structure for distribution is not desirable, as it is prone to uneven load distribution over time, and having to change the partitioning scheme later will require downtime.   Since a distributed data structure, inherently, relies on communication support from the network stack and operating systems, we introduce the notion of conditional lock-freedom that extends the notion of lock-free computation with reasonable assumptions about communication between processes. We present DiLi, a conditional lock-free, linearizable, and distributable linked list that can be asynchronously and dynamically (1) partitioned into multiple sublists and (2) load balanced by distributing sublists across multiple machines. DiLi contains primitives for these that also maintain the lock-free property of the underlying search structure that supports find, remove, and insert of a key as the client operations.   Searching for an item in DiLi is by a novel traversal that involves a binary search on the partitioning scheme, and then a linear traversal on a limitable number of linked nodes. As a result, we are able to empirically show that DiLi performs as well as the state-of-the-art lock-free concurrent search structures that are based off of a linked list when executed on a single-machine. We also show that the throughput of DiLi scales linearly with the number of machines that host it.",
        "authors": [
            "Raaghav Ravishankar",
            "Sandeep Kulkarni",
            "Sathya Peri",
            "Gokarna Sharma"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.06387v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-07"
    },
    "http://arxiv.org/abs/2510.06023v1": {
        "title": "Optimal Good-Case Latency for Sleepy Consensus",
        "link": "http://arxiv.org/abs/2510.06023v1",
        "abstract": "In the context of Byzantine consensus problems such as Byzantine broadcast (BB) and Byzantine agreement (BA), the good-case setting aims to study the minimal possible latency of a BB or BA protocol under certain favorable conditions, namely the designated leader being correct (for BB), or all parties having the same input value (for BA). We provide a full characterization of the feasibility and impossibility of good-case latency, for both BA and BB, in the synchronous sleepy model. Surprisingly to us, we find irrational resilience thresholds emerging: 2-round good-case BB is possible if and only if at all times, at least $\\frac{1}{\\varphi} \\approx 0.618$ fraction of the active parties are correct, where $\\varphi = \\frac{1+\\sqrt{5}}{2} \\approx 1.618$ is the golden ratio; 1-round good-case BA is possible if and only if at least $\\frac{1}{\\sqrt{2}} \\approx 0.707$ fraction of the active parties are correct.",
        "authors": [
            "Yuval Efron",
            "Joachim Neu",
            "Ling Ren",
            "Ertem Nusret Tas"
        ],
        "categories": [
            "cs.CR",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.06023v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-07"
    },
    "http://arxiv.org/abs/2510.06011v1": {
        "title": "How many more is different?",
        "link": "http://arxiv.org/abs/2510.06011v1",
        "abstract": "From the formation of ice in small clusters of water molecules to the mass raids of army ant colonies, the emergent behavior of collectives depends critically on their size. At the same time, common wisdom holds that such behaviors are robust to the loss of individuals. This tension points to the need for a more systematic study of how number influences collective behavior. We initiate this study by focusing on collective behaviors that change abruptly at certain critical numbers of individuals. We show that a subtle modification of standard bifurcation analysis identifies such critical numbers, including those associated with discreteness- and noise-induced transitions. By treating them as instances of the same phenomenon, we show that critical numbers across physical scales and scientific domains commonly arise from competing feedbacks that scale differently with number. We then use this idea to find overlooked critical numbers in past studies of collective behavior and explore the implications for their conclusions. In particular, we highlight how deterministic approximations of stochastic models can fail near critical numbers. We close by distinguishing these qualitative changes from density-dependent phase transitions and by discussing how our approach could generalize to broader classes of collective behaviors.",
        "authors": [
            "Jacob Calvert",
            "Andréa W. Richa",
            "Dana Randall"
        ],
        "categories": [
            "q-bio.PE",
            "cond-mat.stat-mech",
            "cs.DC",
            "nlin.AO"
        ],
        "id": "http://arxiv.org/abs/2510.06011v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-07"
    },
    "http://arxiv.org/abs/2510.05943v1": {
        "id": "http://arxiv.org/abs/2510.05943v1",
        "title": "EARL: Efficient Agentic Reinforcement Learning Systems for Large Language Models",
        "link": "http://arxiv.org/abs/2510.05943v1",
        "tags": [
            "RL",
            "training"
        ],
        "relevant": true,
        "indexed_date": "2025-10-08",
        "tldr": "Addresses scalability bottlenecks in agentic RL training for LLMs by dynamically adapting parallelism and decentralizing intermediate data exchange. Achieves 2.1× higher throughput and eliminates OOM failures during long-context RL training.",
        "abstract": "Reinforcement learning (RL) has become a pivotal component of large language model (LLM) post-training, and agentic RL extends this paradigm to operate as agents through multi-turn interaction and tool use. Scaling such systems exposes two practical bottlenecks: (1) context length grows rapidly during training, inflating memory usage and latency, and triggering out-of-memory (OOM) failures; and (2) intermediate tensors accumulate with context length, making cross-device data movement a major system bottleneck.   We present EARL, a scalable system for efficient agentic RL. EARL designs a parallelism selector that dynamically adapts model and training parallelism across RL stages based on sequence length and system load, and a data dispatcher that performs layout-aware, decentralized exchange of intermediate data batches. Together, these components increase throughput, reduce long-context failures, and enable stable large-scale training of agentic LLMs without relying on hard limits or penalties of context length.",
        "authors": [
            "Zheyue Tan",
            "Mustapha Abdullahi",
            "Tuo Shi",
            "Huining Yuan",
            "Zelai Xu",
            "Chao Yu",
            "Boxun Li",
            "Bo Zhao"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-10-07"
    },
    "http://arxiv.org/abs/2511.11581v1": {
        "id": "http://arxiv.org/abs/2511.11581v1",
        "title": "The Anatomy of a Triton Attention Kernel",
        "link": "http://arxiv.org/abs/2511.11581v1",
        "tags": [
            "serving",
            "kernel",
            "hardware"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Develops a portable paged attention kernel for LLM inference using Triton to eliminate manual tuning. The kernel achieves 105.9% of state-of-the-art performance across NVIDIA and AMD GPUs through JIT compilation and auto-tuning, improving portability and efficiency.",
        "abstract": "A long-standing goal in both industry and academia is to develop an LLM inference platform that is portable across hardware architectures, eliminates the need for low-level hand-tuning, and still delivers best-in-class efficiency. In this work, we demonstrate that portable, efficient cross-platform LLM inference is indeed possible and share our experience. We develop a state-of-the-art paged attention kernel, the core performance-critical component of many LLM deployments, that builds exclusively on the domain-specific just-in-time compiled language Triton to achieve state-of-the-art performance on both NVIDIA and AMD GPUs. We describe our high-level approach, the key algorithmic and system-level improvements, the parameter auto-tuning required to unlock efficiency, and the integrations into a popular inference server that are necessary to bring the performance of a generic Triton attention kernel from 19.7% of the state-of-the-art to 105.9%. Our results highlight how open-source domain-specific languages can be leveraged to unlock model portability across different GPU vendors.",
        "authors": [
            "Burkhard Ringlein",
            "Jan van Lunteren",
            "Radu Stoica",
            "Thomas Parnell"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "cs.DC",
            "cs.PL"
        ],
        "submit_date": "2025-10-07"
    },
    "http://arxiv.org/abs/2510.05738v1": {
        "title": "A Review of Ontology-Driven Big Data Analytics in Healthcare: Challenges, Tools, and Applications",
        "link": "http://arxiv.org/abs/2510.05738v1",
        "abstract": "Exponential growth in heterogeneous healthcare data arising from electronic health records (EHRs), medical imaging, wearable sensors, and biomedical research has accelerated the adoption of data lakes and centralized architectures capable of handling the Volume, Variety, and Velocity of Big Data for advanced analytics. However, without effective governance, these repositories risk devolving into disorganized data swamps. Ontology-driven semantic data management offers a robust solution by linking metadata to healthcare knowledge graphs, thereby enhancing semantic interoperability, improving data discoverability, and enabling expressive, domain-aware access. This review adopts a systematic research strategy, formulating key research questions and conducting a structured literature search across major academic databases, with selected studies analyzed and classified into six categories of ontology-driven healthcare analytics: (i) ontology-driven integration frameworks, (ii) semantic modeling for metadata enrichment, (iii) ontology-based data access (OBDA), (iv) basic semantic data management, (v) ontology-based reasoning for decision support, and (vi) semantic annotation for unstructured data. We further examine the integration of ontology technologies with Big Data frameworks such as Hadoop, Spark, Kafka, and so on, highlighting their combined potential to deliver scalable and intelligent healthcare analytics. For each category, recent techniques, representative case studies, technical and organizational challenges, and emerging trends such as artificial intelligence, machine learning, the Internet of Things (IoT), and real-time analytics are reviewed to guide the development of sustainable, interoperable, and high-performance healthcare data ecosystems.",
        "authors": [
            "Ritesh Chandra",
            "Sonali Agarwal",
            "Navjot Singh",
            "Sadhana Tiwari"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.05738v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-07"
    },
    "http://arxiv.org/abs/2510.05711v1": {
        "title": "Intertemporal Pricing of Time-Bound Stablecoins: Measuring and Controlling the Liquidity-of-Time Premium",
        "link": "http://arxiv.org/abs/2510.05711v1",
        "abstract": "Time-bound stablecoins are DeFi assets that temporarily tokenize traditional securities during market off-hours, enabling continuous cross-market liquidity. We introduce the Liquidity-of-Time Premium (TLP): the extra return or cost of providing liquidity when the primary market is closed. We build a no-arbitrage pricing model that yields a band for fair values over different expiries, and a dynamic risk-control mechanism that adjusts loan-to-value (LTV) ratios in real time to keep TLP within a target range. Our analysis blends financial engineering (no-arbitrage conditions, option-style pricing) with empirical finance (event studies on cross-listed stocks and futures) to measure TLP under time-zone frictions. We define TLP formally, derive closed-form expressions for its term structure under idealized assumptions, and simulate scenarios that vary volatility and collateralization. We then propose an LTV policy that raises or lowers collateral to expand or curtail time-bound stablecoin supply, analogous to a central bank adjusting rates to defend a peg. We outline empirical proxies for TLP, including ADR premiums, overseas index futures versus cash index divergence, and pre-market versus official close gaps. Results show that TLP grows with closure length and volatility, yet can be contained by adaptive LTV. We provide backtests and figures (term-structure curves, capital-efficiency versus tail-risk trade-offs, time-liquidity heatmaps) and discuss protocol design (vault structure, closing-price oracles, on-chain auction liquidations). The findings position time-bound stablecoins as a tool to reduce temporal market inefficiencies and inform future research and deployment.",
        "authors": [
            "Ailiya Borjigin",
            "Cong He"
        ],
        "categories": [
            "cs.DC",
            "cs.CE"
        ],
        "id": "http://arxiv.org/abs/2510.05711v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-07"
    },
    "http://arxiv.org/abs/2510.21745v1": {
        "title": "Simopt-Power: Leveraging Simulation Metadata for Low-Power Design Synthesis",
        "link": "http://arxiv.org/abs/2510.21745v1",
        "abstract": "Excessive switching activity is a primary contributor to dynamic power dissipation in modern FPGAs, where fine-grained configurability amplifies signal toggling and associated capacitance. Conventional low-power techniques -- gating, clock-domain partitioning, and placement-aware netlist rewrites - either require intrusive design changes or offer diminishing returns as device densities grow. In this work, we present Simopt-power, a simulator-driven optimisation framework that leverages simulation analysis to identify and selectively reconfigure high-toggle paths. By feeding activity profiles back into a lightweight transformation pass, Simopt-power judiciously inserts duplicate truth table logic using Shannon Decomposition principle and relocates critical nets, thereby attenuating unnecessary transitions without perturbing functional behaviour. We evaluated this framework on open-source RTLLM benchmark, with Simopt-power achieves an average switching-induced power reduction of ~9\\% while incurring only ~9\\% additional LUT-equivalent resources for arithmetic designs. These results demonstrate that coupling simulation insights with targeted optimisations can yield a reduced dynamic power, offering a practical path toward using simulation metadata in the FPGA-CAD flow.",
        "authors": [
            "Eashan Wadhwa",
            "Shanker Shreejith"
        ],
        "categories": [
            "cs.AR",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.21745v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-07"
    },
    "http://arxiv.org/abs/2510.05621v1": {
        "title": "Decoupling Correctness from Policy: A Deterministic Causal Structure for Multi-Agent Systems",
        "link": "http://arxiv.org/abs/2510.05621v1",
        "abstract": "In distributed multi-agent systems, correctness is often entangled with operational policies such as scheduling, batching, or routing, which makes systems brittle since performance-driven policy evolution may break integrity guarantees. This paper introduces the Deterministic Causal Structure (DCS), a formal foundation that decouples correctness from policy. We develop a minimal axiomatic theory and prove four results: existence and uniqueness, policy-agnostic invariance, observational equivalence, and axiom minimality. These results show that DCS resolves causal ambiguities that value-centric convergence models such as CRDTs cannot address, and that removing any axiom collapses determinism into ambiguity. DCS thus emerges as a boundary principle of asynchronous computation, analogous to CAP and FLP: correctness is preserved only within the expressive power of a join-semilattice. All guarantees are established by axioms and proofs, with only minimal illustrative constructions included to aid intuition. This work establishes correctness as a fixed, policy-agnostic substrate, a Correctness-as-a-Chassis paradigm, on which distributed intelligent systems can be built modularly, safely, and evolvably.",
        "authors": [
            "Zhiyuan Ren",
            "Tao Zhang",
            "Wenchi Chen"
        ],
        "categories": [
            "cs.DC",
            "cs.MA"
        ],
        "id": "http://arxiv.org/abs/2510.05621v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-07"
    },
    "http://arxiv.org/abs/2510.05583v1": {
        "title": "When Does Global Attention Help? A Unified Empirical Study on Atomistic Graph Learning",
        "link": "http://arxiv.org/abs/2510.05583v1",
        "abstract": "Graph neural networks (GNNs) are widely used as surrogates for costly experiments and first-principles simulations to study the behavior of compounds at atomistic scale, and their architectural complexity is constantly increasing to enable the modeling of complex physics. While most recent GNNs combine more traditional message passing neural networks (MPNNs) layers to model short-range interactions with more advanced graph transformers (GTs) with global attention mechanisms to model long-range interactions, it is still unclear when global attention mechanisms provide real benefits over well-tuned MPNN layers due to inconsistent implementations, features, or hyperparameter tuning. We introduce the first unified, reproducible benchmarking framework - built on HydraGNN - that enables seamless switching among four controlled model classes: MPNN, MPNN with chemistry/topology encoders, GPS-style hybrids of MPNN with global attention, and fully fused local - global models with encoders. Using seven diverse open-source datasets for benchmarking across regression and classification tasks, we systematically isolate the contributions of message passing, global attention, and encoder-based feature augmentation. Our study shows that encoder-augmented MPNNs form a robust baseline, while fused local-global models yield the clearest benefits for properties governed by long-range interaction effects. We further quantify the accuracy - compute trade-offs of attention, reporting its overhead in memory. Together, these results establish the first controlled evaluation of global attention in atomistic graph learning and provide a reproducible testbed for future model development.",
        "authors": [
            "Arindam Chowdhury",
            "Massimiliano Lupo Pasini"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.05583v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-07"
    },
    "http://arxiv.org/abs/2510.05556v1": {
        "id": "http://arxiv.org/abs/2510.05556v1",
        "title": "Toward Systems Foundations for Agentic Exploration",
        "link": "http://arxiv.org/abs/2510.05556v1",
        "tags": [
            "agentic",
            "serving",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-09-16",
        "tldr": "Addresses the lack of efficient system support for LLM agentic exploration. Proposes fundamental challenges in fork semantics, side-effect handling, and microsecond-level native forking. Achieves scalable branching with minimal overhead in multi-agent deployments.",
        "abstract": "Agentic exploration, letting LLM-powered agents branch, backtrack, and search across many execution paths, demands systems support well beyond today's pass-at-k resets. Our benchmark of six snapshot/restore mechanisms shows that generic tools such as CRIU or container commits are not fast enough even in isolated testbeds, and they crumble entirely in real deployments where agents share files, sockets, and cloud APIs with other agents and human users. In this talk, we pinpoint three open fundamental challenges: fork semantics, which concerns how branches reveal or hide tentative updates; external side-effects, where fork awareness must be added to services or their calls intercepted; and native forking, which requires cloning databases and runtimes in microseconds without bulk copying.",
        "authors": [
            "Jiakai Xu",
            "Tianle Zhou",
            "Eugene Wu",
            "Kostis Kaffes"
        ],
        "categories": [
            "cs.DC",
            "cs.OS"
        ],
        "submit_date": "2025-10-07"
    },
    "http://arxiv.org/abs/2510.05497v3": {
        "id": "http://arxiv.org/abs/2510.05497v3",
        "title": "Orders in Chaos: Enhancing Large-Scale MoE LLM Serving with Data Movement Forecasting",
        "link": "http://arxiv.org/abs/2510.05497v3",
        "tags": [
            "MoE",
            "serving",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-10-01",
        "tldr": "Reduces data movement overhead in large-scale MoE LLM serving. Proposes insights from data-movement profiling and implements wafer-scale GPU modifications. Achieves 5.3x and 3.1x speedups on DeepSeek V3 and Qwen3 models.",
        "abstract": "Large-scale Mixture of Experts (MoE) Large Language Models (LLMs) have recently become the frontier open weight models, achieving remarkable model capability similar to proprietary ones. But their random expert selection mechanism introduces significant data movement overhead that becomes the dominant bottleneck in multi-unit LLM serving systems.   To understand the patterns underlying this data movement, we conduct comprehensive data-movement-centric profiling across four state-of-the-art large-scale MoE models released in 2025 (200B-1000B) using over 24,000 requests spanning diverse workloads. We perform systematic analysis from both temporal and spatial perspectives and distill six key insights to guide the design of diverse future serving systems. With our insights, we then demonstrate how to improve wafer-scale GPUs as a case study, and show that minor architectural modifications leveraging the insights achieve substantial performance gains, delivering 5.3x and 3.1x average speedups on DeepSeek V3 and Qwen3, respectively. Our work presents the first comprehensive data-centric analysis of large-scale MoE models and a concrete design study using the learned lessons, with profiling traces and simulation framework already open-sourced with $>$1k downloads. Our traces and results are publicly available at https://huggingface.co/datasets/core12345/MoE_expert_selection_trace",
        "authors": [
            "Zhongkai Yu",
            "Yue Guan",
            "Zihao Yu",
            "Chenyang Zhou",
            "Zhengding Hu",
            "Shuyi Pei",
            "Yangwook Kang",
            "Yufei Ding",
            "Po-An Tsai"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.AR",
            "cs.LG"
        ],
        "submit_date": "2025-10-07"
    },
    "http://arxiv.org/abs/2510.05476v2": {
        "id": "http://arxiv.org/abs/2510.05476v2",
        "title": "cMPI: Using CXL Memory Sharing for MPI One-Sided and Two-Sided Inter-Node Communications",
        "link": "http://arxiv.org/abs/2510.05476v2",
        "tags": [
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Optimizes MPI inter-node communication using CXL memory sharing to bypass network protocols. Proposes cMPI, transforming cross-node communication into CXL memory transactions. Achieves up to 72x higher bandwidth and 49x lower latency versus TCP over Ethernet and SmartNIC for small messages.",
        "abstract": "Message Passing Interface (MPI) is a foundational programming model for high-performance computing. MPI libraries traditionally employ network interconnects (e.g., Ethernet and InfiniBand) and network protocols (e.g., TCP and RoCE) with complex software stacks for cross-node communication. We present cMPI, the first work to optimize MPI point-to-point communication (both one-sided and two-sided) using CXL memory sharing on a real CXL platform, transforming cross-node communication into memory transactions and data copies within CXL memory, bypassing traditional network protocols. We analyze performance across various interconnects and find that CXL memory sharing achieves 7.2x-8.1x lower latency than TCP-based interconnects deployed in small- and medium-scale clusters. We address challenges of CXL memory sharing for MPI communication, including data object management over the dax representation [50], cache coherence, and atomic operations. Overall, cMPI outperforms TCP over standard Ethernet NIC and high-end SmartNIC by up to 49x and 72x in latency and bandwidth, respectively, for small messages.",
        "authors": [
            "Xi Wang",
            "Bin Ma",
            "Jongryool Kim",
            "Byungil Koh",
            "Hoshik Kim",
            "Dong Li"
        ],
        "categories": [
            "cs.DC",
            "cs.AR",
            "cs.NI"
        ],
        "submit_date": "2025-10-07"
    },
    "http://arxiv.org/abs/2510.05254v1": {
        "title": "Performance of a high-order MPI-Kokkos accelerated fluid solver",
        "link": "http://arxiv.org/abs/2510.05254v1",
        "abstract": "This work discusses the performance of a modern numerical scheme for fluid dynamical problems on modern high-performance computing architectures. Our code implements a spatial nodal discontinuous Galerkin scheme that we test up to an order of convergence of eight. It is temporally coupled to a set of Runge-Kutta methods of orders up to six. The code integrates the linear advection equations as well as the isothermal Euler equations in one, two, and three dimensions. In order to target modern hardware involving many-core Central Processing Units and accelerators such as Graphic Processing Units we use the Kokkos library in conjunction with the Message Passing Interface to run our single source code on various GPU systems. We find that the higher the order the faster is the code. Eighth-order simulations attain a given global error with much less computing time than third- or fourth-order simulations. The RK scheme has a smaller impact on the code performance and a classical fourth-order scheme seems to generally be a good choice. The code performs very well on all considered GPUs. The many-CPU performance is also very good and perfect weak scaling is observed up to many hundreds of CPU cores using MPI. We note that small grid-size simulations are faster on CPUs than on GPUs while GPUs win significantly over CPUs for simulations involving more than $10^7$ degrees of freedom ($\\approx 3100^2$ grid points). When it comes to the environmental impact of numerical simulations we estimate that GPUs consume less energy than CPUs for large grid-size simulations but more energy on small grids. We observe a tendency that the more modern is the GPU the larger needs to be the grid in order to use it efficiently. This yields a rebound effect because larger simulations need longer computing times and in turn more energy that is not compensated by the energy efficiency gain of the newer GPUs.",
        "authors": [
            "Filipp Sporykhin",
            "Holger Homann"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.05254v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-06"
    },
    "http://arxiv.org/abs/2510.05068v1": {
        "title": "Multi-Agent Distributed Optimization With Feasible Set Privacy",
        "link": "http://arxiv.org/abs/2510.05068v1",
        "abstract": "We consider the problem of decentralized constrained optimization with multiple agents $E_1,\\ldots,E_N$ who jointly wish to learn the optimal solution set while keeping their feasible sets $\\mathcal{P}_1,\\ldots,\\mathcal{P}_N$ private from each other. We assume that the objective function $f$ is known to all agents and each feasible set is a collection of points from a universal alphabet $\\mathcal{P}_{alph}$. A designated agent (leader) starts the communication with the remaining (non-leader) agents, and is the first to retrieve the solution set. The leader searches for the solution by sending queries to and receiving answers from the non-leaders, such that the information on the individual feasible sets revealed to the leader should be no more than nominal, i.e., what is revealed from learning the solution set alone. We develop achievable schemes for obtaining the solution set at nominal information leakage, and characterize their communication costs under two communication setups between agents. In this work, we focus on two kinds of network setups: i) ring, where each agent communicates with two adjacent agents, and ii) star, where only the leader communicates with the remaining agents. We show that, if the leader first learns the joint feasible set through an existing private set intersection (PSI) protocol and then deduces the solution set, the information leaked to the leader is greater than nominal. Moreover, we draw connection of our schemes to threshold PSI (ThPSI), which is a PSI-variant where the intersection is revealed only when its cardinality is larger than a threshold value. Finally, for various realizations of $f$ mapped uniformly at random to a fixed range of values, our schemes are more communication-efficient with a high probability compared to retrieving the entire feasible set through PSI.",
        "authors": [
            "Shreya Meel",
            "Sennur Ulukus"
        ],
        "categories": [
            "cs.IT",
            "cs.CR",
            "cs.DC",
            "cs.NI",
            "eess.SP"
        ],
        "id": "http://arxiv.org/abs/2510.05068v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-06"
    },
    "http://arxiv.org/abs/2510.04952v2": {
        "title": "Safe and Compliant Cross-Market Trade Execution via Constrained RL and Zero-Knowledge Audits",
        "link": "http://arxiv.org/abs/2510.04952v2",
        "abstract": "We present a cross-market algorithmic trading system that balances execution quality with rigorous compliance enforcement. The architecture comprises a high-level planner, a reinforcement learning execution agent, and an independent compliance agent. We formulate trade execution as a constrained Markov decision process with hard constraints on participation limits, price bands, and self-trading avoidance. The execution agent is trained with proximal policy optimization, while a runtime action-shield projects any unsafe action into a feasible set. To support auditability without exposing proprietary signals, we add a zero-knowledge compliance audit layer that produces cryptographic proofs that all actions satisfied the constraints. We evaluate in a multi-venue, ABIDES-based simulator and compare against standard baselines (e.g., TWAP, VWAP). The learned policy reduces implementation shortfall and variance while exhibiting no observed constraint violations across stress scenarios including elevated latency, partial fills, compliance module toggling, and varying constraint limits. We report effects at the 95% confidence level using paired t-tests and examine tail risk via CVaR. We situate the work at the intersection of optimal execution, safe reinforcement learning, regulatory technology, and verifiable AI, and discuss ethical considerations, limitations (e.g., modeling assumptions and computational overhead), and paths to real-world deployment.",
        "authors": [
            "Ailiya Borjigin",
            "Cong He"
        ],
        "categories": [
            "cs.AI",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.04952v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-06"
    },
    "http://arxiv.org/abs/2510.04665v1": {
        "title": "Interactive High-Performance Visualization for Astronomy and Cosmology",
        "link": "http://arxiv.org/abs/2510.04665v1",
        "abstract": "The exponential growth of data in Astrophysics and Cosmology demands scalable computational tools and intuitive interfaces for analysis and visualization. In this work, we present an innovative integration of the VisIVO scientific visualization framework with the InterActive Computing (IAC) service at Cineca, enabling interactive, high-performance visual workflows directly within HPC environments. Through seamless integration into Jupyter-based science gateways, users can now access GPU-enabled compute nodes to perform complex 3D visualizations using VisIVO via custom Python wrappers and preconfigured interactive notebooks. We demonstrate how this infrastructure simplifies access to advanced HPC resources, enhances reproducibility, and accelerates exploratory workflows in astronomical research. Our approach has been validated through a set of representative use cases involving large-scale simulations from the GADGET code, highlighting the effectiveness of this system in visualizing the large-scale structure of the Universe. This work exemplifies how science gateways can bridge domain-specific tools and advanced infrastructures, fostering user-centric, scalable, and reproducible research environments.",
        "authors": [
            "Eva Sciacca",
            "Nicola Tuccari",
            "Umer Arshad",
            "Fabio Pitari",
            "Giuseppa Muscianisi",
            "Emiliano Tramontana"
        ],
        "categories": [
            "astro-ph.IM",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.04665v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-06"
    },
    "http://arxiv.org/abs/2510.04644v1": {
        "title": "The R(1)W(1) Communication Model for Self-Stabilizing Distributed Algorithms",
        "link": "http://arxiv.org/abs/2510.04644v1",
        "abstract": "Self-stabilization is a versatile methodology in the design of fault-tolerant distributed algorithms for transient faults. A self-stabilizing system automatically recovers from any kind and any finite number of transient faults. This property is specifically useful in modern distributed systems with a large number of components. In this paper, we propose a new communication and execution model named the R(1)W(1) model in which each process can read and write its own and neighbors' local variables in a single step. We propose self-stabilizing distributed algorithms in the R(1)W(1) model for the problems of maximal matching, minimal k-dominating set and maximal k-dependent set. Finally, we propose an example transformer, based on randomized distance-two local mutual exclusion, to simulate algorithms designed for the R(1)W(1) model in the synchronous message passing model with synchronized clocks.",
        "authors": [
            "Hirotsugu Kakugawa",
            "Sayaka Kamei",
            "Masahiro Shibata",
            "Fukuhito Ooshita"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.04644v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-06"
    },
    "http://arxiv.org/abs/2510.04478v2": {
        "title": "Overlapping Schwarz Scheme for Linear-Quadratic Programs in Continuous Time",
        "link": "http://arxiv.org/abs/2510.04478v2",
        "abstract": "We present an optimize-then-discretize framework for solving linear-quadratic optimal control problems (OCP) governed by time-inhomogeneous ordinary differential equations (ODEs). Our method employs a modified overlapping Schwarz decomposition based on the Pontryagin Minimum Principle, partitioning the temporal domain into overlapping intervals and independently solving Hamiltonian systems in continuous time. We demonstrate that the convergence is ensured by appropriately updating the boundary conditions of the individual Hamiltonian dynamics. The cornerstone of our analysis is to prove that the exponential decay of sensitivity (EDS) exhibited in discrete-time OCPs carries over to the continuous-time setting. Unlike the discretize-then-optimize approach, our method can flexibly incorporate different numerical integration methods for solving the resulting Hamiltonian two-point boundary-value subproblems, including adaptive-time integrators. A numerical experiment on a linear-quadratic OCP illustrates the practicality of our approach in broad scientific applications.",
        "authors": [
            "Hongli Zhao",
            "Mihai Anitescu",
            "Sen Na"
        ],
        "categories": [
            "math.OC",
            "cs.CE",
            "cs.DC",
            "math.DS",
            "math.NA"
        ],
        "id": "http://arxiv.org/abs/2510.04478v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-06"
    },
    "http://arxiv.org/abs/2510.05186v1": {
        "id": "http://arxiv.org/abs/2510.05186v1",
        "title": "OptPipe: Memory- and Scheduling-Optimized Pipeline Parallelism for LLM Training",
        "link": "http://arxiv.org/abs/2510.05186v1",
        "tags": [
            "training",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-10-01",
        "tldr": "Optimizes pipeline parallelism for LLM training by formulating scheduling as a constrained optimization problem that balances memory, computation, and bubble minimization. Dynamically adjusts activation offloading and scheduling to reduce idle time by up to 50% under fixed memory constraints.",
        "abstract": "Pipeline parallelism (PP) has become a standard technique for scaling large language model (LLM) training across multiple devices. However, despite recent progress in reducing memory consumption through activation offloading, existing approaches remain largely heuristic and coarse-grained, often overlooking the fine-grained trade-offs between memory, computation, and scheduling latency. In this work, we revisit the pipeline scheduling problem from a principled optimization perspective. We observe that prevailing strategies either rely on static rules or aggressively offload activations without fully leveraging the interaction between memory constraints and scheduling efficiency. To address this, we formulate scheduling as a constrained optimization problem that jointly accounts for memory capacity, activation reuse, and pipeline bubble minimization. Solving this model yields fine-grained schedules that reduce pipeline bubbles while adhering to strict memory budgets. Our approach complements existing offloading techniques: whereas prior approaches trade memory for time in a fixed pattern, we dynamically optimize the tradeoff with respect to model structure and hardware configuration. Experimental results demonstrate that our method consistently improves both throughput and memory utilization. In particular, we reduce idle pipeline time by up to 50% under the same per-device memory limit, and in some cases, enable the training of larger models within limited memory budgets.",
        "authors": [
            "Hongpei Li",
            "Han Zhang",
            "Huikang Liu",
            "Dongdong Ge",
            "Yinyu Ye"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "math.OC"
        ],
        "submit_date": "2025-10-06"
    },
    "http://arxiv.org/abs/2510.04404v2": {
        "id": "http://arxiv.org/abs/2510.04404v2",
        "title": "Next-Generation Event-Driven Architectures: Performance, Scalability, and Intelligent Orchestration Across Messaging Frameworks",
        "link": "http://arxiv.org/abs/2510.04404v2",
        "tags": [
            "networking",
            "serving",
            "offline"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Studies performance and scalability of event-driven architectures for AI inference pipelines. Introduces AIEO with ML-driven predictive scaling and RL-based resource allocation. Achieves 34% latency reduction, 28% resource utilization improvement across messaging systems.",
        "abstract": "Modern distributed systems demand low-latency, fault-tolerant event processing that exceeds traditional messaging architecture limits. While frameworks including Apache Kafka, RabbitMQ, Apache Pulsar, NATS JetStream, and serverless event buses have matured significantly, no unified comparative study evaluates them holistically under standardized conditions. This paper presents the first comprehensive benchmarking framework evaluating 12 messaging systems across three representative workloads: e-commerce transactions, IoT telemetry ingestion, and AI inference pipelines. We introduce AIEO (AI-Enhanced Event Orchestration), employing machine learning-driven predictive scaling, reinforcement learning for dynamic resource allocation, and multi-objective optimization. Our evaluation reveals fundamental trade-offs: Apache Kafka achieves peak throughput (1.2M messages/sec, 18ms p95 latency) but requires substantial operational expertise; Apache Pulsar provides balanced performance (950K messages/sec, 22ms p95) with superior multi-tenancy; serverless solutions offer elastic scaling for variable workloads despite higher baseline latency (80-120ms p95). AIEO demonstrates 34\\% average latency reduction, 28\\% resource utilization improvement, and 42% cost optimization across all platforms. We contribute standardized benchmarking methodologies, open-source intelligent orchestration, and evidence-based decision guidelines. The evaluation encompasses 2,400+ experimental configurations with rigorous statistical analysis, providing comprehensive performance characterization and establishing foundations for next-generation distributed system design.",
        "authors": [
            "Jahidul Arafat",
            "Fariha Tasmin",
            "Sanjaya Poudel"
        ],
        "categories": [
            "cs.DC",
            "cs.PF"
        ],
        "submit_date": "2025-10-06"
    },
    "http://arxiv.org/abs/2510.04371v1": {
        "id": "http://arxiv.org/abs/2510.04371v1",
        "title": "Speculative Actions: A Lossless Framework for Faster Agentic Systems",
        "link": "http://arxiv.org/abs/2510.04371v1",
        "tags": [
            "agentic",
            "serving",
            "thinking"
        ],
        "relevant": true,
        "indexed_date": "2025-10-01",
        "tldr": "Proposes speculative actions to accelerate agentic systems by predicting future actions with faster models, enabling parallel execution. Achieves up to 55% accuracy in action prediction and significantly reduces end-to-end latency in real-world environments.",
        "abstract": "Despite growing interest in AI agents across industry and academia, their execution in an environment is often slow, hampering training, evaluation, and deployment. For example, a game of chess between two state-of-the-art agents may take hours. A critical bottleneck is that agent behavior unfolds sequentially: each action requires an API call, and these calls can be time-consuming. Inspired by speculative execution in microprocessors and speculative decoding in LLM inference, we propose speculative actions, a lossless framework for general agentic systems that predicts likely actions using faster models, enabling multiple steps to be executed in parallel. We evaluate this framework across three agentic environments: gaming, e-commerce, web search, and a \"lossy\" extension for an operating systems environment. In all cases, speculative actions achieve substantial accuracy in next-action prediction (up to 55%), translating into significant reductions in end-to-end latency. Moreover, performance can be further improved through stronger guessing models, top-K action prediction, multi-step speculation, and uncertainty-aware optimization, opening a promising path toward deploying low-latency agentic systems in the real world.",
        "authors": [
            "Naimeng Ye",
            "Arnav Ahuja",
            "Georgios Liargkovas",
            "Yunan Lu",
            "Kostis Kaffes",
            "Tianyi Peng"
        ],
        "categories": [
            "cs.AI",
            "cs.DC",
            "cs.MA"
        ],
        "submit_date": "2025-10-05"
    },
    "http://arxiv.org/abs/2510.04310v1": {
        "title": "Beyond Canonical Rounds: Communication Abstractions for Optimal Byzantine Resilience",
        "link": "http://arxiv.org/abs/2510.04310v1",
        "abstract": "We study communication abstractions for asynchronous Byzantine fault tolerance with optimal failure resilience, where $n > 3f$. Two classic patterns -- canonical asynchronous rounds and communication-closed layers -- have long been considered as general frameworks for designing distributed algorithms, making asynchronous executions appear synchronous and enabling modular reasoning.   We show that these patterns are inherently limited in the critical resilience regime $3f < n \\le 5f$. Several key tasks -- such as approximate and crusader agreement, reliable broadcast and gather -- cannot be solved by bounded-round canonical-round algorithms, and are unsolvable if communication closure is imposed. These results explain the historical difficulty of achieving optimal-resilience algorithms within round-based frameworks.   On the positive side, we show that the gather abstraction admits constant-time solutions with optimal resilience ($n > 3f$), and supports modular reductions. Specifically, we present the first optimally-resilient algorithm for connected consensus by reducing it to gather.   Our results demonstrate that while round-based abstractions are analytically convenient, they obscure the true complexity of Byzantine fault-tolerant algorithms. Richer communication patterns such as gather provide a better foundation for modular, optimal-resilience design.",
        "authors": [
            "Hagit Attiya",
            "Itay Flam",
            "Jennifer L. Welch"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.04310v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-05"
    },
    "http://arxiv.org/abs/2510.04186v3": {
        "title": "From Patchwork to Network: A Comprehensive Framework for Demand Analysis and Fleet Optimization of Urban Air Mobility",
        "link": "http://arxiv.org/abs/2510.04186v3",
        "abstract": "Urban Air Mobility (UAM) presents a transformative vision for metropolitan transportation, but its practical implementation is hindered by substantial infrastructure costs and operational complexities. We address these challenges by modeling a UAM network that leverages existing regional airports and operates with an optimized, heterogeneous fleet of aircraft. We introduce LPSim, a Large-Scale Parallel Simulation framework that utilizes multi-GPU computing to co-optimize UAM demand, fleet operations, and ground transportation interactions simultaneously.   Our equilibrium search algorithm is extended to accurately forecast demand and determine the most efficient fleet composition. Applied to a case study of the San Francisco Bay Area, our results demonstrate that this UAM model can yield over 20 minutes' travel time savings for 230,000 selected trips. However, the analysis also reveals that system-wide success is critically dependent on seamless integration with ground access and dynamic scheduling.",
        "authors": [
            "Xuan Jiang",
            "Xuanyu Zhou",
            "Yibo Zhao",
            "Shangqing Cao",
            "Dingyi Zhuang",
            "Jinhua Zhao",
            "Haris Koutsopoulos",
            "Shenhao Wang",
            "Mark Hansen",
            "Raja Sengupta"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.04186v3",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-05"
    },
    "http://arxiv.org/abs/2510.03970v1": {
        "title": "Towards Carbon-Aware Container Orchestration: Predicting Workload Energy Consumption with Federated Learning",
        "link": "http://arxiv.org/abs/2510.03970v1",
        "abstract": "The growing reliance on large-scale data centers to run resource-intensive workloads has significantly increased the global carbon footprint, underscoring the need for sustainable computing solutions. While container orchestration platforms like Kubernetes help optimize workload scheduling to reduce carbon emissions, existing methods often depend on centralized machine learning models that raise privacy concerns and struggle to generalize across diverse environments. In this paper, we propose a federated learning approach for energy consumption prediction that preserves data privacy by keeping sensitive operational data within individual enterprises. By extending the Kubernetes Efficient Power Level Exporter (Kepler), our framework trains XGBoost models collaboratively across distributed clients using Flower's FedXgbBagging aggregation using a bagging strategy, eliminating the need for centralized data sharing. Experimental results on the SPECPower benchmark dataset show that our FL-based approach achieves 11.7 percent lower Mean Absolute Error compared to a centralized baseline. This work addresses the unresolved trade-off between data privacy and energy prediction efficiency in prior systems such as Kepler and CASPER and offers enterprises a viable pathway toward sustainable cloud computing without compromising operational privacy.",
        "authors": [
            "Zainab Saad",
            "Jialin Yang",
            "Henry Leung",
            "Steve Drew"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "id": "http://arxiv.org/abs/2510.03970v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-04"
    },
    "http://arxiv.org/abs/2510.05164v1": {
        "id": "http://arxiv.org/abs/2510.05164v1",
        "title": "SATER: A Self-Aware and Token-Efficient Approach to Routing and Cascading",
        "link": "http://arxiv.org/abs/2510.05164v1",
        "tags": [
            "serving",
            "thinking",
            "MoE"
        ],
        "relevant": true,
        "indexed_date": "2025-10-01",
        "tldr": "How to reduce cost and latency of LLM inference while maintaining performance? SATER introduces a self-aware routing and cascade framework with confidence-aware rejection and preference optimization, cutting computational cost by >50% and cascade latency by >80%.",
        "abstract": "Large language models (LLMs) demonstrate remarkable performance across diverse tasks, yet their effectiveness frequently depends on costly commercial APIs or cloud services. Model selection thus entails a critical trade-off between performance and cost: high-performing LLMs typically incur substantial expenses, whereas budget-friendly small language models (SLMs) are constrained by limited capabilities. Current research primarily proposes two routing strategies: pre-generation routing and cascade routing. Both approaches have distinct characteristics, with cascade routing typically offering superior cost-effectiveness and accuracy despite its higher latency. To further address the limitations of both approaches, we introduce SATER, a dual-mode compatible approach that fine-tunes models through shortest-response preference optimization and a confidence-aware rejection mechanism. SATER significantly reduces redundant outputs and response times, while improving both the performance of pre-generation routing and the efficiency of cascade routing. Experiments across three SLMs and six datasets, varying in type and complexity, demonstrate that SATER achieves comparable performance while consistently reducing computational costs by over 50\\% and cascade latency by over 80\\%.",
        "authors": [
            "Yuanzhe Shen",
            "Yide Liu",
            "Zisu Huang",
            "Ruicheng Yin",
            "Xiaoqing Zheng",
            "Xuanjing Huang"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.LG"
        ],
        "submit_date": "2025-10-04"
    },
    "http://arxiv.org/abs/2510.03915v1": {
        "title": "OpenFLAME: Federated Visual Positioning System to Enable Large-Scale Augmented Reality Applications",
        "link": "http://arxiv.org/abs/2510.03915v1",
        "abstract": "World-scale augmented reality (AR) applications need a ubiquitous 6DoF localization backend to anchor content to the real world consistently across devices. Large organizations such as Google and Niantic are 3D scanning outdoor public spaces in order to build their own Visual Positioning Systems (VPS). These centralized VPS solutions fail to meet the needs of many future AR applications -- they do not cover private indoor spaces because of privacy concerns, regulations, and the labor bottleneck of updating and maintaining 3D scans. In this paper, we present OpenFLAME, a federated VPS backend that allows independent organizations to 3D scan and maintain a separate VPS service for their own spaces. This enables access control of indoor 3D scans, distributed maintenance of the VPS backend, and encourages larger coverage. Sharding of VPS services introduces several unique challenges -- coherency of localization results across spaces, quality control of VPS services, selection of the right VPS service for a location, and many others. We introduce the concept of federated image-based localization and provide reference solutions for managing and merging data across maps without sharing private data.",
        "authors": [
            "Sagar Bharadwaj",
            "Harrison Williams",
            "Luke Wang",
            "Michael Liang",
            "Tao Jin",
            "Srinivasan Seshan",
            "Anthony Rowe"
        ],
        "categories": [
            "cs.CV",
            "cs.DC",
            "cs.RO"
        ],
        "id": "http://arxiv.org/abs/2510.03915v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-04"
    },
    "http://arxiv.org/abs/2510.03891v1": {
        "id": "http://arxiv.org/abs/2510.03891v1",
        "title": "Toward Co-adapting Machine Learning Job Shape and Cluster Topology",
        "link": "http://arxiv.org/abs/2510.03891v1",
        "tags": [
            "training",
            "networking",
            "sparse"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Addresses resource allocation for distributed ML jobs in torus-topology clusters. Proposes RFold, adapting job shapes and cluster topology via homomorphic shape identification and optical circuit switch reconfiguration. Achieves 57% higher utilization and up to 11× lower job completion time.",
        "abstract": "Allocating resources to distributed machine learning jobs in multi-tenant torus-topology clusters must meet each job's specific placement and communication requirements, which are typically described using shapes. There is an inherent tension between minimizing network contention and maximizing cluster utilization when placing various-shaped jobs. While existing schedulers typically optimize for one objective at the expense of the other, we demonstrate that both can be achieved simultaneously.   Our proposed approach, RFold, adapts both job shapes and the underlying cluster topology at runtime. This is accomplished by combining two techniques: (1) identifying homomorphic job shapes that support the jobs communication needs, and (2) reconfiguring the optical circuit switch-enabled topology to support more diverse job shapes. Preliminary evaluation performed on a 4096-node torus cluster simulator indicates that RFold can improve absolute cluster utilization by 57% and reduce job completion time by up to 11x relative to existing methods",
        "authors": [
            "Shawn Shuoshuo Chen",
            "Daiyaan Arfeen",
            "Minlan Yu",
            "Peter Steenkiste",
            "Srinivasan Seshan"
        ],
        "categories": [
            "cs.DC",
            "cs.NI"
        ],
        "submit_date": "2025-10-04"
    },
    "http://arxiv.org/abs/2510.03872v2": {
        "id": "http://arxiv.org/abs/2510.03872v2",
        "title": "Datacenter Energy Optimized Power Profiles",
        "link": "http://arxiv.org/abs/2510.03872v2",
        "tags": [
            "training",
            "hardware",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Introduces datacenter power profiles for NVIDIA Blackwell GPUs to optimize energy efficiency in AI/HPC workloads. Employs hardware-software innovations and application-aware optimization recipes. Achieves up to 15% energy savings and 13% throughput increase within power constraints.",
        "abstract": "This paper presents datacenter power profiles, a new NVIDIA software feature released with Blackwell B200, aimed at improving energy efficiency and/or performance. The initial feature provides coarse-grain user control for HPC and AI workloads leveraging hardware and software innovations for intelligent power management and domain knowledge of HPC and AI workloads. The resulting workload-aware optimization recipes maximize computational throughput while operating within strict facility power constraints. The phase-1 Blackwell implementation achieves up to 15% energy savings while maintaining performance levels above 97% for critical applications, enabling an overall throughput increase of up to 13% in a power-constrained facility.   KEYWORDS GPU power management, energy efficiency, power profile, HPC optimization, Max-Q, Blackwell architecture",
        "authors": [
            "Sreedhar Narayanaswamy",
            "Pratikkumar Dilipkumar Patel",
            "Ian Karlin",
            "Apoorv Gupta",
            "Sudhir Saripalli",
            "Janey Guo"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-10-04"
    },
    "http://arxiv.org/abs/2510.03744v1": {
        "title": "HydroFusion-LMF: Semi-Supervised Multi-Network Fusion with Large-Model Adaptation for Long-Term Daily Runoff Forecasting",
        "link": "http://arxiv.org/abs/2510.03744v1",
        "abstract": "Accurate decade-scale daily runoff forecasting in small watersheds is difficult because signals blend drifting trends, multi-scale seasonal cycles, regime shifts, and sparse extremes. Prior deep models (DLinear, TimesNet, PatchTST, TiDE, Nonstationary Transformer, LSTNet, LSTM) usually target single facets and under-utilize unlabeled spans, limiting regime adaptivity. We propose HydroFusion-LMF, a unified framework that (i) performs a learnable trend-seasonal-residual decomposition to reduce non-stationarity, (ii) routes residuals through a compact heterogeneous expert set (linear refinement, frequency kernel, patch Transformer, recurrent memory, dynamically normalized attention), (iii) fuses expert outputs via a hydrologic context-aware gate conditioned on day-of-year phase, antecedent precipitation, local variance, flood indicators, and static basin attributes, and (iv) augments supervision with a semi-supervised multi-task objective (composite MSE/MAE + extreme emphasis + NSE/KGE, masked reconstruction, multi-scale contrastive alignment, augmentation consistency, variance-filtered pseudo-labeling). Optional adapter / LoRA layers inject a frozen foundation time-series encoder efficiently. On a ~10-year daily dataset HydroFusion-LMF attains MSE 1.0128 / MAE 0.5818, improving the strongest baseline (DLinear) by 10.2% / 10.3% and the mean baseline by 24.6% / 17.1%. We observe simultaneous MSE and MAE reductions relative to baselines. The framework balances interpretability (explicit components, sparse gating) with performance, advancing label-efficient hydrologic forecasting under non-stationarity.",
        "authors": [
            "Qianfei Fan",
            "Jiayu Wei",
            "Peijun Zhu",
            "Wensheng Ye",
            "Meie Fang"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC",
            "cs.NE",
            "physics.geo-ph"
        ],
        "id": "http://arxiv.org/abs/2510.03744v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-04"
    },
    "http://arxiv.org/abs/2510.03625v1": {
        "title": "On the Limits of Consensus under Dynamic Availability and Reconfiguration",
        "link": "http://arxiv.org/abs/2510.03625v1",
        "abstract": "Proof-of-stake blockchains require consensus protocols that support Dynamic Availability and Reconfiguration (so-called DAR setting), where the former means that the consensus protocol should remain live even if a large number of nodes temporarily crash, and the latter means it should be possible to change the set of operating nodes over time. State-of-the-art protocols for the DAR setting, such as Ethereum, Cardano's Ouroboros, or Snow White, require unrealistic additional assumptions, such as social consensus, or that key evolution is performed even while nodes are not participating. In this paper, we identify the necessary and sufficient adversarial condition under which consensus can be achieved in the DAR setting without additional assumptions. We then introduce a new and realistic additional assumption: honest nodes dispose of their cryptographic keys the moment they express intent to exit from the set of operating nodes. To add reconfiguration to any dynamically available consensus protocol, we provide a bootstrapping gadget that is particularly simple and efficient in the common optimistic case of few reconfigurations and no double-spending attempts.",
        "authors": [
            "Joachim Neu",
            "Javier Nieto",
            "Ling Ren"
        ],
        "categories": [
            "cs.CR",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.03625v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-04"
    },
    "http://arxiv.org/abs/2510.03601v1": {
        "title": "MECKD: Deep Learning-Based Fall Detection in Multilayer Mobile Edge Computing With Knowledge Distillation",
        "link": "http://arxiv.org/abs/2510.03601v1",
        "abstract": "The rising aging population has increased the importance of fall detection (FD) systems as an assistive technology, where deep learning techniques are widely applied to enhance accuracy. FD systems typically use edge devices (EDs) worn by individuals to collect real-time data, which are transmitted to a cloud center (CC) or processed locally. However, this architecture faces challenges such as a limited ED model size and data transmission latency to the CC. Mobile edge computing (MEC), which allows computations at MEC servers deployed between EDs and CC, has been explored to address these challenges. We propose a multilayer MEC (MLMEC) framework to balance accuracy and latency. The MLMEC splits the architecture into stations, each with a neural network model. If front-end equipment cannot detect falls reliably, data are transmitted to a station with more robust back-end computing. The knowledge distillation (KD) approach was employed to improve front-end detection accuracy by allowing high-power back-end stations to provide additional learning experiences, enhancing precision while reducing latency and processing loads. Simulation results demonstrate that the KD approach improved accuracy by 11.65% on the SisFall dataset and 2.78% on the FallAllD dataset. The MLMEC with KD also reduced the data latency rate by 54.15% on the FallAllD dataset and 46.67% on the SisFall dataset compared to the MLMEC without KD. In summary, the MLMEC FD system exhibits improved accuracy and reduced latency.",
        "authors": [
            "Wei-Lung Mao",
            "Chun-Chi Wang",
            "Po-Heng Chou",
            "Kai-Chun Liu",
            "Yu Tsao"
        ],
        "categories": [
            "cs.LG",
            "cs.DC",
            "cs.NI",
            "eess.SP"
        ],
        "id": "http://arxiv.org/abs/2510.03601v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-04"
    },
    "http://arxiv.org/abs/2510.03557v1": {
        "title": "Cosmological Hydrodynamics at Exascale: A Trillion-Particle Leap in Capability",
        "link": "http://arxiv.org/abs/2510.03557v1",
        "abstract": "Resolving the most fundamental questions in cosmology requires simulations that match the scale, fidelity, and physical complexity demanded by next-generation sky surveys. To achieve the realism needed for this critical scientific partnership, detailed gas dynamics, along with a host of astrophysical effects, must be treated self-consistently with gravity for end-to-end modeling of structure formation. As an important step on this roadmap, exascale computing enables simulations that span survey-scale volumes while incorporating key subgrid processes that shape complex cosmic structures. We present results from CRK-HACC, a cosmological hydrodynamics code built for the extreme scalability requirements set by modern cosmological surveys. Using separation-of-scale techniques, GPU-resident tree solvers, in situ analysis pipelines, and multi-tiered I/O, CRK-HACC executed Frontier-E: a four trillion particle full-sky simulation, over an order of magnitude larger than previous efforts. The run achieved 513.1 PFLOPs peak performance, processing 46.6 billion particles per second and writing more than 100 PB of data in just over one week of runtime.",
        "authors": [
            "Nicholas Frontiere",
            "J. D. Emberson",
            "Michael Buehlmann",
            "Esteban M. Rangel",
            "Salman Habib",
            "Katrin Heitmann",
            "Patricia Larsen",
            "Vitali Morozov",
            "Adrian Pope",
            "Claude-André Faucher-Giguère",
            "Antigoni Georgiadou",
            "Damien Lebrun-Grandié",
            "Andrey Prokopenko"
        ],
        "categories": [
            "cs.DC",
            "astro-ph.CO",
            "astro-ph.IM",
            "cs.PF",
            "physics.comp-ph"
        ],
        "id": "http://arxiv.org/abs/2510.03557v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-03"
    },
    "http://arxiv.org/abs/2510.03513v1": {
        "title": "A Lightweight Federated Learning Approach for Privacy-Preserving Botnet Detection in IoT",
        "link": "http://arxiv.org/abs/2510.03513v1",
        "abstract": "The rapid growth of the Internet of Things (IoT) has expanded opportunities for innovation but also increased exposure to botnet-driven cyberattacks. Conventional detection methods often struggle with scalability, privacy, and adaptability in resource-constrained IoT environments. To address these challenges, we present a lightweight and privacy-preserving botnet detection framework based on federated learning. This approach enables distributed devices to collaboratively train models without exchanging raw data, thus maintaining user privacy while preserving detection accuracy. A communication-efficient aggregation strategy is introduced to reduce overhead, ensuring suitability for constrained IoT networks. Experiments on benchmark IoT botnet datasets demonstrate that the framework achieves high detection accuracy while substantially reducing communication costs. These findings highlight federated learning as a practical path toward scalable, secure, and privacy-aware intrusion detection for IoT ecosystems.",
        "authors": [
            "Taha M. Mahmoud",
            "Naima Kaabouch"
        ],
        "categories": [
            "cs.LG",
            "cs.CR",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.03513v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-03"
    },
    "http://arxiv.org/abs/2510.03491v1": {
        "id": "http://arxiv.org/abs/2510.03491v1",
        "title": "Short-circuiting Rings for Low-Latency AllReduce",
        "link": "http://arxiv.org/abs/2510.03491v1",
        "tags": [
            "training",
            "networking",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Challenges the assumption that Recursive Doubling is superior to Ring for small AllReduce messages in GPU clusters. Proposes a heuristic for circuit-switching photonic interconnects to dynamically balance reconfiguration delays and congestion. Achieves faster completion times than static Ring AllReduce in evaluations with realistic delays.",
        "abstract": "Efficient collective communication is critical for many distributed ML and HPC applications. In this context, it is widely believed that the Ring algorithm for the AllReduce collective communication operation is optimal only for large messages, while Recursive Doubling is preferable for small ones due to its logarithmic number of steps compared to the linear number for Ring. In this paper, we challenge this long-held assumption and show that the Ring algorithm can remain optimal even for short messages in ring-based GPU-to-GPU topologies, once realistic propagation delays and link capacity constraints are accounted for. We find that the total propagation delay for both Ring and Recursive Doubling essentially sums to the same value, but the latter incurs significantly higher congestion due to longer hop counts, leading to increased completion times. This surprising result motivates our case for in-collective adaptive topologies, particularly in the context of emerging photonic interconnects, which can break through the limitations of static topology designs at the collective communication granularity. We design a \\emph{simple and fast} heuristic for circuit-switching that enables Recursive Doubling to exploit dynamically reconfigurable photonic paths, carefully balancing reconfiguration delays, propagation latencies, and link congestion to minimize overall completion time. Our preliminary evaluations, using realistic reconfiguration delays, show that our circuit-switching schedules enable faster completion times for Recursive Doubling, even compared to Ring AllReduce on static ring topologies. We conclude by highlighting key challenges and future research directions for realizing practical, in-collective photonic switching.",
        "authors": [
            "Sarah-Michelle Hammer",
            "Stefan Schmid",
            "Rachee Singh",
            "Vamsi Addanki"
        ],
        "categories": [
            "cs.NI",
            "cs.DC"
        ],
        "submit_date": "2025-10-03"
    },
    "http://arxiv.org/abs/2510.03434v1": {
        "id": "http://arxiv.org/abs/2510.03434v1",
        "title": "Paris: A Decentralized Trained Open-Weight Diffusion Model",
        "link": "http://arxiv.org/abs/2510.03434v1",
        "tags": [
            "diffusion",
            "MoE"
        ],
        "relevant": true,
        "indexed_date": "2025-10-01",
        "tldr": "Can high-quality diffusion models be trained decentralized without gradient synchronization? Paris uses 8 isolated expert diffusion models with a router for inference, achieving comparable quality to centralized baselines using 16× less compute and 14× less data.",
        "abstract": "We present Paris, the first publicly released diffusion model pre-trained entirely through decentralized computation. Paris demonstrates that high-quality text-to-image generation can be achieved without centrally coordinated infrastructure. Paris is open for research and commercial use. Paris required implementing our Distributed Diffusion Training framework from scratch. The model consists of 8 expert diffusion models (129M-605M parameters each) trained in complete isolation with no gradient, parameter, or intermediate activation synchronization. Rather than requiring synchronized gradient updates across thousands of GPUs, we partition data into semantically coherent clusters where each expert independently optimizes its subset while collectively approximating the full distribution. A lightweight transformer router dynamically selects appropriate experts at inference, achieving generation quality comparable to centrally coordinated baselines. Eliminating synchronization enables training on heterogeneous hardware without specialized interconnects. Empirical validation confirms that Paris's decentralized training maintains generation quality while removing the dedicated GPU cluster requirement for large-scale diffusion models. Paris achieves this using 14$\\times$ less training data and 16$\\times$ less compute than the prior decentralized baseline.",
        "authors": [
            "Zhiying Jiang",
            "Raihan Seraj",
            "Marcos Villagra",
            "Bidhan Roy"
        ],
        "categories": [
            "cs.GR",
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-10-03"
    },
    "http://arxiv.org/abs/2510.03000v2": {
        "title": "Sensors in viticulture: functions, benefits, and data-driven insights",
        "link": "http://arxiv.org/abs/2510.03000v2",
        "abstract": "Use of sensors and related analytical predictions can be a powerful tool in providing data-informed input to viticulturalists' decision process, complementing their vineyard observations and intuition. Their up-to-date measurements, predictions, and alerts offer actionable insights and suggestions for managing key vineyard operations, such as irrigation, disease and pest control, canopy management, and harvest timing. In many cases, anticipatory interventions can mitigate risks before problems become apparent. By offering guidance on the targeting, timing, and dosage of vineyard practices, sensor data platforms can enhance operational effectiveness and efficiency while conserving labor and resources when they are not required. They also enable implementation of the principles of precision viticulture - doing the right thing, at the right time, in the right place. This paper provides a succinct summary of the functions, benefits, and practical considerations of sensor data platforms in viticulture. It may be of interest to viticulturalists as well as agricultural and IoT researchers.",
        "authors": [
            "Milan Milenkovic"
        ],
        "categories": [
            "cs.CY",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.03000v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-03"
    },
    "http://arxiv.org/abs/2510.02930v2": {
        "title": "iDDS: Intelligent Distributed Dispatch and Scheduling for Workflow Orchestration",
        "link": "http://arxiv.org/abs/2510.02930v2",
        "abstract": "The intelligent Distributed Dispatch and Scheduling (iDDS) service is a versatile workflow orchestration system designed for large-scale, distributed scientific computing. iDDS extends traditional workload and data management by integrating data-aware execution, conditional logic, and programmable workflows, enabling automation of complex and dynamic processing pipelines. Originally developed for the ATLAS experiment at the Large Hadron Collider, iDDS has evolved into an experiment-agnostic platform that supports both template-driven workflows and a Function-as-a-Task model for Python-based orchestration.   This paper presents the architecture and core components of iDDS, highlighting its scalability, modular message-driven design, and integration with systems such as PanDA and Rucio. We demonstrate its versatility through real-world use cases: fine-grained tape resource optimization for ATLAS, orchestration of large Directed Acyclic Graph (DAG) workflows for the Rubin Observatory, distributed hyperparameter optimization for machine learning applications, active learning for physics analyses, and AI-assisted detector design at the Electron-Ion Collider.   By unifying workload scheduling, data movement, and adaptive decision-making, iDDS reduces operational overhead and enables reproducible, high-throughput workflows across heterogeneous infrastructures. We conclude with current challenges and future directions, including interactive, cloud-native, and serverless workflow support.",
        "authors": [
            "Wen Guan",
            "Tadashi Maeno",
            "Aleksandr Alekseev",
            "Fernando Harald Barreiro Megino",
            "Kaushik De",
            "Edward Karavakis",
            "Alexei Klimentov",
            "Tatiana Korchuganova",
            "FaHui Lin",
            "Paul Nilsson",
            "Torre Wenaus",
            "Zhaoyu Yang",
            "Xin Zhao"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.02930v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-03"
    },
    "http://arxiv.org/abs/2510.02894v1": {
        "title": "PyRadiomics-cuda: a GPU-accelerated 3D features extraction from medical images within PyRadiomics",
        "link": "http://arxiv.org/abs/2510.02894v1",
        "abstract": "PyRadiomics-cuda is a GPU-accelerated extension of the PyRadiomics library, designed to address the computational challenges of extracting three-dimensional shape features from medical images. By offloading key geometric computations to GPU hardware it dramatically reduces processing times for large volumetric datasets. The system maintains full compatibility with the original PyRadiomics API, enabling seamless integration into existing AI workflows without code modifications. This transparent acceleration facilitates efficient, scalable radiomics analysis, supporting rapid feature extraction essential for high-throughput AI pipeline. Tests performed on a typical computational cluster, budget and home devices prove usefulness in all scenarios. PyRadiomics-cuda is implemented in Python and C/CUDA and is freely available under the BSD license at https://github.com/mis-wut/pyradiomics-CUDA Additionally PyRadiomics-cuda test suite is available at https://github.com/mis-wut/pyradiomics-cuda-data-gen. It provides detailed handbook and sample scripts suited for different kinds of workflows plus detailed installation instructions. The dataset used for testing is available at Kaggle https://www.kaggle.com/datasets/sabahesaraki/kidney-tumor-segmentation-challengekits-19",
        "authors": [
            "Jakub Lisowski",
            "Piotr Tyrakowski",
            "Szymon Zyguła",
            "Krzysztof Kaczmarski"
        ],
        "categories": [
            "cs.DC",
            "cs.CV"
        ],
        "id": "http://arxiv.org/abs/2510.02894v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-03"
    },
    "http://arxiv.org/abs/2510.02882v1": {
        "id": "http://arxiv.org/abs/2510.02882v1",
        "title": "Energy Efficiency in Cloud-Based Big Data Processing for Earth Observation: Gap Analysis and Future Directions",
        "link": "http://arxiv.org/abs/2510.02882v1",
        "tags": [
            "training",
            "quantization",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Addresses energy inefficiency in cloud-based big data processing for Earth Observation with foundation models. Proposes energy-aware monitoring, optimization techniques, and task scheduling for distributed frameworks. Aims to reduce power consumption and carbon footprint while maintaining processing performance.",
        "abstract": "Earth observation (EO) data volumes are rapidly increasing. While cloud computing are now used for processing large EO datasets, the energy efficiency aspects of such a processing have received much less attention. This issue is notable given the increasing awareness of energy costs and carbon footprint in big data processing, particularly with increased attention on compute-intensive foundation models. In this paper we identify gaps in energy efficiency practices within cloud-based EO big data (EOBD) processing and propose several research directions for improvement. We first examine the current EOBD landscape, focus on the requirements that necessitate cloud-based processing and analyze existing cloud-based EOBD solutions. We then investigate energy efficiency strategies that have been successfully employed in well-studied big data domains. Through this analysis, we identify several critical gaps in existing EOBD processing platforms, which primarily focus on data accessibility and computational feasibility, instead of energy efficiency. These gaps include insufficient energy monitoring mechanisms, lack of energy awareness in data management, inadequate implementation of energy-aware resource allocation and lack of energy efficiency criteria on task scheduling. Based on these findings, we propose the development of energy-aware performance monitoring and benchmarking frameworks, the use of optimization techniques for infrastructure orchestration, and of energy-efficient task scheduling approaches for distributed cloud-based EOBD processing frameworks. These proposed approaches aim to foster more energy awareness in EOBD processing , potentially reducing power consumption and environmental impact while maintaining or minimally impacting processing performance.",
        "authors": [
            "Adhitya Bhawiyuga",
            "Serkan Girgin",
            "Rolf A. de By",
            "Raul Zurita-Milla"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-10-03"
    },
    "http://arxiv.org/abs/2510.02878v1": {
        "title": "On the energy efficiency of sparse matrix computations on multi-GPU clusters",
        "link": "http://arxiv.org/abs/2510.02878v1",
        "abstract": "We investigate the energy efficiency of a library designed for parallel computations with sparse matrices. The library leverages high-performance, energy-efficient Graphics Processing Unit (GPU) accelerators to enable large-scale scientific applications. Our primary development objective was to maximize parallel performance and scalability in solving sparse linear systems whose dimensions far exceed the memory capacity of a single node. To this end, we devised methods that expose a high degree of parallelism while optimizing algorithmic implementations for efficient multi-GPU usage. Previous work has already demonstrated the library's performance efficiency on large-scale systems comprising thousands of NVIDIA GPUs, achieving improvements over state-of-the-art solutions. In this paper, we extend those results by providing energy profiles that address the growing sustainability requirements of modern HPC platforms. We present our methodology and tools for accurate runtime energy measurements of the library's core components and discuss the findings. Our results confirm that optimizing GPU computations and minimizing data movement across memory and computing nodes reduces both time-to-solution and energy consumption. Moreover, we show that the library delivers substantial advantages over comparable software frameworks on standard benchmarks.",
        "authors": [
            "Massimo Bernaschi",
            "Alessandro Celestini",
            "Pasqua D'Ambra",
            "Giorgio Richelli"
        ],
        "categories": [
            "cs.DC",
            "cs.MS",
            "cs.PF"
        ],
        "id": "http://arxiv.org/abs/2510.02878v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-03"
    },
    "http://arxiv.org/abs/2510.02851v2": {
        "id": "http://arxiv.org/abs/2510.02851v2",
        "title": "Action Deviation-Aware Inference for Low-Latency Wireless Robots",
        "link": "http://arxiv.org/abs/2510.02851v2",
        "tags": [
            "serving",
            "edge",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes Action Deviation-Aware Hybrid Inference (ADAHI) to reduce latency for robot policy inference by selectively offloading drafts to a server based on action deviation. Combines lightweight on-device model with remote verification, cutting latency by 39.2% and transmission by 40%, preserving 97.2% task-success.",
        "abstract": "To support latency-sensitive AI applications ranging from autonomous driving to industrial robot manipulation, 6G envisions distributed ML with computational resources in mobile, edge, and cloud connected over hyper-reliable low-latency communication (HRLLC). In this setting, speculative decoding can facilitate collaborative inference of models distributively deployed: a lightweight on-device model locally generates drafts while a more capable remote target model on a server verifies and corrects them in parallel with speculative sampling, thus resulting in lower latency without compromising accuracy. However, unlike autoregressive text generation, behavior cloning policies, typically used for embodied AI applications, cannot parallelize verification and correction for multiple drafts as each generated action depends on observation updated by a previous action. To this end, we propose Action Deviation-Aware Hybrid Inference (ADAHI), wherein drafts are selectively transmitted and verified based on action deviation, which has a strong correlation with action's rejection probability by the target model. By invoking server operation only when necessary, communication and computational overhead can be reduced while accuracy gain from speculative sampling is preserved. Experiments on our testbed show that ADAHI reduces transmission and server operations by approximately 40%, lowers end-to-end latency by 39.2%, and attains up to 97.2% of the task-success rate of baseline that invokes speculative sampling for every draft embedding vector.",
        "authors": [
            "Jeyoung Park",
            "Yeonsub Lim",
            "Seungeun Oh",
            "Jihong Park",
            "Jinho Choi",
            "Seong-Lyun Kim"
        ],
        "categories": [
            "cs.RO",
            "cs.DC"
        ],
        "submit_date": "2025-10-03"
    },
    "http://arxiv.org/abs/2510.02838v1": {
        "id": "http://arxiv.org/abs/2510.02838v1",
        "title": "TridentServe: A Stage-level Serving System for Diffusion Pipelines",
        "link": "http://arxiv.org/abs/2510.02838v1",
        "tags": [
            "serving",
            "diffusion"
        ],
        "relevant": true,
        "indexed_date": "2025-10-01",
        "tldr": "Addresses inefficient static serving of diffusion pipelines by introducing dynamic stage-level resource allocation. TridentServe co-optimizes stage placement and request routing, achieving up to 4.1x lower P95 latency while improving SLO attainment.",
        "abstract": "Diffusion pipelines, renowned for their powerful visual generation capabilities, have seen widespread adoption in generative vision tasks (e.g., text-to-image/video). These pipelines typically follow an encode--diffuse--decode three-stage architecture. Current serving systems deploy diffusion pipelines within a static, manual, and pipeline-level paradigm, allocating the same resources to every request and stage. However, through an in-depth analysis, we find that such a paradigm is inefficient due to the discrepancy in resource needs across the three stages of each request, as well as across different requests. Following the analysis, we propose the dynamic stage-level serving paradigm and develop TridentServe, a brand new diffusion serving system. TridentServe automatically, dynamically derives the placement plan (i.e., how each stage resides) for pipeline deployment and the dispatch plan (i.e., how the requests are routed) for request processing, co-optimizing the resource allocation for both model and requests. Extensive experiments show that TridentServe consistently improves SLO attainment and reduces average/P95 latencies by up to 2.5x and 3.6x/4.1x over existing works across a variety of workloads.",
        "authors": [
            "Yifei Xia",
            "Fangcheng Fu",
            "Hao Yuan",
            "Hanke Zhang",
            "Xupeng Miao",
            "Yijun Liu",
            "Suhan Ling",
            "Jie Jiang",
            "Bin Cui"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-10-03"
    },
    "http://arxiv.org/abs/2510.03371v1": {
        "id": "http://arxiv.org/abs/2510.03371v1",
        "title": "Distributed Low-Communication Training with Decoupled Momentum Optimization",
        "link": "http://arxiv.org/abs/2510.03371v1",
        "tags": [
            "training",
            "networking",
            "sparse"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Reduces communication overhead in distributed LLM training by decomposing Nesterov momentum into high/low-frequency components via DCT, synchronizing only high-frequency components every H steps. Achieves 16x less communication than DiLoCo baseline while maintaining generalization across architectures.",
        "abstract": "The training of large models demands substantial computational resources, typically available only in data centers with high-bandwidth interconnects. However, reducing the reliance on high-bandwidth interconnects between nodes enables the use of distributed compute resources as an alternative to centralized data center training. Building on recent advances in distributed model training, we propose an approach that further reduces communication by combining infrequent synchronizations across distributed model replicas with gradient momentum compression. In particular, we treat the optimizer momentum as a signal and decompose the Nesterov momentum into high- and low-frequency components via the discrete cosine transform (DCT). Only the high-frequency components are synchronized across model replicas every $H$ steps. Empirically, our method achieves up to a $16\\times$ reduction in communication compared to the baseline DiLoCo, and it generalizes across architectures, including transformer-based language models and convolutional neural networks for images. Overall, this work advances the feasibility of training large models on distributed nodes with low-bandwidth interconnects.",
        "authors": [
            "Sasho Nedelkoski",
            "Alexander Acker",
            "Odej Kao",
            "Soeren Becker",
            "Dominik Scheinert"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC"
        ],
        "submit_date": "2025-10-03"
    },
    "http://arxiv.org/abs/2510.15927v1": {
        "id": "http://arxiv.org/abs/2510.15927v1",
        "title": "UPMEM Unleashed: Software Secrets for Speed",
        "link": "http://arxiv.org/abs/2510.15927v1",
        "tags": [
            "hardware",
            "kernel",
            "quantization"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Examines software inefficiencies in UPMEM PIM platforms for low-precision kernels. Proposes assembly-level optimizations, bit-serial processing INT4/INT8, and NUMA-aware allocations. Achieves 1.4-5.9x arithmetic speedups and over 3x faster INT8 GEMV vs CPU after optimizations.",
        "abstract": "Developing kernels for Processing-In-Memory (PIM) platforms poses unique challenges in data management and parallel programming on limited processing units. Although software development kits (SDKs) for PIM, such as the UPMEM SDK, provide essential tools, these emerging platforms still leave significant room for performance optimization. In this paper, we reveal surprising inefficiencies in UPMEM software stack and play with non-standard programming techniques. By making simple modifications to the assembly generated by the UPMEM compiler, we achieve speedups of 1.6-2x in integer addition and 1.4-5.9x in integer multiplication, depending on the data type. We also demonstrate that bit-serial processing of low precision data is a viable option for UPMEM: in INT4 bit-serial dot-product calculation, UPMEM can achieve over 2.7x speedup over the baseline. Minor API extensions for PIM allocation that account for the non-uniform memory access (NUMA) architecture of the server further improve the consistency and throughput of host-PIM data transfers by up to 2.9x. Finally, we show that, when the matrix is preloaded into PIM, our optimized kernels outperform a dual-socket CPU server by over 3x for INT8 generalized matrix-vector multiplication (GEMV) and by 10x for INT4 GEMV. Our optimized INT8 GEMV kernel outperforms the baseline 3.5x.",
        "authors": [
            "Krystian Chmielewski",
            "Jarosław Ławnicki",
            "Uladzislau Lukyanau",
            "Tadeusz Kobus",
            "Maciej Maciejewski"
        ],
        "categories": [
            "cs.AR",
            "cs.DC",
            "cs.PF"
        ],
        "submit_date": "2025-10-03"
    },
    "http://arxiv.org/abs/2510.02774v1": {
        "title": "GRNND: A GPU-Parallel Relative NN-Descent Algorithm for Efficient Approximate Nearest Neighbor Graph Construction",
        "link": "http://arxiv.org/abs/2510.02774v1",
        "abstract": "Relative Nearest Neighbor Descent (RNN-Descent) is a state-of-the-art algorithm for constructing sparse approximate nearest neighbor (ANN) graphs by combining the iterative refinement of NN-Descent with the edge-pruning rules of the Relative Neighborhood Graph (RNG). It has demonstrated strong effectiveness in large-scale search tasks such as information retrieval and related tasks. However, as the amount and dimensionality of data increase, the complexity of graph construction in RNN-Descent rises sharply, making this stage increasingly time-consuming and even prohibitive for subsequent query processing. In this paper, we propose GRNND, the first GPU-parallel algorithm of RNN-Descent designed to fully exploit GPU architecture. GRNND introduces a disordered neighbor propagation strategy to mitigate synchronized update traps, enhancing structural diversity, and avoiding premature convergence during parallel execution. It also leverages warp-level cooperative operations and a double-buffered neighbor pool with fixed capacity for efficient memory access, eliminate contention, and enable highly parallelized neighbor updates. Extensive experiments demonstrate that GRNND consistently outperforms existing CPU- and GPU-based methods. GRNND achieves 2.4 to 51.7x speedup over existing GPU methods, and 17.8 to 49.8x speedup over CPU methods.",
        "authors": [
            "Xiang Li",
            "Qiong Chang",
            "Yun Li",
            "Jun Miyazaki"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.02774v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-03"
    },
    "http://arxiv.org/abs/2510.02613v1": {
        "id": "http://arxiv.org/abs/2510.02613v1",
        "title": "ElasticMoE: An Efficient Auto Scaling Method for Mixture-of-Experts Models",
        "link": "http://arxiv.org/abs/2510.02613v1",
        "tags": [
            "MoE",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-10-01",
        "tldr": "Enables fine-grained, zero-downtime scaling of MoE LLMs during inference by decoupling execution from memory operations and using zero-copy remapping. Achieves up to 9× lower scale-up latency and 2× higher throughput during scaling.",
        "abstract": "Mixture-of-Experts (MoE) models promise efficient scaling of large language models (LLMs) by activating only a small subset of experts per token, but their parallelized inference pipelines make elastic serving challenging. Existing strategies fall short: horizontal scaling provisions entire replicas of the current configuration, often tens to hundreds of accelerators, leading to coarse granularity, long provisioning delays, and costly overprovisioning. Vertical scaling offers finer adjustments but typically requires instance restarts, incurring downtime. These limitations make current approaches ill-suited for the bursty, short-lived traffic patterns common in cloud deployments.   We present ElasticMoE, an elastic scaling framework for MoE LLMs that achieves fine-grained, low-latency, and zero-downtime scaling. ElasticMoE decouples inference execution from memory operations, enabling scaling steps to proceed concurrently with serving. An HBM Management Module (HMM) reuses weights and KV caches via zero-copy remapping, while high-bandwidth peer-to-peer transfers bring newly added accelerators online without interrupting service. A virtual memory based expert redistribution mechanism migrates MoE experts without costly buffer reallocations, reducing peak memory usage during expert parallelism reconfiguration.   Our evaluation on Ascend NPUs with three popular MoE LLMs shows that ElasticMoE achieves up to 9x lower scale-up latency, up to 2x better throughput during scaling, and significantly improves SLO attainment compared to baselines. By enabling fine-grained, concurrent scaling with minimal disruption, ElasticMoE advances the practicality of deploying massive MoE LLMs in dynamic cloud environments.",
        "authors": [
            "Gursimran Singh",
            "Timothy Yu",
            "Haley Li",
            "Cheng Chen",
            "Hanieh Sadri",
            "Qintao Zhang",
            "Yu Zhang",
            "Ying Xiong",
            "Yong Zhang",
            "Zhenan Fan"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-10-02"
    },
    "http://arxiv.org/abs/2510.01885v1": {
        "id": "http://arxiv.org/abs/2510.01885v1",
        "title": "Accuracy vs Performance: An abstraction model for deadline constrained offloading at the mobile-edge",
        "link": "http://arxiv.org/abs/2510.01885v1",
        "tags": [
            "edge",
            "offloading",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes a deadline-aware scheduling algorithm for DNN offloading to mobile edge devices, using lightweight network state representation and dynamic bandwidth estimation. Improves task throughput under resource scarcity and reduces latency in high-volume workloads.",
        "abstract": "In this paper, we present a solution for low-latency deadline-constrained DNN offloading on mobile edge devices. We design a scheduling algorithm with lightweight network state representation, considering device availability, communication on the network link, priority-aware pre-emption, and task deadlines. The scheduling algorithm aims to reduce latency by designing a resource availability representation, as well as a network discretisation and a dynamic bandwidth estimation mechanism. We implement the scheduling algorithm into a system composed of four Raspberry Pi 2 (model Bs) mobile edge devices, sampling a waste classification conveyor belt at a set frame rate. The system is evaluated and compared to a previous approach of ours, which was proven to outcompete work-stealers and a non-pre-emption based scheduling heuristic under the aforementioned waste classification scenario. Our findings show the novel lower latency abstraction models yield better performance under high-volume workloads, with the dynamic bandwidth estimation assisting the task placement while, ultimately, increasing task throughput in times of resource scarcity.",
        "authors": [
            "Jamie Cotter",
            "Ignacio Castineiras",
            "Victor Cionca"
        ],
        "categories": [
            "cs.DC",
            "cs.NI"
        ],
        "submit_date": "2025-10-02"
    },
    "http://arxiv.org/abs/2510.05149v1": {
        "id": "http://arxiv.org/abs/2510.05149v1",
        "title": "Percepta: High Performance Stream Processing at the Edge",
        "link": "http://arxiv.org/abs/2510.05149v1",
        "tags": [
            "edge",
            "serving",
            "RL"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes Percepta, a lightweight edge stream processing system for AI workloads, focusing on reinforcement learning. It includes reward computation, real-time data prep, and handles normalization/harmonization. Achieves reduced latency and efficient data handling for continuous decision-making at the edge.",
        "abstract": "The rise of real-time data and the proliferation of Internet of Things (IoT) devices have highlighted the limitations of cloud-centric solutions, particularly regarding latency, bandwidth, and privacy. These challenges have driven the growth of Edge Computing. Associated with IoT appears a set of other problems, like: data rate harmonization between multiple sources, protocol conversion, handling the loss of data and the integration with Artificial Intelligence (AI) models. This paper presents Percepta, a lightweight Data Stream Processing (DSP) system tailored to support AI workloads at the edge, with a particular focus on such as Reinforcement Learning (RL). It introduces specialized features such as reward function computation, data storage for model retraining, and real-time data preparation to support continuous decision-making. Additional functionalities include data normalization, harmonization across heterogeneous protocols and sampling rates, and robust handling of missing or incomplete data, making it well suited for the challenges of edge-based AI deployment.",
        "authors": [
            "Clarisse Sousa",
            "Tiago Fonseca",
            "Luis Lino Ferreira",
            "Ricardo Venâncio",
            "Ricardo Severino"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-10-02"
    },
    "http://arxiv.org/abs/2510.01657v1": {
        "title": "Exponential Quantum Advantage for Message Complexity in Distributed Algorithms",
        "link": "http://arxiv.org/abs/2510.01657v1",
        "abstract": "We investigate how much quantum distributed algorithms can outperform classical distributed algorithms with respect to the message complexity (the overall amount of communication used by the algorithm). Recently, Dufoulon, Magniez and Pandurangan (PODC 2025) have shown a polynomial quantum advantage for several tasks such as leader election and agreement. In this paper, we show an exponential quantum advantage for a fundamental task: routing information between two specified nodes of a network. We prove that for the family of ``welded trees\" introduced in the seminal work by Childs, Cleve, Deotto, Farhi, Gutmann and Spielman (STOC 2003), there exists a quantum distributed algorithm that transfers messages from the entrance of the graph to the exit with message complexity exponentially smaller than any classical algorithm. Our quantum algorithm is based on the recent \"succinct\" implementation of quantum walks over the welded trees by Li, Li and Luo (SODA 2024). Our classical lower bound is obtained by ``lifting'' the lower bound from Childs, Cleve, Deotto, Farhi, Gutmann and Spielman (STOC 2003) from query complexity to message complexity.",
        "authors": [
            "François Le Gall",
            "Maël Luce",
            "Joseph Marchand",
            "Mathieu Roget"
        ],
        "categories": [
            "quant-ph",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.01657v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-02"
    },
    "http://arxiv.org/abs/2510.03334v1": {
        "id": "http://arxiv.org/abs/2510.03334v1",
        "title": "Semantic-Aware Scheduling for GPU Clusters with Large Language Models",
        "link": "http://arxiv.org/abs/2510.03334v1",
        "tags": [
            "training",
            "serving",
            "storage"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes SchedMate, a semantic-aware GPU cluster scheduler using LLMs to analyze unstructured data (source code, logs, history). Integrates with existing schedulers to reduce profiling overhead and improve estimates. Achieves up to 1.91× reduction in average job completion time.",
        "abstract": "Deep learning (DL) schedulers are pivotal in optimizing resource allocation in GPU clusters, but operate with a critical limitation: they are largely blind to the semantic context of the jobs they manage. This forces them to rely on limited metadata, leading to high profiling overhead, unreliable duration estimation, inadequate failure handling, and poor observability. To this end, we propose SchedMate, a framework that bridges this semantic gap by systematically extracting deep insights from overlooked, unstructured data sources: source code, runtime logs, and historical jobs. SchedMate enhances existing schedulers non-intrusively through three LLM-based components. Our implementation integrates seamlessly with existing deep learning schedulers. Evaluations on a 128-GPU physical cluster and extensive simulations on production traces show SchedMate reduces average job completion times by up to 1.91x, substantially enhancing the scheduling performance, demonstrating the critical role of semantic-awareness in modern DL scheduling.",
        "authors": [
            "Zerui Wang",
            "Qinghao Hu",
            "Ana Klimovic",
            "Tianwei Zhang",
            "Yonggang Wen",
            "Peng Sun",
            "Dahua Lin"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "submit_date": "2025-10-02"
    },
    "http://arxiv.org/abs/2510.01565v2": {
        "id": "http://arxiv.org/abs/2510.01565v2",
        "title": "TetriServe: Efficient DiT Serving for Heterogeneous Image Generation",
        "link": "http://arxiv.org/abs/2510.01565v2",
        "tags": [
            "serving",
            "diffusion",
            "autoscaling"
        ],
        "relevant": true,
        "indexed_date": "2025-10-01",
        "tldr": "Proposes TetriServe for efficient DiT serving. Introduces step-level sequence parallelism and round-based scheduling to adapt parallelism per request deadlines. Achieves 32% higher SLO attainment than existing solutions.",
        "abstract": "Diffusion Transformer (DiT) models excel at generating highquality images through iterative denoising steps, but serving them under strict Service Level Objectives (SLOs) is challenging due to their high computational cost, particularly at large resolutions. Existing serving systems use fixed degree sequence parallelism, which is inefficient for heterogeneous workloads with mixed resolutions and deadlines, leading to poor GPU utilization and low SLO attainment.   In this paper, we propose step-level sequence parallelism to dynamically adjust the parallel degree of individual requests according to their deadlines. We present TetriServe, a DiT serving system that implements this strategy for highly efficient image generation. Specifically, TetriServe introduces a novel round-based scheduling mechanism that improves SLO attainment: (1) discretizing time into fixed rounds to make deadline-aware scheduling tractable, (2) adapting parallelism at the step level and minimize GPU hour consumption, and (3) jointly packing requests to minimize late completions. Extensive evaluation on state-of-the-art DiT models shows that TetriServe achieves up to 32% higher SLO attainment compared to existing solutions without degrading image quality.",
        "authors": [
            "Runyu Lu",
            "Shiqi He",
            "Wenxuan Tan",
            "Shenggui Li",
            "Ruofan Wu",
            "Jeff J. Ma",
            "Ang Chen",
            "Mosharaf Chowdhury"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "submit_date": "2025-10-02"
    },
    "http://arxiv.org/abs/2510.05145v1": {
        "id": "http://arxiv.org/abs/2510.05145v1",
        "title": "FlashResearch: Real-time Agent Orchestration for Efficient Deep Research",
        "link": "http://arxiv.org/abs/2510.05145v1",
        "tags": [
            "agentic",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-10-01",
        "tldr": "How to accelerate deep research agents by parallelizing sequential reasoning? FlashResearch dynamically decomposes queries into tree-structured tasks and orchestrates parallel execution across breadth and depth, achieving 5x speedup while maintaining report quality.",
        "abstract": "Deep research agents, which synthesize information across diverse sources, are significantly constrained by their sequential reasoning processes. This architectural bottleneck results in high latency, poor runtime adaptability, and inefficient resource allocation, making them impractical for interactive applications. To overcome this, we introduce FlashResearch, a novel framework for efficient deep research that transforms sequential processing into parallel, runtime orchestration by dynamically decomposing complex queries into tree-structured sub-tasks. Our core contributions are threefold: (1) an adaptive planner that dynamically allocates computational resources by determining research breadth and depth based on query complexity; (2) a real-time orchestration layer that monitors research progress and prunes redundant paths to reallocate resources and optimize efficiency; and (3) a multi-dimensional parallelization framework that enables concurrency across both research breadth and depth. Experiments show that FlashResearch consistently improves final report quality within fixed time budgets, and can deliver up to a 5x speedup while maintaining comparable quality.",
        "authors": [
            "Lunyiu Nie",
            "Nedim Lipka",
            "Ryan A. Rossi",
            "Swarat Chaudhuri"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.MA"
        ],
        "submit_date": "2025-10-02"
    },
    "http://arxiv.org/abs/2510.01536v1": {
        "title": "QScale: Probabilistic Chained Consensus for Moderate-Scale Systems",
        "link": "http://arxiv.org/abs/2510.01536v1",
        "abstract": "Existing distributed ledger protocols either incur a high communication complexity and are thus suited to systems with a small number of processes (e.g., PBFT), or rely on committee-sampling-based approaches that only work for a very large number of processes (e.g., Algorand). Neither of these lines of work is well-suited for moderate-scale distributed ledgers ranging from a few hundred to a thousand processes, which are common in production (e.g, Redbelly, Sui). The goal of this work is to design a distributed ledger with sub-linear communication complexity per process, sub-quadratic total communication complexity, and low latency for finalizing a block into the ledger, such that it can be used for moderate-scale systems. We propose QScale, a protocol in which every process incurs only $\\widetilde{O}(κ\\sqrt{n})$ communication complexity per-block in expectation, $\\widetilde{O}(nκ)$ total communication complexity per-block in expectation, and a best-case latency of $O(κ)$ rounds while ensuring safety and liveness with overwhelming probability, with $κ$ being a small security parameter.",
        "authors": [
            "Hasan Heydari",
            "Alysson Bessani",
            "Kartik Nayak"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.01536v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-02"
    },
    "http://arxiv.org/abs/2510.01511v1": {
        "title": "Exponential convergence of a distributed divide-and-conquer algorithm for constrained convex optimization on networks",
        "link": "http://arxiv.org/abs/2510.01511v1",
        "abstract": "We propose a divide-and-conquer (DAC) algorithm for constrained convex optimization over networks, where the global objective is the sum of local objectives attached to individual agents. The algorithm is fully distributed: each iteration solves local subproblems around selected fusion centers and coordinates only with neighboring fusion centers. Under standard assumptions of smoothness, strong convexity, and locality on the objective function, together with polynomial growth conditions on the underlying graph, we establish exponential convergence of the DAC iterations and derive explicit bounds for both exact and inexact local solvers. Numerical experiments on three representative losses ($L_2$ distance, quadratic, and entropy) confirm the theory and demonstrate scalability and effectiveness.",
        "authors": [
            "Nazar Emirov",
            "Guohui Song",
            "Qiyu Sun"
        ],
        "categories": [
            "math.OC",
            "cs.DC",
            "math.NA"
        ],
        "id": "http://arxiv.org/abs/2510.01511v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-01"
    },
    "http://arxiv.org/abs/2510.00991v1": {
        "id": "http://arxiv.org/abs/2510.00991v1",
        "title": "An Efficient, Reliable and Observable Collective Communication Library in Large-scale GPU Training Clusters",
        "link": "http://arxiv.org/abs/2510.00991v1",
        "tags": [
            "training",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-10-01",
        "tldr": "Designs ICCL, a collective communication library for large-scale LLM training, to improve P2P efficiency, tolerate NIC failures, and enable microsecond-level anomaly observability, achieving 28.5% lower latency and 6.02% higher training throughput than NCCL.",
        "abstract": "Large-scale LLM training requires collective communication libraries to exchange data among distributed GPUs. As a company dedicated to building and operating large-scale GPU training clusters, we encounter several challenges when using NCCL in production, including 1) limited efficiency with costly and cumbersome P2P communication, 2) poor tolerance to frequent RNIC port failures, and 3) insufficient observability of transient collective communication anomalies. To address these issues, we propose ICCL, an efficient, reliable, and observable collective communication library in large-scale GPU training clusters. ICCL offloads the P2P communication from GPU kernels to CPU threads for minimal SM consumption, and removes the redundant memory copies irrelevant to the actual communicating process. ICCL also introduces a primary-backup QP mechanism to tolerate frequent NIC port failures, and designs a window-based monitor to observe network anomalies at O(us) level. We open-source ICCL and deploy it in production training clusters for several months, with results showing that compared to NCCL, ICCL achieves a 23.4%/28.5% improvement in P2P throughput/latency as well as a 6.02% increase in training throughput. We also share the operating experience of ICCL in large-scale clusters, hoping to give the communities more insights on production-level collective communication libraries in LLM training.",
        "authors": [
            "Ziteng Chen",
            "Xiaohe Hu",
            "Menghao Zhang",
            "Yanmin Jia",
            "Yan Zhang",
            "Mingjun Zhang",
            "Da Liu",
            "Fangzheng Jiao",
            "Jun Chen",
            "He Liu",
            "Aohan Zeng",
            "Shuaixing Duan",
            "Ruya Gu",
            "Yang Jing",
            "Bowen Han",
            "Jiahao Cao",
            "Wei Chen",
            "Wenqi Xie",
            "Jinlong Hou",
            "Yuan Cheng",
            "Bohua Xu",
            "Mingwei Xu",
            "Chunming Hu"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-10-01"
    },
    "http://arxiv.org/abs/2510.00976v1": {
        "title": "Adaptive Federated Few-Shot Rare-Disease Diagnosis with Energy-Aware Secure Aggregation",
        "link": "http://arxiv.org/abs/2510.00976v1",
        "abstract": "Rare-disease diagnosis remains one of the most pressing challenges in digital health, hindered by extreme data scarcity, privacy concerns, and the limited resources of edge devices. This paper proposes the Adaptive Federated Few-Shot Rare-Disease Diagnosis (AFFR) framework, which integrates three pillars: (i) few-shot federated optimization with meta-learning to generalize from limited patient samples, (ii) energy-aware client scheduling to mitigate device dropouts and ensure balanced participation, and (iii) secure aggregation with calibrated differential privacy to safeguard sensitive model updates. Unlike prior work that addresses these aspects in isolation, AFFR unifies them into a modular pipeline deployable on real-world clinical networks. Experimental evaluation on simulated rare-disease detection datasets demonstrates up to 10% improvement in accuracy compared with baseline FL, while reducing client dropouts by over 50% without degrading convergence. Furthermore, privacy-utility trade-offs remain within clinically acceptable bounds. These findings highlight AFFR as a practical pathway for equitable and trustworthy federated diagnosis of rare conditions.",
        "authors": [
            "Aueaphum Aueawatthanaphisut"
        ],
        "categories": [
            "cs.AI",
            "cs.CR",
            "cs.DC",
            "cs.LG",
            "q-bio.QM"
        ],
        "id": "http://arxiv.org/abs/2510.00976v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-01"
    },
    "http://arxiv.org/abs/2510.00833v1": {
        "title": "Towards Verifiable Federated Unlearning: Framework, Challenges, and The Road Ahead",
        "link": "http://arxiv.org/abs/2510.00833v1",
        "abstract": "Federated unlearning (FUL) enables removing the data influence from the model trained across distributed clients, upholding the right to be forgotten as mandated by privacy regulations. FUL facilitates a value exchange where clients gain privacy-preserving control over their data contributions, while service providers leverage decentralized computing and data freshness. However, this entire proposition is undermined because clients have no reliable way to verify that their data influence has been provably removed, as current metrics and simple notifications offer insufficient assurance. We envision unlearning verification becoming a pivotal and trust-by-design part of the FUL life-cycle development, essential for highly regulated and data-sensitive services and applications like healthcare. This article introduces veriFUL, a reference framework for verifiable FUL that formalizes verification entities, goals, approaches, and metrics. Specifically, we consolidate existing efforts and contribute new insights, concepts, and metrics to this domain. Finally, we highlight research challenges and identify potential applications and developments for verifiable FUL and veriFUL.",
        "authors": [
            "Thanh Linh Nguyen",
            "Marcela Tuler de Oliveira",
            "An Braeken",
            "Aaron Yi Ding",
            "Quoc-Viet Pham"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "id": "http://arxiv.org/abs/2510.00833v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-01"
    },
    "http://arxiv.org/abs/2510.00828v1": {
        "title": "Data Management System Analysis for Distributed Computing Workloads",
        "link": "http://arxiv.org/abs/2510.00828v1",
        "abstract": "Large-scale international collaborations such as ATLAS rely on globally distributed workflows and data management to process, move, and store vast volumes of data. ATLAS's Production and Distributed Analysis (PanDA) workflow system and the Rucio data management system are each highly optimized for their respective design goals. However, operating them together at global scale exposes systemic inefficiencies, including underutilized resources, redundant or unnecessary transfers, and altered error distributions. Moreover, PanDA and Rucio currently lack shared performance awareness and coordinated, adaptive strategies.   This work charts a path toward co-optimizing the two systems by diagnosing data-management pitfalls and prioritizing end-to-end improvements. With the observation of spatially and temporally imbalanced transfer activities, we develop a metadata-matching algorithm that links PanDA jobs and Rucio datasets at the file level, yielding a complete, fine-grained view of data access and movement. Using this linkage, we identify anomalous transfer patterns that violate PanDA's data-centric job-allocation principle. We then outline mitigation strategies for these patterns and highlight opportunities for tighter PanDA-Rucio coordination to improve resource utilization, reduce unnecessary data movement, and enhance overall system resilience.",
        "authors": [
            "Kuan-Chieh Hsu",
            "Sairam Sri Vatsavai",
            "Ozgur O. Kilic",
            "Tatiana Korchuganova",
            "Paul Nilsson",
            "Sankha Dutta",
            "Yihui Ren",
            "David K. Park",
            "Joseph Boudreau",
            "Tasnuva Chowdhury",
            "Shengyu Feng",
            "Raees Khan",
            "Jaehyung Kim",
            "Scott Klasky",
            "Tadashi Maeno",
            "Verena Ingrid Martinez Outschoorn",
            "Norbert Podhorszki",
            "Frédéric Suter",
            "Wei Yang",
            "Yiming Yang",
            "Shinjae Yoo",
            "Alexei Klimentov",
            "Adolfy Hoisie"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.00828v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-01"
    },
    "http://arxiv.org/abs/2510.00822v1": {
        "title": "CGSim: A Simulation Framework for Large Scale Distributed Computing Environment",
        "link": "http://arxiv.org/abs/2510.00822v1",
        "abstract": "Large-scale distributed computing infrastructures such as the Worldwide LHC Computing Grid (WLCG) require comprehensive simulation tools for evaluating performance, testing new algorithms, and optimizing resource allocation strategies. However, existing simulators suffer from limited scalability, hardwired algorithms, lack of real-time monitoring, and inability to generate datasets suitable for modern machine learning approaches. We present CGSim, a simulation framework for large-scale distributed computing environments that addresses these limitations. Built upon the validated SimGrid simulation framework, CGSim provides high-level abstractions for modeling heterogeneous grid environments while maintaining accuracy and scalability. Key features include a modular plugin mechanism for testing custom workflow scheduling and data movement policies, interactive real-time visualization dashboards, and automatic generation of event-level datasets suitable for AI-assisted performance modeling. We demonstrate CGSim's capabilities through a comprehensive evaluation using production ATLAS PanDA workloads, showing significant calibration accuracy improvements across WLCG computing sites. Scalability experiments show near-linear scaling for multi-site simulations, with distributed workloads achieving 6x better performance compared to single-site execution. The framework enables researchers to simulate WLCG-scale infrastructures with hundreds of sites and thousands of concurrent jobs within practical time budget constraints on commodity hardware.",
        "authors": [
            "Sairam Sri Vatsavai",
            "Raees Khan",
            "Kuan-Chieh Hsu",
            "Ozgur O. Kilic",
            "Paul Nilsson",
            "Tatiana Korchuganova",
            "David K. Park",
            "Sankha Dutta",
            "Yihui Ren",
            "Joseph Boudreau",
            "Tasnuva Chowdhury",
            "Shengyu Feng",
            "Jaehyung Kim",
            "Scott Klasky",
            "Tadashi Maeno",
            "Verena Ingrid Martinez",
            "Norbert Podhorszki",
            "Frédéric Suter",
            "Wei Yang",
            "Yiming Yang",
            "Shinjae Yoo",
            "Alexei Klimentov",
            "Adolfy Hoisie"
        ],
        "categories": [
            "cs.DC",
            "cs.PF"
        ],
        "id": "http://arxiv.org/abs/2510.00822v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-01"
    },
    "http://arxiv.org/abs/2510.00758v1": {
        "title": "Decentralized and Self-adaptive Core Maintenance on Temporal Graphs",
        "link": "http://arxiv.org/abs/2510.00758v1",
        "abstract": "Key graph-based problems play a central role in understanding network topology and uncovering patterns of similarity in homogeneous and temporal data. Such patterns can be revealed by analyzing communities formed by nodes, which in turn can be effectively modeled through temporal $k$-cores. This paper introduces a novel decentralized and incremental algorithm for computing the core decomposition of temporal networks. Decentralized solutions leverage the ability of network nodes to communicate and coordinate locally, addressing complex problems in a scalable, adaptive, and timely manner. By leveraging previously computed coreness values, our approach significantly reduces the activation of nodes and the volume of message exchanges when the network changes over time. This enables scalability with only a minimal trade-off in precision. Experimental evaluations on large real-world networks under varying levels of dynamism demonstrate the efficiency of our solution compared to a state-of-the-art approach, particularly in terms of active nodes, communication overhead, and convergence speed.",
        "authors": [
            "Davide Rucci",
            "Emanuele Carlini",
            "Patrizio Dazzi",
            "Hanna Kavalionak",
            "Matteo Mordacchini"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.00758v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-01"
    },
    "http://arxiv.org/abs/2510.00678v1": {
        "title": "Net-Zero 6G from Earth to Orbit: Sustainable Design of Integrated Terrestrial and Non-Terrestrial Networks",
        "link": "http://arxiv.org/abs/2510.00678v1",
        "abstract": "The integration of Terrestrial Networks (TN) and Non-Terrestrial Networks (NTN) plays a crucial role in bridging the digital divide and enabling Sixth Generation (6G) and beyond to achieve truly ubiquitous connectivity. However, combining TN and NTN introduces significant energy challenges due to the diverse characteristics and operational environments of these systems. In this paper, we present for the first time a comprehensive overview of the design challenges associated with achieving Net-Zero energy targets in integrated TN and NTN systems. We outline a set of key enabling technologies that can support the energy demands of such networks while aligning with Net-Zero objectives. To enhance the Energy Efficiency (EE) of integrated TN and NTN systems, we provide a use case analysis that leverages Artificial Intelligence (AI) to deliver adaptable solutions across diverse deployment scenarios. Finally, we highlight promising research directions that can guide the sustainable evolution of integrated TN and NTN.",
        "authors": [
            "Muhammad Ali Jamshed",
            "Malik Muhammad Saad",
            "Muhammad Ahmed Mohsin",
            "Dongkyun Kim",
            "Octavia A. Dobre",
            "Halim Yanikomeroglu",
            "Lina Mohjazi"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.00678v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-01"
    },
    "http://arxiv.org/abs/2510.00606v3": {
        "id": "http://arxiv.org/abs/2510.00606v3",
        "title": "ElasWave: An Elastic-Native System for Scalable Hybrid-Parallel Training",
        "link": "http://arxiv.org/abs/2510.00606v3",
        "tags": [
            "training",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-10-01",
        "tldr": "Designs an elastic-native LLM training system that maintains parameter consistency and low recovery time during dynamic scaling. Introduces multi-dimensional scheduling, online resharding, and asynchronous migration with in-memory snapshots. Achieves 1.6× higher throughput and 51% lower MTTR than baselines.",
        "abstract": "Large-scale LLM pretraining now runs across $10^5$--$10^6$ accelerators, making failures routine and elasticity mandatory. We posit that an elastic-native training system must jointly deliver (i) parameter consistency, (ii) low mean time to recovery (MTTR), (iii) high post-change throughput, and (iv) computation consistency. No prior system achieves all four simultaneously. To achieve these goals, we present ElasWave, which delivers per-step fault tolerance via multi-dimensional scheduling across graph, dataflow, DVFS, and RNG. ElasWave reshapes and reshards micro-batches while preserving the global batch size and gradient scale. It performs online pipeline resharding with asynchronous parameter migration and interleaves ZeRO partitions, reducing parameter recovery processes to disjoint rank-to-rank transfers. It further leverages DVFS to absorb pipeline bubbles and reshards RNG to keep computation consistency. Together, a dynamic communicator enables in-place communication group edits, while per-step in-memory snapshots support online verification and redistribution. We evaluate ElasWave on 96 NPUs and benchmark it against state-of-the-art baselines: throughput improves by $1.35\\times$ over ReCycle and $1.60\\times$ over TorchFT; communicator recovery completes within one second (up to $82\\times/3.6\\times$ faster than full/partial rebuilds); migration MTTR drops by as much as $51\\%$; and convergence deviation is reduced by approximately $78\\%$.",
        "authors": [
            "Xueze Kang",
            "Guangyu Xiang",
            "Yuxin Wang",
            "Hao Zhang",
            "Yuchu Fang",
            "Yuhang Zhou",
            "Zhenheng Tang",
            "Youhui Lv",
            "Eliran Maman",
            "Mark Wasserman",
            "Alon Zameret",
            "Zhipeng Bian",
            "Shushu Chen",
            "Zhiyou Yu",
            "Jin Wang",
            "Xiaoyu Wu",
            "Yang Zheng",
            "Chen Tian",
            "Xiaowen Chu"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-10-01"
    },
    "http://arxiv.org/abs/2510.00541v1": {
        "title": "Towards Efficient VM Placement: A Two-Stage ACO-PSO Approach for Green Cloud Infrastructure",
        "link": "http://arxiv.org/abs/2510.00541v1",
        "abstract": "Datacenters consume a growing share of energy, prompting the need for sustainable resource management. This paper presents a Hybrid ACO-PSO (HAPSO) algorithm for energy-aware virtual machine (VM) placement and migration in green cloud datacenters. In the first stage, Ant Colony Optimization (ACO) performs energy-efficient initial placement across physical hosts, ensuring global feasibility. In the second stage, a discrete Particle Swarm Optimization (PSO) refines allocations by migrating VMs from overloaded or underutilized hosts. HAPSO introduces several innovations: sequential hybridization of metaheuristics, system-informed particle initialization using ACO output, heuristic-guided discretization for constraint handling, and a multi-objective fitness function that minimizes active servers and resource wastage. Implemented in CloudSimPlus, extensive simulations demonstrate that HAPSO consistently outperforms classical heuristics (BFD, FFD), Unified Ant Colony System (UACS), and ACO-only. Notably, HAPSO achieves up to 25% lower energy consumption and 18% fewer SLA violations compared to UACS at large-scale workloads, while sustaining stable cost and carbon emissions. These results highlight the effectiveness of two-stage bio-inspired hybridization in addressing the dynamic and multi-objective nature of cloud resource management.",
        "authors": [
            "Ali M. Baydoun",
            "Ahmed S. Zekri"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.00541v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-01"
    },
    "http://arxiv.org/abs/2510.02395v1": {
        "id": "http://arxiv.org/abs/2510.02395v1",
        "title": "PolyLink: A Blockchain Based Decentralized Edge AI Platform for LLM Inference",
        "link": "http://arxiv.org/abs/2510.02395v1",
        "tags": [
            "edge",
            "serving",
            "RAG"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes PolyLink, a blockchain-based decentralized platform for LLM inference at the edge. Uses crowdsourced devices, TIQE protocol for integrity, and token incentives. Achieves low verification latency and resists security attacks in geo-distributed deployment.",
        "abstract": "The rapid advancement of large language models (LLMs) in recent years has revolutionized the AI landscape. However, the deployment model and usage of LLM services remain highly centralized, creating significant trust issues and costs for end users and developers. To address these issues, we propose PolyLink, a blockchain-based decentralized AI platform that decentralizes LLM development and inference. Specifically, PolyLink introduces a decentralized crowdsourcing architecture that supports single-device and cross-device model deployment and inference across heterogeneous devices at the edge. Moreover, to ensure the inference integrity, we design the TIQE protocol, which combines a lightweight cross-encoder model and an LLM-as-a-Judge for a high-accuracy inference evaluation. Lastly, we integrate a comprehensive token-based incentive model with dynamic pricing and reward mechanisms for all participants. We have deployed PolyLink and conducted an extensive real-world evaluation through geo-distributed deployment across heterogeneous devices. Results indicate that the inference and verification latency is practical. Our security analysis demonstrates that the system is resistant to model degradation attacks and validator corruptions. PolyLink is now available at https://github.com/IMCL-PolyLink/PolyLink.",
        "authors": [
            "Hongbo Liu",
            "Jiannong Cao",
            "Bo Yang",
            "Dongbin Bai",
            "Yinfeng Cao",
            "Xiaoming Shen",
            "Yinan Zhang",
            "Jinwen Liang",
            "Shan Jiang",
            "Mingjin Zhang"
        ],
        "categories": [
            "cs.CR",
            "cs.DC"
        ],
        "submit_date": "2025-10-01"
    },
    "http://arxiv.org/abs/2510.00471v1": {
        "title": "ThirstyFLOPS: Water Footprint Modeling and Analysis Toward Sustainable HPC Systems",
        "link": "http://arxiv.org/abs/2510.00471v1",
        "abstract": "High-performance computing (HPC) systems are becoming increasingly water-intensive due to their reliance on water-based cooling and the energy used in power generation. However, the water footprint of HPC remains relatively underexplored-especially in contrast to the growing focus on carbon emissions. In this paper, we present ThirstyFLOPS - a comprehensive water footprint analysis framework for HPC systems. Our approach incorporates region-specific metrics, including Water Usage Effectiveness, Power Usage Effectiveness, and Energy Water Factor, to quantify water consumption using real-world data. Using four representative HPC systems - Marconi, Fugaku, Polaris, and Frontier - as examples, we provide implications for HPC system planning and management. We explore the impact of regional water scarcity and nuclear-based energy strategies on HPC sustainability. Our findings aim to advance the development of water-aware, environmentally responsible computing infrastructures.",
        "authors": [
            "Yankai Jiang",
            "Raghavendra Kanakagiri",
            "Rohan Basu Roy",
            "Devesh Tiwari"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.00471v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-01"
    },
    "http://arxiv.org/abs/2510.00306v1": {
        "title": "BlockSDN-VC: A SDN-Based Virtual Coordinate-Enhanced Transaction Broadcast Framework for High-Performance Blockchains",
        "link": "http://arxiv.org/abs/2510.00306v1",
        "abstract": "Modern blockchains need fast, reliable propagation to balance security and throughput. Virtual-coordinate methods speed dissemination but rely on slow iterative updates, leaving nodes out of sync. We present BlockSDN-VC, a transaction-broadcast protocol that centralises coordinate computation and forwarding control in an SDN controller, delivering global consistency, minimal path stretch and rapid response to churn or congestion. In geo-distributed simulations, BlockSDN-VC cuts median latency by up to 62% and accelerates convergence fourfold over state-of-the-art schemes with under 3% control-plane overhead. In a real blockchain environment, BlockSDN-VC boosts confirmed-transaction throughput by 17% under adversarial workloads, requiring no modifications to existing clients.",
        "authors": [
            "Wenyang Jia",
            "Jingjing Wang",
            "Kai Lei"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.00306v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-09-30"
    },
    "http://arxiv.org/abs/2510.05127v1": {
        "title": "Artificial Intelligence for Cost-Aware Resource Prediction in Big Data Pipelines",
        "link": "http://arxiv.org/abs/2510.05127v1",
        "abstract": "Efficient resource allocation is a key challenge in modern cloud computing. Over-provisioning leads to unnecessary costs, while under-provisioning risks performance degradation and SLA violations. This work presents an artificial intelligence approach to predict resource utilization in big data pipelines using Random Forest regression. We preprocess the Google Borg cluster traces to clean, transform, and extract relevant features (CPU, memory, usage distributions). The model achieves high predictive accuracy (R Square = 0.99, MAE = 0.0048, RMSE = 0.137), capturing non-linear relationships between workload characteristics and resource utilization. Error analysis reveals impressive performance on small-to-medium jobs, with higher variance in rare large-scale jobs. These results demonstrate the potential of AI-driven prediction for cost-aware autoscaling in cloud environments, reducing unnecessary provisioning while safeguarding service quality.",
        "authors": [
            "Harshit Goyal"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "id": "http://arxiv.org/abs/2510.05127v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-09-30"
    },
    "http://arxiv.org/abs/2510.00207v2": {
        "id": "http://arxiv.org/abs/2510.00207v2",
        "title": "FlowMoE: A Scalable Pipeline Scheduling Framework for Distributed Mixture-of-Experts Training",
        "link": "http://arxiv.org/abs/2510.00207v2",
        "tags": [
            "MoE",
            "training"
        ],
        "relevant": true,
        "indexed_date": "2025-10-01",
        "tldr": "FlowMoE develops a unified pipeline scheduling framework for distributed MoE training by integrating MHA, gating, expert computation, and communication. It uses tensor chunk-based priority scheduling to overlap all-reduce with computing, reducing training time by up to 57%.",
        "abstract": "The parameter size of modern large language models (LLMs) can be scaled up via the sparsely-activated Mixture-of-Experts (MoE) technique to avoid excessive increase of the computational costs. To further improve training efficiency, pipelining computation and communication has become a promising solution for distributed MoE training. However, existing work primarily focuses on scheduling tasks within the MoE layer, such as expert computing and all-to-all (A2A) communication, while neglecting other key operations including multi-head attention (MHA) computing, gating, and all-reduce communication. In this paper, we propose FlowMoE, a scalable framework for scheduling multi-type task pipelines. First, FlowMoE constructs a unified pipeline to consistently scheduling MHA computing, gating, expert computing, and A2A communication. Second, FlowMoE introduces a tensor chunk-based priority scheduling mechanism to overlap the all-reduce communication with all computing tasks. We implement FlowMoE as an adaptive and generic framework atop PyTorch. Extensive experiments with 675 typical MoE layers and four real-world MoE models across two GPU clusters demonstrate that our proposed FlowMoE framework outperforms state-of-the-art MoE training frameworks, reducing training time by 13%-57%, energy consumption by 10%-39%, and memory usage by 7%-32%.",
        "authors": [
            "Yunqi Gao",
            "Bing Hu",
            "Mahdi Boloursaz Mashhadi",
            "A-Long Jin",
            "Yanfeng Zhang",
            "Pei Xiao",
            "Rahim Tafazolli",
            "Merouane Debbah"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-30"
    },
    "http://arxiv.org/abs/2510.00206v1": {
        "id": "http://arxiv.org/abs/2510.00206v1",
        "title": "LoRAFusion: Efficient LoRA Fine-Tuning for LLMs",
        "link": "http://arxiv.org/abs/2510.00206v1",
        "tags": [
            "training",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-10-01",
        "tldr": "Improves LoRA fine-tuning efficiency by fusing memory-bound ops via kernel optimization and scheduling multiple LoRA adapters with adaptive batching. Achieves up to 1.96× speedup over Megatron-LM and 1.46× over mLoRA.",
        "abstract": "Low-Rank Adaptation (LoRA) has become the leading Parameter-Efficient Fine-Tuning (PEFT) method for Large Language Models (LLMs), as it significantly reduces GPU memory usage while maintaining competitive fine-tuned model quality on downstream tasks. Despite these benefits, we identify two key inefficiencies in existing LoRA fine-tuning systems. First, they incur substantial runtime overhead due to redundant memory accesses on large activation tensors. Second, they miss the opportunity to concurrently fine-tune multiple independent LoRA adapters that share the same base model on the same set of GPUs. This leads to missed performance gains such as reduced pipeline bubbles, better communication overlap, and improved GPU load balance.   To address these issues, we introduce LoRAFusion, an efficient LoRA fine-tuning system for LLMs. At the kernel level, we propose a graph-splitting method that fuses memory-bound operations. This design eliminates unnecessary memory accesses and preserves the performance of compute-bound GEMMs without incurring the cost of recomputation or synchronization. At the scheduling level, LoRAFusion introduces an adaptive batching algorithm for multi-job fine-tuning. It first splits LoRA adapters into groups to intentionally stagger batch execution across jobs, and then solves a bin-packing problem within each group to generate balanced, dependency-aware microbatches. LoRAFusion achieves up to $1.96\\times$ ($1.47\\times$ on average) end-to-end speedup compared to Megatron-LM, and up to $1.46\\times$ ($1.29\\times$ on average) improvement over mLoRA, the state-of-the-art multi-LoRA fine-tuning system. Our fused kernel achieves up to $1.39\\times$ ($1.27\\times$ on average) kernel performance improvement and can directly serve as a plug-and-play replacement in existing LoRA systems. We open-source LoRAFusion at https://github.com/CentML/lorafusion.",
        "authors": [
            "Zhanda Zhu",
            "Qidong Su",
            "Yaoyao Ding",
            "Kevin Song",
            "Shang Wang",
            "Gennady Pekhimenko"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC"
        ],
        "submit_date": "2025-09-30"
    },
    "http://arxiv.org/abs/2510.00183v2": {
        "id": "http://arxiv.org/abs/2510.00183v2",
        "title": "Lattica: A Decentralized Cross-NAT Communication Framework for Scalable AI Inference and Training",
        "link": "http://arxiv.org/abs/2510.00183v2",
        "tags": [
            "networking",
            "training",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-10-01",
        "tldr": "Designs a decentralized cross-NAT framework to enable scalable AI training and inference without centralized infrastructure. Uses NAT traversal, CRDTs, and DHT-based discovery for peer-to-peer model synchronization. Achieves reliable communication in permissionless environments with low latency and high throughput.",
        "abstract": "The rapid expansion of distributed Artificial Intelligence (AI) workloads beyond centralized data centers creates a demand for new communication substrates. These substrates must operate reliably in heterogeneous and permissionless environments, where Network Address Translators (NATs) and firewalls impose significant constraints. Existing solutions, however, are either designed for controlled data center deployments or implemented as monolithic systems that tightly couple machine learning logic with networking code. To address these limitations, we present Lattica, a decentralized cross-NAT communication framework designed to support distributed AI systems. Lattica integrates three core components. First, it employs a robust suite of NAT traversal mechanisms to establish a globally addressable peer-to-peer mesh. Second, it provides a decentralized data store based on Conflict-free Replicated Data Types (CRDTs), ensuring verifiable and eventually consistent state replication. Third, it incorporates a content discovery layer that leverages distributed hash tables (DHTs) together with an optimized RPC protocol for efficient model synchronization. By integrating these components, Lattica delivers a complete protocol stack for sovereign, resilient, and scalable AI systems that operate independently of centralized intermediaries. It is directly applicable to edge intelligence, collaborative reinforcement learning, and other large-scale distributed machine learning scenarios.",
        "authors": [
            "Ween Yang",
            "Jason Liu",
            "Suli Wang",
            "Xinyuan Song",
            "Lynn Ai",
            "Eric Yang",
            "Bill Shi"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-30"
    },
    "http://arxiv.org/abs/2509.26541v2": {
        "id": "http://arxiv.org/abs/2509.26541v2",
        "title": "TASP: Topology-aware Sequence Parallelism",
        "link": "http://arxiv.org/abs/2509.26541v2",
        "tags": [
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-10-01",
        "tldr": "Addresses inefficient communication in sequence parallelism for long-context LLMs by decomposing Ring AllGather into topology-aware concurrent ring paths. TASP exploits AlltoAll accelerator topology to boost communication efficiency, achieving up to 3.58x speedup over Ring Attention.",
        "abstract": "Long-context large language models (LLMs) face constraints due to the quadratic complexity of the self-attention mechanism. The mainstream sequence parallelism (SP) method, Ring Attention, attempts to solve this by distributing the query into multiple query chunks across accelerators and enable each Q tensor to access all KV tensors from other accelerators via the Ring AllGather communication primitive. However, it exhibits low communication efficiency, restricting its practical applicability. This inefficiency stems from the mismatch between the Ring AllGather communication primitive it adopts and the AlltoAll topology of modern accelerators. A Ring AllGather primitive is composed of iterations of ring-styled data transfer, which can only utilize a very limited fraction of an AlltoAll topology.   Inspired by the Hamiltonian decomposition of complete directed graphs, we identify that modern accelerator topology can be decomposed into multiple orthogonal ring datapaths which can concurrently transfer data without interference. Based on this, we further observe that the Ring AllGather primitive can also be decomposed into the same number of concurrent ring-styled data transfer at every iteration. Based on these insights, we propose TASP, a topology-aware SP method for long-context LLMs that fully utilizes the communication capacity of modern accelerators via topology decomposition and primitive decomposition. Experimental results on both single-node and multi-node NVIDIA H100 systems and a single-node AMD MI300X system demonstrate that TASP achieves higher communication efficiency than Ring Attention on these modern accelerator topologies and achieves up to 3.58 speedup than Ring Attention and its variant Zigzag-Ring Attention. The code is available at https://github.com/infinigence/HamiltonAttention.",
        "authors": [
            "Yida Wang",
            "Ke Hong",
            "Xiuhong Li",
            "Yuanchao Xu",
            "Wenxun Wang",
            "Guohao Dai",
            "Yu Wang"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "submit_date": "2025-09-30"
    },
    "http://arxiv.org/abs/2509.26534v1": {
        "id": "http://arxiv.org/abs/2509.26534v1",
        "title": "Rearchitecting Datacenter Lifecycle for AI: A TCO-Driven Framework",
        "link": "http://arxiv.org/abs/2509.26534v1",
        "tags": [
            "offline",
            "hardware",
            "training"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes a holistic TCO-driven framework for AI datacenter lifecycle management across building, hardware refresh, and operation stages. Coordinates design choices in power, cooling, networking, refresh strategies, and software optimizations. Achieves up to 40% reduction in total cost of ownership.",
        "abstract": "The rapid rise of large language models (LLMs) has been driving an enormous demand for AI inference infrastructure, mainly powered by high-end GPUs. While these accelerators offer immense computational power, they incur high capital and operational costs due to frequent upgrades, dense power consumption, and cooling demands, making total cost of ownership (TCO) for AI datacenters a critical concern for cloud providers. Unfortunately, traditional datacenter lifecycle management (designed for general-purpose workloads) struggles to keep pace with AI's fast-evolving models, rising resource needs, and diverse hardware profiles. In this paper, we rethink the AI datacenter lifecycle scheme across three stages: building, hardware refresh, and operation. We show how design choices in power, cooling, and networking provisioning impact long-term TCO. We also explore refresh strategies aligned with hardware trends. Finally, we use operation software optimizations to reduce cost. While these optimizations at each stage yield benefits, unlocking the full potential requires rethinking the entire lifecycle. Thus, we present a holistic lifecycle management framework that coordinates and co-optimizes decisions across all three stages, accounting for workload dynamics, hardware evolution, and system aging. Our system reduces the TCO by up to 40\\% over traditional approaches. Using our framework we provide guidelines on how to manage AI datacenter lifecycle for the future.",
        "authors": [
            "Jovan Stojkovic",
            "Chaojie Zhang",
            "Íñigo Goiri",
            "Ricardo Bianchini"
        ],
        "categories": [
            "cs.AI",
            "cs.AR",
            "cs.DC"
        ],
        "submit_date": "2025-09-30"
    },
    "http://arxiv.org/abs/2509.26529v2": {
        "title": "CSnake: Detecting Self-Sustaining Cascading Failure via Causal Stitching of Fault Propagations",
        "link": "http://arxiv.org/abs/2509.26529v2",
        "abstract": "Recent studies have revealed that self-sustaining cascading failures in distributed systems frequently lead to widespread outages, which are challenging to contain and recover from. Existing failure detection techniques struggle to expose such failures prior to deployment, as they typically require a complex combination of specific conditions to be triggered. This challenge stems from the inherent nature of cascading failures, as they typically involve a sequence of fault propagations, each activated by distinct conditions.   This paper presents CSnake, a fault injection framework to expose self-sustaining cascading failures in distributed systems. CSnake uses the novel idea of causal stitching, which causally links multiple single-fault injections in different tests to simulate complex fault propagation chains. To identify these chains, CSnake designs a counterfactual causality analysis of fault propagations - fault causality analysis (FCA): FCA compares the execution trace of a fault injection run with its corresponding profile run (i.e., same test w/o the injection) and identifies any additional faults triggered, which are considered to have a causal relationship with the injected fault.   To address the large search space of fault and workload combinations, CSnake employs a three-phase allocation protocol of test budget that prioritizes faults with unique and diverse causal consequences, increasing the likelihood of uncovering conditional fault propagations. Furthermore, to avoid incorrectly connecting fault propagations from workloads with incompatible conditions, CSnake performs a local compatibility check that approximately checks the compatibility of the path constraints associated with connected fault propagations with low overhead.   CSnake detected 15 bugs that cause self-sustaining cascading failures in five systems, five of which have been confirmed with two fixed.",
        "authors": [
            "Shangshu Qian",
            "Lin Tan",
            "Yongle Zhang"
        ],
        "categories": [
            "cs.DC",
            "cs.SE"
        ],
        "id": "http://arxiv.org/abs/2509.26529v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-09-30"
    },
    "http://arxiv.org/abs/2509.26300v1": {
        "title": "Tuning the Tuner: Introducing Hyperparameter Optimization for Auto-Tuning",
        "link": "http://arxiv.org/abs/2509.26300v1",
        "abstract": "Automatic performance tuning (auto-tuning) is widely used to optimize performance-critical applications across many scientific domains by finding the best program variant among many choices. Efficient optimization algorithms are crucial for navigating the vast and complex search spaces in auto-tuning. As is well known in the context of machine learning and similar fields, hyperparameters critically shape optimization algorithm efficiency. Yet for auto-tuning frameworks, these hyperparameters are almost never tuned, and their potential performance impact has not been studied.   We present a novel method for general hyperparameter tuning of optimization algorithms for auto-tuning, thus \"tuning the tuner\". In particular, we propose a robust statistical method for evaluating hyperparameter performance across search spaces, publish a FAIR data set and software for reproducibility, and present a simulation mode that replays previously recorded tuning data, lowering the costs of hyperparameter tuning by two orders of magnitude. We show that even limited hyperparameter tuning can improve auto-tuner performance by 94.8% on average, and establish that the hyperparameters themselves can be optimized efficiently with meta-strategies (with an average improvement of 204.7%), demonstrating the often overlooked hyperparameter tuning as a powerful technique for advancing auto-tuning research and practice.",
        "authors": [
            "Floris-Jan Willemsen",
            "Rob V. van Nieuwpoort",
            "Ben van Werkhoven"
        ],
        "categories": [
            "cs.LG",
            "cs.DC",
            "cs.PF"
        ],
        "id": "http://arxiv.org/abs/2509.26300v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-09-30"
    },
    "http://arxiv.org/abs/2509.26253v1": {
        "title": "Efficient Construction of Large Search Spaces for Auto-Tuning",
        "link": "http://arxiv.org/abs/2509.26253v1",
        "abstract": "Automatic performance tuning, or auto-tuning, accelerates high-performance codes by exploring vast spaces of code variants. However, due to the large number of possible combinations and complex constraints, constructing these search spaces can be a major bottleneck. Real-world applications have been encountered where the search space construction takes minutes to hours or even days. Current state-of-the-art techniques for search space construction, such as chain-of-trees, lack a formal foundation and only perform adequately on a specific subset of search spaces.   We show that search space construction for constraint-based auto-tuning can be reformulated as a Constraint Satisfaction Problem (CSP). Building on this insight with a CSP solver, we develop a runtime parser that translates user-defined constraint functions into solver-optimal expressions, optimize the solver to exploit common structures in auto-tuning constraints, and integrate these and other advances in open-source tools. These contributions substantially improve performance and accessibility while preserving flexibility.   We evaluate our approach using a diverse set of benchmarks, demonstrating that our optimized solver reduces construction time by four orders of magnitude versus brute-force enumeration, three orders of magnitude versus an unoptimized CSP solver, and one to two orders of magnitude versus leading auto-tuning frameworks built on chain-of-trees. We thus eliminate a critical scalability barrier for auto-tuning and provide a drop-in solution that enables the exploration of previously unattainable problem scales in auto-tuning and related domains.",
        "authors": [
            "Floris-Jan Willemsen",
            "Rob V. van Nieuwpoort",
            "Ben van Werkhoven"
        ],
        "categories": [
            "cs.DC",
            "cs.PF"
        ],
        "id": "http://arxiv.org/abs/2509.26253v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-09-30"
    },
    "http://arxiv.org/abs/2509.26193v1": {
        "title": "I Like To Move It -- Computation Instead of Data in the Brain",
        "link": "http://arxiv.org/abs/2509.26193v1",
        "abstract": "The detailed functioning of the human brain is still poorly understood. Brain simulations are a well-established way to complement experimental research, but must contend with the computational demands of the approximately $10^{11}$ neurons and the $10^{14}$ synapses connecting them, the network of the latter referred to as the connectome. Studies suggest that changes in the connectome (i.e., the formation and deletion of synapses, also known as structural plasticity) are essential for critical tasks such as memory formation and learning. The connectivity update can be efficiently computed using a Barnes-Hut-inspired approximation that lowers the computational complexity from $O(n^2)$ to $O(n log n)$, where n is the number of neurons. However, updating synapses, which relies heavily on RMA, and the spike exchange between neurons, which requires all-to-all communication at every time step, still hinder scalability. We present a new algorithm that significantly reduces the communication overhead by moving computation instead of data. This shrinks the time it takes to update connectivity by a factor of six and the time it takes to exchange spikes by more than two orders of magnitude.",
        "authors": [
            "Fabian Czappa",
            "Marvin Kaster",
            "Felix Wolf"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2509.26193v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-09-30"
    },
    "http://arxiv.org/abs/2509.26182v1": {
        "id": "http://arxiv.org/abs/2509.26182v1",
        "title": "Parallax: Efficient LLM Inference Service over Decentralized Environment",
        "link": "http://arxiv.org/abs/2509.26182v1",
        "tags": [
            "serving",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-10-01",
        "tldr": "How to efficiently serve LLMs over decentralized, heterogeneous GPU pools? Parallax uses a two-phase scheduler for layer-wise model allocation and dynamic pipeline construction, reducing latency by up to 40% and improving throughput versus decentralized baselines.",
        "abstract": "Deploying a large language model (LLM) inference service remains costly because centralized serving depends on specialized GPU clusters and high-bandwidth interconnects in datacenters. An appealing alternative is to leverage collaborative decentralized GPU pools. However, heterogeneity in GPU and limited interconnected network bandwidth, along with potentially dynamic availability, make efficient scheduling the central challenge in this scenario. In this paper, we present Parallax, a decentralized LLM serving system that turns a pool of heterogeneous GPUs into an efficient inference platform via a two-phase scheduler. Parallax decomposes planning into (i) model allocation, which places layers of each replica across diverse GPUs to jointly optimize latency and throughput under memory and link-bandwidth constraints, and (ii) request-time GPU pipeline selection, which stitches layers from different replicas into end-to-end execution chains that balance load and adapt to current conditions. We implement Parallax and evaluate it on open-source LLMs deployed over real volunteer nodes. Parallax consistently reduces latency and increases throughput relative to decentralized baselines, demonstrating that principled scheduling can make volunteer compute a practical, affordable substrate for LLM inference.   Github Repo at: https://github.com/GradientHQ/parallax.",
        "authors": [
            "Chris Tong",
            "Youhe Jiang",
            "Gufeng Chen",
            "Tianyi Zhao",
            "Sibian Lu",
            "Wenjie Qu",
            "Eric Yang",
            "Lynn Ai",
            "Binhang Yuan"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-30"
    },
    "http://arxiv.org/abs/2509.26120v1": {
        "title": "AGOCS -- Accurate Google Cloud Simulator Framework",
        "link": "http://arxiv.org/abs/2509.26120v1",
        "abstract": "This paper presents the Accurate Google Cloud Simulator (AGOCS) - a novel high-fidelity Cloud workload simulator based on parsing real workload traces, which can be conveniently used on a desktop machine for day-to-day research. Our simulation is based on real-world workload traces from a Google Cluster with 12.5K nodes, over a period of a calendar month. The framework is able to reveal very precise and detailed parameters of the executed jobs, tasks and nodes as well as to provide actual resource usage statistics. The system has been implemented in Scala language with focus on parallel execution and an easy-to-extend design concept. The paper presents the detailed structural framework for AGOCS and discusses our main design decisions, whilst also suggesting alternative and possibly performance enhancing future approaches. The framework is available via the Open Source GitHub repository.",
        "authors": [
            "Leszek Sliwko",
            "Vladimir Getov"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.PF"
        ],
        "id": "http://arxiv.org/abs/2509.26120v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-09-30"
    },
    "http://arxiv.org/abs/2509.26092v2": {
        "title": "Hybrid Dual-Batch and Cyclic Progressive Learning for Efficient Distributed Training",
        "link": "http://arxiv.org/abs/2509.26092v2",
        "abstract": "Distributed machine learning is critical for training deep learning models on large datasets with numerous parameters. Current research primarily focuses on leveraging additional hardware resources and powerful computing units to accelerate the training process. As a result, larger batch sizes are often employed to speed up training. However, training with large batch sizes can lead to lower accuracy due to poor generalization. To address this issue, we propose the dual-batch learning scheme, a distributed training method built on the parameter server framework. This approach maximizes training efficiency by utilizing the largest batch size that the hardware can support while incorporating a smaller batch size to enhance model generalization. By using two different batch sizes simultaneously, this method improves accuracy with minimal additional training time. Additionally, to mitigate the time overhead caused by dual-batch learning, we propose the cyclic progressive learning scheme. This technique repeatedly and gradually increases image resolution from low to high during training, thereby reducing training time. By combining cyclic progressive learning with dual-batch learning, our hybrid approach improves both model generalization and training efficiency. Experimental results with ResNet-18 demonstrate that, compared to conventional training methods, our approach improves accuracy by 3.3% while reducing training time by 10.1% on CIFAR-100, and further achieves a 34.8% reduction in training time on ImageNet.",
        "authors": [
            "Kuan-Wei Lu",
            "Ding-Yong Hong",
            "Pangfeng Liu",
            "Jan-Jan Wu"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "id": "http://arxiv.org/abs/2509.26092v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-09-30"
    },
    "http://arxiv.org/abs/2509.26043v1": {
        "title": "Enabling Time-Aware Priority Traffic Management over Distributed FPGA Nodes",
        "link": "http://arxiv.org/abs/2509.26043v1",
        "abstract": "Network Interface Cards (NICs) greatly evolved from simple basic devices moving traffic in and out of the network to complex heterogeneous systems offloading host CPUs from performing complex tasks on in-transit packets. These latter comprise different types of devices, ranging from NICs accelerating fixed specific functions (e.g., on-the-fly data compression/decompression, checksum computation, data encryption, etc.) to complex Systems-on-Chip (SoC) equipped with both general purpose processors and specialized engines (Smart-NICs). Similarly, Field Programmable Gate Arrays (FPGAs) moved from pure reprogrammable devices to modern heterogeneous systems comprising general-purpose processors, real-time cores and even AI-oriented engines. Furthermore, the availability of high-speed network interfaces (e.g., SFPs) makes modern FPGAs a good choice for implementing Smart-NICs. In this work, we extended the functionalities offered by an open-source NIC implementation (Corundum) by enabling time-aware traffic management in hardware, and using this feature to control the bandwidth associated with different traffic classes. By exposing dedicated control registers on the AXI bus, the driver of the NIC can easily configure the transmission bandwidth of different prioritized queues. Basically, each control register is associated with a specific transmission queue (Corundum can expose up to thousands of transmission and receiving queues), and sets up the fraction of time in a transmission window which the queue is supposed to get access the output port and transmit the packets. Queues are then prioritized and associated to different traffic classes through the Linux QDISC mechanism. Experimental evaluation demonstrates that the approach allows to properly manage the bandwidth reserved to the different transmission flows.",
        "authors": [
            "Alberto Scionti",
            "Paolo Savio",
            "Francesco Lubrano",
            "Federico Stirano",
            "Antonino Nespola",
            "Olivier Terzo",
            "Corrado De Sio",
            "Luca Sterpone"
        ],
        "categories": [
            "cs.DC",
            "cs.AR"
        ],
        "id": "http://arxiv.org/abs/2509.26043v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-09-30"
    },
    "http://arxiv.org/abs/2509.25919v1": {
        "id": "http://arxiv.org/abs/2509.25919v1",
        "title": "Accelerating LLM Inference with Precomputed Query Storage",
        "link": "http://arxiv.org/abs/2509.25919v1",
        "tags": [
            "serving",
            "storage"
        ],
        "relevant": true,
        "indexed_date": "2025-09-16",
        "tldr": "Reduces LLM inference latency by precomputing and storing response pairs for predictable queries. Uses LLM-driven query generation and disk-backed vector indexing for efficient retrieval. Achieves up to 17.3% latency reduction without compromising response quality.",
        "abstract": "Large language model (LLM) inference often suffers from high latency, particularly in resource-constrained environments such as on-device or edge deployments. To address this challenge, we present StorInfer, a novel storage-assisted LLM inference system that accelerates response time by precomputing and storing predictable query-response pairs offline. When a user query semantically matches a precomputed query, StorInfer bypasses expensive GPU inference and instantly returns the stored response, significantly reducing latency and compute costs. To maximize coverage and effectiveness, StorInfer employs an LLM-driven generator that adaptively produces diverse and deduplicated queries based on a given knowledge base. This is achieved via two techniques: adaptive query masking, which prevents regeneration of similar queries, and adaptive sampling, which dynamically tunes generation parameters to promote semantic diversity. The resulting query-response pairs are embedded and indexed using a disk-backed vector database to enable fast, similarity-based retrieval at runtime. Using this approach, we generated 150K unique precomputed pairs (taking up to 830 MB of storage space), achieving up to 17.3% latency reduction with no loss in response quality. Our evaluation across multiple QA datasets demonstrates the practicality and scalability of storage-assisted inference, especially in scenarios with predictable query distributions. StorInfer highlights a promising direction in leveraging storage as a primary enabler for efficient, low-latency LLM deployment.",
        "authors": [
            "Jay H. Park",
            "Youngju Cho",
            "Choungsol Lee",
            "Moonwook Oh",
            "Euiseong Seo"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-09-30"
    },
    "http://arxiv.org/abs/2509.25700v1": {
        "title": "PAST: Pilot and Adaptive Orchestration for Timely and Resilient Service Delivery in Edge-Assisted UAV Networks under Spatio-Temporal Dynamics",
        "link": "http://arxiv.org/abs/2509.25700v1",
        "abstract": "Incentive-driven resource trading is essential for UAV applications with intensive, time-sensitive computing demands. Traditional spot trading suffers from negotiation delays and high energy costs, while conventional futures trading struggles to adapt to the dynamic, uncertain UAV-edge environment. To address these challenges, we propose PAST (pilot-and-adaptive stable trading), a novel framework for edge-assisted UAV networks with spatio-temporal dynamism. PAST integrates two complementary mechanisms: PilotAO (pilot trading agreements with overbooking), a risk-aware, overbooking-enabled early-stage decision-making module that establishes long-term, mutually beneficial agreements and boosts resource utilization; and AdaptAO (adaptive trading agreements with overbooking rate update), an intelligent adaptation module that dynamically updates agreements and overbooking rates based on UAV mobility, supply-demand variations, and agreement performance. Together, these mechanisms enable both stability and flexibility, guaranteeing individual rationality, strong stability, competitive equilibrium, and weak Pareto optimality. Extensive experiments on real-world datasets show that PAST consistently outperforms benchmark methods in decision-making overhead, task completion latency, resource utilization, and social welfare. By combining predictive planning with real-time adjustments, PAST offers a valuable reference on robust and adaptive practice for improving low-altitude mission performance.",
        "authors": [
            "Houyi Qi",
            "Minghui Liwang",
            "Liqun Fu",
            "Sai Zou",
            "Xinlei Yi",
            "Wei Ni",
            "Huaiyu Dai"
        ],
        "categories": [
            "cs.DC",
            "cs.GT",
            "cs.NI"
        ],
        "id": "http://arxiv.org/abs/2509.25700v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-09-30"
    },
    "http://arxiv.org/abs/2510.06228v1": {
        "title": "Layerwise Federated Learning for Heterogeneous Quantum Clients using Quorus",
        "link": "http://arxiv.org/abs/2510.06228v1",
        "abstract": "Quantum machine learning (QML) holds the promise to solve classically intractable problems, but, as critical data can be fragmented across private clients, there is a need for distributed QML in a quantum federated learning (QFL) format. However, the quantum computers that different clients have access to can be error-prone and have heterogeneous error properties, requiring them to run circuits of different depths. We propose a novel solution to this QFL problem, Quorus, that utilizes a layerwise loss function for effective training of varying-depth quantum models, which allows clients to choose models for high-fidelity output based on their individual capacity. Quorus also presents various model designs based on client needs that optimize for shot budget, qubit count, midcircuit measurement, and optimization space. Our simulation and real-hardware results show the promise of Quorus: it increases the magnitude of gradients of higher depth clients and improves testing accuracy by 12.4% on average over the state-of-the-art.",
        "authors": [
            "Jason Han",
            "Nicholas S. DiBrita",
            "Daniel Leeds",
            "Jianqiang Li",
            "Jason Ludmir",
            "Tirthak Patel"
        ],
        "categories": [
            "quant-ph",
            "cs.DC",
            "cs.ET",
            "cs.LG"
        ],
        "id": "http://arxiv.org/abs/2510.06228v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-09-30"
    },
    "http://arxiv.org/abs/2510.00078v1": {
        "id": "http://arxiv.org/abs/2510.00078v1",
        "title": "Adaptive and Resource-efficient Agentic AI Systems for Mobile and Embedded Devices: A Survey",
        "link": "http://arxiv.org/abs/2510.00078v1",
        "tags": [
            "agentic",
            "edge",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Surveys adaptive and resource-efficient agentic AI systems for mobile/embedded devices, covering techniques like elastic inference and test-time adaptation. Focuses on balancing model complexity with constraints like memory, energy, and latency in edge deployments.",
        "abstract": "Foundation models have reshaped AI by unifying fragmented architectures into scalable backbones with multimodal reasoning and contextual adaptation. In parallel, the long-standing notion of AI agents, defined by the sensing-decision-action loop, is entering a new paradigm: with FMs as their cognitive core, agents transcend rule-based behaviors to achieve autonomy, generalization, and self-reflection. This dual shift is reinforced by real-world demands such as autonomous driving, robotics, virtual assistants, and GUI agents, as well as ecosystem advances in embedded hardware, edge computing, mobile deployment platforms, and communication protocols that together enable large-scale deployment. Yet this convergence collides with reality: while applications demand long-term adaptability and real-time interaction, mobile and edge deployments remain constrained by memory, energy, bandwidth, and latency. This creates a fundamental tension between the growing complexity of FMs and the limited resources of deployment environments. This survey provides the first systematic characterization of adaptive, resource-efficient agentic AI systems. We summarize enabling techniques into elastic inference, test-time adaptation, dynamic multimodal integration, and agentic AI applications, and identify open challenges in balancing accuracy-latency-communication trade-offs and sustaining robustness under distribution shifts. We further highlight future opportunities in algorithm-system co-design, cognitive adaptation, and collaborative edge deployment. By mapping FM structures, cognition, and hardware resources, this work establishes a unified perspective toward scalable, adaptive, and resource-efficient agentic AI. We believe this survey can help readers to understand the connections between enabling technologies while promoting further discussions on the fusion of agentic intelligence and intelligent agents.",
        "authors": [
            "Sicong Liu",
            "Weiye Wu",
            "Xiangrui Xu",
            "Teng Li",
            "Bowen Pang",
            "Bin Guo",
            "Zhiwen Yu"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC"
        ],
        "submit_date": "2025-09-30"
    },
    "http://arxiv.org/abs/2509.25605v1": {
        "id": "http://arxiv.org/abs/2509.25605v1",
        "title": "LAPIS: A Performance Portable, High Productivity Compiler Framework",
        "link": "http://arxiv.org/abs/2509.25605v1",
        "tags": [
            "kernel",
            "hardware",
            "sparse"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes LAPIS, an MLIR-based compiler framework for high performance portability across architectures. Focuses on automatic lowering of sparse/dense linear algebra kernels from scientific and AI use cases, facilitating integration of PyTorch and Kokkos. Achieves comparable kernel performance to MLIR on diverse architectures.",
        "abstract": "Portability, performance, and productivity are three critical dimensions for evaluating a programming model or compiler infrastructure. Several modern programming models for computational science focus on performance and portability. On the other end, several machine learning focused programming models focus on portability and productivity. A clear solution that is strong in all three dimensions has yet to emerge. A second related problem arises when use cases from computational science converge with machine learning. The disparate popular frameworks of these fields require programmers to manually integrate codes written in different frameworks. Finally, several programming frameworks lack easy options for extensibility as any new computer architecture change require complex changes to the programming models. We present LAPIS, an MLIR-based compiler that addresses all three of these challenges. We demonstrate that LAPIS can automatically lower sparse and dense linear algebra kernels from computational science and artificial intelligence use cases. We also show how LAPIS facilitates the integration of codes between PyTorch and Kokkos. We compare kernel performance with the default MLIR implementations on diverse architectures to demonstrate portability. By developing a dialect that is built on the principles of the Kokkos ecosystem, LAPIS also allows extensibility of the framework to new architectures.",
        "authors": [
            "Brian Kelley",
            "Sivasankaran Rajamanickam"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-30"
    },
    "http://arxiv.org/abs/2509.25555v1": {
        "id": "http://arxiv.org/abs/2509.25555v1",
        "title": "Enhancing Split Learning with Sharded and Blockchain-Enabled SplitFed Approaches",
        "link": "http://arxiv.org/abs/2509.25555v1",
        "tags": [
            "training",
            "security",
            "edge"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes Sharded SplitFed Learning (SSFL) and Blockchain-enabled SplitFed (BSFL) to address scalability, performance, and security in federated settings. SSFL distributes server workload across shards; BSFL adds blockchain consensus for integrity. Achieves 85.2% scalability improvement and 62.7% higher attack resilience.",
        "abstract": "Collaborative and distributed learning techniques, such as Federated Learning (FL) and Split Learning (SL), hold significant promise for leveraging sensitive data in privacy-critical domains. However, FL and SL suffer from key limitations -- FL imposes substantial computational demands on clients, while SL leads to prolonged training times. To overcome these challenges, SplitFed Learning (SFL) was introduced as a hybrid approach that combines the strengths of FL and SL. Despite its advantages, SFL inherits scalability, performance, and security issues from SL. In this paper, we propose two novel frameworks: Sharded SplitFed Learning (SSFL) and Blockchain-enabled SplitFed Learning (BSFL). SSFL addresses the scalability and performance constraints of SFL by distributing the workload and communication overhead of the SL server across multiple parallel shards. Building upon SSFL, BSFL replaces the centralized server with a blockchain-based architecture that employs a committee-driven consensus mechanism to enhance fairness and security. BSFL incorporates an evaluation mechanism to exclude poisoned or tampered model updates, thereby mitigating data poisoning and model integrity attacks. Experimental evaluations against baseline SL and SFL approaches show that SSFL improves performance and scalability by 31.2% and 85.2%, respectively. Furthermore, BSFL increases resilience to data poisoning attacks by 62.7% while maintaining superior performance under normal operating conditions. To the best of our knowledge, BSFL is the first blockchain-enabled framework to implement an end-to-end decentralized SplitFed Learning system.",
        "authors": [
            "Amirreza Sokhankhosh",
            "Khalid Hassan",
            "Sara Rouhani"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-09-29"
    },
    "http://arxiv.org/abs/2510.03298v2": {
        "title": "CAFL-L: Constraint-Aware Federated Learning with Lagrangian Dual Optimization for On-Device Language Models",
        "link": "http://arxiv.org/abs/2510.03298v2",
        "abstract": "We introduce Constraint-Aware Federated Learning with Lagrangian Dual Optimization (CAFL-L), a principled extension of FedAvg that explicitly incorporates device-level resource constraints including energy, communication, memory, and thermal budgets. CAFL-L employs Lagrangian dual optimization to dynamically adapt training hyperparameters -- freezing depth, local steps, batch size, and communication compression -- while preserving training stability through token-budget preservation via gradient accumulation. Experiments on a character-level language model demonstrate that CAFL-L achieves superior constraint satisfaction compared to standard FedAvg (reducing memory usage by 20% and communication by 95%) while maintaining competitive validation performance, making it practical for deployment on resource-constrained edge devices.",
        "authors": [
            "Dongqi Zheng",
            "Wenjin Fu"
        ],
        "categories": [
            "cs.LG",
            "cs.CL",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.03298v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-09-29"
    },
    "http://arxiv.org/abs/2509.25415v1": {
        "title": "Permuting Transactions in Ethereum Blocks: An Empirical Study",
        "link": "http://arxiv.org/abs/2509.25415v1",
        "abstract": "Several recent proposals implicitly or explicitly suggest making use of randomized transaction ordering within a block to mitigate centralization effects and to improve fairness in the Ethereum ecosystem. However, transactions and blocks are subject to gas limits and protocol rules. In a randomized transaction order, the behavior of transactions may change depending on other transactions in the same block, leading to invalid blocks and varying gas consumptions. In this paper, we quantify and characterize protocol violations, execution errors and deviations in gas consumption of blocks and transactions to examine technical deployability. For that, we permute and execute the transactions of over 335,000 Ethereum Mainnet blocks multiple times. About 22% of block permutations are invalid due to protocol violations caused by privately mined transactions or blocks close to their gas limit. Also, almost all transactions which show execution errors under permutation but not in the original order are privately mined transactions. Only 6% of transactions show deviations in gas consumption and 98% of block permutations deviate at most 10% from their original gas consumption. From a technical perspective, these results suggest that randomized transaction ordering may be feasible if transaction selection is handled carefully.",
        "authors": [
            "Jan Droll"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2509.25415v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-09-29"
    },
    "http://arxiv.org/abs/2509.25155v2": {
        "id": "http://arxiv.org/abs/2509.25155v2",
        "title": "Context-Driven Performance Modeling for Causal Inference Operators on Neural Processing Units",
        "link": "http://arxiv.org/abs/2509.25155v2",
        "tags": [
            "edge",
            "kernel",
            "inference"
        ],
        "relevant": true,
        "indexed_date": "2025-09-16",
        "tldr": "Analyzes performance bottlenecks of causal inference operators for LLMs on NPUs. Benchmarks quadratic attention and sub-quadratic alternatives like structured state-space models to identify memory-bound and compute-bound limitations. Provides insights for hardware-aware optimizations on edge devices.",
        "abstract": "The proliferation of large language models has driven demand for long-context inference on resource-constrained edge platforms. However, deploying these models on Neural Processing Units (NPUs) presents significant challenges due to architectural mismatch: the quadratic complexity of standard attention conflicts with NPU memory and compute patterns. This paper presents a comprehensive performance analysis of causal inference operators on a modern NPU, benchmarking quadratic attention against sub-quadratic alternatives including structured state-space models and causal convolutions. Our analysis reveals a spectrum of critical bottlenecks: quadratic attention becomes severely memory-bound with catastrophic cache inefficiency, while sub-quadratic variants span from compute-bound on programmable vector cores to memory-bound by data movement. These findings provide essential insights for co-designing hardware-aware models and optimization strategies to enable efficient long-context inference on edge platforms.",
        "authors": [
            "Neelesh Gupta",
            "Rakshith Jayanth",
            "Dhruv Parikh",
            "Viktor Prasanna"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-09-29"
    },
    "http://arxiv.org/abs/2509.25121v1": {
        "id": "http://arxiv.org/abs/2509.25121v1",
        "title": "Accelerating Dynamic Image Graph Construction on FPGA for Vision GNNs",
        "link": "http://arxiv.org/abs/2509.25121v1",
        "tags": [
            "hardware",
            "kernel",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Addresses the bottleneck of dynamic image graph construction in Vision GNNs. Proposes a streaming FPGA accelerator with on-chip buffers and parallel sorting to minimize memory traffic. Achieves up to 16.6x speedup over CPU baselines.",
        "abstract": "Vision Graph Neural Networks (Vision GNNs, or ViGs) represent images as unstructured graphs, achieving state of the art performance in computer vision tasks such as image classification, object detection, and instance segmentation. Dynamic Image Graph Construction (DIGC) builds image graphs by connecting patches (nodes) based on feature similarity, and is dynamically repeated in each ViG layer following GNN based patch (node) feature updates. However, DIGC constitutes over 50% of end to end ViG inference latency, rising to 95% at high image resolutions, making it the dominant computational bottleneck. While hardware acceleration holds promise, prior works primarily optimize graph construction algorithmically, often compromising DIGC flexibility, accuracy, or generality. To address these limitations, we propose a streaming, deeply pipelined FPGA accelerator for DIGC, featuring on chip buffers that process input features in small, uniform blocks. Our design minimizes external memory traffic via localized computation and performs efficient parallel sorting with local merge sort and global k way merging directly on streaming input blocks via heap insertion. This modular architecture scales seamlessly across image resolutions, ViG layer types, and model sizes and variants, and supports DIGC across diverse ViG based vision backbones. The design achieves high clock frequencies post place and route due to the statically configured parallelism minimizing critical path delay and delivers up to 16.6x and 6.8x speedups over optimized CPU and GPU DIGC baselines.",
        "authors": [
            "Anvitha Ramachandran",
            "Dhruv Parikh",
            "Viktor Prasanna"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-29"
    },
    "http://arxiv.org/abs/2509.25044v1": {
        "id": "http://arxiv.org/abs/2509.25044v1",
        "title": "A Scalable Distributed Framework for Multimodal GigaVoxel Image Registration",
        "link": "http://arxiv.org/abs/2509.25044v1",
        "tags": [
            "training",
            "kernel",
            "storage"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes FFDP with fused kernels and distributed framework for large-scale image registration. Optimizes non-GEMM bottlenecks with IO-aware kernels and convolution-aware sharding. Achieves 7x speedup and 59% peak memory reduction in multimodal brain registration.",
        "abstract": "In this work, we propose FFDP, a set of IO-aware non-GEMM fused kernels supplemented with a distributed framework for image registration at unprecedented scales. Image registration is an inverse problem fundamental to biomedical and life sciences, but algorithms have not scaled in tandem with image acquisition capabilities. Our framework complements existing model parallelism techniques proposed for large-scale transformer training by optimizing non-GEMM bottlenecks and enabling convolution-aware tensor sharding. We demonstrate unprecedented capabilities by performing multimodal registration of a 100 micron ex-vivo human brain MRI volume at native resolution - an inverse problem more than 570x larger than a standard clinical datum in about a minute using only 8 A6000 GPUs. FFDP accelerates existing state-of-the-art optimization and deep learning registration pipelines by upto 6 - 7x while reducing peak memory consumption by 20 - 59%. Comparative analysis on a 250 micron dataset shows that FFDP can fit upto 64x larger problems than existing SOTA on a single GPU, and highlights both the performance and efficiency gains of FFDP compared to SOTA image registration methods.",
        "authors": [
            "Rohit Jena",
            "Vedant Zope",
            "Pratik Chaudhari",
            "James C. Gee"
        ],
        "categories": [
            "cs.CV",
            "cs.DC"
        ],
        "submit_date": "2025-09-29"
    },
    "http://arxiv.org/abs/2509.25041v2": {
        "id": "http://arxiv.org/abs/2509.25041v2",
        "title": "GRACE-MoE: Grouping and Replication with Locality-Aware Routing for Efficient Distributed MoE Inference",
        "link": "http://arxiv.org/abs/2509.25041v2",
        "tags": [
            "MoE",
            "serving",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-09-16",
        "tldr": "Addresses communication overhead and load imbalance in distributed Mixture-of-Experts inference. Proposes GRACE-MoE, a framework with expert grouping, dynamic replication, and locality-aware routing. Achieves up to 3.79x end-to-end inference speedup.",
        "abstract": "Sparse Mixture of Experts (SMoE) performs conditional computation by selectively activating a subset of experts, thereby enabling scalable parameter growth in large language models (LLMs). However, the expanded parameter scale exceeds the memory capacity of a single device, necessitating distributed deployment for inference. This setup introduces two critical challenges: (1) Communication Issue: Transferring features to devices with activated experts leads to significant communication overhead. (2) Computational Load Issue: Skewed expert activation overloads certain GPUs, resulting in load imbalance across devices. Among these, communication overhead is identified as the main bottleneck in SMoE inference. Nevertheless, reducing communication between devices may exacerbate computational load imbalance, leading to device idleness and resource waste. Therefore, we present GRACE-MoE, short for Grouping and Replication with Locality-Aware Routing for SMoE inference. GRACE-MoE is a co-optimization framework that jointly reduces communication overhead and alleviates computational load imbalance. Specifically, the framework comprises two key phases: (1) Grouping & Replication: This phase groups experts based on their affinity to reduce cross-device communication. Additionally, dynamic replication is applied to address load skew, improving computational load balance across GPUs. (2) Routing: This phase employs a locality-aware routing strategy with load prediction. It prioritizes local replicas to minimize communication overhead and balances requests across remote replicas when necessary. Experiments on diverse models and multi-node, multi-GPU environments demonstrate that GRACE-MoE efficiently reduces end-to-end inference latency, achieving up to 3.79x speedup over state-of-the-art systems. Code for GRACE-MoE will be released upon acceptance.",
        "authors": [
            "Yu Han",
            "Lehan Pan",
            "Jie Peng",
            "Ziyang Tao",
            "Wuyang Zhang",
            "Yanyong Zhang"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-29"
    },
    "http://arxiv.org/abs/2510.03293v1": {
        "id": "http://arxiv.org/abs/2510.03293v1",
        "title": "From Score Distributions to Balance: Plug-and-Play Mixture-of-Experts Routing",
        "link": "http://arxiv.org/abs/2510.03293v1",
        "tags": [
            "MoE",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-09-16",
        "tldr": "How to improve MoE inference efficiency without retraining? LASER dynamically balances expert load using gate score distributions, routing tokens to least-loaded experts when scores are ambiguous. Achieves up to 30% lower latency and higher throughput on Mixtral and DeepSeek-MoE with near-zero accuracy drop.",
        "abstract": "Mixture-of-Experts (MoE) models can scale parameter capacity by routing each token to a subset of experts through a learned gate function. While conditional routing reduces training costs, it shifts the burden on inference memory: expert parameters and activations consume memory, limiting the number of experts per device. As tokens are routed, some experts become overloaded while others are underutilized. Because experts are mapped to GPUs, this imbalance translates directly into degraded system performance in terms of latency, throughput, and cost. We present LASER, a plug-and-play, inference-time routing algorithm that balances load while preserving accuracy. LASER adapts to the shape of the gate's score distribution. When scores provide a clear preference, it routes to the strongest experts; when scores are more uniform, it broadens the set of viable experts and routes to the least-loaded among them. Because LASER relies only on gate scores from a trained model, it integrates directly into existing MoE inference pipelines without retraining or finetuning. We evaluate LASER on Mixtral-8x7B and DeepSeek-MoE-16b-chat across four datasets (ARC-Easy, ARC-Challenge, MMLU, and GSM8K). LASER improves load balancing, translating into lower latency and higher throughput, while keeping the accuracy changes negligible.",
        "authors": [
            "Rana Shahout",
            "Colin Cai",
            "Yilun Du",
            "Minlan Yu",
            "Michael Mitzenmacher"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC"
        ],
        "submit_date": "2025-09-29"
    },
    "http://arxiv.org/abs/2509.24946v2": {
        "title": "A Management Framework for Vehicular Cloudtoward Economic and Environmental Efficiency",
        "link": "http://arxiv.org/abs/2509.24946v2",
        "abstract": "Vehicular Cloud Computing (VCC) leverages the idle computing capacity of vehicles to execute end-users' offloaded tasks without requiring new computation infrastructure. Despite its conceptual appeal, VCC adoption is hindered by the lack of quantitative evidence demonstrating its profitability and environmental advantages in real-world scenarios. This paper tackles the fundamental question: Can VCC be both profitable and sustainable? We address this problem by proposing a management scheme for VCC that combines energy-aware task allocation with a game-theoretic revenue-sharing mechanism. Our framework is the first to jointly model latency, energy consumption, monetary incentives, and carbon emissions within urban mobility and 5G communication settings. The task allocation strategy maximizes the aggregate stakeholder utility while satisfying deadlines and minimizing energy costs. The payoffs are distributed via a coalitional game theory adapted to dynamic vehicular environments, to prevent disincentivizing participants with potentially negative contributions. Extensive simulations demonstrate that our approach supports low-latency task execution, enables effective monetization of vehicular resources, and reduces CO2 emissions by more than 99% compared to conventional edge infrastructures, making VCC a practical and sustainable alternative to edge computing.",
        "authors": [
            "Rosario Patanè",
            "Andrea Araldo",
            "Nadjib Achir",
            "Lila Boukhatem"
        ],
        "categories": [
            "cs.GT",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2509.24946v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-09-29"
    },
    "http://arxiv.org/abs/2510.05118v1": {
        "title": "Lumos: Performance Characterization of WebAssembly as a Serverless Runtime in the Edge-Cloud Continuum",
        "link": "http://arxiv.org/abs/2510.05118v1",
        "abstract": "WebAssembly has emerged as a lightweight and portable runtime to execute serverless functions, particularly in heterogeneous and resource-constrained environments such as the Edge Cloud Continuum. However, the performance benefits versus trade-offs remain insufficiently understood. This paper presents Lumos, a performance model and benchmarking tool for characterizing serverless runtimes. Lumos identifies workload, system, and environment-level performance drivers in the Edge-Cloud Continuum. We benchmark state-of-the-art containers and the Wasm runtime in interpreted mode and with ahead-of-time compilation. Our performance characterization shows that AoT-compiled Wasm images are up to 30x smaller and decrease cold-start latency by up to 16% compared to containers, while interpreted Wasm suffers up to 55x higher warm latency and up to 10x I/O-serialization overhead.",
        "authors": [
            "Cynthia Marcelino",
            "Noah Krennmair",
            "Thomas Pusztai",
            "Stefan Nastic"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.05118v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-09-29"
    },
    "http://arxiv.org/abs/2509.24932v3": {
        "title": "Graph Theory Meets Federated Learning over Satellite Constellations: Spanning Aggregations, Network Formation, and Performance Optimization",
        "link": "http://arxiv.org/abs/2509.24932v3",
        "abstract": "In this work, we introduce Fed-Span: \\textit{\\underline{fed}erated learning with \\underline{span}ning aggregation over low Earth orbit (LEO) satellite constellations}. Fed-Span aims to address critical challenges inherent to distributed learning in dynamic satellite networks, including intermittent satellite connectivity, heterogeneous computational capabilities of satellites, and time-varying satellites' datasets. At its core, Fed-Span leverages minimum spanning tree (MST) and minimum spanning forest (MSF) topologies to introduce spanning model aggregation and dispatching processes for distributed learning. To formalize Fed-Span, we offer a fresh perspective on MST/MSF topologies by formulating them through a set of continuous constraint representations (CCRs), thereby integrating these topologies into a distributed learning framework for satellite networks. Using these CCRs, we obtain the energy consumption and latency of operations in Fed-Span. Moreover, we derive novel convergence bounds for Fed-Span, accommodating its key system characteristics and degrees of freedom (i.e., tunable parameters). Finally, we propose a comprehensive optimization problem that jointly minimizes model prediction loss, energy consumption, and latency of {Fed-Span}. We unveil that this problem is NP-hard and develop a systematic approach to transform it into a geometric programming formulation, solved via successive convex optimization with performance guarantees. Through evaluations on real-world datasets, we demonstrate that Fed-Span outperforms existing methods, with faster model convergence, greater energy efficiency, and reduced latency.",
        "authors": [
            "Fardis Nadimi",
            "Payam Abdisarabshali",
            "Jacob Chakareski",
            "Nicholas Mastronarde",
            "Seyyedali Hosseinalipour"
        ],
        "categories": [
            "cs.DC",
            "cs.LG",
            "cs.NI"
        ],
        "id": "http://arxiv.org/abs/2509.24932v3",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-09-29"
    },
    "http://arxiv.org/abs/2509.24859v1": {
        "id": "http://arxiv.org/abs/2509.24859v1",
        "title": "HAPT: Heterogeneity-Aware Automated Parallel Training on Heterogeneous Clusters",
        "link": "http://arxiv.org/abs/2509.24859v1",
        "tags": [
            "training"
        ],
        "relevant": true,
        "indexed_date": "2025-09-16",
        "tldr": "HAPT automates parallel training on heterogeneous GPU clusters by optimizing inter-operator parallel strategies and adaptive 1F1B scheduling to maximize computation-communication overlap, achieving 1.3x-1.6x higher training throughput than existing frameworks.",
        "abstract": "With the rapid evolution of GPU architectures, the heterogeneity of model training infrastructures is steadily increasing. In such environments, effectively utilizing all available heterogeneous accelerators becomes critical for distributed model training. However, existing frameworks, which are primarily designed for homogeneous clusters, often exhibit significant resource underutilization when deployed on heterogeneous accelerators and networks. In this paper, we present Hapt, an automated parallel training framework designed specifically for heterogeneous clusters. Hapt introduces a fine-grained planner that efficiently searches a wide space for the inter-operator parallel strategy, enabling Hapt to alleviate communication overheads while maintaining balanced loads across heterogeneous accelerators. In addition, Hapt implements a heterogeneity-aware 1F1B scheduler that adaptively adjusts the execution timing and ordering of microbatches based on network characteristics, maximizing computation-communication overlap under cross-cluster interconnects while incurring only minimal memory overhead. Our evaluation results show that Hapt can deliver 1.3x-1.6x higher performance on heterogeneous clusters than state-of-the-art training frameworks.",
        "authors": [
            "Antian Liang",
            "Zhigang Zhao",
            "Kai Zhang",
            "Xuri Shi",
            "Chuantao Li",
            "Chunxiao Wang",
            "Zhenying He",
            "Yinan Jing",
            "X. Sean Wang"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-29"
    },
    "http://arxiv.org/abs/2510.15917v1": {
        "id": "http://arxiv.org/abs/2510.15917v1",
        "title": "Intent-Driven Storage Systems: From Low-Level Tuning to High-Level Understanding",
        "link": "http://arxiv.org/abs/2510.15917v1",
        "tags": [
            "RAG",
            "storage",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes Intent-Driven Storage Systems (IDSS) using LLMs to infer workload intent for adaptive storage parameter reconfiguration. Integrates LLMs into control loops with policy guardrails, generating configurations for components like caching. Achieves up to 2.45x IOPS improvement on FileBench workloads.",
        "abstract": "Existing storage systems lack visibility into workload intent, limiting their ability to adapt to the semantics of modern, large-scale data-intensive applications. This disconnect leads to brittle heuristics and fragmented, siloed optimizations. To address these limitations, we propose Intent-Driven Storage Systems (IDSS), a vision for a new paradigm where large language models (LLMs) infer workload and system intent from unstructured signals to guide adaptive and cross-layer parameter reconfiguration. IDSS provides holistic reasoning for competing demands, synthesizing safe and efficient decisions within policy guardrails. We present four design principles for integrating LLMs into storage control loops and propose a corresponding system architecture. Initial results on FileBench workloads show that IDSS can improve IOPS by up to 2.45X by interpreting intent and generating actionable configurations for storage components such as caching and prefetching. These findings suggest that, when constrained by guardrails and embedded within structured workflows, LLMs can function as high-level semantic optimizers, bridging the gap between application goals and low-level system control. IDSS points toward a future in which storage systems are increasingly adaptive, autonomous, and aligned with dynamic workload demands.",
        "authors": [
            "Shai Bergman",
            "Won Wook Song",
            "Lukas Cavigelli",
            "Konstantin Berestizshevsky",
            "Ke Zhou",
            "Ji Zhang"
        ],
        "categories": [
            "cs.AR",
            "cs.AI",
            "cs.DC"
        ],
        "submit_date": "2025-09-29"
    },
    "http://arxiv.org/abs/2509.24626v1": {
        "id": "http://arxiv.org/abs/2509.24626v1",
        "title": "SparseServe: Unlocking Parallelism for Dynamic Sparse Attention in Long-Context LLM Serving",
        "link": "http://arxiv.org/abs/2509.24626v1",
        "tags": [
            "serving",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-09-16",
        "tldr": "How to enable efficient serving of long-context LLMs using dynamic sparse attention? SparseServe introduces hierarchical HBM-DRAM KV cache management with fragmentation-aware transfers, working-set-aware batching, and layer-segmented prefill, achieving up to 9.26x lower TTFT and 3.14x higher throughput.",
        "abstract": "Serving long-context LLMs is costly because attention computation grows linearly with context length. Dynamic sparse attention algorithms (DSAs) mitigate this by attending only to the key-value (KV) cache of critical tokens. However, with DSAs, the main performance bottleneck shifts from HBM bandwidth to HBM capacity: KV caches for unselected tokens must remain in HBM for low-latency decoding, constraining parallel batch size and stalling further throughput gains. Offloading these underutilized KV caches to DRAM could free HBM capacity, allowing larger parallel batch sizes. Yet, achieving such hierarchical HBM-DRAM storage raises new challenges, including fragmented KV cache access, HBM cache contention, and high HBM demands of hybrid batching, that remain unresolved in prior work.   This paper proposes SparseServe, an LLM serving system that unlocks the parallel potential of DSAs through efficient hierarchical HBM-DRAM management. SparseServe introduces three key innovations to address the challenges mentioned above: (1) fragmentation-aware KV cache transfer, which accelerates HBM-DRAM data movement through GPU-direct loading (FlashH2D) and CPU-assisted saving (FlashD2H); (2) working-set-aware batch size control that adjusts batch sizes based on real-time working set estimation to minimize HBM cache thrashing; (3) layer-segmented prefill that bounds HBM use during prefill to a single layer, enabling efficient execution even for long prompts. Extensive experimental results demonstrate that SparseServe achieves up to 9.26x lower mean time-to-first-token (TTFT) latency and up to 3.14x higher token generation throughput compared to state-of-the-art LLM serving systems.",
        "authors": [
            "Qihui Zhou",
            "Peiqi Yin",
            "Pengfei Zuo",
            "James Cheng"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-29"
    },
    "http://arxiv.org/abs/2509.25285v1": {
        "title": "ActorDB: A Unified Database Model Integrating Single-Writer Actors, Incremental View Maintenance, and Zero-Trust Messaging",
        "link": "http://arxiv.org/abs/2509.25285v1",
        "abstract": "This paper presents ActorDB ( Dekigoto ) , a novel database architecture that tightly integrates a single-writer actor model for writes, Incremental View Maintenance (IVM), and a zero-trust security model as a core component. The primary contribution of this work is the unification of these powerful but complex concepts into a single, cohesive system designed to reduce architectural complexity for developers of modern, data-intensive applications. We argue that by providing these capabilities out-of-the-box, ActorDB can offer a more robust, secure, and developer-friendly platform compared to solutions that require manual integration of separate systems for actor persistence, stream processing, and security. We present the core architecture, discuss the critical trade-offs in its design, and define the performance criteria for a Minimum Viable Product (MVP) to validate our approach.",
        "authors": [
            "Jun Kawasaki"
        ],
        "categories": [
            "cs.DB",
            "cs.CL",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2509.25285v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-09-29"
    },
    "http://arxiv.org/abs/2510.02371v1": {
        "title": "Federated Spatiotemporal Graph Learning for Passive Attack Detection in Smart Grids",
        "link": "http://arxiv.org/abs/2510.02371v1",
        "abstract": "Smart grids are exposed to passive eavesdropping, where attackers listen silently to communication links. Although no data is actively altered, such reconnaissance can reveal grid topology, consumption patterns, and operational behavior, creating a gateway to more severe targeted attacks. Detecting this threat is difficult because the signals it produces are faint, short-lived, and often disappear when traffic is examined by a single node or along a single timeline. This paper introduces a graph-centric, multimodal detector that fuses physical-layer and behavioral indicators over ego-centric star subgraphs and short temporal windows to detect passive attacks. To capture stealthy perturbations, a two-stage encoder is introduced: graph convolution aggregates spatial context across ego-centric star subgraphs, while a bidirectional GRU models short-term temporal dependencies. The encoder transforms heterogeneous features into a unified spatio-temporal representation suitable for classification. Training occurs in a federated learning setup under FedProx, improving robustness to heterogeneous local raw data and contributing to the trustworthiness of decentralized training; raw measurements remain on client devices. A synthetic, standards-informed dataset is generated to emulate heterogeneous HAN/NAN/WAN communications with wireless-only passive perturbations, event co-occurrence, and leak-safe splits. The model achieves a testing accuracy of 98.32% per-timestep (F1_{attack}=0.972) and 93.35% per-sequence at 0.15% FPR using a simple decision rule with run-length m=2 and threshold $τ=0.55$. The results demonstrate that combining spatial and temporal context enables reliable detection of stealthy reconnaissance while maintaining low false-positive rates, making the approach suitable for non-IID federated smart-grid deployments.",
        "authors": [
            "Bochra Al Agha",
            "Razane Tajeddine"
        ],
        "categories": [
            "cs.CR",
            "cs.AI",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.02371v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-09-29"
    },
    "http://arxiv.org/abs/2509.24444v2": {
        "title": "BugMagnifier: TON Transaction Simulator for Revealing Smart Contract Vulnerabilities",
        "link": "http://arxiv.org/abs/2509.24444v2",
        "abstract": "The Open Network (TON) blockchain employs an asynchronous execution model that introduces unique security challenges for smart contracts, particularly race conditions arising from unpredictable message processing order. While previous work established vulnerability patterns through static analysis of audit reports, dynamic detection of temporal dependencies through systematic testing remains an open problem. We present BugMagnifier, a transaction simulation framework that systematically reveals vulnerabilities in TON smart contracts through controlled message orchestration. Built atop TON Sandbox and integrated with the TON Virtual Machine (TVM), our tool combines precise message queue manipulation with differential state analysis and probabilistic permutation testing to detect asynchronous execution flaws. Experimental evaluation demonstrates BugMagnifier's effectiveness through extensive parametric studies on purpose-built vulnerable contracts, revealing message ratio-dependent detection complexity that aligns with theoretical predictions. This quantitative model enables predictive vulnerability assessment while shifting discovery from manual expert analysis to automated evidence generation. By providing reproducible test scenarios for temporal vulnerabilities, BugMagnifier addresses a critical gap in the TON security tooling, offering practical support for safer smart contract development in asynchronous blockchain environments.",
        "authors": [
            "Yury Yanovich",
            "Victoria Kovalevskaya",
            "Maksim Egorov",
            "Elizaveta Smirnova",
            "Matvey Mishuris",
            "Yash Madhwal",
            "Kirill Ziborov",
            "Vladimir Gorgadze",
            "Subodh Sharma"
        ],
        "categories": [
            "cs.CR",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2509.24444v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-09-29"
    },
    "http://arxiv.org/abs/2509.24381v1": {
        "id": "http://arxiv.org/abs/2509.24381v1",
        "title": "RServe: Overlapping Encoding and Prefill for Efficient LMM Inference",
        "link": "http://arxiv.org/abs/2509.24381v1",
        "tags": [
            "serving",
            "multi-modal"
        ],
        "relevant": true,
        "indexed_date": "2025-09-16",
        "tldr": "Addressing high latency in LMM inference, REDServe overlaps multimodal encoding with language model prefill via disaggregation and fine-grained scheduling, achieving up to 66% lower latency and 109% higher throughput.",
        "abstract": "Large multimodal models (LMMs) typically employ an encoding module to transform multimodal data inputs into embeddings, which are then fed to language models for further processing. However, efficiently serving LMMs remains highly challenging due to the inherent complexity of their inference pipelines. Traditional serving engines co-locate the encoding module and the language model, leading to significant resource interference and tight data dependency. Recent studies have alleviated this issue by disaggregating the encoding module from the model, following a design style of prefill-decode disaggregation. Nevertheless, these approaches fail to fully exploit parallelism both within individual requests (intra-request) and across multiple requests (inter-request).   To overcome the limitation, we propose REDServe, an LMM inference system that efficiently orchestrates intra- and inter-request pipelines. REDServe is designed to reduce low latency and maximize parallelism at both intra- and inter-request granularities. Built on the disaggregated architecture of the encoding module and language model, REDServe adopts a fine-grained scheduling method that overlaps multimodal encoding with the forward computation of the language model within a single request. For inter-request pipeline, REDServe leverages schedulable tokens and token budgets to balance computational loads across micro-batches. Combined with chunked prefill, this enables a novel scheduling strategy that coordinates the execution of intra- and inter-request pipelines. Experimental evaluations on representative LMMs show that REDServe achieves substantial latency reduction of up to 66% while improving throughput by up to 109%, significantly outperforming existing serving approaches.",
        "authors": [
            "Tianyu Guo",
            "Tianming Xu",
            "Xianjie Chen",
            "Junru Chen",
            "Nong Xiao",
            "Xianwei Zhang"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-29"
    },
    "http://arxiv.org/abs/2510.03288v2": {
        "title": "LogAction: Consistent Cross-system Anomaly Detection through Logs via Active Domain Adaptation",
        "link": "http://arxiv.org/abs/2510.03288v2",
        "abstract": "Log-based anomaly detection is a essential task for ensuring the reliability and performance of software systems. However, the performance of existing anomaly detection methods heavily relies on labeling, while labeling a large volume of logs is highly challenging. To address this issue, many approaches based on transfer learning and active learning have been proposed. Nevertheless, their effectiveness is hindered by issues such as the gap between source and target system data distributions and cold-start problems. In this paper, we propose LogAction, a novel log-based anomaly detection model based on active domain adaptation. LogAction integrates transfer learning and active learning techniques. On one hand, it uses labeled data from a mature system to train a base model, mitigating the cold-start issue in active learning. On the other hand, LogAction utilize free energy-based sampling and uncertainty-based sampling to select logs located at the distribution boundaries for manual labeling, thus addresses the data distribution gap in transfer learning with minimal human labeling efforts. Experimental results on six different combinations of datasets demonstrate that LogAction achieves an average 93.01% F1 score with only 2% of manual labels, outperforming some state-of-the-art methods by 26.28%. Website: https://logaction.github.io",
        "authors": [
            "Chiming Duan",
            "Minghua He",
            "Pei Xiao",
            "Tong Jia",
            "Xin Zhang",
            "Zhewei Zhong",
            "Xiang Luo",
            "Yan Niu",
            "Lingzhe Zhang",
            "Yifan Wu",
            "Siyu Yu",
            "Weijie Hong",
            "Ying Li",
            "Gang Huang"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC",
            "cs.SE"
        ],
        "id": "http://arxiv.org/abs/2510.03288v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-09-29"
    },
    "http://arxiv.org/abs/2509.24305v1": {
        "id": "http://arxiv.org/abs/2509.24305v1",
        "title": "Asynchronous Policy Gradient Aggregation for Efficient Distributed Reinforcement Learning",
        "link": "http://arxiv.org/abs/2509.24305v1",
        "tags": [
            "training",
            "RL",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes asynchronous policy gradient aggregation algorithms (Rennala/Malenia NIGT) for efficient distributed RL. Addresses heterogeneous computations and communication bottlenecks, with improved theoretical complexity and experiments showing significant speedup over prior methods.",
        "abstract": "We study distributed reinforcement learning (RL) with policy gradient methods under asynchronous and parallel computations and communications. While non-distributed methods are well understood theoretically and have achieved remarkable empirical success, their distributed counterparts remain less explored, particularly in the presence of heterogeneous asynchronous computations and communication bottlenecks. We introduce two new algorithms, Rennala NIGT and Malenia NIGT, which implement asynchronous policy gradient aggregation and achieve state-of-the-art efficiency. In the homogeneous setting, Rennala NIGT provably improves the total computational and communication complexity while supporting the AllReduce operation. In the heterogeneous setting, Malenia NIGT simultaneously handles asynchronous computations and heterogeneous environments with strictly better theoretical guarantees. Our results are further corroborated by experiments, showing that our methods significantly outperform prior approaches.",
        "authors": [
            "Alexander Tyurin",
            "Andrei Spiridonov",
            "Varvara Rudenko"
        ],
        "categories": [
            "cs.LG",
            "cs.DC",
            "math.OC"
        ],
        "submit_date": "2025-09-29"
    },
    "http://arxiv.org/abs/2509.25279v2": {
        "id": "http://arxiv.org/abs/2509.25279v2",
        "title": "RL in the Wild: Characterizing RLVR Training in LLM Deployment",
        "link": "http://arxiv.org/abs/2509.25279v2",
        "tags": [
            "training",
            "RL",
            "sparse"
        ],
        "relevant": true,
        "indexed_date": "2025-09-16",
        "tldr": "Characterizes system challenges in RLVR training for LLMs, identifying GPU idling and load imbalance due to skewed sequence lengths and workload variations. Proposes PolyTrace benchmark suite for realistic evaluation, achieving 94.7% accuracy in workload simulation.",
        "abstract": "Large Language Models (LLMs) are now widely used across many domains. With their rapid development, Reinforcement Learning with Verifiable Rewards (RLVR) has surged in recent months to enhance their reasoning and understanding abilities. However, its complex data flows and diverse tasks pose substantial challenges to RL training systems, and there is limited understanding of RLVR from a system perspective. To thoroughly understand the system challenges introduced by RLVR, we present a characterization study of RLVR tasks in our LLM deployment. Specifically, we investigate the distribution and variation trends of workloads across different RL tasks across training steps. We identify issues such as GPU idling caused by skewed sequence length distribution, inefficient parallel strategies in dynamically varying workloads, inefficient data management mechanisms, and load imbalance. We describe our observations and call for further investigation into the remaining open challenges. Furthermore, we propose PolyTrace benchmark suite to conduct evaluation with realistic workloads, and a practical use case validates that PolyTrace benchmark suite exhibits 94.7% accuracy.",
        "authors": [
            "Jiecheng Zhou",
            "Qinghao Hu",
            "Yuyang Jin",
            "Zerui Wang",
            "Peng Sun",
            "Yuzhe Gu",
            "Wenwei Zhang",
            "Mingshu Zhai",
            "Xingcheng Zhang",
            "Weiming Zhang"
        ],
        "categories": [
            "cs.AI",
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-09-29"
    },
    "http://arxiv.org/abs/2509.24063v1": {
        "title": "TeraAgent: A Distributed Agent-Based Simulation Engine for Simulating Half a Trillion Agents",
        "link": "http://arxiv.org/abs/2509.24063v1",
        "abstract": "Agent-based simulation is an indispensable paradigm for studying complex systems. These systems can comprise billions of agents, requiring the computing resources of multiple servers to simulate. Unfortunately, the state-of-the-art platform, BioDynaMo, does not scale out across servers due to its shared-memory-based implementation.   To overcome this key limitation, we introduce TeraAgent, a distributed agent-based simulation engine. A critical challenge in distributed execution is the exchange of agent information across servers, which we identify as a major performance bottleneck. We propose two solutions: 1) a tailored serialization mechanism that allows agents to be accessed and mutated directly from the receive buffer, and 2) leveraging the iterative nature of agent-based simulations to reduce data transfer with delta encoding.   Built on our solutions, TeraAgent enables extreme-scale simulations with half a trillion agents (an 84x improvement), reduces time-to-result with additional compute nodes, improves interoperability with third-party tools, and provides users with more hardware flexibility.",
        "authors": [
            "Lukas Breitwieser",
            "Ahmad Hesam",
            "Abdullah Giray Yağlıkçı",
            "Mohammad Sadrosadati",
            "Fons Rademakers",
            "Onur Mutlu"
        ],
        "categories": [
            "cs.DC",
            "cs.CE",
            "cs.MA",
            "cs.PF",
            "q-bio.QM"
        ],
        "id": "http://arxiv.org/abs/2509.24063v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-09-28"
    },
    "http://arxiv.org/abs/2509.24030v1": {
        "title": "From Edge to HPC: Investigating Cross-Facility Data Streaming Architectures",
        "link": "http://arxiv.org/abs/2509.24030v1",
        "abstract": "In this paper, we investigate three cross-facility data streaming architectures, Direct Streaming (DTS), Proxied Streaming (PRS), and Managed Service Streaming (MSS). We examine their architectural variations in data flow paths and deployment feasibility, and detail their implementation using the Data Streaming to HPC (DS2HPC) architectural framework and the SciStream memory-to-memory streaming toolkit on the production-grade Advanced Computing Ecosystem (ACE) infrastructure at Oak Ridge Leadership Computing Facility (OLCF). We present a workflow-specific evaluation of these architectures using three synthetic workloads derived from the streaming characteristics of scientific workflows. Through simulated experiments, we measure streaming throughput, round-trip time, and overhead under work sharing, work sharing with feedback, and broadcast and gather messaging patterns commonly found in AI-HPC communication motifs. Our study shows that DTS offers a minimal-hop path, resulting in higher throughput and lower latency, whereas MSS provides greater deployment feasibility and scalability across multiple users but incurs significant overhead. PRS lies in between, offering a scalable architecture whose performance matches DTS in most cases.",
        "authors": [
            "Anjus George",
            "Michael Brim",
            "Christopher Zimmer",
            "David Rogers",
            "Sarp Oral",
            "Zach Mayes"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.SE"
        ],
        "id": "http://arxiv.org/abs/2509.24030v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-09-28"
    },
    "http://arxiv.org/abs/2510.03283v1": {
        "id": "http://arxiv.org/abs/2510.03283v1",
        "title": "MACE: A Hybrid LLM Serving System with Colocated SLO-aware Continuous Retraining Alignment",
        "link": "http://arxiv.org/abs/2510.03283v1",
        "tags": [
            "serving",
            "training",
            "edge"
        ],
        "relevant": true,
        "indexed_date": "2025-09-16",
        "tldr": "How to jointly serve LLM inference and fine-tuning on edge devices without violating SLOs? MACE collocates inference and iteration-level retraining with dynamic GPU resource allocation, achieving up to 63% lower latency while maintaining >85% GPU utilization on NVIDIA AGX Orin.",
        "abstract": "Large language models (LLMs) deployed on edge servers are increasingly used in latency-sensitive applications such as personalized assistants, recommendation, and content moderation. However, the non-stationary nature of user data necessitates frequent retraining, which introduces a fundamental tension between inference latency and model accuracy under constrained GPU resources. Existing retraining strategies either delay model updates, over-commit resources to retraining, or overlook iteration-level retraining granularity. In this paper, we identify that iteration-level scheduling is crucial for adapting retraining frequency to model drift without violating service-level objectives (SLOs). We propose MACE, a hybrid LLM system that colocates concurrent inference (prefill, decode) and fine-tuning, with intelligent memory management to maximize task performance while promising inference throughput. MACE leverages the insight that not all model updates equally affect output alignment and allocates GPU cycles accordingly to balance throughput, latency, and update freshness. Our trace-driven evaluation shows that MACE matches or exceeds continuous retraining while reducing inference latency by up to 63% and maintaining throughput under resource constraints. Compared to periodic retraining, MACE improves latency breakdown across prefill, decode, and finetune stages, and sustains GPU utilization above 85% in NVIDIA AGX Orin. These results demonstrate that iteration-level hybrid scheduling is a promising direction for deploying LLMs with continual learning capabilities on edge platforms.",
        "authors": [
            "Yufei Li",
            "Yu Fu",
            "Yue Dong",
            "Cong Liu"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "cs.DC"
        ],
        "submit_date": "2025-09-28"
    },
    "http://arxiv.org/abs/2509.23984v1": {
        "title": "Multiple Concurrent Proposers: Why and How",
        "link": "http://arxiv.org/abs/2509.23984v1",
        "abstract": "Traditional single-proposer blockchains suffer from miner extractable value (MEV), where validators exploit their serial monopoly on transaction inclusion and ordering to extract rents from users. While there have been many developments at the application layer to reduce the impact of MEV, these approaches largely require auctions as a subcomponent. Running auctions efficiently on chain requires two key properties of the underlying consensus protocol: selective-censorship resistance and hiding. These properties guarantee that an adversary can neither selectively delay transactions nor see their contents before they are confirmed. We propose a multiple concurrent proposer (MCP) protocol offering exactly these properties.",
        "authors": [
            "Pranav Garimidi",
            "Joachim Neu",
            "Max Resnick"
        ],
        "categories": [
            "cs.CR",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2509.23984v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-09-28"
    },
    "http://arxiv.org/abs/2509.23803v1": {
        "title": "FedAgentBench: Towards Automating Real-world Federated Medical Image Analysis with Server-Client LLM Agents",
        "link": "http://arxiv.org/abs/2509.23803v1",
        "abstract": "Federated learning (FL) allows collaborative model training across healthcare sites without sharing sensitive patient data. However, real-world FL deployment is often hindered by complex operational challenges that demand substantial human efforts. This includes: (a) selecting appropriate clients (hospitals), (b) coordinating between the central server and clients, (c) client-level data pre-processing, (d) harmonizing non-standardized data and labels across clients, and (e) selecting FL algorithms based on user instructions and cross-client data characteristics. However, the existing FL works overlook these practical orchestration challenges. These operational bottlenecks motivate the need for autonomous, agent-driven FL systems, where intelligent agents at each hospital client and the central server agent collaboratively manage FL setup and model training with minimal human intervention. To this end, we first introduce an agent-driven FL framework that captures key phases of real-world FL workflows from client selection to training completion and a benchmark dubbed FedAgentBench that evaluates the ability of LLM agents to autonomously coordinate healthcare FL. Our framework incorporates 40 FL algorithms, each tailored to address diverse task-specific requirements and cross-client characteristics. Furthermore, we introduce a diverse set of complex tasks across 201 carefully curated datasets, simulating 6 modality-specific real-world healthcare environments, viz., Dermatoscopy, Ultrasound, Fundus, Histopathology, MRI, and X-Ray. We assess the agentic performance of 14 open-source and 10 proprietary LLMs spanning small, medium, and large model scales. While some agent cores such as GPT-4.1 and DeepSeek V3 can automate various stages of the FL pipeline, our results reveal that more complex, interdependent tasks based on implicit goals remain challenging for even the strongest models.",
        "authors": [
            "Pramit Saha",
            "Joshua Strong",
            "Divyanshu Mishra",
            "Cheng Ouyang",
            "J. Alison Noble"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "cs.DC",
            "cs.MA"
        ],
        "id": "http://arxiv.org/abs/2509.23803v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-09-28"
    },
    "http://arxiv.org/abs/2509.23722v1": {
        "id": "http://arxiv.org/abs/2509.23722v1",
        "title": "AdaPtis: Reducing Pipeline Bubbles with Adaptive Pipeline Parallelism on Heterogeneous Models",
        "link": "http://arxiv.org/abs/2509.23722v1",
        "tags": [
            "training",
            "sparse"
        ],
        "relevant": true,
        "indexed_date": "2025-09-16",
        "tldr": "Addresses pipeline bubbles in LLM training caused by model heterogeneity by jointly optimizing partition, placement, and scheduling. AdaPtis uses a performance model to guide adaptive pipeline parallelism, achieving up to 2.14x speedup over Megatron-LM.",
        "abstract": "Pipeline parallelism is widely used to train large language models (LLMs). However, increasing heterogeneity in model architectures exacerbates pipeline bubbles, thereby reducing training efficiency. Existing approaches overlook the co-optimization of model partition, model placement, and workload scheduling, resulting in limited efficiency improvement or even performance degradation. To respond, we propose AdaPtis, an LLM training system that supports adaptive pipeline parallelism. First, we develop a pipeline performance model to accurately estimate training throughput. Second, AdaPtis jointly optimizes model partition, model placement, and workload scheduling policies guided by this performance model. Third, we design a unified pipeline executor that efficiently supports the execution of diverse pipeline strategies. Extensive experiments show that AdaPtis achieves an average speedup of 1.42x (up to 2.14x) over Megatron-LM I-1F1B across various LLM architectures and scales.",
        "authors": [
            "Jihu Guo",
            "Tenghui Ma",
            "Wei Gao",
            "Peng Sun",
            "Jiaxing Li",
            "Xun Chen",
            "Yuyang Jin",
            "Dahua Lin"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-09-28"
    },
    "http://arxiv.org/abs/2509.23706v1": {
        "title": "Parallel Algorithms for the One Sided Crossing Minimization Problem",
        "link": "http://arxiv.org/abs/2509.23706v1",
        "abstract": "The One Sided Crossing Minimization (OSCM) problem is an optimization problem in graph drawing that aims to minimize the number of edge crossings in bipartite graph layouts. It has practical applications in areas such as network visualization and VLSI (Very Large Scale Integration) design, where reducing edge crossings improves the arrangement of circuit components and their interconnections. Despite the rise of multi-core systems, the parallelization of exact and fixed-parameter tractable (FPT) algorithms for OSCM remains largely unexplored. Parallel variants offer significant potential for scaling to larger graphs but require careful handling of synchronization and memory management. In this paper, we explore various previously studied exact and FPT algorithms for OSCM, implementing and analyzing them in both sequential and parallel forms. Our main contribution lies in empirically proving that these algorithms can achieve close to linear speedup under parallelization. In particular, our best result achieves a speedup of nearly 19 on a 16-core, 32-thread machine. We further investigate and discuss the reasons why linear speedup is not always attained.",
        "authors": [
            "Bogdan-Ioan Popa",
            "Adrian-Marius Dumitran",
            "Livia Magureanu"
        ],
        "categories": [
            "cs.DC",
            "cs.DS"
        ],
        "id": "http://arxiv.org/abs/2509.23706v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-09-28"
    },
    "http://arxiv.org/abs/2509.23448v1": {
        "title": "Lyte Quorum: Off-Chain Ready Smart Contract Hosted with Choice",
        "link": "http://arxiv.org/abs/2509.23448v1",
        "abstract": "This paper introduces Lyquor, a decentralized platform that reimagines blockchain infrastructure through a service-centric model where nodes selectively host smart contracts (called Lyquids) while preserving global composability. We present three key innovations: (1) Fate-Constrained Ordering (FCO), which decouples consensus from execution to enable selective hosting without sacrificing Layer-1 grade composability; (2) Direct Memory Architecture (DMA), which eliminates state access bottlenecks by providing each contract with persistent, byte-addressable virtual memory; and (3) Universal Procedure Call (UPC), which enables fault-tolerant, programmable coordination across distributed off-chain computation. Together, these components are powered by a Rust-macroed unified programming model where on-chain and off-chain logic coexist seamlessly, supporting both traditional smart contract patterns and novel distributed applications. Lyquor addresses critical limitations in existing systems while maintaining compatibility with Ethereum APIs, offering a path toward truly scalable decentralized computation.",
        "authors": [
            "Hao Hao",
            "Dahlia Malkhi",
            "Maofan Yin",
            "Lizan Zhou"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2509.23448v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-09-27"
    },
    "http://arxiv.org/abs/2509.23427v1": {
        "title": "StarveSpam: Mitigating Spam with Local Reputation in Permissionless Blockchains",
        "link": "http://arxiv.org/abs/2509.23427v1",
        "abstract": "Spam poses a growing threat to blockchain networks. Adversaries can easily create multiple accounts to flood transaction pools, inflating fees and degrading service quality. Existing defenses against spam, such as fee markets and staking requirements, primarily rely on economic deterrence, which fails to distinguish between malicious and legitimate users and often exclude low-value but honest activity. To address these shortcomings, we present StarveSpam, a decentralized reputation-based protocol that mitigates spam by operating at the transaction relay layer. StarveSpam combines local behavior tracking, peer scoring, and adaptive rate-limiting to suppress abusive actors, without requiring global consensus, protocol changes, or trusted infrastructure. We evaluate StarveSpam using real Ethereum data from a major NFT spam event and show that it outperforms existing fee-based and rule-based defenses, allowing each node to block over 95% of spam while dropping just 3% of honest traffic, and reducing the fraction of the network exposed to spam by 85% compared to existing rule-based methods. StarveSpam offers a scalable and deployable alternative to traditional spam defenses, paving the way toward more resilient and equitable blockchain infrastructure.",
        "authors": [
            "Rowdy Chotkan",
            "Bulat Nasrulin",
            "Jérémie Decouchant",
            "Johan Pouwelse"
        ],
        "categories": [
            "cs.CR",
            "cs.DC",
            "cs.NI"
        ],
        "id": "http://arxiv.org/abs/2509.23427v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-09-27"
    },
    "http://arxiv.org/abs/2509.23419v1": {
        "title": "Enhancing Communication Efficiency in FL with Adaptive Gradient Quantization and Communication Frequency Optimization",
        "link": "http://arxiv.org/abs/2509.23419v1",
        "abstract": "Federated Learning (FL) enables participant devices to collaboratively train deep learning models without sharing their data with the server or other devices, effectively addressing data privacy and computational concerns. However, FL faces a major bottleneck due to high communication overhead from frequent model updates between devices and the server, limiting deployment in resource-constrained wireless networks. In this paper, we propose a three-fold strategy. Firstly, an Adaptive Feature-Elimination Strategy to drop less important features while retaining high-value ones; secondly, Adaptive Gradient Innovation and Error Sensitivity-Based Quantization, which dynamically adjusts the quantization level for innovative gradient compression; and thirdly, Communication Frequency Optimization to enhance communication efficiency. We evaluated our proposed model's performance through extensive experiments, assessing accuracy, loss, and convergence compared to baseline techniques. The results show that our model achieves high communication efficiency in the framework while maintaining accuracy.",
        "authors": [
            "Asadullah Tariq",
            "Tariq Qayyum",
            "Mohamed Adel Serhani",
            "Farag Sallabi",
            "Ikbal Taleb",
            "Ezedin S. Barka"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "id": "http://arxiv.org/abs/2509.23419v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-09-27"
    },
    "http://arxiv.org/abs/2509.23384v3": {
        "id": "http://arxiv.org/abs/2509.23384v3",
        "title": "A Predictive and Synergistic Two-Layer Scheduling Framework for LLM Serving",
        "link": "http://arxiv.org/abs/2509.23384v3",
        "tags": [
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-09-16",
        "tldr": "Addresses inefficient two-layer LLM serving by introducing predictive, synergistic scheduling to bridge cluster- and engine-layer information gaps. Uses a performancemodel for adaptive batching and state-driven routing, improving SLO attainment by 43% and throughput by 3x.",
        "abstract": "LLM inference serving typically scales out with a two-tier architecture: a cluster router distributes requests to multiple inference engines, each of which then in turn performs its own internal scheduling. However, this commonly used paradigm suffers from critical, systemic inefficiency caused by the information gaps across two layers. At the cluster-layer, the router mainly relies on lagging, coarse-grained metrics, such as average latency and queue length to make decisions, resulting in \"decision lag\" that leads to suboptimal request routing. At the engine-layer, static heuristic scheduling policies cannot effectively handle the dynamic workloads, leading a poor balance between latency and throughput. Besides, these gaps may cause SLO violations and resource waste, especially in heterogeneous cloud environments.   To bridge such gaps, we propose NexusSched, a cross-layer framework that shifts LLM serving system from reactive load balancing to predictive orchestration. The core of NexusSched lies in a structurally-informed online performance model that provides accurate, forward-looking per-step latency and capacity estimations. This model empowers two key components. At the engine-layer, LENS performs SLO-aware, adaptive scheduling, dynamically optimizing batching to meet SLOs under real-time loads. At the cluster-layer, PRISM uses predictive signals to perform state-driven routing, maximizing cluster-wide performance and SLO attainment. Performance evaluations show that NexusSched improves SLO attainment by 43% on average and achieves up to 3x throughput speedup in long-context and heterogeneous scenarios. Besides, we also deploy NexusSched on FlowGPT's clusters to demonstrate its advantages in production environment.",
        "authors": [
            "Yue Zhang",
            "Yuansheng Chen",
            "Xuan Mo",
            "Alex Xi",
            "Jialun Li",
            "WeiGang Wu"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-27"
    },
    "http://arxiv.org/abs/2509.23340v3": {
        "title": "CrediBench: Building Web-Scale Network Datasets for Information Integrity",
        "link": "http://arxiv.org/abs/2509.23340v3",
        "abstract": "Online misinformation poses an escalating threat, amplified by the Internet's open nature and increasingly capable LLMs that generate persuasive yet deceptive content. Existing misinformation detection methods typically focus on either textual content or network structure in isolation, failing to leverage the rich, dynamic interplay between website content and hyperlink relationships that characterizes real-world misinformation ecosystems. We introduce CrediBench: a large-scale data processing pipeline for constructing temporal web graphs that jointly model textual content and hyperlink structure for misinformation detection. Unlike prior work, our approach captures the dynamic evolution of general misinformation domains, including changes in both content and inter-site references over time. Our processed one-month snapshot extracted from the Common Crawl archive in December 2024 contains 45 million nodes and 1 billion edges, representing the largest web graph dataset made publicly available for misinformation research to date. From our experiments on this graph snapshot, we demonstrate the strength of both structural and webpage content signals for learning credibility scores, which measure source reliability. The pipeline and experimentation code are all available here, and the dataset is in this folder.",
        "authors": [
            "Emma Kondrup",
            "Sebastian Sabry",
            "Hussein Abdallah",
            "Zachary Yang",
            "James Zhou",
            "Kellin Pelrine",
            "Jean-François Godbout",
            "Michael M. Bronstein",
            "Reihaneh Rabbany",
            "Shenyang Huang"
        ],
        "categories": [
            "cs.SI",
            "cs.DC",
            "cs.LG"
        ],
        "id": "http://arxiv.org/abs/2509.23340v3",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-09-27"
    },
    "http://arxiv.org/abs/2509.23324v1": {
        "id": "http://arxiv.org/abs/2509.23324v1",
        "title": "Scaling LLM Test-Time Compute with Mobile NPU on Smartphones",
        "link": "http://arxiv.org/abs/2509.23324v1",
        "tags": [
            "serving",
            "quantization",
            "edge"
        ],
        "relevant": true,
        "indexed_date": "2025-09-16",
        "tldr": "Can smaller LLMs match larger models' accuracy on smartphones by leveraging underused NPU compute via test-time scaling? Proposes hardware-aware quantization and LUT optimizations for NPU-efficient inference, achieving up to 2.2× speedup and matching larger model accuracy.",
        "abstract": "Deploying Large Language Models (LLMs) on mobile devices faces the challenge of insufficient performance in smaller models and excessive resource consumption in larger ones. This paper highlights that mobile Neural Processing Units (NPUs) have underutilized computational resources, particularly their matrix multiplication units, during typical LLM inference. To leverage this wasted compute capacity, we propose applying parallel test-time scaling techniques on mobile NPUs to enhance the performance of smaller LLMs. However, this approach confronts inherent NPU challenges, including inadequate hardware support for fine-grained quantization and low efficiency in general-purpose computations. To overcome these, we introduce two key techniques: a hardware-aware tile quantization scheme that aligns group quantization with NPU memory access patterns, and efficient LUT-based replacements for complex operations such as Softmax and dequantization. We design and implement an end-to-end inference system that leverages the NPU's compute capability to support test-time scaling on Qualcomm Snapdragon platforms. Experiments show our approach brings significant speedups: up to 19.0 for mixed-precision GEMM and 2.2 for Softmax. More importantly, we demonstrate that smaller models using test-time scaling can match or exceed the accuracy of larger models, achieving a new performance-cost Pareto frontier.",
        "authors": [
            "Zixu Hao",
            "Jianyu Wei",
            "Tuowei Wang",
            "Minxing Huang",
            "Huiqiang Jiang",
            "Shiqi Jiang",
            "Ting Cao",
            "Ju Ren"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-09-27"
    },
    "http://arxiv.org/abs/2510.05112v2": {
        "id": "http://arxiv.org/abs/2510.05112v2",
        "title": "A Flexible Programmable Pipeline Parallelism Framework for Efficient DNN Training",
        "link": "http://arxiv.org/abs/2510.05112v2",
        "tags": [
            "training"
        ],
        "relevant": true,
        "indexed_date": "2025-09-16",
        "tldr": "Designs FlexPipe, a programmable framework for automated and customizable pipeline parallelism in DNN training, using a DSL and scheduler to explore efficient schedules; achieves up to 2.28× speedup over Megtron-LM.",
        "abstract": "Pipeline parallelism is an essential distributed parallelism method. Increasingly complex and diverse DNN models necessitate meticulously customized pipeline schedules for performance. However, existing practices typically rely on predefined schedules, each with strengths, but fail to adapt automatically to the emerging model architectures. Exploring novel high-efficiency schedules is daunting due to the enormous and varying schedule space. Besides, manually implementing schedules can be challenging due to the onerous coding burdens and constantly changing needs. Unfortunately, existing frameworks have limitations in automated schedule exploration and lack flexibility and controllability.   This paper presents FlexPipe, a programmable pipeline parallelism framework with enhanced productivity, programmability, debuggability, and ease of tuning. FlexPipe has two main components: a succinct domain-specific language (DSL) and an automated scheduler. FlexPipe enables automated schedule exploration for various parallel scenarios within a broad spectrum of schedule types at a small search cost. Besides, users can swiftly develop and customize schedules using the FlexPipe DSL, which embodies flexible controllability in the pipeline order of micro-batch computations over stages. It also provides convenient mechanisms to include new operations in schedules to meet changing demands. Our evaluation results demonstrate that FlexPipe achieves up to 2.28X performance speedup compared to the popular large-scale parallel framework Megtron-LM, and gains up to 1.49X performance speedup compared to the state-of-the-art automated pipeline parallelism framework.",
        "authors": [
            "Lijuan Jiang",
            "Xingjian Qian",
            "Zhenxiang Ma",
            "Zan Zong",
            "Hengjie Li",
            "Chao Yang",
            "Jidong Zhai"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-27"
    },
    "http://arxiv.org/abs/2510.02345v1": {
        "id": "http://arxiv.org/abs/2510.02345v1",
        "title": "Breaking the MoE LLM Trilemma: Dynamic Expert Clustering with Structured Compression",
        "link": "http://arxiv.org/abs/2510.02345v1",
        "tags": [
            "MoE",
            "quantization",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-09-16",
        "tldr": "Addresses the MoE trilemma by dynamically clustering experts and applying structured compression with hierarchical routing, mixed-precision storage (FP16/INT4), and dynamic offloading, reducing parameters by 80% while improving throughput by 10–20% and load balance by 3×.",
        "abstract": "Mixture-of-Experts (MoE) Large Language Models (LLMs) face a trilemma of load imbalance, parameter redundancy, and communication overhead. We introduce a unified framework based on dynamic expert clustering and structured compression to address these issues cohesively. Our method employs an online clustering procedure that periodically regroups experts using a fused metric of parameter and activation similarity, which stabilizes expert utilization. To our knowledge, this is one of the first frameworks to leverage the semantic embedding capability of the router to dynamically reconfigure the model's architecture during training for substantial efficiency gains. Within each cluster, we decompose expert weights into a shared base matrix and extremely low-rank residual adapters, achieving up to fivefold parameter reduction per group while preserving specialization. This structure enables a two-stage hierarchical routing strategy: tokens are first assigned to a cluster, then to specific experts within it, drastically reducing the routing search space and the volume of all-to-all communication. Furthermore, a heterogeneous precision scheme, which stores shared bases in FP16 and residual factors in INT4, coupled with dynamic offloading of inactive clusters, reduces peak memory consumption to levels comparable to dense models. Evaluated on GLUE and WikiText-103, our framework matches the quality of standard MoE models while reducing total parameters by approximately 80%, improving throughput by 10% to 20%, and lowering expert load variance by a factor of over three. Our work demonstrates that structural reorganization is a principled path toward scalable, efficient, and memory-effective MoE LLMs.",
        "authors": [
            "Peijun Zhu",
            "Ning Yang",
            "Jiayu Wei",
            "Jinghang Wu",
            "Haijun Zhang"
        ],
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.DC",
            "cs.LG",
            "cs.NE"
        ],
        "submit_date": "2025-09-27"
    },
    "http://arxiv.org/abs/2509.23241v1": {
        "id": "http://arxiv.org/abs/2509.23241v1",
        "title": "Memory Efficient and Staleness Free Pipeline Parallel DNN Training Framework with Improved Convergence Speed",
        "link": "http://arxiv.org/abs/2509.23241v1",
        "tags": [
            "training"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes V-TiMePReSt and I-TiMePReSt frameworks for pipeline-parallel DNN training to eliminate weight staleness and improve memory efficiency. V-TiMePReSt ensures staleness-free training with latest weights, while I-TiMePReSt uses intermediate weights to balance memory and convergence. Achieves improved GPU memory efficiency and optimal convergence speed.",
        "abstract": "High resource requirement for Deep Neural Network (DNN) training across multiple GPUs necessitates development of various parallelism techniques. In this paper, we introduce two interconnected DNN training frameworks, namely, V-TiMePReSt and I-TiMePReSt, based on pipeline parallelism, a variant of model parallelism. V-TiMePReSt is a completely staleness-free system which enables the DNNs to be trained on the latest updated weights in each stage of all forward and backward passes. Developing staleness-aware systems at the expense of weight stashing reduces GPU-memory consumption, however, increases the number of epochs to converge. Thus, we introduce I-TiMePReSt, which is also a staleness-aware system, but not at the expense of weight stashing. It does not rely solely on the stale weights or the latest updated weights. I-TiMePReSt computes an intermediate weight towards the latter and performs backward pass on it. Additionally, we formulate the significance of the stale weights mathematically depending on the degree of staleness. In contrast to V-TiMePReSt, I-TiMePReSt works based on the assumption that stale weights have a significant contribution in training, which can be quantified mathematically based on the degree of staleness, although there are other contributory factors which should not be ignored. Experimental results show that V-TiMePReSt is advantageous over existing models in terms of $1)$ the extent of staleness of the weight parameter values and $2)$ GPU memory efficiency, while I-TiMePReSt is superior in terms of $1)$ removing staleness of the weight parameters without removing weight stashing and $2)$ maintaining the trade-off between GPU memory consumption and convergence speed (number of epochs).",
        "authors": [
            "Ankita Dutta",
            "Nabendu Chaki",
            "Rajat K. De"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-27"
    },
    "http://arxiv.org/abs/2509.23130v2": {
        "title": "SysMoBench: Evaluating AI on Formally Modeling Complex Real-World Systems",
        "link": "http://arxiv.org/abs/2509.23130v2",
        "abstract": "Formal models are essential to specifying large, complex computer systems and verifying their correctness, but are notoriously expensive to write and maintain. Recent advances in generative AI show promise in generating certain forms of specifications. However, existing work mostly targets small code, not complete systems. It is unclear whether AI can deal with realistic system artifacts, as this requires abstracting their complex behavioral properties into formal models. We present SysMoBench, a benchmark that evaluates AI's ability to formally model large, complex systems. We focus on concurrent and distributed systems, which are keystones of today's critical computing infrastructures, encompassing operating systems and cloud infrastructure. We use TLA+, the de facto specification language for concurrent and distributed systems, though the benchmark can be extended to other specification languages. We address the primary challenge of evaluating AI-generated models by automating metrics like syntactic and runtime correctness, conformance to system code, and invariant correctness. SysMoBench currently includes nine diverse system artifacts: the Raft implementation of Etcd and Redis, the Spinlock and Mutex in Asterinas OS, etc.; more artifacts are being actively added. SysMoBench enables us to understand the capabilities and limitations of today's LLMs and agents, putting tools in this area on a firm footing and opening up promising new research directions.",
        "authors": [
            "Qian Cheng",
            "Ruize Tang",
            "Emilie Ma",
            "Finn Hackett",
            "Peiyang He",
            "Yiming Su",
            "Ivan Beschastnikh",
            "Yu Huang",
            "Xiaoxing Ma",
            "Tianyin Xu"
        ],
        "categories": [
            "cs.AI",
            "cs.DC",
            "cs.SE"
        ],
        "id": "http://arxiv.org/abs/2509.23130v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-09-27"
    },
    "http://arxiv.org/abs/2509.23101v1": {
        "title": "Towards Quantum-Ready Blockchain Fraud Detection via Ensemble Graph Neural Networks",
        "link": "http://arxiv.org/abs/2509.23101v1",
        "abstract": "Blockchain Business applications and cryptocurrencies such as enable secure, decentralized value transfer, yet their pseudonymous nature creates opportunities for illicit activity, challenging regulators and exchanges in anti money laundering (AML) enforcement. Detecting fraudulent transactions in blockchain networks requires models that can capture both structural and temporal dependencies while remaining resilient to noise, imbalance, and adversarial behavior. In this work, we propose an ensemble framework that integrates Graph Convolutional Networks (GCN), Graph Attention Networks (GAT), and Graph Isomorphism Networks (GIN) to enhance blockchain fraud detection. Using the real-world Elliptic dataset, our tuned soft voting ensemble achieves high recall of illicit transactions while maintaining a false positive rate below 1%, beating individual GNN models and baseline methods. The modular architecture incorporates quantum-ready design hooks, allowing seamless future integration of quantum feature mappings and hybrid quantum classical graph neural networks. This ensures scalability, robustness, and long-term adaptability as quantum computing technologies mature. Our findings highlight ensemble GNNs as a practical and forward-looking solution for real-time cryptocurrency monitoring, providing both immediate AML utility and a pathway toward quantum-enhanced financial security analytics.",
        "authors": [
            "M. Z. Haider",
            "Tayyaba Noreen",
            "M. Salman"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CR",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2509.23101v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-09-27"
    },
    "http://arxiv.org/abs/2509.23049v1": {
        "title": "Beyond Aggregation: Guiding Clients in Heterogeneous Federated Learning",
        "link": "http://arxiv.org/abs/2509.23049v1",
        "abstract": "Federated learning (FL) is increasingly adopted in domains like healthcare, where data privacy is paramount. A fundamental challenge in these systems is statistical heterogeneity-the fact that data distributions vary significantly across clients (e.g., different hospitals may treat distinct patient demographics). While current FL algorithms focus on aggregating model updates from these heterogeneous clients, the potential of the central server remains under-explored. This paper is motivated by a healthcare scenario: could a central server not only build a model but also guide a new patient to the hospital best equipped for their specific condition? We generalize this idea to propose a novel paradigm for FL systems where the server actively guides the allocation of new tasks or queries to the most appropriate client in the network. To enable this, we introduce an empirical likelihood-based framework that simultaneously addresses two goals: (1) learning effective local models on each client, and (2) finding the best matching client for a new query. Empirical results demonstrate the framework's effectiveness on benchmark datasets, showing improvements in both model accuracy and the precision of client guidance compared to standard FL approaches. This work opens a new direction for building more intelligent and resource-efficient federated systems that leverage heterogeneity as a feature, not just a bug. Code is available at https://github.com/zijianwang0510/FedDRM.git.",
        "authors": [
            "Zijian Wang",
            "Xiaofei Zhang",
            "Xin Zhang",
            "Yukun Liu",
            "Qiong Zhang"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2509.23049v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-09-27"
    },
    "http://arxiv.org/abs/2509.23013v1": {
        "title": "Characterizing FaaS Workflows on Public Clouds: The Good, the Bad and the Ugly",
        "link": "http://arxiv.org/abs/2509.23013v1",
        "abstract": "Function-as-a-service (FaaS) is a popular serverless computing paradigm for developing event-driven functions that elastically scale on public clouds. FaaS workflows, such as AWS Step Functions and Azure Durable Functions, are composed from FaaS functions, like AWS Lambda and Azure Functions, to build practical applications. But, the complex interactions between functions in the workflow and the limited visibility into the internals of proprietary FaaS platforms are major impediments to gaining a deeper understanding of FaaS workflow platforms. While several works characterize FaaS platforms to derive such insights, there is a lack of a principled and rigorous study for FaaS workflow platforms, which have unique scaling, performance and costing behavior influenced by the platform design, dataflow and workloads. In this article, we perform extensive evaluations of three popular FaaS workflow platforms from AWS and Azure, running 25 micro-benchmark and application workflows over 132k invocations. Our detailed analysis confirms some conventional wisdom but also uncovers unique insights on the function execution, workflow orchestration, inter-function interactions, cold-start scaling and monetary costs. Our observations help developers better configure and program these platforms, set performance and scalability expectations, and identify research gaps on enhancing the platforms.",
        "authors": [
            "Varad Kulkarni",
            "Nikhil Reddy",
            "Tuhin Khare",
            "Abhinandan S. Prasad",
            "Chitra Babu",
            "Yogesh Simmhan"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2509.23013v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-09-27"
    },
    "http://arxiv.org/abs/2509.22922v1": {
        "title": "OptimES: Optimizing Federated Learning Using Remote Embeddings for Graph Neural Networks",
        "link": "http://arxiv.org/abs/2509.22922v1",
        "abstract": "Graph Neural Networks (GNNs) have experienced rapid advancements in recent years due to their ability to learn meaningful representations from graph data structures. However, in most real-world settings, such as financial transaction networks and healthcare networks, this data is localized to different data owners and cannot be aggregated due to privacy concerns. Federated Learning (FL) has emerged as a viable machine learning approach for training a shared model that iteratively aggregates local models trained on decentralized data. This addresses privacy concerns while leveraging parallelism. State-of-the-art methods enhance the privacy-respecting convergence accuracy of federated GNN training by sharing remote embeddings of boundary vertices through a server (EmbC). However, they are limited by diminished performance due to large communication costs. In this article, we propose OptimES, an optimized federated GNN training framework that employs remote neighbourhood pruning, overlapping the push of embeddings to the server with local training, and dynamic pulling of embeddings to reduce network costs and training time. We perform a rigorous evaluation of these strategies for four common graph datasets with up to $111M$ vertices and $1.8B$ edges. We see that a modest drop in per-round accuracy due to the preemptive push of embeddings is out-stripped by the reduction in per-round training time for large and dense graphs like Reddit and Products, converging up to $\\approx 3.5\\times$ faster than EmbC and giving up to $\\approx16\\%$ better accuracy than the default federated GNN learning. While accuracy improvements over default federated GNNs are modest for sparser graphs like Arxiv and Papers, they achieve the target accuracy about $\\approx11\\times$ faster than EmbC.",
        "authors": [
            "Pranjal Naman",
            "Yogesh Simmhan"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2509.22922v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-09-26"
    },
    "http://arxiv.org/abs/2509.22860v2": {
        "title": "Ringleader ASGD: The First Asynchronous SGD with Optimal Time Complexity under Data Heterogeneity",
        "link": "http://arxiv.org/abs/2509.22860v2",
        "abstract": "Asynchronous stochastic gradient methods are central to scalable distributed optimization, particularly when devices differ in computational capabilities. Such settings arise naturally in federated learning, where training takes place on smartphones and other heterogeneous edge devices. In addition to varying computation speeds, these devices often hold data from different distributions. However, existing asynchronous SGD methods struggle in such heterogeneous settings and face two key limitations. First, many rely on unrealistic assumptions of similarity across workers' data distributions. Second, methods that relax this assumption still fail to achieve theoretically optimal performance under heterogeneous computation times. We introduce Ringleader ASGD, the first asynchronous SGD algorithm that attains the theoretical lower bounds for parallel first-order stochastic methods in the smooth nonconvex regime, thereby achieving optimal time complexity under data heterogeneity and without restrictive similarity assumptions. Our analysis further establishes that Ringleader ASGD remains optimal under arbitrary and even time-varying worker computation speeds, closing a fundamental gap in the theory of asynchronous optimization.",
        "authors": [
            "Artavazd Maranjyan",
            "Peter Richtárik"
        ],
        "categories": [
            "math.OC",
            "cs.DC",
            "cs.LG",
            "stat.ML"
        ],
        "id": "http://arxiv.org/abs/2509.22860v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-09-26"
    },
    "http://arxiv.org/abs/2509.22832v1": {
        "id": "http://arxiv.org/abs/2509.22832v1",
        "title": "Efficient Fine-Grained GPU Performance Modeling for Distributed Deep Learning of LLM",
        "link": "http://arxiv.org/abs/2509.22832v1",
        "tags": [
            "training",
            "hardware"
        ],
        "relevant": true,
        "indexed_date": "2025-09-16",
        "tldr": "Predicts end-to-end LLM training time across distributed GPUs by decomposing models into primitives and using lightweight hardware-aware models. Achieves <10% prediction error on 20B models across 128 GPUs while running entirely on CPU.",
        "abstract": "Training Large Language Models(LLMs) is one of the most compute-intensive tasks in high-performance computing. Predicting end-to-end training time for multi-billion parameter models distributed across hundreds of GPUs remains challenging due to complex interactions between transformer components, parallelism strategies(data, model, pipeline, tensor), and multi-tier communication. Learned models require costly sampling, while analytical models often struggle with real-world network and hardware complexities. We address this by decomposing LLMs into core computational primitives and modeling them with: (1) operator-level decomposition for fine-grained analysis; (2) lightweight sampling based hardware-aware prediction models for key operations; (3) an end-to-end prediction system integrating these components across complex parallelization strategies. Crucially, our methodology has been validated on two large-scale HPC systems. Our framework achieves low average prediction errors-4.98\\% on Perlmutter(A100) and 9.38\\% on Vista(GH200)-for models up to 20B parameters across 128 GPUs. Importantly, it runs entirely on CPUs, enabling rapid iteration over hardware configurations and training strategies without costly on-cluster experimentation.",
        "authors": [
            "Biyao Zhang",
            "Mingkai Zheng",
            "Debargha Ganguly",
            "Xuecen Zhang",
            "Vikash Singh",
            "Vipin Chaudhary",
            "Zhao Zhang"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.LG"
        ],
        "submit_date": "2025-09-26"
    },
    "http://arxiv.org/abs/2510.05111v1": {
        "id": "http://arxiv.org/abs/2510.05111v1",
        "title": "Agora: Bridging the GPU Cloud Resource-Price Disconnect",
        "link": "http://arxiv.org/abs/2510.05111v1",
        "tags": [
            "serving",
            "storage",
            "hardware"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Addresses market inefficiency in cloud GPU pricing for bandwidth-bound workloads. Proposes Agora, a feature-based pricing framework linked to resource consumption like memory bandwidth. Implementation with 10us sampling shows only 2.4% revenue loss compared to ideal sampling.",
        "abstract": "The historic trend of Moore's Law, which predicted exponential growth in computational performance per dollar, has diverged for modern Graphics Processing Units (GPUs). While Floating Point Operations per Second (FLOPs) capabilities have continued to scale economically, memory bandwidth has not, creating a significant price-performance disconnect. This paper argues that the prevailing time-based pricing models for cloud GPUs are economically inefficient for bandwidth-bound workloads. These models fail to account for the rising marginal cost of memory bandwidth, leading to market distortions and suboptimal hardware allocation. To address this, we propose a novel feature-based pricing framework that directly links cost to resource consumption, including but not limited to memory bandwidth. We provide a robust economic and algorithmic definition of this framework and introduce Agora, a practical and secure system architecture for its implementation. Our implementation of Agora shows that a 50us sampling provides nearly perfect pricing as what ideal sampling would provide - losing only 5\\% of revenue. 10us sampling is even better result in 2.4\\% loss. Modern telemetry systems can already provide this rate of measurement, and our prototype implementation shows the system design for feature-based pricing is buildable. Our evaluation across diverse GPU applications and hardware generations empirically validates the effectiveness of our approach in creating a more transparent and efficient market for cloud GPU resources.",
        "authors": [
            "Ian McDougall",
            "Noah Scott",
            "Joon Huh",
            "Kirthevasan Kandasamy",
            "Karthikeyan Sankaralingam"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-26"
    },
    "http://arxiv.org/abs/2509.22369v2": {
        "title": "Role-Aware Multi-modal federated learning system for detecting phishing webpages",
        "link": "http://arxiv.org/abs/2509.22369v2",
        "abstract": "We present a federated, multi-modal phishing website detector that supports URL, HTML, and IMAGE inputs without binding clients to a fixed modality at inference: any client can invoke any modality head trained elsewhere. Methodologically, we propose role-aware bucket aggregation on top of FedProx, inspired by Mixture-of-Experts and FedMM. We drop learnable routing and use hard gating (selecting the IMAGE/HTML/URL expert by sample modality), enabling separate aggregation of modality-specific parameters to isolate cross-embedding conflicts and stabilize convergence. On TR-OP, the Fusion head reaches Acc 97.5% with FPR 2.4% across two data types; on the image subset (ablation) it attains Acc 95.5% with FPR 5.9%. For text, we use GraphCodeBERT for URLs and an early three-way embedding for raw, noisy HTML. On WebPhish (HTML) we obtain Acc 96.5% / FPR 1.8%; on TR-OP (raw HTML) we obtain Acc 95.1% / FPR 4.6%. Results indicate that bucket aggregation with hard-gated experts enables stable federated training under strict privacy, while improving the usability and flexibility of multi-modal phishing detection.",
        "authors": [
            "Bo Wang",
            "Imran Khan",
            "Martin White",
            "Natalia Beloff"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2509.22369v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-09-26"
    },
    "http://arxiv.org/abs/2509.22233v1": {
        "title": "Orientation does not help with 3-coloring a grid in online-LOCAL",
        "link": "http://arxiv.org/abs/2509.22233v1",
        "abstract": "The online-LOCAL and SLOCAL models are extensions of the LOCAL model where nodes are processed in a sequential but potentially adversarial order. So far, the only problem we know of where the global memory of the online-LOCAL model has an advantage over SLOCAL is 3-coloring bipartite graphs. Recently, Chang et al. [PODC 2024] showed that even in grids, 3-coloring requires $Ω(\\log n)$ locality in deterministic online-LOCAL. This result was subsequently extended by Akbari et al. [STOC 2025] to also hold in randomized online-LOCAL. However, both proofs heavily rely on the assumption that the algorithm does not have access to the orientation of the underlying grid. In this paper, we show how to lift this requirement and obtain the same lower bound (against either model) even when the algorithm is explicitly given a globally consistent orientation of the grid.",
        "authors": [
            "Thomas Boudier",
            "Filippo Casagrande",
            "Avinandan Das",
            "Massimo Equi",
            "Henrik Lievonen",
            "Augusto Modanese",
            "Ronja Stimpert"
        ],
        "categories": [
            "cs.DC",
            "cs.DS"
        ],
        "id": "http://arxiv.org/abs/2509.22233v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-09-26"
    },
    "http://arxiv.org/abs/2509.22117v2": {
        "id": "http://arxiv.org/abs/2509.22117v2",
        "title": "The AI_INFN Platform: Artificial Intelligence Development in the Cloud",
        "link": "http://arxiv.org/abs/2509.22117v2",
        "tags": [
            "offloading",
            "training",
            "hardware"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Presents AI_INFN, a Kubernetes-based platform for efficient GPU resource sharing in AI workflows. Integrates offloading via Virtual Kubelet and InterLink API to span resources across distributed providers. Demonstrated scalable execution on heterogeneous resources, including WLCG sites and HPC centers.",
        "abstract": "Machine Learning (ML) is profoundly reshaping the way researchers create, implement, and operate data-intensive software. Its adoption, however, introduces notable challenges for computing infrastructures, particularly when it comes to coordinating access to hardware accelerators across development, testing, and production environments. The INFN initiative AI_INFN (Artificial Intelligence at INFN) seeks to promote the use of ML methods across various INFN research scenarios by offering comprehensive technical support, including access to AI-focused computational resources. Leveraging the INFN Cloud ecosystem and cloud-native technologies, the project emphasizes efficient sharing of accelerator hardware while maintaining the breadth of the Institute's research activities. This contribution describes the deployment and commissioning of a Kubernetes-based platform designed to simplify GPU-powered data analysis workflows and enable their scalable execution on heterogeneous distributed resources. By integrating offloading mechanisms through Virtual Kubelet and the InterLink API, the platform allows workflows to span multiple resource providers, from Worldwide LHC Computing Grid sites to high-performance computing centers like CINECA Leonardo. We will present preliminary benchmarks, functional tests, and case studies, demonstrating both performance and integration outcomes.",
        "authors": [
            "Lucio Anderlini",
            "Giulio Bianchini",
            "Diego Ciangottini",
            "Stefano Dal Pra",
            "Diego Michelotto",
            "Rosa Petrini",
            "Daniele Spiga"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-09-26"
    },
    "http://arxiv.org/abs/2509.22068v1": {
        "title": "Code once, Run Green: Automated Green Code Translation in Serverless Computing",
        "link": "http://arxiv.org/abs/2509.22068v1",
        "abstract": "The rapid digitization and the increasing use of emerging technologies such as AI models have significantly contributed to the emissions of computing infrastructure. Efforts to mitigate this impact typically focus on the infrastructure level such as powering data centers with renewable energy, or through the specific design of energy-efficient software. However, both strategies rely on stakeholder intervention, making their adoption in legacy and already-deployed systems unlikely. As a result, past architectural and implementation decisions continue to incur additional energy usage - a phenomenon we refer to as energy debt.   Hence, in this paper, we investigate the potential of serverless computing platforms to automatically reduce energy debt by leveraging the unique access to function source code. Specifically, we explore whether large language models (LLMs) can translate serverless functions into more energy-efficient programming languages while preserving functional correctness. To this end, we design and implement ReFaaS and integrate it into the Fission serverless framework. We evaluate multiple LLMs on their ability to perform such code translations and analyze their impact on energy consumption.   Our preliminary results indicate that translated functions can reduce invocation energy by up to 70%, achieving net energy savings after approximately 3,000 to 5,000 invocations, depending on the LLM used. Nonetheless, the approach faces several challenges: not all functions are suitable for translation, and for some, the amortization threshold is significantly higher or unreachable. Despite these limitations, we identify four key research challenges whose resolution could unlock long-term, automated mitigation of energy debt in serverless computing.",
        "authors": [
            "Sebastian Werner",
            "Mathis Kähler",
            "Alireza Hakamian"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2509.22068v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-09-26"
    },
    "http://arxiv.org/abs/2510.00031v1": {
        "title": "VibeCodeHPC: An Agent-Based Iterative Prompting Auto-Tuner for HPC Code Generation Using LLMs",
        "link": "http://arxiv.org/abs/2510.00031v1",
        "abstract": "We propose VibeCodeHPC, an automatic tuning system for HPC programs based on multi-agent LLMs for code generation. VibeCodeHPC tunes programs through multi-agent role allocation and iterative prompt refinement. We describe the system configuration with four roles: Project Manager (PM), System Engineer (SE), Programmer (PG), and Continuous Delivery (CD). We introduce dynamic agent deployment and activity monitoring functions to facilitate effective multi-agent collaboration. In our case study, we convert and optimize CPU-based matrix-matrix multiplication code written in C to GPU code using CUDA. The multi-agent configuration of VibeCodeHPC achieved higher-quality code generation per unit time compared to a solo-agent configuration. Additionally, the dynamic agent deployment and activity monitoring capabilities facilitated more effective identification of requirement violations and other issues.",
        "authors": [
            "Shun-ichiro Hayashi",
            "Koki Morita",
            "Daichi Mukunoki",
            "Tetsuya Hoshino",
            "Takahiro Katagiri"
        ],
        "categories": [
            "cs.SE",
            "cs.AI",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2510.00031v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-09-26"
    },
    "http://arxiv.org/abs/2509.21841v2": {
        "id": "http://arxiv.org/abs/2509.21841v2",
        "title": "Zeppelin: Balancing Variable-length Workloads in Data Parallel Large Model Training",
        "link": "http://arxiv.org/abs/2509.21841v2",
        "tags": [
            "training",
            "sparse"
        ],
        "relevant": true,
        "indexed_date": "2025-09-16",
        "tldr": "Addresses load imbalance in data-parallel LLM training due to variable sequence lengths. Introduces hierarchical sequence partitioning, dynamic NIC routing, and module-aware remapping to balance computation and communication. Achieves 2.80x speedup over state-of-the-art.",
        "abstract": "Training large language models (LLMs) with increasingly long and varying sequence lengths introduces severe load imbalance challenges in large-scale data-parallel training. Recent frameworks attempt to mitigate these issues through data reorganization or hybrid parallel strategies. However, they often overlook how computational and communication costs scale with sequence length, resulting in suboptimal performance. We identify three critical challenges: (1) varying computation-to-communication ratios across sequences of different lengths in distributed attention, (2) mismatch between static NIC-GPU affinity and dynamic parallel workloads, and (3) distinct optimal partitioning strategies required for quadratic attention versus linear components. To address these challenges, we present Zeppelin, a novel training system that integrates three key techniques: (1) a hierarchical sequence partitioning method for the attention module that reduces communication overhead and balances computation, supported by an efficient attention engine that applies divergent parallel strategies; (2) a routing layer that orchestrates inter-node transfers to fully utilize NIC bandwidth; and (3) a remapping layer that transforms sequence layouts between attention and linear modules, ensuring high computational efficiency across both. Comprehensive evaluations across diverse configurations show that Zeppelin delivers an average 2.80x speedup over state-of-the-art methods.",
        "authors": [
            "Chang Chen",
            "Tiancheng Chen",
            "Jiangfei Duan",
            "Qianchao Zhu",
            "Zerui Wang",
            "Qinghao Hu",
            "Peng Sun",
            "Xiuhong Li",
            "Chao Yang",
            "Torsten Hoefler"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-26"
    },
    "http://arxiv.org/abs/2510.05109v3": {
        "id": "http://arxiv.org/abs/2510.05109v3",
        "title": "Tiny but Mighty: A Software-Hardware Co-Design Approach for Efficient Multimodal Inference on Battery-Powered Small Devices",
        "link": "http://arxiv.org/abs/2510.05109v3",
        "tags": [
            "multi-modal",
            "offloading",
            "edge"
        ],
        "relevant": true,
        "indexed_date": "2025-09-16",
        "tldr": "Proposes NANOMIND, a hardware-software co-design for modular LMM inference on small devices. Dynamically offloads model components to heterogeneous accelerators using token-aware buffer management. Reduces energy by 42.3% and GPU memory by 11.2%, enabling 20.8 hours runtime.",
        "abstract": "Large Multimodal Models (LMMs) are inherently modular, consisting of vision and audio encoders, projectors, and large language models. Yet, they are almost always executed monolithically, which underutilizes the heterogeneous accelerators (NPUs, GPUs, DSPs) in modern SoCs and leads to high end-to-end latency. In this paper, we present NANOMIND, a hardware--software co-design inference framework for Large Multimodal Models (LMMs) that breaks large models into modular ``bricks'' (vision, language, audio, etc.) and maps each to its ideal accelerator. The key insight is that large models can be broken into modular components and scheduled to run on the most appropriate compute units. It performs module-level dynamic offloading across accelerators on unified-memory SoCs. By combining customized hardware design, system-level scheduling, and optimized low-bit computation kernels, we demonstrate our framework with a compact, battery-powered device capable of running LMMs entirely on device. This prototype functions as a self-contained intelligent assistant that requires no network connectivity, while achieving higher throughput and superior power efficiency under strict resource constraints. The design further bypasses CPU bottlenecks and reduces redundant memory usage through token-aware buffer management and module-level coordination. Our system outperforms existing implementations in resource efficiency, cutting energy consumption by 42.3\\% and GPU memory usage by 11.2\\%. This enables a battery-powered device to run LLaVA-OneVision with a camera for nearly 20.8 hours.",
        "authors": [
            "Yilong Li",
            "Shuai Zhang",
            "Yijing Zeng",
            "Hao Zhang",
            "Xinmiao Xiong",
            "Jingyu Liu",
            "Pan Hu",
            "Suman Banerjee"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.CL",
            "eess.SP"
        ],
        "submit_date": "2025-09-25"
    },
    "http://arxiv.org/abs/2509.21527v1": {
        "title": "Redesigning GROMACS Halo Exchange: Improving Strong Scaling with GPU-initiated NVSHMEM",
        "link": "http://arxiv.org/abs/2509.21527v1",
        "abstract": "Improving time-to-solution in molecular dynamics simulations often requires strong scaling due to fixed-sized problems. GROMACS is highly latency-sensitive, with peak iteration rates in the sub-millisecond, making scalability on heterogeneous supercomputers challenging. MPI's CPU-centric nature introduces additional latencies on GPU-resident applications' critical path, hindering GPU utilization and scalability. To address these limitations, we present an NVSHMEM-based GPU kernel-initiated redesign of the GROMACS domain decomposition halo-exchange algorithm. Highly tuned GPU kernels fuse data packing and communication, leveraging hardware latency-hiding for fine-grained overlap. We employ kernel fusion across overlapped data forwarding communication phases and utilize the asynchronous copy engine over NVLink to optimize latency and bandwidth. Our GPU-resident formulation greatly increases communication-computation overlap, improving GROMACS strong scaling performance across NVLink by up to 1.5x (intra-node) and 2x (multi-node), and up to 1.3x multi-node over NVLink+InfiniBand. This demonstrates the profound benefits of GPU-initiated communication for strong-scaling a broad range of latency-sensitive applications.",
        "authors": [
            "Mahesh Doijade",
            "Andrey Alekseenko",
            "Ania Brown",
            "Alan Gray",
            "Szilárd Páll"
        ],
        "categories": [
            "cs.DC",
            "cs.PF",
            "physics.comp-ph"
        ],
        "id": "http://arxiv.org/abs/2509.21527v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-09-25"
    },
    "http://arxiv.org/abs/2509.21275v2": {
        "id": "http://arxiv.org/abs/2509.21275v2",
        "title": "Data-Centric Elastic Pipeline Parallelism for Efficient Long-Context LLM Training",
        "link": "http://arxiv.org/abs/2509.21275v2",
        "tags": [
            "training",
            "sparse",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-09-16",
        "tldr": "Proposes Elastic Pipeline Parallelism (EPP) and InfiniPipe system for efficient long-context LLM training. Adaptively combines token-level and batch-level pipeline parallelism with workload balancing and adaptive checkpointing to reduce communication and memory overhead. Achieves 1.69x speedup over state-of-the-art.",
        "abstract": "Long context training is crucial for LLM's context extension. Existing schemes, such as sequence parallelism, incur substantial communication overhead. Pipeline parallelism (PP) reduces this cost, but its effectiveness hinges on partitioning granularity. Batch-level PP dividing input samples exhibits high memory consumption in long-context scenario, whereas token-level PP splitting sequences into slices alleviates memory overhead but may incur hardware under-utilization. This trade-off motivates adaptively selecting PP granularity to match resource and workload characteristics. Moreover, sequence length distribution of the real-world dataset exhibits skewness, posing a challenge on PP's workload balance and efficient scheduling. Current static PP scheduling methods overlook the variance of sequence length, leading to suboptimal performance. In this paper, we propose Elastic Pipeline Parallelism (EPP) that orchestrates token-level PP and batch-level PP to adapt to resource and workload heterogeneity. We build InfiniPipe, a distributed training system that unleashes the potential of EPP via (1) a resource-aware and workload-balanced sequence processor that splits long sequences and packs short ones; and (2) a co-optimization methodology that jointly optimizes pipeline schedule and gradient checkpointing via a mechanism named stage-aware chunk-level adaptive checkpointing. Comprehensive experiments demonstrate that InfiniPipe achieves a 1.69x speedup over state-of-the-art systems.",
        "authors": [
            "Shiju Wang",
            "Yujie Wang",
            "Ao Sun",
            "Fangcheng Fu",
            "Zijian Zhu",
            "Bin Cui",
            "Xu Han",
            "Kaisheng Ma"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-09-25"
    },
    "http://arxiv.org/abs/2509.21271v1": {
        "id": "http://arxiv.org/abs/2509.21271v1",
        "title": "SuperOffload: Unleashing the Power of Large-Scale LLM Training on Superchips",
        "link": "http://arxiv.org/abs/2509.21271v1",
        "tags": [
            "training",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-09-16",
        "tldr": "How to optimize large-scale LLM training on Superchips using offloading? SuperOffload introduces Superchip-aware techniques like adaptive weight offloading and CPU-optimized Adam, achieving 2.5x higher throughput and enabling 25B model training on a single GH200.",
        "abstract": "The emergence of Superchips represents a significant advancement in next-generation AI hardware. These Superchips employ a tightly coupled heterogeneous architecture that integrates GPU and CPU on the same package, which offers unprecedented computational power. However, there has been scant research investigating how LLM training benefits from this new architecture. In this work, for the first time, we study LLM training solutions based on offloading for Superchips. We observe important differences between Superchips and traditional loosely-coupled GPU-CPU architecture, which necessitate revisiting prevailing assumptions about offloading. Based on that, we present SuperOffload, a Superchip-centric offloading system that simultaneously uses Hopper GPU, Grace CPU, and NVLink-C2C interconnect more efficiently. SuperOffload accomplishes this via a combination of techniques, such as adaptive weight offloading, bucketization repartitioning, Superchip-aware casting, speculative execution, and a highly optimized Adam optimizer for Grace CPUs. Our evaluation of SuperOffload on NVIDIA GH200 demonstrates up to 2.5x throughput improvement compared to state-of-the-art offloading-based systems, enabling training of up to 25B model on a single Superchip while achieving high training throughput. We also extend SuperOffload with ZeRO-style data parallelism and DeepSpeed-Ulysses sequence parallelism, enabling training of 13B model with sequence lengths up to 1 million tokens on 8 GH200 while achieving 55% MFU.",
        "authors": [
            "Xinyu Lian",
            "Masahiro Tanaka",
            "Olatunji Ruwase",
            "Minjia Zhang"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "submit_date": "2025-09-25"
    },
    "http://arxiv.org/abs/2509.21221v1": {
        "id": "http://arxiv.org/abs/2509.21221v1",
        "title": "Go With The Flow: Churn-Tolerant Decentralized Training of Large Language Models",
        "link": "http://arxiv.org/abs/2509.21221v1",
        "tags": [
            "training",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-09-16",
        "tldr": "Proposes GWTF, a decentralized framework for LLM training tolerant to node churn and network instability. Uses a novel flow-based routing algorithm to optimize microbatch scheduling across heterogeneous clients. Reduces training time by up to 45% under high churn.",
        "abstract": "Motivated by the emergence of large language models (LLMs) and the importance of democratizing their training, we propose GWTF, the first crash tolerant practical decentralized training framework for LLMs. Differently from existing distributed and federated training frameworks, GWTF enables the efficient collaborative training of a LLM on heterogeneous clients that volunteer their resources. In addition, GWTF addresses node churn, i.e., clients joining or leaving the system at any time, and network instabilities, i.e., network links becoming unstable or unreliable. The core of GWTF is a novel decentralized flow algorithm that finds the most effective routing that maximizes the number of microbatches trained with the lowest possible delay. We extensively evaluate GWTF on GPT-like and LLaMa-like models and compare it against the prior art. Our results indicate that GWTF reduces the training time by up to 45% in realistic and challenging scenarios that involve heterogeneous client nodes distributed over 10 different geographic locations with a high node churn rate.",
        "authors": [
            "Nikolay Blagoev",
            "Bart Cox",
            "Jérémie Decouchant",
            "Lydia Y. Chen"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "submit_date": "2025-09-25"
    },
    "http://arxiv.org/abs/2509.21137v3": {
        "id": "http://arxiv.org/abs/2509.21137v3",
        "title": "From GPUs to RRAMs: Distributed In-Memory Primal-Dual Hybrid Gradient Method for Solving Large-Scale Linear Optimization Problem",
        "link": "http://arxiv.org/abs/2509.21137v3",
        "tags": [
            "hardware",
            "offloading",
            "training"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes a distributed in-memory PDHG method for large-scale linear optimization using RRAM arrays to reduce energy and latency. Minimizes write cycles and unifies operations across crossbars; achieves up to 1000× energy and latency reductions versus GPUs while maintaining accuracy.",
        "abstract": "The exponential growth of computational workloads is surpassing the capabilities of conventional architectures, which are constrained by fundamental limits. In-memory computing (IMC) with RRAM provides a promising alternative by providing analog computations with significant gains in latency and energy use. However, existing algorithms developed for conventional architectures do not translate to IMC, particularly for constrained optimization problems where frequent matrix reprogramming remains cost-prohibitive for IMC applications. Here we present a distributed in-memory primal-dual hybrid gradient (PDHG) method, specifically co-designed for arrays of RRAM devices. Our approach minimizes costly write cycles, incorporates robustness against device non-idealities, and leverages a symmetric block-matrix formulation to unify operations across distributed crossbars. We integrate a physics-based simulation framework called MELISO+ to evaluate performance under realistic device conditions. Benchmarking against GPU-accelerated solvers on large-scale linear programs demonstrates that our RRAM-based solver achieves comparable accuracy with up to three orders of magnitude reductions in energy consumption and latency. These results demonstrate the first PDHG-based LP solver implemented on RRAMs, showcasing the transformative potential of algorithm-hardware co-design for solving large-scale optimization through distributed in-memory computing.",
        "authors": [
            "Huynh Q. N. Vo",
            "Md Tawsif Rahman Chowdhury",
            "Paritosh Ramanan",
            "Gozde Tutuncuoglu",
            "Junchi Yang",
            "Feng Qiu",
            "Murat Yildirim"
        ],
        "categories": [
            "cs.DC",
            "cs.AR",
            "cs.ET"
        ],
        "submit_date": "2025-09-25"
    },
    "http://arxiv.org/abs/2509.21075v1": {
        "title": "Communication Bias in Large Language Models: A Regulatory Perspective",
        "link": "http://arxiv.org/abs/2509.21075v1",
        "abstract": "Large language models (LLMs) are increasingly central to many applications, raising concerns about bias, fairness, and regulatory compliance. This paper reviews risks of biased outputs and their societal impact, focusing on frameworks like the EU's AI Act and the Digital Services Act. We argue that beyond constant regulation, stronger attention to competition and design governance is needed to ensure fair, trustworthy AI. This is a preprint of the Communications of the ACM article of the same title.",
        "authors": [
            "Adrian Kuenzler",
            "Stefan Schmid"
        ],
        "categories": [
            "cs.CY",
            "cs.AI",
            "cs.CL",
            "cs.DC",
            "cs.HC",
            "cs.LG"
        ],
        "id": "http://arxiv.org/abs/2509.21075v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-09-25"
    },
    "http://arxiv.org/abs/2509.21039v1": {
        "id": "http://arxiv.org/abs/2509.21039v1",
        "title": "Mojo: MLIR-Based Performance-Portable HPC Science Kernels on GPUs for the Python Ecosystem",
        "link": "http://arxiv.org/abs/2509.21039v1",
        "tags": [
            "kernel",
            "hardware"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Explores Mojo, an MLIR-based language for efficient science kernels on GPUs. Combines Python compatibility with low-level optimizations for cross-platform GPU portability. Achieves competitive performance to CUDA/HIP on memory-bound kernels, reducing development time while maintaining efficiency.",
        "abstract": "We explore the performance and portability of the novel Mojo language for scientific computing workloads on GPUs. As the first language based on the LLVM's Multi-Level Intermediate Representation (MLIR) compiler infrastructure, Mojo aims to close performance and productivity gaps by combining Python's interoperability and CUDA-like syntax for compile-time portable GPU programming. We target four scientific workloads: a seven-point stencil (memory-bound), BabelStream (memory-bound), miniBUDE (compute-bound), and Hartree-Fock (compute-bound with atomic operations); and compare their performance against vendor baselines on NVIDIA H100 and AMD MI300A GPUs. We show that Mojo's performance is competitive with CUDA and HIP for memory-bound kernels, whereas gaps exist on AMD GPUs for atomic operations and for fast-math compute-bound kernels on both AMD and NVIDIA GPUs. Although the learning curve and programming requirements are still fairly low-level, Mojo can close significant gaps in the fragmented Python ecosystem in the convergence of scientific computing and AI.",
        "authors": [
            "William F. Godoy",
            "Tatiana Melnichenko",
            "Pedro Valero-Lara",
            "Wael Elwasif",
            "Philip Fackler",
            "Rafael Ferreira Da Silva",
            "Keita Teranishi",
            "Jeffrey S. Vetter"
        ],
        "categories": [
            "cs.DC",
            "cs.CE",
            "cs.ET",
            "cs.PL"
        ],
        "submit_date": "2025-09-25"
    },
    "http://arxiv.org/abs/2509.21037v1": {
        "title": "Utilizing Sparsity in the GPU-accelerated Assembly of Schur Complement Matrices in Domain Decomposition Methods",
        "link": "http://arxiv.org/abs/2509.21037v1",
        "abstract": "Schur complement matrices emerge in many domain decomposition methods that can solve complex engineering problems using supercomputers. Today, as most of the high-performance clusters' performance lies in GPUs, these methods should also be accelerated.   Typically, the offloaded components are the explicitly assembled dense Schur complement matrices used later in the iterative solver for multiplication with a vector. As the explicit assembly is expensive, it represents a significant overhead associated with this approach to acceleration. It has already been shown that the overhead can be minimized by assembling the Schur complements directly on the GPU.   This paper shows that the GPU assembly can be further improved by wisely utilizing the sparsity of the input matrices. In the context of FETI methods, we achieved a speedup of 5.1 in the GPU section of the code and 3.3 for the whole assembly, making the acceleration beneficial from as few as 10 iterations.",
        "authors": [
            "Jakub Homola",
            "Ondřej Meca",
            "Lubomír Říha",
            "Tomáš Brzobohatý"
        ],
        "categories": [
            "cs.DC",
            "cs.MS"
        ],
        "id": "http://arxiv.org/abs/2509.21037v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-09-25"
    },
    "http://arxiv.org/abs/2509.21009v1": {
        "id": "http://arxiv.org/abs/2509.21009v1",
        "title": "RollPacker: Mitigating Long-Tail Rollouts for Fast, Synchronous RL Post-Training",
        "link": "http://arxiv.org/abs/2509.21009v1",
        "tags": [
            "RL",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-09-16",
        "tldr": "Addresses GPU underutilization in synchronous RL post-training caused by long-tail response delays. Introduces tail batching to schedule long responses separately, enabling balanced workloads across training stages. Achieves up to 2.56x faster training time on 128 H800 GPUs.",
        "abstract": "Reinforcement Learning (RL) is a pivotal post-training technique for enhancing the reasoning capabilities of Large Language Models (LLMs). However, synchronous RL post-training often suffers from significant GPU underutilization, referred to as bubbles, caused by imbalanced response lengths within rollout steps. Many RL systems attempt to alleviate this problem by relaxing synchronization, but this can compromise training accuracy. In this paper, we introduce tail batching, a novel rollout scheduling strategy for synchronous RL that systematically consolidates prompts leading to long-tail responses into a small subset of rollout steps (long rounds), while ensuring that the majority of steps (short rounds) involve only balanced, short rollouts. By excluding long responses from short rounds and rescheduling them into a few designated long rounds, tail batching effectively reduces GPU idle time during rollouts and significantly accelerates RL training without sacrificing accuracy. We present RollPacker, a system that fully harnesses the benefits of tail batching through holistic optimizations across all three RL stages: elastic parallelism adaptation for rollout, dynamic resource allocation and scheduling for reward, and stream-based training. Empirical results show that RollPacker achieves a 2.03x-2.56x end-to-end training time reduction compared to veRL and up to 2.24x speedup compared to RLHFuse for the Qwen2.5 family of LLMs on up to 128 H800 GPUs.",
        "authors": [
            "Wei Gao",
            "Yuheng Zhao",
            "Dakai An",
            "Tianyuan Wu",
            "Lunxi Cao",
            "Shaopan Xiong",
            "Ju Huang",
            "Weixun Wang",
            "Siran Yang",
            "Wenbo Su",
            "Jiamang Wang",
            "Lin Qu",
            "Bo Zheng",
            "Wei Wang"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-09-25"
    },
    "http://arxiv.org/abs/2510.01260v1": {
        "id": "http://arxiv.org/abs/2510.01260v1",
        "title": "IoT-MCP: Bridging LLMs and IoT Systems Through Model Context Protocol",
        "link": "http://arxiv.org/abs/2510.01260v1",
        "tags": [
            "edge",
            "RAG"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes IoT-MCP, a framework using Model Context Protocol for standardized communication between LLMs and IoT systems via edge servers. Achieves 100% task success rate and 205ms avg response time on complex tasks with only 74KB peak memory footprint on microcontrollers.",
        "abstract": "The integration of Large Language Models (LLMs) with Internet-of-Things (IoT) systems faces significant challenges in hardware heterogeneity and control complexity. The Model Context Protocol (MCP) emerges as a critical enabler, providing standardized communication between LLMs and physical devices. We propose IoT-MCP, a novel framework that implements MCP through edge-deployed servers to bridge LLMs and IoT ecosystems. To support rigorous evaluation, we introduce IoT-MCP Bench, the first benchmark containing 114 Basic Tasks (e.g., ``What is the current temperature?'') and 1,140 Complex Tasks (e.g., ``I feel so hot, do you have any ideas?'') for IoT-enabled LLMs. Experimental validation across 22 sensor types and 6 microcontroller units demonstrates IoT-MCP's 100% task success rate to generate tool calls that fully meet expectations and obtain completely accurate results, 205ms average response time, and 74KB peak memory footprint. This work delivers both an open-source integration framework (https://github.com/Duke-CEI-Center/IoT-MCP-Servers) and a standardized evaluation methodology for LLM-IoT systems.",
        "authors": [
            "Ningyuan Yang",
            "Guanliang Lyu",
            "Mingchen Ma",
            "Yiyi Lu",
            "Yiming Li",
            "Zhihui Gao",
            "Hancheng Ye",
            "Jianyi Zhang",
            "Tingjun Chen",
            "Yiran Chen"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-09-25"
    },
    "http://arxiv.org/abs/2509.20883v1": {
        "title": "RecIS: Sparse to Dense, A Unified Training Framework for Recommendation Models",
        "link": "http://arxiv.org/abs/2509.20883v1",
        "abstract": "In this paper, we propose RecIS, a unified Sparse-Dense training framework designed to achieve two primary goals: 1. Unified Framework To create a Unified sparse-dense training framework based on the PyTorch ecosystem that meets the training needs of industrial-grade recommendation models that integrated with large models. 2.System Optimization To optimize the sparse component, offering superior efficiency over the TensorFlow-based recommendation models. The dense component, meanwhile, leverages existing optimization technologies within the PyTorch ecosystem. Currently, RecIS is being used in Alibaba for numerous large-model enhanced recommendation training tasks, and some traditional sparse models have also begun training in it.",
        "authors": [
            "Hua Zong",
            "Qingtao Zeng",
            "Zhengxiong Zhou",
            "Zhihua Han",
            "Zhensong Yan",
            "Mingjie Liu",
            "Hechen Sun",
            "Jiawei Liu",
            "Yiwen Hu",
            "Qi Wang",
            "YiHan Xian",
            "Wenjie Guo",
            "Houyuan Xiang",
            "Zhiyuan Zeng",
            "Xiangrong Sheng",
            "Bencheng Yan",
            "Nan Hu",
            "Yuheng Huang",
            "Jinqing Lian",
            "Ziru Xu",
            "Yan Zhang",
            "Ju Huang",
            "Siran Yang",
            "Huimin Yi",
            "Jiamang Wang",
            "Pengjie Wang",
            "Han Zhu",
            "Jian Wu",
            "Dan Ou",
            "Jian Xu",
            "Haihong Tang",
            "Yuning Jiang",
            "Bo Zheng",
            "Lin Qu"
        ],
        "categories": [
            "cs.IR",
            "cs.DC",
            "cs.LG"
        ],
        "id": "http://arxiv.org/abs/2509.20883v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-09-25"
    },
    "http://arxiv.org/abs/2510.03243v2": {
        "id": "http://arxiv.org/abs/2510.03243v2",
        "title": "Prompt-Aware Scheduling for Low-Latency LLM Serving",
        "link": "http://arxiv.org/abs/2510.03243v2",
        "tags": [
            "serving",
            "thinking"
        ],
        "relevant": true,
        "indexed_date": "2025-09-16",
        "tldr": "Proposes PARS, a prompt-aware LLM scheduler that approximates SJF through pairwise ranking to mitigate HOL blocking. Integrated into vLLM, it predicts response lengths to reduce latency. Improves throughput by up to 1.7x and reduces latency by 38% compared to FCFS.",
        "abstract": "Efficient scheduling of LLM inference tasks is essential for achieving low latency and high throughput, particularly with the growing use of reasoning-capable LLMs. Traditional strategies like First-Come-First-Serve (FCFS) often suffer from Head-of-Line (HOL) blocking, where long-running tasks delay shorter ones queued behind them. In this paper, we introduce PARS, a prompt-aware LLM task scheduler that improves serving efficiency by approximating shortest-job-first (SJF) scheduling through pairwise ranking with margin ranking loss. PARS focuses on impactful scheduling decisions and is seamlessly integrated into the state-of-the-art LLM serving system vLLM. It effectively predicts response-length-based task ordering, reducing latency with minimal overhead. Extensive experiments across multiple LLMs and real-world inference datasets show that PARS significantly improves performance, including for reasoning workloads. Furthermore, our cross-model evaluations demonstrate that the design generalizes well, enabling effective scheduling even when predictors are trained on different LLMs.",
        "authors": [
            "Yiheng Tao",
            "Yihe Zhang",
            "Matthew T. Dearing",
            "Xin Wang",
            "Yuping Fan",
            "Zhiling Lan"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC",
            "cs.PF"
        ],
        "submit_date": "2025-09-25"
    },
    "http://arxiv.org/abs/2509.20819v1": {
        "id": "http://arxiv.org/abs/2509.20819v1",
        "title": "Integrating and Characterizing HPC Task Runtime Systems for hybrid AI-HPC workloads",
        "link": "http://arxiv.org/abs/2509.20819v1",
        "tags": [
            "training",
            "inference",
            "edge"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Studies how to manage hybrid AI-HPC workloads combining MPI, training, and inference. Integrates RADICAL-Pilot with Flux and Dragon for hierarchical resource management. Achieves over 1,500 tasks/s with 99.6% utilization, reducing makespan by 30-60% and increasing throughput 4x on Frontier.",
        "abstract": "Scientific workflows increasingly involve both HPC and machine-learning tasks, combining MPI-based simulations, training, and inference in a single execution. Launchers such as Slurm's srun constrain concurrency and throughput, making them unsuitable for dynamic and heterogeneous workloads. We present a performance study of RADICAL-Pilot (RP) integrated with Flux and Dragon, two complementary runtime systems that enable hierarchical resource management and high-throughput function execution. Using synthetic and production-scale workloads on Frontier, we characterize the task execution properties of RP across runtime configurations. RP+Flux sustains up to 930 tasks/s, and RP+Flux+Dragon exceeds 1,500 tasks/s with over 99.6% utilization. In contrast, srun peaks at 152 tasks/s and degrades with scale, with utilization below 50%. For IMPECCABLE.v2 drug discovery campaign, RP+Flux reduces makespan by 30-60% relative to srun/Slurm and increases throughput more than four times on up to 1,024. These results demonstrate hybrid runtime integration in RP as a scalable approach for hybrid AI-HPC workloads.",
        "authors": [
            "Andre Merzky",
            "Mikhail Titov",
            "Matteo Turilli",
            "Shantenu Jha"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-25"
    },
    "http://arxiv.org/abs/2509.20776v1": {
        "title": "Distributed-memory Algorithms for Sparse Matrix Permutation, Extraction, and Assignment",
        "link": "http://arxiv.org/abs/2509.20776v1",
        "abstract": "We present scalable distributed-memory algorithms for sparse matrix permutation, extraction, and assignment. Our methods follow an Identify-Exchange-Build (IEB) strategy where each process identifies the local nonzeros to be sent, exchanges the required data, and then builds its local submatrix from the received elements. This approach reduces communication compared to SpGEMM-based methods in distributed memory. By employing synchronization-free multithreaded algorithms, we further accelerate local computations, achieving substantially better performance than existing libraries such as CombBLAS and PETSc. We design efficient software for these operations and evaluate their performance on two university clusters and the Perlmutter supercomputer. Our experiments span a variety of application scenarios, including matrix permutation for load balancing, matrix reordering, subgraph extraction, and streaming graph applications. In all cases, we compare our algorithms against CombBLAS, the most comprehensive distributed library for these operations, and, in some scenarios, against PETSc. Overall, this work provides a comprehensive study of algorithms, software implementations, experimental evaluations, and applications for sparse matrix permutation, extraction, and assignment.",
        "authors": [
            "Elaheh Hassani",
            "Md Taufique Hussain",
            "Ariful Azad"
        ],
        "categories": [
            "cs.DC",
            "cs.MS"
        ],
        "id": "http://arxiv.org/abs/2509.20776v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-09-25"
    },
    "http://arxiv.org/abs/2510.01256v1": {
        "id": "http://arxiv.org/abs/2510.01256v1",
        "title": "Kant: An Efficient Unified Scheduling System for Large-Scale AI Clusters",
        "link": "http://arxiv.org/abs/2510.01256v1",
        "tags": [
            "training",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-09-16",
        "tldr": "Designs Kant, a unified scheduler for co-scheduling LLM training and inference in large AI clusters. Uses Backfill and E-Binpack to improve GPU utilization and reduce fragmentation. Achieves up to 30% higher GPU Allocation Ratio (GAR) compared to baseline schedulers.",
        "abstract": "As AI cluster sizes continue to expand and the demand for large-language-model (LLM) training and inference workloads grows rapidly, traditional scheduling systems face significant challenges in balancing resource utilization, scheduling efficiency, and service quality. This paper presents and evaluates Kant: an efficient unified scheduling platform designed for large-scale AI container clusters, supporting the co-scheduling of both training and inference jobs. Based on the practical implementation of the Kant system, we systematically define a set of key evaluation metrics for AI clusters, including GPU Allocation Ratio (GAR), Scheduling Occupancy Rate (SOR), GPU Node Fragmentation Ratio (GFR), Job Waiting Time Distribution (JWTD), and Job Training Time Estimation Distribution (JTTED), providing a foundation for quantitative performance analysis. Experimental results demonstrate that Kant achieves exceptional performance in clusters ranging from hundreds to tens of thousands of GPUs. By leveraging scheduling strategies such as Backfill and Enhanced Binpack (E-Binpack), the system significantly improves resource utilization and scheduling efficiency, while effectively reducing resource fragmentation and communication overhead in distributed training. The system has been deployed in multiple AI data center clusters, where it stably supports large-scale intelligent computing workloads. This work provides a practical engineering approach for building high-performance, highly available, AI-native scheduling infrastructure.",
        "authors": [
            "Lingling Zeng",
            "Gen Zhang",
            "Jialin Peng",
            "Xiang Xu",
            "Yuan Xu",
            "Lijun Ma"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.IT",
            "cs.LG"
        ],
        "submit_date": "2025-09-25"
    },
    "http://arxiv.org/abs/2509.20667v1": {
        "title": "Guiding Application Users via Estimation of Computational Resources for Massively Parallel Chemistry Computations",
        "link": "http://arxiv.org/abs/2509.20667v1",
        "abstract": "In this work, we develop machine learning (ML) based strategies to predict resources (costs) required for massively parallel chemistry computations, such as coupled-cluster methods, to guide application users before they commit to running expensive experiments on a supercomputer. By predicting application execution time, we determine the optimal runtime parameter values such as number of nodes and tile sizes. Two key questions of interest to users are addressed. The first is the shortest-time question, where the user is interested in knowing the parameter configurations (number of nodes and tile sizes) to achieve the shortest execution time for a given problem size and a target supercomputer. The second is the cheapest-run question in which the user is interested in minimizing resource usage, i.e., finding the number of nodes and tile size that minimizes the number of node-hours for a given problem size.   We evaluate a rich family of ML models and strategies, developed based on the collections of runtime parameter values for the CCSD (Coupled Cluster with Singles and Doubles) application executed on the Department of Energy (DOE) Frontier and Aurora supercomputers. Our experiments show that when predicting the total execution time of a CCSD iteration, a Gradient Boosting (GB) ML model achieves a Mean Absolute Percentage Error (MAPE) of 0.023 and 0.073 for Aurora and Frontier, respectively. In the case where it is expensive to run experiments just to collect data points, we show that active learning can achieve a MAPE of about 0.2 with just around 450 experiments collected from Aurora and Frontier.",
        "authors": [
            "Tanzila Tabassum",
            "Omer Subasi",
            "Ajay Panyala",
            "Epiya Ebiapia",
            "Gerald Baumgartner",
            "Erdal Mutlu",
            "P.",
            "Sadayappan",
            "Karol Kowalski"
        ],
        "categories": [
            "cs.LG",
            "cs.CE",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2509.20667v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-09-25"
    },
    "http://arxiv.org/abs/2509.20603v2": {
        "id": "http://arxiv.org/abs/2509.20603v2",
        "title": "Experience Deploying Containerized GenAI Services at an HPC Center",
        "link": "http://arxiv.org/abs/2509.20603v2",
        "tags": [
            "serving",
            "RAG"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Explores deployment of GenAI services, including LLM inference servers, at an HPC center. Proposes a converged HPC and Kubernetes architecture for containerized workloads using vLLM for inference. Achieves reproducible multi-platform deployment across different container runtimes.",
        "abstract": "Generative Artificial Intelligence (GenAI) applications are built from specialized components -- inference servers, object storage, vector and graph databases, and user interfaces -- interconnected via web-based APIs. While these components are often containerized and deployed in cloud environments, such capabilities are still emerging at High-Performance Computing (HPC) centers. In this paper, we share our experience deploying GenAI workloads within an established HPC center, discussing the integration of HPC and cloud computing environments. We describe our converged computing architecture that integrates HPC and Kubernetes platforms running containerized GenAI workloads, helping with reproducibility. A case study illustrates the deployment of the Llama Large Language Model (LLM) using a containerized inference server (vLLM) across both Kubernetes and HPC platforms using multiple container runtimes. Our experience highlights practical considerations and opportunities for the HPC container community, guiding future research and tool development.",
        "authors": [
            "Angel M. Beltre",
            "Jeff Ogden",
            "Kevin Pedretti"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.AR",
            "cs.ET",
            "cs.LG"
        ],
        "submit_date": "2025-09-24"
    },
    "http://arxiv.org/abs/2509.20563v1": {
        "id": "http://arxiv.org/abs/2509.20563v1",
        "title": "FZModules: A Heterogeneous Computing Framework for Customizable Scientific Data Compression Pipelines",
        "link": "http://arxiv.org/abs/2509.20563v1",
        "tags": [
            "training",
            "offline",
            "storage"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Addresses high data volume challenges in scientific computing with customizable compression pipelines. Proposes FZModules, a heterogeneous framework with asynchronous task execution and dependency management for modular lossy compression. Achieves significant end-to-end speedup comparable to GPU compressors with improved rate-distortion fidelity.",
        "abstract": "Modern scientific simulations and instruments generate data volumes that overwhelm memory and storage, throttling scalability. Lossy compression mitigates this by trading controlled error for reduced footprint and throughput gains, yet optimal pipelines are highly data and objective specific, demanding compression expertise. GPU compressors supply raw throughput but often hard-code fused kernels that hinder rapid experimentation, and underperform in rate-distortion. We present FZModules, a heterogeneous framework for assembling error-bounded custom compression pipelines from high-performance modules through a concise extensible interface. We further utilize an asynchronous task-backed execution library that infers data dependencies, manages memory movement, and exposes branch and stage level concurrency for powerful asynchronous compression pipelines. Evaluating three pipelines built with FZModules on four representative scientific datasets, we show they can compare end-to-end speedup of fused-kernel GPU compressors while achieving similar rate-distortion to higher fidelity CPU or hybrid compressors, enabling rapid, domain-tailored design.",
        "authors": [
            "Skyler Ruiter",
            "Jiannan Tian",
            "Fengguang Song"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-24"
    },
    "http://arxiv.org/abs/2509.20520v1": {
        "id": "http://arxiv.org/abs/2509.20520v1",
        "title": "Adaptive Approach to Enhance Machine Learning Scheduling Algorithms During Runtime Using Reinforcement Learning in Metascheduling Applications",
        "link": "http://arxiv.org/abs/2509.20520v1",
        "tags": [
            "scheduling",
            "RL",
            "training"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes an online reinforcement learning unit for metaschedulers to adapt AI-based scheduling during runtime. Integrates RL models to discover new scheduling solutions and optimize existing ones under dynamic events. Achieves continuous refinement for robustness in safety-critical environments.",
        "abstract": "Metascheduling in time-triggered architectures has been crucial in adapting to dynamic and unpredictable environments, ensuring the reliability and efficiency of task execution. However, traditional approaches face significant challenges when training Artificial Intelligence (AI) scheduling inferences offline, particularly due to the complexities involved in constructing a comprehensive Multi-Schedule Graph (MSG) that accounts for all possible scenarios. The process of generating an MSG that captures the vast probability space, especially when considering context events like hardware failures, slack variations, or mode changes, is resource-intensive and often infeasible. To address these challenges, we propose an adaptive online learning unit integrated within the metascheduler to enhance performance in real-time. The primary motivation for developing this unit stems from the limitations of offline training, where the MSG created is inherently a subset of the complete space, focusing only on the most probable and critical context events. In the online mode, Reinforcement Learning (RL) plays a pivotal role by continuously exploring and discovering new scheduling solutions, thus expanding the MSG and enhancing system performance over time. This dynamic adaptation allows the system to handle unexpected events and complex scheduling scenarios more effectively. Several RL models were implemented within the online learning unit, each designed to address specific challenges in scheduling. These models not only facilitate the discovery of new solutions but also optimize existing schedulers, particularly when stricter deadlines or new performance criteria are introduced. By continuously refining the AI inferences through real-time training, the system remains flexible and capable of meeting evolving demands, thus ensuring robustness and efficiency in large-scale, safety-critical environments.",
        "authors": [
            "Samer Alshaer",
            "Ala Khalifeh",
            "Roman Obermaisser"
        ],
        "categories": [
            "cs.AI",
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-09-24"
    },
    "http://arxiv.org/abs/2509.20513v1": {
        "id": "http://arxiv.org/abs/2509.20513v1",
        "title": "Reconstruction-Based Adaptive Scheduling Using AI Inferences in Safety-Critical Systems",
        "link": "http://arxiv.org/abs/2509.20513v1",
        "tags": [
            "RL",
            "edge",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes a reconstruction framework to dynamically validate and assemble schedules in time-triggered safety-critical systems. Transforms AI-generated or heuristic scheduling priorities into executable schedules with safety checks and recovery mechanisms. Improves system adaptability and runtime performance, achieving significant enhancement in operational integrity.",
        "abstract": "Adaptive scheduling is crucial for ensuring the reliability and safety of time-triggered systems (TTS) in dynamic operational environments. Scheduling frameworks face significant challenges, including message collisions, locked loops from incorrect precedence handling, and the generation of incomplete or invalid schedules, which can compromise system safety and performance. To address these challenges, this paper presents a novel reconstruction framework designed to dynamically validate and assemble schedules. The proposed reconstruction models operate by systematically transforming AI-generated or heuristically derived scheduling priorities into fully executable schedules, ensuring adherence to critical system constraints such as precedence rules and collision-free communication. It incorporates robust safety checks, efficient allocation algorithms, and recovery mechanisms to handle unexpected context events, including hardware failures and mode transitions. Comprehensive experiments were conducted across multiple performance profiles, including makespan minimisation, workload balancing, and energy efficiency, to validate the operational effectiveness of the reconstruction models. Results demonstrate that the proposed framework significantly enhances system adaptability, operational integrity, and runtime performance while maintaining computational efficiency. Overall, this work contributes a practical and scalable solution to the problem of safe schedule generation in safety-critical TTS, enabling reliable and flexible real-time scheduling even under highly dynamic and uncertain operational conditions.",
        "authors": [
            "Samer Alshaer",
            "Ala Khalifeh",
            "Roman Obermaisser"
        ],
        "categories": [
            "cs.AI",
            "cs.DC"
        ],
        "submit_date": "2025-09-24"
    },
    "http://arxiv.org/abs/2509.20340v1": {
        "title": "xGFabric: Coupling Sensor Networks and HPC Facilities with Private 5G Wireless Networks for Real-Time Digital Agriculture",
        "link": "http://arxiv.org/abs/2509.20340v1",
        "abstract": "Advanced scientific applications require coupling distributed sensor networks with centralized high-performance computing facilities. Citrus Under Protective Screening (CUPS) exemplifies this need in digital agriculture, where citrus research facilities are instrumented with numerous sensors monitoring environmental conditions and detecting protective screening damage. CUPS demands access to computational fluid dynamics codes for modeling environmental conditions and guiding real-time interventions like water application or robotic repairs. These computing domains have contrasting properties: sensor networks provide low-performance, limited-capacity, unreliable data access, while high-performance facilities offer enormous computing power through high-latency batch processing. Private 5G networks present novel capabilities addressing this challenge by providing low latency, high throughput, and reliability necessary for near-real-time coupling of edge sensor networks with HPC simulations. This work presents xGFabric, an end-to-end system coupling sensor networks with HPC facilities through Private 5G networks. The prototype connects remote sensors via 5G network slicing to HPC systems, enabling real-time digital agriculture simulation.",
        "authors": [
            "Liubov Kurafeeva",
            "Alan Subedi",
            "Ryan Hartung",
            "Michael Fay",
            "Avhishek Biswas",
            "Shantenu Jha",
            "Ozgur O. Kilic",
            "Chandra Krintz",
            "Andre Merzky",
            "Douglas Thain",
            "Mehmet C. Vuran",
            "Rich Wolski"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2509.20340v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-09-24"
    },
    "http://arxiv.org/abs/2509.20241v1": {
        "id": "http://arxiv.org/abs/2509.20241v1",
        "title": "Energy Use of AI Inference: Efficiency Pathways and Test-Time Compute",
        "link": "http://arxiv.org/abs/2509.20241v1",
        "tags": [
            "serving",
            "hardware",
            "offline"
        ],
        "relevant": true,
        "indexed_date": "2025-09-16",
        "tldr": "Estimates energy use per LLM inference query at scale, accounting for real-world GPU utilization and PUE. Proposes a bottom-up methodology to quantify efficiency gains at model, platform, and hardware levels, achieving up to 20x reduction in energy per query for 1B queries/day.",
        "abstract": "As AI inference scales to billions of queries and emerging reasoning and agentic workflows increase token demand, reliable estimates of per-query energy use are increasingly important for capacity planning, emissions accounting, and efficiency prioritization. Many public estimates are inconsistent and overstate energy use, because they extrapolate from limited benchmarks and fail to reflect efficiency gains achievable at scale. In this perspective, we introduce a bottom-up methodology to estimate the per-query energy of large-scale LLM systems based on token throughput. For models running on an H100 node under realistic workloads, GPU utilization and PUE constraints, we estimate a median energy per query of 0.34 Wh (IQR: 0.18-0.67) for frontier-scale models (>200 billion parameters). These results are consistent with measurements using production-scale configurations and show that non-production estimates and assumptions can overstate energy use by 4-20x. Extending to test-time scaling scenarios with 15x more tokens per typical query, the median energy rises 13x to 4.32 Wh, indicating that targeting efficiency in this regime will deliver the largest fleet-wide savings. We quantify achievable efficiency gains at the model, serving platform, and hardware levels, finding individual median reductions of 1.5-3.5x in energy per query, while combined advances can plausibly deliver 8-20x reductions. To illustrate the system-level impact, we estimate the baseline daily energy use of a deployment serving 1 billion queries to be 0.8 GWh/day. If 10% are long queries, demand could grow to 1.8 GWh/day. With targeted efficiency interventions, it falls to 0.9 GWh/day, similar to the energy footprint of web search at that scale. This echoes how data centers historically tempered energy growth through efficiency gains during the internet and cloud build-up.",
        "authors": [
            "Felipe Oviedo",
            "Fiodar Kazhamiaka",
            "Esha Choukse",
            "Allen Kim",
            "Amy Luers",
            "Melanie Nakagawa",
            "Ricardo Bianchini",
            "Juan M. Lavista Ferres"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "submit_date": "2025-09-24"
    },
    "http://arxiv.org/abs/2509.20223v1": {
        "title": "An Empirical Analysis of Secure Federated Learning for Autonomous Vehicle Applications",
        "link": "http://arxiv.org/abs/2509.20223v1",
        "abstract": "Federated Learning lends itself as a promising paradigm in enabling distributed learning for autonomous vehicles applications and ensuring data privacy while enhancing and refining predictive model performance through collaborative training on edge client vehicles. However, it remains vulnerable to various categories of cyber-attacks, necessitating more robust security measures to effectively mitigate potential threats. Poisoning attacks and inference attacks are commonly initiated within the federated learning environment to compromise secure system performance. Secure aggregation can limit the disclosure of sensitive information from outsider and insider attackers of the federated learning environment. In this study, our aim is to conduct an empirical analysis on the transportation image dataset (e.g., LISA traffic light) using various secure aggregation techniques and multiparty computation in the presence of diverse categories of cyber-attacks. Multiparty computation serves as a state-of-the-art security mechanism, offering standard privacy for secure aggregation of edge autonomous vehicles local model updates through various security protocols. The presence of adversaries can mislead the autonomous vehicle learning model, leading to the misclassification of traffic lights, and resulting in detrimental impacts. This empirical study explores the resilience of various secure federated learning aggregation techniques and multiparty computation in safeguarding autonomous vehicle applications against various cyber threats during both training and inference times.",
        "authors": [
            "Md Jueal Mia",
            "M. Hadi Amini"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2509.20223v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-09-24"
    },
    "http://arxiv.org/abs/2509.20205v1": {
        "id": "http://arxiv.org/abs/2509.20205v1",
        "title": "Fulcrum: Optimizing Concurrent DNN Training and Inferencing on Edge Accelerators",
        "link": "http://arxiv.org/abs/2509.20205v1",
        "tags": [
            "training",
            "inference",
            "edge"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Addresses optimizing concurrent DNN training and inference on edge accelerators under power and latency constraints. Proposes Fulcrum scheduler with gradient descent (GMD) and active learning (ALS) techniques to interleave workloads and select power modes. Satisfies latency/power budgets in 97% of runs, achieving within 7% of optimal throughput.",
        "abstract": "The proliferation of GPU accelerated edge devices like Nvidia Jetsons and the rise in privacy concerns are placing an emphasis on concurrent DNN training and inferencing on edge devices. Inference and training have different computing and QoS goals. But edge accelerators like Jetson do not support native GPU sharing and expose 1000s of power modes. This requires careful time-sharing of concurrent workloads to meet power--performance goals, while limiting costly profiling. In this paper, we design an intelligent time-slicing approach for concurrent DNN training and inferencing on Jetsons. We formulate an optimization problem to interleave training and inferencing minibatches, and decide the device power mode and inference minibatch size, while maximizing the training throughput and staying within latency and power budgets, with modest profiling costs. We propose GMD, an efficient multi-dimensional gradient descent search which profiles just $15$ power modes; and ALS, an Active Learning technique which identifies reusable Pareto-optimal power modes, but profiles $50$--$150$ power modes. We evaluate these within our Fulcrum scheduler for $273,000+$ configurations across $15$ DNN workloads. We also evaluate our strategies on dynamic arrival inference and concurrent inferences. ALS and GMD outperform simpler and more complex baselines with larger-scale profiling. Their solutions satisfy the latency and power budget for $>97\\%$ of our runs, and on average are within $7\\%$ of the optimal throughput.",
        "authors": [
            "Prashanthi S. K.",
            "Saisamarth Taluri",
            "Pranav Gupta",
            "Amartya Ranjan Saikia",
            "Kunal Kumar Sahoo",
            "Atharva Vinay Joshi",
            "Lakshya Karwa",
            "Kedar Dhule",
            "Yogesh Simmhan"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-24"
    },
    "http://arxiv.org/abs/2509.20189v1": {
        "id": "http://arxiv.org/abs/2509.20189v1",
        "title": "Pagoda: An Energy and Time Roofline Study for DNN Workloads on Edge Accelerators",
        "link": "http://arxiv.org/abs/2509.20189v1",
        "tags": [
            "edge",
            "serving",
            "hardware"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Studies power-performance trade-offs for DNN workloads on Nvidia Jetson edge accelerators. Develops energy and time roofline models coupled with analytical workload analysis. Achieves up to 15% energy reduction with minimal latency degradation by tuning power modes.",
        "abstract": "Edge accelerators such as Nvidia Jetsons are becoming an integral part of the computing continuum, and are often used for DNN inferencing and training. Nvidia Jetson edge devices have $2000$+ CUDA cores within a $70$W power envelope and offer $1000$s of power modes to customize CPU, GPU and memory frequencies. Their widely varying power--performance trade-offs can be exploited for energy and power-constrained deployments. While data-driven methods to predict the power and latency of DNN workloads for edge devices exist, there is a lack of principled study to understand why edge accelerators and their power modes perform the way they do. We develop a time roofline and a novel energy roofline model for the Jetson Orin AGX for diverse power modes, and couple it with an analytical model of the compute (FLOP) and memory access (bytes) for DNN inference workloads to analyze them from first principles. These reveal unique, sometimes counter-intuitive, insights into the power and performance behavior of DNN workloads on edge accelerators, e.g., the default power mode MAXN is not the most energy efficient and time efficiency implies energy efficiency for all power modes. We also extend our analytical roofline models to DNN training. Finally, we apply these methods to tune the power mode (and hence the roofline) of the edge device to optimize the latency and energy for DNN inference, with up to $15\\%$ lower energy and minimal degradation in inference time.",
        "authors": [
            "Prashanthi S. K.",
            "Kunal Kumar Sahoo",
            "Amartya Ranjan Saikia",
            "Pranav Gupta",
            "Atharva Vinay Joshi",
            "Priyanshu Pansari",
            "Yogesh Simmhan"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-24"
    },
    "http://arxiv.org/abs/2509.20160v1": {
        "title": "Characterizing the Performance of Accelerated Jetson Edge Devices for Training Deep Learning Models",
        "link": "http://arxiv.org/abs/2509.20160v1",
        "abstract": "Deep Neural Networks (DNNs) have had a significant impact on domains like autonomous vehicles and smart cities through low-latency inferencing on edge computing devices close to the data source. However, DNN training on the edge is poorly explored. Techniques like federated learning and the growing capacity of GPU-accelerated edge devices like NVIDIA Jetson motivate the need for a holistic characterization of DNN training on the edge. Training DNNs is resource-intensive and can stress an edge's GPU, CPU, memory and storage capacities. Edge devices also have different resources compared to workstations and servers, such as slower shared memory and diverse storage media. Here, we perform a principled study of DNN training on individual devices of three contemporary Jetson device types: AGX Xavier, Xavier NX and Nano for three diverse DNN model--dataset combinations. We vary device and training parameters such as I/O pipelining and parallelism, storage media, mini-batch sizes and power modes, and examine their effect on CPU and GPU utilization, fetch stalls, training time, energy usage, and variability. Our analysis exposes several resource inter-dependencies and counter-intuitive insights, while also helping quantify known wisdom. Our rigorous study can help tune the training performance on the edge, trade-off time and energy usage on constrained devices, and even select an ideal edge hardware for a DNN workload, and, in future, extend to federated learning too. As an illustration, we use these results to build a simple model to predict the training time and energy per epoch for any given DNN across different power modes, with minimal additional profiling.",
        "authors": [
            "Prashanthi S. K.",
            "Sai Anuroop Kesanapalli",
            "Yogesh Simmhan"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2509.20160v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-09-24"
    },
    "http://arxiv.org/abs/2509.19836v1": {
        "id": "http://arxiv.org/abs/2509.19836v1",
        "title": "BurstEngine: an Efficient Distributed Framework for Training Transformers on Extremely Long Sequences of over 1M Tokens",
        "link": "http://arxiv.org/abs/2509.19836v1",
        "tags": [
            "training",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-09-16",
        "tldr": "Designs BurstEngine to efficiently train LLMs on sequences >1M tokens by introducing topology-aware BurstAttention, selective checkpointing, and fused loss computation, achieving 1.2× speedup and reduced memory overhead compared to state-of-the-art methods.",
        "abstract": "Existing methods for training LLMs on long-sequence data, such as Tensor Parallelism and Context Parallelism, exhibit low Model FLOPs Utilization as sequence lengths and number of GPUs increase, especially when sequence lengths exceed 1M tokens. To address these challenges, we propose BurstEngine, an efficient framework designed to train LLMs on long-sequence data. BurstEngine introduces BurstAttention, an optimized distributed attention with lower communication cost than RingAttention. BurstAttention leverages topology-aware ring communication to fully utilize network bandwidth and incorporates fine-grained communication-computation overlap. Furthermore, BurstEngine introduces sequence-level selective checkpointing and fuses the language modeling head with the loss function to reduce memory cost. Additionally, BurstEngine introduces workload balance optimization for various types of attention masking. By integrating these optimizations, BurstEngine achieves a $1.2\\times$ speedup with much lower memory overhead than the state-of-the-art baselines when training LLMs on extremely long sequences of over 1M tokens. We have made our code publicly available on GitHub: https://github.com/thunlp/BurstEngine.",
        "authors": [
            "Ao Sun",
            "Weilin Zhao",
            "Xu Han",
            "Cheng Yang",
            "Zhiyuan Liu",
            "Chuan Shi",
            "Maosong sun"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-24"
    },
    "http://arxiv.org/abs/2509.20408v1": {
        "title": "A Theory of Multi-Agent Generative Flow Networks",
        "link": "http://arxiv.org/abs/2509.20408v1",
        "abstract": "Generative flow networks utilize a flow-matching loss to learn a stochastic policy for generating objects from a sequence of actions, such that the probability of generating a pattern can be proportional to the corresponding given reward. However, a theoretical framework for multi-agent generative flow networks (MA-GFlowNets) has not yet been proposed. In this paper, we propose the theory framework of MA-GFlowNets, which can be applied to multiple agents to generate objects collaboratively through a series of joint actions. We further propose four algorithms: a centralized flow network for centralized training of MA-GFlowNets, an independent flow network for decentralized execution, a joint flow network for achieving centralized training with decentralized execution, and its updated conditional version. Joint Flow training is based on a local-global principle allowing to train a collection of (local) GFN as a unique (global) GFN. This principle provides a loss of reasonable complexity and allows to leverage usual results on GFN to provide theoretical guarantees that the independent policies generate samples with probability proportional to the reward function. Experimental results demonstrate the superiority of the proposed framework compared to reinforcement learning and MCMC-based methods.",
        "authors": [
            "Leo Maxime Brunswic",
            "Haozhi Wang",
            "Shuang Luo",
            "Jianye Hao",
            "Amir Rasouli",
            "Yinchuan Li"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2509.20408v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-09-24"
    },
    "http://arxiv.org/abs/2509.19729v1": {
        "id": "http://arxiv.org/abs/2509.19729v1",
        "title": "Gyges: Dynamic Cross-Instance Parallelism Transformation for Efficient LLM Inference",
        "link": "http://arxiv.org/abs/2509.19729v1",
        "tags": [
            "serving",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-09-16",
        "tldr": "Addresses dynamic context length variance in LLM serving by adaptively transforming parallelism strategies across instances. Proposes a header-centric KV cache layout, weight padding, and transformation-aware scheduler, achieving up to 6.57× higher throughput than state-of-the-art systems.",
        "abstract": "Efficiently processing the dynamics of requests, especially the context length variance, is important in Large Language Model (LLM) serving scenarios. However, there is an intrinsic trade-off: while leveraging parallelism strategies, such as Tensor Parallelism (TP), can coordinate multiple GPUs to accommodate larger context lengths, it inevitably results in degraded overall throughput. In this paper, we propose Cross-Instance Parallelism Transformation (Gyges), which adaptively adjusts the parallelism strategies of running instances to align with the dynamics of incoming requests. We design (1) a page-friendly, header-centric layout to accelerate KV cache transformations; (2) dedicated weight padding to accelerate model weight transformations; and (3) a transformation-aware scheduler to cooperatively schedule requests and parallelism transformations, optimizing the overall performance. Evaluations using real-world traces show that Gyges improves throughput by 1.75x-6.57x compared to state-of-the-art solutions.",
        "authors": [
            "Haoyu Chen",
            "Xue Li",
            "Kun Qian",
            "Yu Guan",
            "Jin Zhao",
            "Xin Wang"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-24"
    },
    "http://arxiv.org/abs/2509.19701v1": {
        "title": "Characterizing Adaptive Mesh Refinement on Heterogeneous Platforms with Parthenon-VIBE",
        "link": "http://arxiv.org/abs/2509.19701v1",
        "abstract": "Hero-class HPC simulations rely on Adaptive Mesh Refinement (AMR) to reduce compute and memory demands while maintaining accuracy. This work analyzes the performance of Parthenon, a block-structured AMR benchmark, on CPU-GPU systems. We show that smaller mesh blocks and deeper AMR levels degrade GPU performance due to increased communication, serial overheads, and inefficient GPU utilization. Through detailed profiling, we identify inefficiencies, low occupancy, and memory access bottlenecks. We further analyze rank scalability and memory constraints, and propose optimizations to improve GPU throughput and reduce memory footprint. Our insights can inform future AMR deployments on Department of Energy's upcoming heterogeneous supercomputers.",
        "authors": [
            "Akash Poptani",
            "Alireza Khadem",
            "Scott Mahlke",
            "Jonah Miller",
            "Joshua Dolence",
            "Reetuparna Das"
        ],
        "categories": [
            "cs.DC",
            "cs.PF"
        ],
        "id": "http://arxiv.org/abs/2509.19701v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-09-24"
    },
    "http://arxiv.org/abs/2512.13047v1": {
        "title": "Sharpen the Spec, Cut the Code: A Case for Generative File System with SYSSPEC",
        "link": "http://arxiv.org/abs/2512.13047v1",
        "abstract": "File systems are critical OS components that require constant evolution to support new hardware and emerging application needs. However, the traditional paradigm of developing features, fixing bugs, and maintaining the system incurs significant overhead, especially as systems grow in complexity. This paper proposes a new paradigm, generative file systems, which leverages Large Language Models (LLMs) to generate and evolve a file system from prompts, effectively addressing the need for robust evolution. Despite the widespread success of LLMs in code generation, attempts to create a functional file system have thus far been unsuccessful, mainly due to the ambiguity of natural language prompts.   This paper introduces SYSSPEC, a framework for developing generative file systems. Its key insight is to replace ambiguous natural language with principles adapted from formal methods. Instead of imprecise prompts, SYSSPEC employs a multi-part specification that accurately describes a file system's functionality, modularity, and concurrency. The specification acts as an unambiguous blueprint, guiding LLMs to generate expected code flexibly. To manage evolution, we develop a DAG-structured patch that operates on the specification itself, enabling new features to be added without violating existing invariants. Moreover, the SYSSPEC toolchain features a set of LLM-based agents with mechanisms to mitigate hallucination during construction and evolution. We demonstrate our approach by generating SPECFS, a concurrent file system. SPECFS passes hundreds of regression tests, matching a manually-coded baseline. We further confirm its evolvability by seamlessly integrating 10 real-world features from Ext4. Our work shows that a specification-guided approach makes generating and evolving complex systems not only feasible but also highly effective.",
        "authors": [
            "Qingyuan Liu",
            "Zou Mo",
            "Hengbin Zhang",
            "Dong Du",
            "Yubin Xia",
            "Haibo Chen"
        ],
        "categories": [
            "cs.OS",
            "cs.SE"
        ],
        "id": "http://arxiv.org/abs/2512.13047v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-15"
    },
    "http://arxiv.org/abs/2512.12615v2": {
        "id": "http://arxiv.org/abs/2512.12615v2",
        "title": "gpu_ext: Extensible OS Policies for GPUs via eBPF",
        "link": "http://arxiv.org/abs/2512.12615v2",
        "tags": [
            "serving",
            "training",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-12-14",
        "tldr": "Proposes GPU kernel extensibility via eBPF for dynamic resource management. Designs device-side hooks and runtime to enforce policies without kernel modifications. Achieves up to 4.8x throughput gain and 2x tail latency reduction in ML workloads including inference and training.",
        "abstract": "Performance in modern GPU-centric systems increasingly depends on resource management policies, including memory placement, scheduling, and observability. However, uniform policies typically yield suboptimal performance across diverse workloads. Existing approaches present a tradeoff: user-space runtimes provide programmability and flexibility but lack cross-tenant visibility and fine-grained control of hardware resources; meanwhile, modifications to the OS kernel introduce significant complexity and safety risks. To address this, we argue that the GPU driver and device layer should provide an extensible OS interface for policy enforcement. While the emerging eBPF technology shows potential, directly applying existing host-side eBPF is insufficient because they lack visibility and control into critical device-side events, and directly embedding policy code into GPU kernels could compromise safety and efficiency. We propose gpu_ext, an eBPF-based runtime that treats the GPU driver and device as a programmable OS subsystem. gpu_ext extends GPU drivers by exposing safe programmable hooks and introduces a device-side eBPF runtime capable of executing verified policy logic within GPU kernels, enabling coherent and transparent policies. Evaluation across realistic workloads including inference, training, and vector search demonstrates that gpu_ext improves throughput by up to 4.8x and reduces tail latency by up to 2x, incurring low overhead, without modifying or restarting applications",
        "authors": [
            "Yusheng Zheng",
            "Tong Yu",
            "Yiwei Yang",
            "Minghui Jiang",
            "Xiangyu Gao",
            "Jianchang Su",
            "Yanpeng Hu",
            "Wenan Mao",
            "Wei Zhang",
            "Dan Williams",
            "Andi Quinn"
        ],
        "categories": [
            "cs.OS"
        ],
        "submit_date": "2025-12-14"
    },
    "http://arxiv.org/abs/2512.12530v1": {
        "id": "http://arxiv.org/abs/2512.12530v1",
        "title": "Principled Performance Tunability in Operating System Kernels",
        "link": "http://arxiv.org/abs/2512.12530v1",
        "tags": [
            "kernel",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-12-14",
        "tldr": "Addresses the problem of safely tuning Linux kernel performance constants (perf-consts). Proposes KernelX, using Scoped Indirect Execution (SIE) to enable live updates without rebuilds. Achieves millisecond-scale updates and significant performance improvements in case studies.",
        "abstract": "The Linux kernel source code contains numerous constant values that critically influence system performance. Many of these constants, which we term perf-consts, are magic numbers that encode brittle assumptions about hardware and workloads. As systems and workloads evolve, such constants often become suboptimal. Unfortunately, deployed kernels lack support for safe and efficient in-situ tuning of perf-consts without a long and disruptive process of rebuilding and redeploying the kernel image.   This paper advocates principled OS performance tunability. We present KernelX, a system that provides a safe, efficient, and programmable interface for in-situ tuning of arbitrary perf-consts on a running kernel. KernelX transforms any perf-const into a tunable knob on demand using a novel mechanism called Scoped Indirect Execution (SIE). SIE precisely identifies the binary boundaries where a perf-const influences system state and redirects execution to synthesized instructions that update the state as if new values were used. KernelX goes beyond version atomicity to guarantee side-effect safety, a property not provided by existing kernel update mechanisms. KernelX also provides a programmable interface that allows policies to incorporate application hints, hardware heuristics, and fine-grained isolation, without modifying kernel source code or disrupting deployed OS kernels.   Case studies across multiple kernel subsystems demonstrate that KernelX enables significant performance improvements by making previously untunable perf-consts safely tunable at runtime, while supporting millisecond-scale policy updates.",
        "authors": [
            "Zhongjie Chen",
            "Wentao Zhang",
            "Yulong Tang",
            "Ran Shu",
            "Fengyuan Ren",
            "Tianyin Xu",
            "Jing Liu"
        ],
        "categories": [
            "cs.OS"
        ],
        "submit_date": "2025-12-14"
    },
    "http://arxiv.org/abs/2512.09300v1": {
        "title": "ZeroOS: A Universal Modular Library OS for zkVMs",
        "link": "http://arxiv.org/abs/2512.09300v1",
        "abstract": "zkVMs promise general-purpose verifiable computation through ISA-level compatibility with modern programs and toolchains. However, compatibility extends further than just the ISA; modern programs often cannot run or even compile without an operating system and libc. zkVMs attempt to address this by maintaining forks of language-specific runtimes and statically linking them into applications to create self-contained unikernels, but this ad-hoc approach leads to version hell and burdens verifiable applications (vApps) with an unnecessarily large trusted computing base. We solve this problem with ZeroOS, a modular library operating system (libOS) for vApp unikernels; vApp developers can use off-the-shelf toolchains to compile and link only the exact subset of the Linux ABI their vApp needs. Any zkVM team can easily leverage the ZeroOS ecosystem by writing a ZeroOS bootloader for their platform, resulting in a reduced maintainence burden and unifying the entire zkVM ecosystem with consolidated development and audit resources. ZeroOS is free and open-sourced at https://github.com/LayerZero-Labs/ZeroOS.",
        "authors": [
            "Guangxian Zou",
            "Isaac Zhang",
            "Ryan Zarick",
            "Kelvin Wong",
            "Thomas Kim",
            "Daniel L. -K. Wong",
            "Saeid Yazdinejad",
            "Dan Boneh"
        ],
        "categories": [
            "cs.OS",
            "cs.CR"
        ],
        "id": "http://arxiv.org/abs/2512.09300v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-10"
    },
    "http://arxiv.org/abs/2512.08858v1": {
        "title": "NecoFuzz: Effective Fuzzing of Nested Virtualization via Fuzz-Harness Virtual Machines",
        "link": "http://arxiv.org/abs/2512.08858v1",
        "abstract": "Nested virtualization is now widely supported by major cloud vendors, allowing users to leverage virtualization-based technologies in the cloud. However, supporting nested virtualization significantly increases host hypervisor complexity and introduces a new attack surface in cloud platforms. While many prior studies have explored hypervisor fuzzing, none has explicitly addressed nested virtualization due to the challenge of generating effective virtual machine (VM) instances with a vast state space as fuzzing inputs.   We present NecoFuzz, the first fuzzing framework that systematically targets nested virtualization-specific logic in hypervisors. NecoFuzz synthesizes executable fuzz-harness VMs with internal states near the boundary between valid and invalid, guided by an approximate model of hardware-assisted virtualization specifications. Since vulnerabilities in nested virtualization often stem from incorrect handling of unexpected VM states, this specification-guided, boundary-oriented generation significantly improves coverage of security-critical code across different hypervisors.   We implemented NecoFuzz on Intel VT-x and AMD-V by extending AFL++ to support fuzz-harness VMs. NecoFuzz achieved 84.7% and 74.2% code coverage for nested virtualization-specific code on Intel VT-x and AMD-V, respectively, and uncovered six previously unknown vulnerabilities across three hypervisors, including two assigned CVEs.",
        "authors": [
            "Reima Ishii",
            "Takaaki Fukai",
            "Takahiro Shinagawa"
        ],
        "categories": [
            "cs.OS",
            "cs.CR"
        ],
        "id": "http://arxiv.org/abs/2512.08858v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-09"
    },
    "http://arxiv.org/abs/2512.06331v1": {
        "title": "Defending Event-Triggered Systems against Out-of-Envelope Environments",
        "link": "http://arxiv.org/abs/2512.06331v1",
        "abstract": "The design of real-time systems is based on assumptions about environmental conditions in which they will operate. We call this their safe operational envelope. Violation of these assumptions, i.e., out-of-envelope environments, can jeopardize timeliness and safety of real-time systems, e.g., by overwhelming them with interrupt storms. A long-lasting debate has been going on over which design paradigm, the time- or event-triggered, is more robust against such behavior. In this work, we investigate the claim that time-triggered systems are immune against out-of-envelope behavior and how event-triggered systems can be constructed to defend against being overwhelmed by interrupt showers. We introduce importance (independently of priority and criticality) as a means to express which tasks should still be scheduled in case environmental design assumptions cease to hold, draw parallels to mixed-criticality scheduling, and demonstrate how event-triggered systems can defend against out-of-envelope behavior.",
        "authors": [
            "Marcus Völp",
            "Mohammad Ibrahim Alkoudsi",
            "Azin Bayrami Asl",
            "Kristin Krüger",
            "Julio Rodrigues Mendonca da Neto",
            "Gerhard Fohler"
        ],
        "categories": [
            "cs.OS",
            "eess.SY"
        ],
        "id": "http://arxiv.org/abs/2512.06331v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-06"
    },
    "http://arxiv.org/abs/2512.05555v1": {
        "title": "Compiling Away the Overhead of Race Detection",
        "link": "http://arxiv.org/abs/2512.05555v1",
        "abstract": "Dynamic data race detectors are indispensable for flagging concurrency errors in software, but their high runtime overhead limits their adoption. This overhead stems primarily from pervasive instrumentation of memory accesses - a significant fraction of which is redundant. We addresses this inefficiency through a static, compiler-integrated approach that identifies and eliminates redundant instrumentation, drastically reducing the runtime cost of dynamic data race detectors. We introduce a suite of interprocedural static analyses reasoning about memory access patterns, synchronization, and thread creation to eliminate instrumentation for provably race-free accesses and show that the completeness properties of the data race detector are preserved. We further observe that many inserted checks flag a race if and only if a preceding check has already flagged an equivalent race for the same memory location - albeit potentially at a different access. We characterize this notion of equivalence and show that, when limiting reporting to at least one representative for each equivalence class, a further class of redundant checks can be eliminated. We identify such accesses using a novel dominance-based elimination analysis. Based on these two insights, we have implemented five static analyses within the LLVM, integrated with the instrumentation pass of the race detector ThreadSanitizer. Our experimental evaluation on a diverse suite of real-world applications demonstrates that our approach significantly reduces race detection overhead, achieving a geomean speedup of 1.34x, with peak speedups reaching 2.5x under high thread contention. This performance is achieved with a negligible increase in compilation time and, being fully automatic, places no additional burden on developers. Our optimizations have been accepted by the ThreadSanitizer maintainers and are in the process of being upstreamed.",
        "authors": [
            "Alexey Paznikov",
            "Andrey Kogutenko",
            "Yaroslav Osipov",
            "Michael Schwarz",
            "Umang Mathur"
        ],
        "categories": [
            "cs.PL",
            "cs.OS",
            "cs.SE"
        ],
        "id": "http://arxiv.org/abs/2512.05555v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-05"
    },
    "http://arxiv.org/abs/2512.03279v1": {
        "title": "Getting the MOST out of your Storage Hierarchy with Mirror-Optimized Storage Tiering",
        "link": "http://arxiv.org/abs/2512.03279v1",
        "abstract": "We present Mirror-Optimized Storage Tiering (MOST), a novel tiering-based approach optimized for modern storage hierarchies. The key idea of MOST is to combine the load balancing advantages of mirroring with the space-efficiency advantages of tiering. Specifically, MOST dynamically mirrors a small amount of hot data across storage tiers to efficiently balance load, avoiding costly migrations. As a result, MOST is as space-efficient as classic tiering while achieving better bandwidth utilization under I/O-intensive workloads. We implement MOST in Cerberus, a user-level storage management layer based on CacheLib. We show the efficacy of Cerberus through a comprehensive empirical study: across a range of static and dynamic workloads, Cerberus achieves better throughput than competing approaches on modern storage hierarchies especially under I/O-intensive and dynamic workloads.",
        "authors": [
            "Kaiwei Tu",
            "Kan Wu",
            "Andrea C. Arpaci-Dusseau",
            "Remzi H. Arpaci-Dusseau"
        ],
        "categories": [
            "cs.OS"
        ],
        "id": "http://arxiv.org/abs/2512.03279v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-02"
    },
    "http://arxiv.org/abs/2512.01594v2": {
        "title": "Confidential, Attestable, and Efficient Inter-CVM Communication with Arm CCA",
        "link": "http://arxiv.org/abs/2512.01594v2",
        "abstract": "Confidential Virtual Machines (CVMs) are increasingly adopted to protect sensitive workloads from privileged adversaries such as the hypervisor. While they provide strong isolation guarantees, existing CVM architectures lack first-class mechanisms for inter-CVM data sharing due to their disjoint memory model, making inter-CVM data exchange a performance bottleneck in compartmentalized or collaborative multi-CVM systems. Under this model, a CVM's accessible memory is either shared with the hypervisor or protected from both the hypervisor and all other CVMs. This design simplifies reasoning about memory ownership; however, it fundamentally precludes plaintext data sharing between CVMs because all inter-CVM communication must pass through hypervisor-accessible memory, requiring costly encryption and decryption to preserve confidentiality and integrity. In this paper, we introduce CAEC, a system that enables protected memory sharing between CVMs. CAEC builds on Arm Confidential Compute Architecture (CCA) and extends its firmware to support Confidential Shared Memory (CSM), a memory region securely shared between multiple CVMs while remaining inaccessible to the hypervisor and all non-participating CVMs. CAEC's design is fully compatible with CCA hardware and introduces only a modest increase (4%) in CCA firmware code size. CAEC delivers substantial performance benefits across a range of workloads. For instance, inter-CVM communication over CAEC achieves up to 209$\\times$ reduction in CPU cycles compared to encryption-based mechanisms over hypervisor-accessible shared memory. By combining high performance, strong isolation guarantees, and attestable sharing semantics, CAEC provides a practical and scalable foundation for the next generation of trusted multi-CVM services across both edge and cloud environments.",
        "authors": [
            "Sina Abdollahi",
            "Amir Al Sadi",
            "Marios Kogias",
            "David Kotz",
            "Hamed Haddadi"
        ],
        "categories": [
            "cs.CR",
            "cs.OS"
        ],
        "id": "http://arxiv.org/abs/2512.01594v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-01"
    },
    "http://arxiv.org/abs/2512.01381v1": {
        "title": "Accelerating Probabilistic Response-Time Analysis: Revised Critical Instant and Optimized Convolution",
        "link": "http://arxiv.org/abs/2512.01381v1",
        "abstract": "Accurate estimation of the Worst-Case Deadline Failure Probability (WCDFP) has attracted growing attention as a means to provide safety assurances in complex systems such as robotic platforms and autonomous vehicles. WCDFP quantifies the likelihood of deadline misses under the most pessimistic operating conditions, and safe estimation is essential for dependable real-time applications. However, achieving high accuracy in WCDFP estimation often incurs significant computational cost. Recent studies have revealed that the classical assumption of the critical instant, the activation pattern traditionally considered to trigger the worst-case behavior, can lead to underestimation of WCDFP in probabilistic settings. This observation motivates the use of a revised critical instant formulation that more faithfully captures the true worst-case scenario. This paper investigates convolution-based methods for WCDFP estimation under this revised setting and proposes an optimization technique that accelerates convolution by improving the merge order. Extensive experiments with diverse execution-time distributions demonstrate that the proposed optimized Aggregate Convolution reduces computation time by up to an order of magnitude compared to Sequential Convolution, while retaining accurate and safe-sided WCDFP estimates. These results highlight the potential of the approach to provide both efficiency and reliability in probabilistic timing analysis for safety-critical real-time applications.",
        "authors": [
            "Hiroto Takahashi",
            "Atsushi Yano",
            "Takuya Azumi"
        ],
        "categories": [
            "cs.OS",
            "cs.DS",
            "cs.RO"
        ],
        "id": "http://arxiv.org/abs/2512.01381v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-12-01"
    },
    "http://arxiv.org/abs/2512.00400v2": {
        "title": "TenonOS: A Self-Generating LibOS-on-LibOS Framework for Time-Critical Embedded Operating Systems",
        "link": "http://arxiv.org/abs/2512.00400v2",
        "abstract": "The growing complexity of embedded systems creates tension between rich functionality and strict resource and real-time constraints. Traditional monolithic operating system and hypervisor designs suffer from resource bloat and unpredictable scheduling, making them unsuitable for time-critical workloads where low latency and low jitter are essential. We propose TenonOS, a demand-driven, self-generating, lightweight operating system framework for time-critical embedded systems that rethinks both hypervisor and operating system architectures. TenonOS introduces a LibOS-on-LibOS model that decomposes hypervisor and operating system functionality into fine-grained, reusable micro-libraries. A generative orchestration engine dynamically composes these libraries to synthesize a customized runtime tailored to each application's criticality, timing requirements, and resource profile. TenonOS consists of two core components: Mortise, a minimalist micro-hypervisor, and Tenon, a real-time library operating system. Mortise provides lightweight isolation and removes the usual double-scheduler overhead in virtualized setups, while Tenon provides precise and deterministic task management. By generating only the necessary software stack per workload, TenonOS removes redundant layers, minimizes the trusted computing base, and maximizes responsiveness. Experiments show a 40.28 percent reduction in scheduling latency, an ultra-compact 361 KiB memory footprint, and strong adaptability.",
        "authors": [
            "Xinkui Zhao",
            "Yifan Zhang",
            "Haidan Zhao",
            "Hao Zhang",
            "Qingyu Ma",
            "Lufei Zhang",
            "Guanjie Cheng",
            "Shuiguang Deng",
            "Jianwei Yin",
            "Zuoning Chen"
        ],
        "categories": [
            "cs.OS",
            "eess.SY"
        ],
        "id": "http://arxiv.org/abs/2512.00400v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-29"
    },
    "http://arxiv.org/abs/2511.21235v1": {
        "title": "DynamicAdaptiveClimb: Adaptive Cache Replacement with Dynamic Resizing",
        "link": "http://arxiv.org/abs/2511.21235v1",
        "abstract": "Efficient cache management is critical for optimizing the system performance, and numerous caching mechanisms have been proposed, each exploring various insertion and eviction strategies. In this paper, we present AdaptiveClimb and its extension, DynamicAdaptiveClimb, two novel cache replacement policies that leverage lightweight, cache adaptation to outperform traditional approaches. Unlike classic Least Recently Used (LRU) and Incremental Rank Progress (CLIMB) policies, AdaptiveClimb dynamically adjusts the promotion distance (jump) of the cached objects based on recent hit and miss patterns, requiring only a single tunable parameter and no per-item statistics. This enables rapid adaptation to changing access distributions while maintaining low overhead. Building on this foundation, DynamicAdaptiveClimb further enhances adaptability by automatically tuning the cache size in response to workload demands. Our comprehensive evaluation across a diverse set of real-world traces, including 1067 traces from 6 different datasets, demonstrates that DynamicAdaptiveClimb consistently achieves substantial speedups and higher hit ratios compared to other state-of-the-art algorithms. In particular, our approach achieves up to a 29% improvement in hit ratio and a substantial reduction in miss penalties compared to the FIFO baseline. Furthermore, it outperforms the next-best contenders, AdaptiveClimb and SIEVE [43], by approximately 10% to 15%, especially in environments characterized by fluctuating working set sizes. These results highlight the effectiveness of our approach in delivering efficient performance, making it well-suited for modern, dynamic caching environments.",
        "authors": [
            "Daniel Berend",
            "Shlomi Dolev",
            "Sweta Kumari",
            "Dhruv Mishra",
            "Marina Kogan-Sadetsky",
            "Archit Somani"
        ],
        "categories": [
            "cs.OS",
            "eess.SY"
        ],
        "id": "http://arxiv.org/abs/2511.21235v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-26"
    },
    "http://arxiv.org/abs/2511.19991v1": {
        "id": "http://arxiv.org/abs/2511.19991v1",
        "title": "SARA: A Stall-Aware Memory Allocation Strategy for Mixed-Criticality Systems",
        "link": "http://arxiv.org/abs/2511.19991v1",
        "tags": [
            "edge",
            "offloading",
            "storage"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Proposes SARA, a stall-aware real-time memory allocator for mixed-criticality edge systems. It balances memory allocation between soft RT tasks and non-RT applications, minimizes soft RT memory usage by modeling latency effects, and mitigates stalls via proactive job dropping. Achieves 97.13% deadline hit ratio and up to 22.32x throughput improvement under memory constraints.",
        "abstract": "The memory capacity in edge devices is often limited due to constraints on cost, size, and power. Consequently, memory competition leads to inevitable page swapping in memory-constrained mixed-criticality edge devices, causing slow storage I/O and thus performance degradation. In such scenarios, inefficient memory allocation disrupts the balance between application performance, causing soft real-time (soft RT) tasks to miss deadlines or preventing non-real-time (non-RT) applications from optimizing throughput. Meanwhile, we observe unpredictable, long system-level stalls (called long stalls) under high memory and I/O pressure, which further degrade performance. In this work, we propose a Stall-Aware Real-Time Memory Allocator (SARA), which discovers opportunities for performance balance by allocating just enough memory to soft RT tasks to meet deadlines and, at the same time, optimizing the remaining memory for non-RT applications. To minimize the memory usage of soft RT tasks while meeting real-time requirements, SARA leverages our insight into how latency, caused by memory insufficiency and measured by our proposed PSI-based metric, affects the execution time of each soft RT job, where a job runs per period and a soft RT task consists of multiple periods. Moreover, SARA detects long stalls using our definition and proactively drops affected jobs, minimizing stalls in task execution. Experiments show that SARA achieves an average of 97.13% deadline hit ratio for soft RT tasks and improves non-RT application throughput by up to 22.32x over existing approaches, even with memory capacity limited to 60% of peak demand.",
        "authors": [
            "Meng-Chia Lee",
            "Wen Sheng Lim",
            "Yuan-Hao Chang",
            "Tei-Wei Kuo"
        ],
        "categories": [
            "cs.OS"
        ],
        "submit_date": "2025-11-25"
    },
    "http://arxiv.org/abs/2511.18323v1": {
        "id": "http://arxiv.org/abs/2511.18323v1",
        "title": "Crash-Consistent Checkpointing for AI Training on macOS/APFS",
        "link": "http://arxiv.org/abs/2511.18323v1",
        "tags": [
            "training",
            "storage"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Investigates crash-consistent checkpointing protocols for AI training on macOS/APFS. Implements three write modes with increasing durability and a SHA-256-based integrity guard for auto-rollback. Detects 99.8-100% corruptions with zero false positives at 56.5-570.6% overhead versus unsafe baseline.",
        "abstract": "Deep learning training relies on periodic checkpoints to recover from failures, but unsafe checkpoint installation can leave corrupted files on disk. This paper presents an experimental study of checkpoint installation protocols and integrity validation for AI training on macOS/APFS. We implement three write modes with increasing durability guarantees: unsafe (baseline, no fsync), atomic_nodirsync (file-level durability via fsync()), and atomic_dirsync (file + directory durability). We design a format-agnostic integrity guard using SHA-256 checksums with automatic rollback. Through controlled experiments including crash injection (430 unsafe-mode trials) and corruption injection (1,600 atomic-mode trials), we demonstrate that the integrity guard detects 99.8-100% of corruptions with zero false positives. Performance overhead is 56.5-108.4% for atomic_nodirsync and 84.2-570.6% for atomic_dirsync relative to the unsafe baseline. Our findings quantify the reliability-performance trade-offs and provide deployment guidance for production AI infrastructure.",
        "authors": [
            "Juha Jeon"
        ],
        "categories": [
            "cs.OS",
            "cs.LG"
        ],
        "submit_date": "2025-11-23"
    },
    "http://arxiv.org/abs/2511.18155v1": {
        "id": "http://arxiv.org/abs/2511.18155v1",
        "title": "eBPF-PATROL: Protective Agent for Threat Recognition and Overreach Limitation using eBPF in Containerized and Virtualized Environments",
        "link": "http://arxiv.org/abs/2511.18155v1",
        "tags": [
            "kernel",
            "edge",
            "storage"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Introduces eBPF-PATROL, a lightweight runtime security agent using eBPF to monitor system calls in containerized/virtualized environments. It enforces user-defined policies with context awareness, achieving <2.5% overhead and high detection accuracy against real-world attacks.",
        "abstract": "With the increasing use and adoption of cloud and cloud-native computing, the underlying technologies (i.e., containerization and virtualization) have become foundational. However, strict isolation and maintaining runtime security in these environments has become increasingly challenging. Existing approaches like seccomp and Mandatory Access Control (MAC) frameworks offer some protection up to a limit, but often lack context awareness, syscall argument filtering, and adaptive enforcement, providing the ability to adjust decisions at runtime based on observed application behavior, workload changes, or detected anomalies rather than relying solely on static or predefined rules.This paper introduces eBPF-PATROL (eBPF-Protective Agent for Threat Recognition and Overreach Limitation), an extensible lightweight runtime security agent that uses extended Berkeley Packet Filter (eBPF) technology to monitor and enforce policies in containerized and virtualized environments. By intercepting system calls, analyzing execution context, and applying user-defined rules, eBPF-PATROL detects and prevents real-time boundary violations, such as reverse shells, privilege escalation, and container escape attempts. We describe the architecture, implementation, and evaluation of eBPF-PATROL, demonstrating its low overhead (< 2.5 percent) and high detection accuracy across real-world attack scenarios.",
        "authors": [
            "Sangam Ghimire",
            "Nirjal Bhurtel",
            "Roshan Sahani",
            "Sudan Jha"
        ],
        "categories": [
            "cs.CR",
            "cs.OS"
        ],
        "submit_date": "2025-11-22"
    },
    "http://arxiv.org/abs/2511.13251v1": {
        "title": "Sharpe-Driven Stock Selection and Liquidiy-Constrained Portfolio Optimization: Evidence from the Chinese Equity Market",
        "link": "http://arxiv.org/abs/2511.13251v1",
        "abstract": "This paper develops and empirically evaluates a Sharpe-driven stock selection and liquidity-constrained portfolio optimization framework designed for the Chinese equity market. The proposed methodology integrates three sequential stages: Sharpe-ratio-based universe selection, liquidity-adjusted mean-variance optimization, and multi-layered risk management implemented within an automated trading bot. Using daily price and volume data from 2023 to 2025 across the A-share universe, the framework dynamically identifies stocks exhibiting strong risk-adjusted performance while accounting for trading frictions and liquidity asymmetries that are common in emerging markets. Empirical backtests reveal that the proposed strategy achieves an annualized return of 25 percent, a Sharpe ratio of 1.71, and a maximum drawdown of 8.2 percent. These results significantly outperform the Buy-and-Hold benchmark, which records an annualized return of 21 percent, a Sharpe ratio of 1.62, and a drawdown of 7.6 percent over the same period. The superior performance demonstrates that incorporating risk-adjusted selection and liquidity-aware constraints enhances both profitability and stability, enabling the portfolio to capture upside potential while maintaining drawdown resilience. Beyond its empirical success, this study contributes methodologically by bridging classical mean-variance theory with practical liquidity adjustments and dynamic Sharpe-based screening. The resulting system not only improves the tradability of optimized portfolios but also provides a scalable and adaptive framework for quantitative asset allocation in liquidity-sensitive markets, offering new evidence that disciplined risk-return optimization can outperform passive investment strategies in the post-2023 Chinese equity landscape.",
        "authors": [
            "Thanh Nguyen"
        ],
        "categories": [
            "cs.OS"
        ],
        "id": "http://arxiv.org/abs/2511.13251v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-17"
    },
    "http://arxiv.org/abs/2511.13239v2": {
        "title": "Talyxion: From Speculation to Optimization in Risk Managed Crypto Portfolio Allocation",
        "link": "http://arxiv.org/abs/2511.13239v2",
        "abstract": "Cryptocurrency trading has attracted tremendous attention from both retail and institutional investors. However, most traders fail to scale their assets under management due to fragile strategies that collapse during adverse markets. The primary causes are oversized leverage, speculative position sizing, and the absence of robust risk management or hedging mechanisms. This paper introduces Talyxion, an end to end framework for crypto portfolio allocation that shifts the paradigm from speculation to optimization. The proposed pipeline consists of four stages: universe selection, alpha backtesting, volatility aware portfolio optimization, and dynamic drawdown based risk management. By combining operations research techniques with practical risk controls, Talyxion enables scalable crypto portfolios that can withstand market downturns. In live 30 day trading on Binance Futures, the framework achieved a return on investment (ROI) of +16.68%, with the Sharpe ratio reaching 5.72 and the maximum drawdown contained at just 4.56%, demonstrating strong downside risk control. The system executed 227 trades, of which 131 were profitable, resulting in a win rate of 57.71% and a PnL of +1,137.49 USDT. Importantly, these results outperformed the buy and hold baseline (Sharpe 1.79, ROI 4.36%, MDD 4.96%) as well as several top leader copy trading bots on Binance, highlighting both the competitiveness and scalability of Talyxion in real world trading environments.",
        "authors": [
            "Thanh Nguyen"
        ],
        "categories": [
            "cs.OS"
        ],
        "id": "http://arxiv.org/abs/2511.13239v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-17"
    },
    "http://arxiv.org/abs/2511.09961v1": {
        "title": "Vmem: A Lightweight Hot-Upgradable Memory Management for In-production Cloud Environment",
        "link": "http://arxiv.org/abs/2511.09961v1",
        "abstract": "Traditional memory management suffers from metadata overhead, architectural complexity, and stability degradation, problems intensified in cloud environments. Existing software/hardware optimizations are insufficient for cloud computing's dual demands of flexibility and low overhead. This paper presents Vmem, a memory management architecture for in-production cloud environments that enables flexible, efficient cloud server memory utilization through lightweight reserved memory management. Vmem is the first such architecture to support online upgrades, meeting cloud requirements for high stability and rapid iterative evolution. Experiments show Vmem increases sellable memory rate by about 2%, delivers extreme elasticity and performance, achieves over 3x faster boot time for VFIO-based virtual machines (VMs), and improves network performance by about 10% for DPU-accelerated VMs. Vmem has been deployed at large scale for seven years, demonstrating efficiency and stability on over 300,000 cloud servers supporting hundreds of millions of VMs.",
        "authors": [
            "Hao Zheng",
            "Qiang Wang",
            "Longxiang Wang",
            "Xishi Qiu",
            "Yibin Shen",
            "Xiaoshe Dong",
            "Naixuan Guan",
            "Jia Wei",
            "Fudong Qiu",
            "Xingjun Zhang",
            "Yun Xu",
            "Mao Zhao",
            "Yisheng Xie",
            "Shenglong Zhao",
            "Min He",
            "Yu Li",
            "Xiao Zheng",
            "Ben Luo",
            "Jiesheng Wu"
        ],
        "categories": [
            "cs.OS"
        ],
        "id": "http://arxiv.org/abs/2511.09961v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-13"
    },
    "http://arxiv.org/abs/2511.09936v2": {
        "id": "http://arxiv.org/abs/2511.09936v2",
        "title": "Taiji: A DPU Memory Elasticity Solution for In-production Cloud Environments",
        "link": "http://arxiv.org/abs/2511.09936v2",
        "tags": [
            "hardware",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-11-13",
        "tldr": "Proposes Taiji, a DPU memory elasticity solution using hybrid virtualization and parallel memory swapping to enable memory overcommitment. Transparent to applications, it expands DPU memory by over 50% with 5% virtualization overhead and 10μs swap-in latency for 90% of cases.",
        "abstract": "The growth of cloud computing drives data centers toward higher density and efficiency. Data processing units (DPUs) enhance server network and storage performance but face challenges such as long hardware upgrade cycles and limited resources. To address these, we propose Taiji, a resource-elasticity architecture for DPUs. Combining hybrid virtualization with parallel memory swapping, Taiji switches the DPU's operating system (OS) into a guest OS and inserts a lightweight virtualization layer, making nearly all DPU memory swappable. It achieves memory overcommitment for the switched guest OS via high-performance memory elasticity, fully transparent to upper-layer applications, and supports hot-switch and hot-upgrade to meet in-production cloud requirements. Experiments show that Taiji expands DPU memory resources by over 50%, maintains virtualization overhead around 5%, and ensures 90% of swap-ins complete within 10 microseconds. Taiji delivers an efficient, reliable, low-overhead elasticity solution for DPUs and is deployed in large-scale production systems across more than 30,000 servers.",
        "authors": [
            "Hao Zheng",
            "Longxiang Wang",
            "Yun Xu",
            "Qiang Wang",
            "Yibin Shen",
            "Xiaoshe Dong",
            "Bang Di",
            "Jia Wei",
            "Shenyu Dong",
            "Xingjun Zhang",
            "Weichen Chen",
            "Zhao Han",
            "Sanqian Zhao",
            "Dongdong Huang",
            "Jie Qi",
            "Yifan Yang",
            "Zhao Gao",
            "Yi Wang",
            "Jinhu Li",
            "Xudong Ren",
            "Min He",
            "Hang Yang",
            "Xiao Zheng",
            "Haijiao Hao",
            "Jiesheng Wu"
        ],
        "categories": [
            "cs.OS"
        ],
        "submit_date": "2025-11-13"
    },
    "http://arxiv.org/abs/2511.08297v1": {
        "title": "Work-in-Progress: Function-as-Subtask API Replacing Publish/Subscribe for OS-Native DAG Scheduling",
        "link": "http://arxiv.org/abs/2511.08297v1",
        "abstract": "The Directed Acyclic Graph (DAG) task model for real-time scheduling finds its primary practical target in Robot Operating System 2 (ROS 2). However, ROS 2's publish/subscribe API leaves DAG precedence constraints unenforced: a callback may publish mid-execution, and multi-input callbacks let developers choose topic-matching policies. Thus preserving DAG semantics relies on conventions; once violated, the model collapses. We propose the Function-as-Subtask (FasS) API, which expresses each subtask as a function whose arguments/return values are the subtask's incoming/outgoing edges. By minimizing description freedom, DAG semantics is guaranteed at the API rather than by programmer discipline. We implement a DAG-native scheduler using FasS on a Rust-based experimental kernel and evaluate its semantic fidelity, and we outline design guidelines for applying FasS to Linux Linux sched_ext.",
        "authors": [
            "Takahiro Ishikawa-Aso",
            "Atsushi Yano",
            "Yutaro Kobayashi",
            "Takumi Jin",
            "Yuuki Takano",
            "Shinpei Kato"
        ],
        "categories": [
            "cs.OS",
            "cs.RO"
        ],
        "id": "http://arxiv.org/abs/2511.08297v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-11"
    },
    "http://arxiv.org/abs/2512.00035v1": {
        "title": "WebAssembly on Resource-Constrained IoT Devices: Performance, Efficiency, and Portability",
        "link": "http://arxiv.org/abs/2512.00035v1",
        "abstract": "The increasing heterogeneity of hardware and software in the Internet of Things (IoT) poses a major challenge for the portability, maintainability and deployment of software on devices with limited resources. WebAssembly (WASM), originally designed for the web, is increasingly recognized as a portable, secure and efficient runtime environment that can overcome these challenges. This paper explores the feasibility of using WASM in embedded IoT systems by evaluating its performance, memory footprint and energy consumption on three representative microcontrollers: the Raspberry Pi Pico, the ESP32 C6 and the nRF5340. Two lightweight WASM runtimes, WAMR and wasm3, are compared with the native C execution. The results show that while the native execution remains superior in terms of speed and energy efficiency, WASM offers acceptable trade-offs in return for cross-platform compatibility and sandbox execution. The results highlight that WASM is a viable option for embedded IoT applications when portability and security outweigh strict performance constraints, and that further runtime optimization could extend its practicality in this area.",
        "authors": [
            "Mislav Has",
            "Tao Xiong",
            "Fehmi Ben Abdesslem",
            "Mario Kušek"
        ],
        "categories": [
            "cs.AR",
            "cs.OS"
        ],
        "id": "http://arxiv.org/abs/2512.00035v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-11"
    },
    "http://arxiv.org/abs/2511.07035v1": {
        "id": "http://arxiv.org/abs/2511.07035v1",
        "title": "GoCkpt: Gradient-Assisted Multi-Step overlapped Checkpointing for Efficient LLM Training",
        "link": "http://arxiv.org/abs/2511.07035v1",
        "tags": [
            "training",
            "offloading",
            "storage"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Addresses checkpoint saving overhead in LLM training by overlapping multi-step transfers with gradient data. Proposes GoCkpt for partial checkpoint updates with GPU-CPU optimization. Achieves 38.4% higher throughput and 86.7% less interruption time versus traditional methods.",
        "abstract": "The accuracy of large language models (LLMs) improves with increasing model size, but increasing model complexity also poses significant challenges to training stability. Periodic checkpointing is a key mechanism for fault recovery and is widely used in LLM training. However, traditional checkpointing strategies often pause or delay GPU computation during checkpoint saving for checkpoint GPU-CPU transfer, resulting in significant training interruptions and reduced training throughput. To address this issue, we propose GoCkpt, a method to overlap checkpoint saving with multiple training steps and restore the final checkpoint on the CPU. We transfer the checkpoint across multiple steps, each step transfers part of the checkpoint state, and we transfer some of the gradient data used for parameter updates. After the transfer is complete, each partial checkpoint state is updated to a consistent version on the CPU, thus avoiding the checkpoint state inconsistency problem caused by transferring checkpoints across multiple steps. Furthermore, we introduce a transfer optimization strategy to maximize GPU-CPU bandwidth utilization and SSD persistence throughput. This dual optimization overlapping saves across steps and maximizing I/O efficiency significantly reduces invalid training time. Experimental results show that GoCkpt can increase training throughput by up to 38.4% compared to traditional asynchronous checkpoint solutions in the industry. We also find that GoCkpt can reduce training interruption time by 86.7% compared to the state-of-the-art checkpoint transfer methods, which results in a 4.8% throughput improvement.",
        "authors": [
            "Keyao Zhang",
            "Yiquan Chen",
            "Zhuo Hu",
            "Wenhai Lin",
            "Jiexiong Xu",
            "Wenzhi Chen"
        ],
        "categories": [
            "cs.OS"
        ],
        "submit_date": "2025-11-10"
    },
    "http://arxiv.org/abs/2511.06736v1": {
        "title": "Preemption-Enhanced Benchmark Suite for FPGAs",
        "link": "http://arxiv.org/abs/2511.06736v1",
        "abstract": "Field-Programmable Gate Arrays (FPGAs) have become essential in cloud computing due to their reconfigurability, energy efficiency, and ability to accelerate domain-specific workloads. As FPGA adoption grows, research into task scheduling and preemption techniques has intensified. However, the field lacks a standardized benchmarking framework for consistent and reproducible evaluation. Many existing studies propose innovative scheduling or preemption mechanisms but often rely on proprietary or synthetic benchmarks, limiting generalizability and making comparison difficult. This methodical fragmentation hinders effective evaluation of scheduling strategies and preemption in multi-tenant FPGA environments.   This paper presents the first open-source preemption-enabled benchmark suite for evaluating FPGA preemption strategies and testing new scheduling algorithms, without requiring users to create preemption workloads from scratch. The suite includes 27 diverse applications spanning cryptography, AI/ML, computation-intensive workloads, communication systems, and multimedia processing. Each benchmark integrates comprehensive context-saving and restoration mechanisms, facilitating reproducible research and consistent comparisons. Our suite not only simplifies testing FPGA scheduling policies but also benefits OS research by enabling the evaluation of scheduling fairness, resource allocation efficiency, and context-switching performance in multi-tenant FPGA systems, ultimately supporting the development of better operating systems and scheduling policies for FPGA-based environments. We also provide guidelines for adding new benchmarks, enabling future research to expand and refine FPGA preemption and scheduling evaluation.",
        "authors": [
            "Arsalan Ali Malik",
            "John Buchanan",
            "Aydin Aysu"
        ],
        "categories": [
            "cs.AR",
            "cs.OS"
        ],
        "id": "http://arxiv.org/abs/2511.06736v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-10"
    },
    "http://arxiv.org/abs/2511.06460v1": {
        "id": "http://arxiv.org/abs/2511.06460v1",
        "title": "Guidelines for Building Indexes on Partially Cache-Coherent CXL Shared Memory",
        "link": "http://arxiv.org/abs/2511.06460v1",
        "tags": [
            "hardware",
            "storage",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Investigates efficient indexing on Partial Cache-Coherence (PCC) platforms like CXL. Proposes SP and P$^3$ guidelines to ensure correctness and mitigate overhead. Achieves up to 16× throughput gain versus non-optimized designs.",
        "abstract": "The \\emph{Partial Cache-Coherence (PCC)} model maintains hardware cache coherence only within subsets of cores, enabling large-scale memory sharing with emerging memory interconnect technologies like Compute Express Link (CXL). However, PCC's relaxation of global cache coherence compromises the correctness of existing single-machine software.   This paper focuses on building consistent and efficient indexes on PCC platforms. We present that existing indexes designed for cache-coherent platforms can be made consistent on PCC platforms following SP guidelines, i.e., we identify \\emph{sync-data} and \\emph{protected-data} according to the index's concurrency control mechanisms, and synchronize them accordingly. However, conversion with SP guidelines introduces performance overhead. To mitigate the overhead, we identify several unique performance bottlenecks on PCC platforms, and propose P$^3$ guidelines (i.e., using Out-of-\\underline{P}lace update, Re\\underline{P}licated shared variable, S\\underline{P}eculative Reading) to improve the efficiency of converted indexes on PCC platforms.   With SP and P$^3$ guidelines, we convert and optimize two indexes (CLevelHash and BwTree) for PCC platforms. Evaluation shows that converted indexes' throughput improves up to 16$\\times$ following P$^3$ guidelines, and the optimized indexes outperform their message-passing-based and disaggregated-memory-based counterparts by up to 16$\\times$ and 19$\\times$.",
        "authors": [
            "Fangnuo Wu",
            "Mingkai Dong",
            "Wenjun Cai",
            "Jingsheng Yan",
            "Haibo Chen"
        ],
        "categories": [
            "cs.OS"
        ],
        "submit_date": "2025-11-09"
    },
    "http://arxiv.org/abs/2511.02230v2": {
        "id": "http://arxiv.org/abs/2511.02230v2",
        "title": "Continuum: Efficient and Robust Multi-Turn LLM Agent Scheduling with KV Cache Time-to-Live",
        "link": "http://arxiv.org/abs/2511.02230v2",
        "tags": [
            "serving",
            "offloading",
            "agentic"
        ],
        "relevant": true,
        "indexed_date": "2025-11-04",
        "tldr": "Proposes Continuum, a KV cache management system for multi-turn LLM agents using time-to-live mechanisms to selectively retain cache during tool calls. Optimizes by balancing re-computation costs and GPU load. Achieves reduced job completion times for real workloads (e.g., in SWE-Bench) with scalability across turn counts.",
        "abstract": "KV cache management is essential for efficient LLM inference. To maximize utilization, existing inference engines evict finished requests' KV cache if new requests are waiting. This policy breaks for agentic workloads, which interleave LLM calls with tools, introducing pauses that prevent effective KV reuse across turns. Since some tool calls have much shorter durations than human response multi-turn chatbot, it would be promising to retain the KV cache in during these tools. However, there are many challenges. First, we need to consider both the potential cost of recomputation or reloading (if CPU offloading enabled) and the increasing queueing delays after eviction from GPU. Second, due to the internal variance of tool call durations, we need the method to remain robust under limited predictability of tool call durations.   We present Continuum, a serving system to optimize job completion time for multi-turn agent workloads by introducing time-to-live mechanism for KV cache retaining. For LLM request that generates a tool call, Continuum selectively pins the KV cache in GPU memory with a time-to-live value determined by considering both the reload cost and ordering preserve benefit of retaining KV cache. Moreover, when the TTL expires, the KV cache can be automatically evicted to free up GPU memory, providing robust performance under edge cases. When combined with program-level first-come-first-serve, Continuum preserves multi-turn continuity, and reduces delay for complex agentic workflows. Our evaluation on real-world agentic workloads (SWE-Bench and BFCL) with Llama-3.1 8B/70B shows that Continuum significantly improves the average job completion times and its improvement scales with turn number increase. We release a preview version at: https://github.com/Hanchenli/vllm-continuum",
        "authors": [
            "Hanchen Li",
            "Qiuyang Mang",
            "Runyuan He",
            "Qizheng Zhang",
            "Huanzhi Mao",
            "Xiaokun Chen",
            "Hangrui Zhou",
            "Alvin Cheung",
            "Joseph Gonzalez",
            "Ion Stoica"
        ],
        "categories": [
            "cs.OS",
            "cs.AI",
            "cs.NI"
        ],
        "submit_date": "2025-11-04"
    },
    "http://arxiv.org/abs/2511.00363v1": {
        "title": "Fast Networks for High-Performance Distributed Trust",
        "link": "http://arxiv.org/abs/2511.00363v1",
        "abstract": "Organizations increasingly need to collaborate by performing a computation on their combined dataset, while keeping their data hidden from each other. Certain kinds of collaboration, such as collaborative data analytics and AI, require a level of performance beyond what current cryptographic techniques for distributed trust can provide. This is because the organizations run software in different trust domains, which can require them to communicate over WANs or the public Internet. In this paper, we explore how to instead run such applications using fast datacenter-type LANs. We show that, by carefully redesigning distributed trust frameworks for LANs, we can achieve up to order-of-magnitude better performance than naïvely using a LAN. Then, we develop deployment models for Distributed But Proximate Trust (DBPT) that allow parties to use a LAN while remaining physically and logically distinct. These developments make secure collaborative data analytics and AI significantly more practical and set new research directions for developing systems and cryptographic theory for high-performance distributed trust.",
        "authors": [
            "Yicheng Liu",
            "Rafail Ostrovsky",
            "Scott Shenker",
            "Sam Kumar"
        ],
        "categories": [
            "cs.CR",
            "cs.NI",
            "cs.OS"
        ],
        "id": "http://arxiv.org/abs/2511.00363v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-11-01"
    },
    "http://arxiv.org/abs/2511.00140v1": {
        "title": "Supply Chain Exploitation of Secure ROS 2 Systems: A Proof-of-Concept on Autonomous Platform Compromise via Keystore Exfiltration",
        "link": "http://arxiv.org/abs/2511.00140v1",
        "abstract": "This paper presents a proof-of-concept supply chain attack against the Secure ROS 2 (SROS 2) framework, demonstrated on a Quanser QCar2 autonomous vehicle platform. A Trojan-infected Debian package modifies core ROS 2 security commands to exfiltrate newly generated keystore credentials via DNS in base64-encoded chunks to an attacker-controlled nameserver. Possession of these credentials enables the attacker to rejoin the SROS 2 network as an authenticated participant and publish spoofed control or perception messages without triggering authentication failures. We evaluate this capability on a secure ROS 2 Humble testbed configured for a four-stop-sign navigation routine using an Intel RealSense camera for perception. Experimental results show that control-topic injections can cause forced braking, sustained high-speed acceleration, and continuous turning loops, while perception-topic spoofing can induce phantom stop signs or suppress real detections. The attack generalizes to any data distribution service (DDS)-based robotic system using SROS 2, highlighting the need for both supply chain integrity controls and runtime semantic validation to safeguard autonomous systems against insider and impersonation threats.",
        "authors": [
            "Tahmid Hasan Sakib",
            "Yago Romano Martinez",
            "Carter Brady",
            "Syed Rafay Hasan",
            "Terry N. Guo"
        ],
        "categories": [
            "cs.CR",
            "cs.OS",
            "cs.RO",
            "eess.SY"
        ],
        "id": "http://arxiv.org/abs/2511.00140v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-31"
    },
    "http://arxiv.org/abs/2510.27485v1": {
        "title": "Sockeye: a language for analyzing hardware documentation",
        "link": "http://arxiv.org/abs/2510.27485v1",
        "abstract": "Systems programmers have to consolidate the ever growing hardware mess present on modern System-on-Chips (SoCs). Correctly programming a multitude of components, providing functionality but also security, is a difficult problem: semantics of individual units are described in English prose, descriptions are often underspecified, and prone to inaccuracies. Rigorous statements about platform security are often impossible.   We introduce a domain-specific language to describe hardware semantics, assumptions about software behavior, and desired security properties. We then create machine-readable specifications for a diverse set of eight SoCs from their reference manuals, and formally prove their (in-)security. In addition to security proofs about memory confidentiality and integrity, we discover a handful of documentation errors. Finally, our analysis also revealed a vulnerability on a real-world server chip. Our tooling offers system integrators a way of formally describing security properties for entire SoCs, and means to prove them or find counterexamples to them.",
        "authors": [
            "Ben Fiedler",
            "Samuel Gruetter",
            "Timothy Roscoe"
        ],
        "categories": [
            "cs.CR",
            "cs.OS",
            "cs.PL"
        ],
        "id": "http://arxiv.org/abs/2510.27485v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-31"
    },
    "http://arxiv.org/abs/2510.23895v1": {
        "title": "Modeling and Scheduling of Fusion Patterns in Autonomous Driving Systems (Extended Version)",
        "link": "http://arxiv.org/abs/2510.23895v1",
        "abstract": "In Autonomous Driving Systems (ADS), Directed Acyclic Graphs (DAGs) are widely used to model complex data dependencies and inter-task communication. However, existing DAG scheduling approaches oversimplify data fusion tasks by assuming fixed triggering mechanisms, failing to capture the diverse fusion patterns found in real-world ADS software stacks. In this paper, we propose a systematic framework for analyzing various fusion patterns and their performance implications in ADS. Our framework models three distinct fusion task types: timer-triggered, wait-for-all, and immediate fusion, which comprehensively represent real-world fusion behaviors. Our Integer Linear Programming (ILP)-based approach enables an optimization of multiple real-time performance metrics, including reaction time, time disparity, age of information, and response time, while generating deterministic offline schedules directly applicable to real platforms. Evaluation using real-world ADS case studies, Raspberry Pi implementation, and randomly generated DAGs demonstrates that our framework handles diverse fusion patterns beyond the scope of existing work, and achieves substantial performance improvements in comparable scenarios.",
        "authors": [
            "Hoora Sobhani",
            "Hyoseung Kim"
        ],
        "categories": [
            "eess.SY",
            "cs.OS",
            "cs.RO"
        ],
        "id": "http://arxiv.org/abs/2510.23895v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-27"
    },
    "http://arxiv.org/abs/2510.22869v1": {
        "id": "http://arxiv.org/abs/2510.22869v1",
        "title": "Jenga: Responsive Tiered Memory Management without Thrashing",
        "link": "http://arxiv.org/abs/2510.22869v1",
        "tags": [
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Addresses tiered memory performance issues due to hot/cold object mixing and hotness fluctuation. Proposes Jenga with context-based page allocation and stable hotness measurement for timely migration without thrashing. Runs applications 28% faster with fast tier at working set size.",
        "abstract": "A heterogeneous memory has a single address space with fast access to some addresses (a fast tier of DRAM) and slow access to other addresses (a capacity tier of CXL-attached memory or NVM). A tiered memory system aims to maximize the number of accesses to the fast tier via page migrations between the fast and capacity tiers. Unfortunately, previous tiered memory systems can perform poorly due to (1) allocating hot and cold objects in the same page and (2) abrupt changes in hotness measurements that lead to thrashing.   This paper presents Jenga, a tiered memory system that addresses both problems. Jenga's memory allocator uses a novel context-based page allocation strategy. Jenga's accurate measurements of page hotness enable it to react to memory access behavior changes in a timely manner while avoiding thrashing. Compared to the best previous tiered memory system, Jenga runs memory-intensive applications 28% faster across 10 applications, when the fast tier capacity matches the working set size, at a CPU overhead of <3% of a single core and a memory overhead of <0.3%",
        "authors": [
            "Rohan Kadekodi",
            "Haoran Peng",
            "Gilbert Bernstein",
            "Michael D. Ernst",
            "Baris Kasikci"
        ],
        "categories": [
            "cs.ET",
            "cs.OS"
        ],
        "submit_date": "2025-10-26"
    },
    "http://arxiv.org/abs/2510.19765v1": {
        "title": "Tidying Up the Address Space",
        "link": "http://arxiv.org/abs/2510.19765v1",
        "abstract": "Memory tiering in datacenters does not achieve its full potential due to hotness fragmentation -- the intermingling of hot and cold objects within memory pages. This fragmentation prevents page-based reclamation systems from distinguishing truly hot pages from pages containing mostly cold objects, fundamentally limiting memory efficiency despite highly skewed accesses. We introduce address-space engineering: dynamically reorganizing application virtual address spaces to create uniformly hot and cold regions that any page-level tiering backend can manage effectively. HADES demonstrates this frontend/backend approach through a compiler-runtime system that tracks and migrates objects based on access patterns, requiring minimal developer intervention. Evaluations across ten data structures achieve up to 70% memory reduction with 3% performance overhead, showing that address space engineering enables existing reclamation systems to reclaim memory aggressively without performance degradation.",
        "authors": [
            "Vinay Banakar",
            "Suli Yang",
            "Kan Wu",
            "Andrea C. Arpaci-Dusseau",
            "Remzi H. Arpaci-Dusseau",
            "Kimberly Keeton"
        ],
        "categories": [
            "cs.OS",
            "cs.PF",
            "cs.PL"
        ],
        "id": "http://arxiv.org/abs/2510.19765v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-22"
    },
    "http://arxiv.org/abs/2510.18496v2": {
        "id": "http://arxiv.org/abs/2510.18496v2",
        "title": "LatticeHashForest: An Efficient Data Structure for Repetitive Data and Operations",
        "link": "http://arxiv.org/abs/2510.18496v2",
        "tags": [
            "storage",
            "kernel",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Presents LatticeHashForest (LHF), a data structure for efficient deduplication and computation in repetitive operations, such as compiler optimizations. LHF reduces memory usage and speeds up operations by enabling immediate deduplication and nested construction. In pointer analysis, it achieves over 4x speedup and negligible memory for 10 million inputs.",
        "abstract": "Analysis of entire programs as a single unit, or whole-program analysis, involves propagation of large amounts of information through the control flow of the program. This is especially true for pointer analysis, where, unless significant compromises are made in the precision of the analysis, there is a combinatorial blowup of information. One of the key problems we observed in our own efforts to this end is that a lot of duplicate data was being propagated, and many low-level data structure operations were repeated a large number of times.   We present what we consider to be a novel and generic data structure, LatticeHashForest (LHF), to store and operate on such data in a manner that eliminates a majority of redundant computations and duplicate data in scenarios similar to those encountered in compilers and program optimization. LHF differs from similar work in this vein, such as hash-consing, ZDDs, and BDDs, by not only providing a way to efficiently operate on large, aggregate structures, but also modifying the elements of such structures in a manner that they can be deduplicated immediately. LHF also provides a way to perform a nested construction of elements such that they can be deduplicated at multiple levels, cutting down the need for additional, nested computations.   We provide a detailed structural description, along with an abstract model of this data structure. An entire C++ implementation of LHF is provided as an artifact along with evaluations of LHF using examples and benchmark programs. We also supply API documentation and a user manual for users to make independent applications of LHF. Our main use case in the realm of pointer analysis shows memory usage reduction to an almost negligible fraction, and speedups beyond 4x for input sizes approaching 10 million when compared to other implementations.",
        "authors": [
            "Anamitra Ghorui",
            "Uday P. Khedker"
        ],
        "categories": [
            "cs.DS",
            "cs.IT",
            "cs.OS",
            "cs.PL"
        ],
        "submit_date": "2025-10-21"
    },
    "http://arxiv.org/abs/2510.15263v1": {
        "title": "Maratona Linux a tale of upgrading from Ubuntu 20.04 to 22.04",
        "link": "http://arxiv.org/abs/2510.15263v1",
        "abstract": "Maratona Linux is the development environment used since 2016 on the ``Maratona de Programação'', ICPC's South American regional contest. It consists of Debian packages that modify a standard Ubuntu installation in order to make it suitable for the competition, installing IDEs, documentation, compilers, debuggers, interpreters, and enforcing network restrictions. The project, which began based on Ubuntu 16.04, has been successfully migrated from 20.04 to 22.04, the current Long-term Support (LTS) version. The project has also been improved by adding static analyzers, updating the package dependency map, splitting large packages, and enhancing the packaging pipeline.",
        "authors": [
            "Davi Antônio da Silva Santos",
            "Bruno César Ribas"
        ],
        "categories": [
            "cs.OS"
        ],
        "id": "http://arxiv.org/abs/2510.15263v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-17"
    },
    "http://arxiv.org/abs/2510.11310v1": {
        "title": "Detection of Performance Changes in MooBench Results Using Nyrkiö on GitHub Actions",
        "link": "http://arxiv.org/abs/2510.11310v1",
        "abstract": "In GitHub with its 518 million hosted projects, performance changes within these projects are highly relevant to the project's users. Although performance measurement is supported by GitHub CI/CD, performance change detection is a challenging topic.   In this paper, we demonstrate how we incorporated Nyrkiö to MooBench. Prior to this work, Moobench continuously ran on GitHub virtual machines, measuring overhead of tracing agents, but without change detection. By adding the upload of the measurements to the Nyrkiö change detection service, we made it possible to detect performance changes. We identified one major performance regression and examined the performance change in depth. We report that (1) it is reproducible with GitHub actions, and (2) the performance regression is caused by a Linux Kernel version change.",
        "authors": [
            "Shinhyung Yang",
            "David Georg Reichelt",
            "Henrik Ingo",
            "Wilhelm Hasselbring"
        ],
        "categories": [
            "cs.SE",
            "cs.OS",
            "cs.PF"
        ],
        "id": "http://arxiv.org/abs/2510.11310v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-13"
    },
    "http://arxiv.org/abs/2510.08479v2": {
        "title": "Rethinking Provenance Completeness with a Learning-Based Linux Scheduler",
        "link": "http://arxiv.org/abs/2510.08479v2",
        "abstract": "Provenance plays a critical role in maintaining traceability of a system's actions for root cause analysis of security threats and impacts. Provenance collection is often incorporated into the reference monitor of systems to ensure that an audit trail exists of all events, that events are completely captured, and that logging of such events cannot be bypassed. However, recent research has questioned whether existing state-of-the-art provenance collection systems fail to ensure the security guarantees of a true reference monitor due to the 'super producer threat' in which provenance generation can overload a system to force the system to drop security-relevant events and allow an attacker to hide their actions. One approach towards solving this threat is to enforce resource isolation, but that does not fully solve the problems resulting from hardware dependencies and performance limitations.   In this paper, we show how an operating system's kernel scheduler can mitigate this threat, and we introduce Aegis, a learned scheduler for Linux specifically designed for provenance. Unlike conventional schedulers that ignore provenance completeness requirements, Aegis leverages reinforcement learning to learn provenance task behavior and to dynamically optimize resource allocation. We evaluate Aegis's efficacy and show that Aegis significantly improves both the completeness and efficiency of provenance collection systems compared to traditional scheduling, while maintaining reasonable overheads and even improving overall runtime in certain cases compared to the default Linux scheduler.",
        "authors": [
            "Jinsong Mao",
            "Benjamin E. Ujcich",
            "Shiqing Ma"
        ],
        "categories": [
            "cs.CR",
            "cs.OS"
        ],
        "id": "http://arxiv.org/abs/2510.08479v2",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-09"
    },
    "http://arxiv.org/abs/2510.04607v1": {
        "title": "A Case for Declarative LLM-friendly Interfaces for Improved Efficiency of Computer-Use Agents",
        "link": "http://arxiv.org/abs/2510.04607v1",
        "abstract": "Computer-use agents (CUAs) powered by large language models (LLMs) have emerged as a promising approach to automating computer tasks, yet they struggle with graphical user interfaces (GUIs). GUIs, designed for humans, force LLMs to decompose high-level goals into lengthy, error-prone sequences of fine-grained actions, resulting in low success rates and an excessive number of LLM calls.   We propose Goal-Oriented Interface (GOI), a novel abstraction that transforms existing GUIs into three declarative primitives: access, state, and observation, which are better suited for LLMs. Our key idea is policy-mechanism separation: LLMs focus on high-level semantic planning (policy) while GOI handles low-level navigation and interaction (mechanism). GOI does not require modifying the application source code or relying on application programming interfaces (APIs).   We evaluate GOI with Microsoft Office Suite (Word, PowerPoint, Excel) on Windows. Compared to a leading GUI-based agent baseline, GOI improves task success rates by 67% and reduces interaction steps by 43.5%. Notably, GOI completes over 61% of successful tasks with a single LLM call.",
        "authors": [
            "Yuan Wang",
            "Mingyu Li",
            "Haibo Chen"
        ],
        "categories": [
            "cs.OS",
            "cs.AI",
            "cs.LG"
        ],
        "id": "http://arxiv.org/abs/2510.04607v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-10-06"
    },
    "http://arxiv.org/abs/2510.04360v1": {
        "id": "http://arxiv.org/abs/2510.04360v1",
        "title": "An Early Exploration of Deep-Learning-Driven Prefetching for Far Memory",
        "link": "http://arxiv.org/abs/2510.04360v1",
        "tags": [
            "storage",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Explores deep-learning-driven prefetching for far-memory systems to reduce on-demand data fetches. Proposes Memix, co-designing deep learning with system architecture to predict memory accesses via application semantics and runtime context. Achieves up to 42% fewer far-memory accesses than state-of-the-art.",
        "abstract": "Far-memory systems, where applications store less-active data in more energy-efficient memory media, are increasingly adopted by data centers. However, applications are bottlenecked by on-demand data fetching from far- to local-memory. We present Memix, a far-memory system that embodies a deep-learning-system co-design for efficient and accurate prefetching, minimizing on-demand far-memory accesses. One key observation is that memory accesses are shaped by both application semantics and runtime context, providing an opportunity to optimize each independently. Preliminary evaluation of Memix on data-intensive workloads shows that it outperforms the state-of-the-art far-memory system by up to 42%.",
        "authors": [
            "Yutong Huang",
            "Zhiyuan Guo",
            "Yiying Zhang"
        ],
        "categories": [
            "cs.OS"
        ],
        "submit_date": "2025-10-05"
    },
    "http://arxiv.org/abs/2509.25015v1": {
        "title": "Joyride: Rethinking Linux's network stack design for better performance, security, and reliability",
        "link": "http://arxiv.org/abs/2509.25015v1",
        "abstract": "Contemporary distributed computing workloads, including scientific computation, data mining, and machine learning, increasingly demand OS networking with minimal latency as well as high throughput, security, and reliability. However, Linux's conventional TCP/IP stack becomes increasingly problematic for high-end NICs, particularly those operating at 100 Gbps and beyond.   These limitations come mainly from overheads associated with kernel space processing, mode switching, and data copying in the legacy architecture. Although kernel bypass techniques such as DPDK and RDMA offer alternatives, they introduce significant adoption barriers: both often require extensive application redesign, and RDMA is not universally available on commodity hardware.   This paper proposes Joyride, a high performance framework with a grand vision of replacing Linux's legacy network stack while providing compatibility with existing applications. Joyride aims to integrate kernel bypass ideas, specifically DPDK and a user-space TCP/IP stack, while designing a microkernel-style architecture for Linux networking.",
        "authors": [
            "Yanlin Du",
            "Ruslan Nikolaev"
        ],
        "categories": [
            "cs.OS"
        ],
        "id": "http://arxiv.org/abs/2509.25015v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-09-29"
    },
    "http://arxiv.org/abs/2509.23693v1": {
        "title": "ASIC-based Compression Accelerators for Storage Systems: Design, Placement, and Profiling Insights",
        "link": "http://arxiv.org/abs/2509.23693v1",
        "abstract": "Lossless compression imposes significant computational over head on datacenters when performed on CPUs. Hardware compression and decompression processing units (CDPUs) can alleviate this overhead, but optimal algorithm selection, microarchitectural design, and system-level placement of CDPUs are still not well understood. We present the design of an ASIC-based in-storage CDPU and provide a comprehensive end-to-end evaluation against two leading ASIC accelerators, Intel QAT 8970 and QAT 4xxx. The evaluation spans three dominant CDPU placement regimes: peripheral, on-chip, and in-storage. Our results reveal: (i) acute sensitivity of throughput and latency to CDPU placement and interconnection, (ii) strong correlation between compression efficiency and data patterns/layouts, (iii) placement-driven divergences between microbenchmark gains and real-application speedups, (iv) discrepancies between module and system-level power efficiency, and (v) scalability and multi-tenant interference is sues of various CDPUs. These findings motivate a placement-aware, cross-layer rethinking of hardware (de)compression for hyperscale storage infrastructures.",
        "authors": [
            "Tao Lu",
            "Jiapin Wang",
            "Yelin Shan",
            "Xiangping Zhang",
            "Xiang Chen"
        ],
        "categories": [
            "cs.AR",
            "cs.OS"
        ],
        "id": "http://arxiv.org/abs/2509.23693v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-09-28"
    },
    "http://arxiv.org/abs/2509.22256v2": {
        "id": "http://arxiv.org/abs/2509.22256v2",
        "title": "Secure and Efficient Access Control for Computer-Use Agents via Context Space",
        "link": "http://arxiv.org/abs/2509.22256v2",
        "tags": [
            "RL",
            "agentic",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Addresses security risks in LLM-based computer-use agents by developing CSAgent, a system-level access control framework with intent- and context-aware policies enforced via an optimized OS service. Achieves 99.36% attack defense rate with 6.83% overhead.",
        "abstract": "Large language model (LLM)-based computer-use agents represent a convergence of AI and OS capabilities, enabling natural language to control system- and application-level functions. However, due to LLMs' inherent uncertainty issues, granting agents control over computers poses significant security risks. When agent actions deviate from user intentions, they can cause irreversible consequences. Existing mitigation approaches, such as user confirmation and LLM-based dynamic action validation, still suffer from limitations in usability, security, and performance. To address these challenges, we propose CSAgent, a system-level, static policy-based access control framework for computer-use agents. To bridge the gap between static policy and dynamic context and user intent, CSAgent introduces intent- and context-aware policies, and provides an automated toolchain to assist developers in constructing and refining them. CSAgent enforces these policies through an optimized OS service, ensuring that agent actions can only be executed under specific user intents and contexts. CSAgent supports protecting agents that control computers through diverse interfaces, including API, CLI, and GUI. We implement and evaluate CSAgent, which successfully defends against more than 99.36% of attacks while introducing only 6.83% performance overhead.",
        "authors": [
            "Haochen Gong",
            "Chenxiao Li",
            "Rui Chang",
            "Wenbo Shen"
        ],
        "categories": [
            "cs.CR",
            "cs.AI",
            "cs.OS"
        ],
        "submit_date": "2025-09-26"
    },
    "http://arxiv.org/abs/2509.21550v1": {
        "title": "A Target-Agnostic Protocol-Independent Interface for the Transport Layer",
        "link": "http://arxiv.org/abs/2509.21550v1",
        "abstract": "Transport protocols are fundamental to network communications, continuously evolving to meet the demands of new applications, workloads, and network architectures while running in a wide range of execution environments (a.k.a targets). We argue that this diversity across protocols and targets calls for a high-level, target-agnostic programming abstraction for the transport layer. Specifically, we propose to specify transport protocols as high-level programs that take an event and flow state as input, and using constrained C-like constructs, produce the updated state along with target-agnostic instructions for key transport operations such as data reassembly, packet generation and scheduling, and timer manipulations.   We show the benefits of our high-level transport programs by developing multiple transport protocols in our programming framework called TINF, developing two TINF- compliant backends, one in DPDK and one in Linux eXpress DataPath, and deploying TINF programs for multiple protocols across both backends. Inspired by the benefits unlocked by L2/L3 packet-processing languages like P4, we believe target-agnostic transport programs can reduce the development effort for transport protocols, enable automated analysis and formal verification of the transport layer, and further research in programmable targets for transport protocols.",
        "authors": [
            "Pedro Mizuno",
            "Kimiya Mohammadtaheri",
            "Linfan Qian",
            "Joshua Johnson",
            "Danny Akbarzadeh",
            "Chris Neely",
            "Mario Baldi",
            "Nacihket Kapre",
            "Mina Tahmasbi Arashloo"
        ],
        "categories": [
            "cs.NI",
            "cs.OS",
            "cs.PL"
        ],
        "id": "http://arxiv.org/abs/2509.21550v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-09-25"
    },
    "http://arxiv.org/abs/2509.21301v1": {
        "id": "http://arxiv.org/abs/2509.21301v1",
        "title": "Nova: Real-Time Agentic Vision-Language Model Serving with Adaptive Cross-Stage Parallelization",
        "link": "http://arxiv.org/abs/2509.21301v1",
        "tags": [
            "agentic",
            "serving",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-09-16",
        "tldr": "Nova enables efficient real-time serving of agentic vision-language models by adaptively partitioning GPU resources across vision, prefill, and decode stages, plus lightweight vision encoder offloading. It achieves up to 23.3% lower max latency while maintaining high throughput.",
        "abstract": "This paper presents Nova, a real-time scheduling framework for serving agentic vision-language models (VLMs) on a single GPU with balanced per-request latency and overall request process throughput. Our design begins by enabling effective pipelining across vision encode, LLM prefill, and LLM decode stages of VLMs, by exploiting their heterogeneous resource demands during execution and incorporating elastic GPU spatial partitioning among stages to maximally utilize the compute and memory resources. Building on this, we introduce a real-time scheduling algorithm that adaptively calibrates resource allocation among stages based on a Pareto-optimal analysis of the latency-throughput trade-off, allowing the system to sustain responsiveness and resource efficiency under dynamic request loads. To further alleviate GPU memory pressure, we design a lightweight weight offloading strategy for vision encoders that preserves inference efficiency with minimized memory overhead. Extensive evaluations on both synthetic and real-world agent workloads demonstrate that Nova consistently outperforms the state-of-the-art baselines, improving the maximum latency by up to 23.3%, while keeping competitive throughput.",
        "authors": [
            "Yuhang Xu",
            "Shengzhong Liu",
            "Dong Zhang",
            "Bingheng Yan",
            "Fan Wu",
            "Guihai Chen"
        ],
        "categories": [
            "cs.OS"
        ],
        "submit_date": "2025-09-25"
    },
    "http://arxiv.org/abs/2510.02323v1": {
        "title": "NetCAS: Dynamic Cache and Backend Device Management in Networked Environments",
        "link": "http://arxiv.org/abs/2510.02323v1",
        "abstract": "Modern storage systems often combine fast cache with slower backend devices to accelerate I/O. As performance gaps narrow, concurrently accessing both devices, rather than relying solely on cache hits, can improve throughput. However, in data centers, remote backend storage accessed over networks suffers from unpredictable contention, complicating this split. We present NetCAS, a framework that dynamically splits I/O between cache and backend devices based on real-time network feedback and a precomputed Perf Profile. Unlike traditional hit-rate-based policies, NetCAS adapts split ratios to workload configuration and networking performance. NetCAS employs a low-overhead batched round-robin scheduler to enforce splits, avoiding per-request costs. It achieves up to 174% higher performance than traditional caching in remote storage environments and outperforms converging schemes like Orthus by up to 3.5X under fluctuating network conditions.",
        "authors": [
            "Joon Yong Hwang",
            "Chanseo Park",
            "Ikjun Yeom",
            "Younghoon Kim"
        ],
        "categories": [
            "cs.OS",
            "cs.NI",
            "cs.PF"
        ],
        "id": "http://arxiv.org/abs/2510.02323v1",
        "relevant": false,
        "indexed_date": "2025-12-23",
        "tags": [],
        "submit_date": "2025-09-25"
    },
    "http://arxiv.org/abs/2511.13724v1": {
        "id": "http://arxiv.org/abs/2511.13724v1",
        "title": "Preparation Meets Opportunity: Enhancing Data Preprocessing for ML Training With Seneca",
        "link": "http://arxiv.org/abs/2511.13724v1",
        "tags": [
            "training",
            "storage"
        ],
        "relevant": true,
        "indexed_date": "2025-12-23",
        "tldr": "Addresses input data preprocessing bottlenecks in concurrent ML training. Proposes Seneca, a data loading system featuring cache partitioning for encoded/decoded/augmented data and opportunistic cached data sampling. Reduces makespan by 45.23% and increases throughput by 3.45x over baselines.",
        "abstract": "Input data preprocessing is a common bottleneck when concurrently training multimedia machine learning (ML) models in modern systems. To alleviate these bottlenecks and reduce the training time for concurrent jobs, we present Seneca, a data loading system that optimizes cache partitioning and data sampling for the data storage and ingestion (DSI) pipeline. The design of Seneca contains two key techniques. First, Seneca uses a performance model for the data pipeline to optimally partition the cache for three different forms of data (encoded, decoded, and augmented). Second, Seneca opportunistically serves cached data over uncached ones during random batch sampling so that concurrent jobs benefit from each other. We implement Seneca by modifying PyTorch and demonstrate its effectiveness by comparing it against several state-of-the-art caching systems for DNN training. Seneca reduces the makespan by 45.23% compared to PyTorch and increases data processing throughput by up to 3.45x compared to the next best dataloader.",
        "authors": [
            "Omkar Desai",
            "Ziyang Jiao",
            "Shuyi Pei",
            "Janki Bhimani",
            "Bryan S. Kim"
        ],
        "categories": [
            "cs.OS",
            "cs.AI",
            "cs.LG"
        ],
        "submit_date": "2025-09-24"
    },
    "http://arxiv.org/abs/2509.18869v1": {
        "id": "http://arxiv.org/abs/2509.18869v1",
        "title": "On The Reproducibility Limitations of RAG Systems",
        "link": "http://arxiv.org/abs/2509.18869v1",
        "tags": [
            "RAG"
        ],
        "relevant": true,
        "indexed_date": "2025-09-16",
        "tldr": "Addresses the reproducibility limitations of RAG systems by introducing ReproRAG, a benchmarking framework that quantifies non-determinism across embedding models, retrieval algorithms, and hardware. Evaluates trade-offs using metrics like Exact Match Rate and Jaccard Similarity.",
        "abstract": "Retrieval-Augmented Generation (RAG) is increasingly employed in generative AI-driven scientific workflows to integrate rapidly evolving scientific knowledge bases, yet its reliability is frequently compromised by non-determinism in their retrieval components. This paper introduces ReproRAG, a comprehensive benchmarking framework designed to systematically measure and quantify the reproducibility of vector-based retrieval systems. ReproRAG investigates sources of uncertainty across the entire pipeline, including different embedding models, precision, retrieval algorithms, hardware configurations, and distributed execution environments. Utilizing a suite of metrics, such as Exact Match Rate, Jaccard Similarity, and Kendall's Tau, the proposed framework effectively characterizes the trade-offs between reproducibility and performance. Our large-scale empirical study reveals critical insights; for instance, we observe that different embedding models have remarkable impact on RAG reproducibility. The open-sourced ReproRAG framework provides researchers and engineers productive tools to validate deployments, benchmark reproducibility, and make informed design decisions, thereby fostering more trustworthy AI for science.",
        "authors": [
            "Baiqiang Wang",
            "Dongfang Zhao",
            "Nathan R Tallent",
            "Luanzheng Guo"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-23"
    },
    "http://arxiv.org/abs/2509.17863v1": {
        "id": "http://arxiv.org/abs/2509.17863v1",
        "title": "Expert-as-a-Service: Towards Efficient, Scalable, and Robust Large-scale MoE Serving",
        "link": "http://arxiv.org/abs/2509.17863v1",
        "tags": [
            "MoE",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-09-16",
        "tldr": "Addresses efficient serving of large-scale MoE models by disaggregating experts into stateless services. Uses peer-to-peer communication for low-overhead routing and dynamic resource scaling. Achieves 37.5% resource savings with <2% throughput loss under failures.",
        "abstract": "Mixture-of-Experts (MoE) models challenge serving infrastructures with dynamic, sparse expert utilization, causing instability on conventional systems designed for dense architectures. We propose EaaS, a novel serving system to enable efficient, scalable, and robust MoE deployment. Our system disaggregates MoE modules into independent, stateless services. This design enables fine-grained resource scaling and provides inherent fault tolerance by decoupling compute units. The architecture is powered by a high-performance, CPU-free peer-to-peer communication library that ensures minimal overhead and high throughput. Experiments confirm EaaS's scalability and efficiency, achieving performance comparable to monolithic systems while providing robust fault tolerance and strong scalability. EaaS incurs less than a 2% throughput reduction under simulated hardware failures that would otherwise halt monolithic architectures. It further saves up to 37.5% of computing resources through dynamic fine-grained adaptation to serving traffic, demonstrating strong resilience for large-scale MoE deployment in production.",
        "authors": [
            "Ziming Liu",
            "Boyu Tian",
            "Guoteng Wang",
            "Zhen Jiang",
            "Peng Sun",
            "Zhenhua Han",
            "Tian Tang",
            "Xiaohe Hu",
            "Yanmin Jia",
            "Yan Zhang",
            "He Liu",
            "Mingjun Zhang",
            "Yiqi Zhang",
            "Qiaoling Chen",
            "Shenggan Cheng",
            "Mingyu Gao",
            "Yang You",
            "Siyuan Feng"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-22"
    },
    "http://arxiv.org/abs/2509.17542v2": {
        "id": "http://arxiv.org/abs/2509.17542v2",
        "title": "Disaggregated Prefill and Decoding Inference System for Large Language Model Serving on Multi-Vendor GPUs",
        "link": "http://arxiv.org/abs/2509.17542v2",
        "tags": [
            "serving",
            "hardware"
        ],
        "relevant": true,
        "indexed_date": "2025-09-16",
        "tldr": "Designs a disaggregated LLM inference system using heterogeneous GPUs to separate prefill and decoding stages, enabling cost-efficient deployment. Introduces a heterogeneous-compatible transmission module and joint optimization for parallelism and instance allocation. Achieves 38% higher resource utilization compared to homogeneous setups.",
        "abstract": "LLM-based applications have been widely used in various industries, but with the increasing of models size, an efficient large language model (LLM) inference system is an urgent problem to be solved for service providers. Since the inference system is divided into two stage with different characteristics: Prefill and Decode, the two stage will interfere with each other during the inference process. Toward this end, a P-D disaggregated inference framework is proposed by some researchers. Current research is done on homogeneous GPUs, and lacks deployment solutions based on business scenarios. Compared with homogeneous GPUs, using heterogeneous GPUs to construct inference systems can better improve resource utilization and reduce costs. Even if GPUs from different vendors are used to build inference systems, on the basis of reducing costs, the resource utilization rate can be improved and the dependence on a single vendor can be reduced. Therefore, a P-D disaggreagetd inference system based on heterogeneous GPUs is designed, and the heterogeneous compatible transmission module in the system is designed to address heterogeneous GPU data compatibility issues. Then, a joint optimization algorithm of parallel strategy and instance number allocation is proposed to obtain the deployment solutions. Finally, the experimental results show that the P-D disaggregated inference system can well solve the hybrid inference problem of heterogeneous GPUs from different vendors, and the joint optimization algorithm can obtain the optimal deployment solution.",
        "authors": [
            "Xing Chen",
            "Rong Shi",
            "Lu Zhao",
            "Lingbin Wang",
            "Xiao Jin",
            "Yueqiang Chen",
            "Hongfeng Sun"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-22"
    },
    "http://arxiv.org/abs/2509.17360v1": {
        "id": "http://arxiv.org/abs/2509.17360v1",
        "title": "Asteria: Semantic-Aware Cross-Region Caching for Agentic LLM Tool Access",
        "link": "http://arxiv.org/abs/2509.17360v1",
        "tags": [
            "agentic",
            "serving",
            "RAG"
        ],
        "relevant": true,
        "indexed_date": "2025-09-16",
        "tldr": "Asteria improves agentic LLM performance by introducing semantic-aware cross-region caching for tool access. It uses semantic embeddings and a lightweight LLM judger for precise retrieval, achieving 3.6× higher throughput with 85%+ cache hit rates while maintaining accuracy.",
        "abstract": "Large Language Model (LLM) agents tackle data-intensive tasks such as deep research and code generation. However, their effectiveness depends on frequent interactions with knowledge sources across remote clouds or regions. Such interactions can create non-trivial latency and cost bottlenecks. Existing caching solutions focus on exact-match queries, limiting their effectiveness for semantic knowledge reuse.   To address this challenge, we introduce Asteria, a novel cross-region knowledge caching architecture for LLM agents. At its core are two abstractions: Semantic Element (SE) and Semantic Retrieval Index (Sine). A semantic element captures the semantic embedding representation of an LLM query together with performance-aware metadata such as latency, cost, and staticity. Sine then provides two-stage retrieval: a vector similar index with semantic embedding for fast candidate selection and a lightweight LLM-powered semantic judger for precise validation. Atop these primitives, Asteria builds a new cache interface that includes a new semantic-aware cache hit definition, a cost-efficient eviction policy, and proactive prefetching. To reduce overhead, Asteria co-locates the small LLM judger with the main LLM using adaptive scheduling and resource sharing. Our evaluation demonstrates that Asteria delivers substantial performance improvements without compromising correctness. On representative search workloads, Asteria achieves up to a 3.6$\\times$ increase in throughput by maintaining cache hit rates of over 85%, while preserving accuracy virtually identical to non-cached baselines. Asteria also improves throughput for complex coding tasks by 20%, showcasing its versatility across diverse agentic workloads.",
        "authors": [
            "Chaoyi Ruan",
            "Chao Bi",
            "Kaiwen Zheng",
            "Ziji Shi",
            "Xinyi Wan",
            "Jialin Li"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-22"
    },
    "http://arxiv.org/abs/2509.17357v1": {
        "id": "http://arxiv.org/abs/2509.17357v1",
        "title": "Cronus: Efficient LLM inference on Heterogeneous GPU Clusters via Partially Disaggregated Prefill",
        "link": "http://arxiv.org/abs/2509.17357v1",
        "tags": [
            "serving",
            "heterogeneous"
        ],
        "relevant": true,
        "indexed_date": "2025-09-16",
        "tldr": "Cronus improves LLM inference throughput on heterogeneous GPU clusters by partially disaggregating prefill across low- and high-end GPUs, overlapping stages to balance load. It reduces P99 TTFT and TBT by up to 40% while maintaining high throughput.",
        "abstract": "Efficient LLM inference is critical for real-world applications, especially within heterogeneous GPU clusters commonly found in organizations and on-premise datacenters as GPU architecture rapidly evolves. Current disaggregated prefill strategies, which separate the prefill and decode stages of LLM inference across different GPUs, often suffer from suboptimal performance due to imbalances between GPU capabilities and workload demands. On the other hand, extending conventional data parallelism and pipeline parallelism to heterogeneous setups incurs high inference latencies. To address these challenges, we introduce Cronus, a novel LLM inference system designed to dynamically balance workloads across heterogeneous GPUs using partially disaggregated prefill. Cronus partitions each prefill stage and executes its initial portion on the low-end GPU, while overlapping the remaining prefill and decode stages of earlier requests on the high-end GPU. Extensive evaluations across various high-end and low-end GPU combinations demonstrate that Cronus significantly improves the throughput over disaggregated prefill. It also reduces TTFT P99 and TBT P99 significantly over DP and PP while maintaining similar or better throughput.",
        "authors": [
            "Yunzhao Liu",
            "Qiang Xu",
            "Y. Charlie Hu"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-22"
    },
    "http://arxiv.org/abs/2509.16995v1": {
        "id": "http://arxiv.org/abs/2509.16995v1",
        "title": "MoA-Off: Adaptive Heterogeneous Modality-Aware Offloading with Edge-Cloud Collaboration for Efficient Multimodal LLM Inference",
        "link": "http://arxiv.org/abs/2509.16995v1",
        "tags": [
            "multi-modal",
            "offloading",
            "edge"
        ],
        "relevant": true,
        "indexed_date": "2025-09-16",
        "tldr": "How to efficiently infer multimodal LLMs on edge devices? MoA-Off proposes a modality-aware offloading framework that dynamically splits computation between edge and cloud based on input complexity, reducing latency by 30% and resource overhead by 30%-65% while preserving accuracy.",
        "abstract": "Multimodal large language models (MLLMs) enable powerful cross-modal inference but impose significant computational and latency burdens, posing severe challenges for deployment in resource-constrained environments. In this paper, we propose MoA-Off, an adaptive heterogeneous modality-aware offloading framework with edge-cloud collaboration for efficient MLLM inference. MoA-Off introduces a lightweight heterogeneous modality-aware module that estimates the complexity of heterogeneous inputs through multi-dimensional feature analysis. Then, an adaptive edge-cloud collaborative offloading strategy is proposed that dynamically schedules workloads between edge and cloud based on modality-aware complexity scores and real-time system states. The experimental results demonstrate that MoA-Off can achieve over 30% reduction in latency and 30%-65% decrease in resource overhead while maintaining competitive accuracy compared to traditional approaches.",
        "authors": [
            "Zheming Yang",
            "Qi Guo",
            "Yunqing Hu",
            "Chang Zhao",
            "Chang Zhang",
            "Jian Zhao",
            "Wen Ji"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-21"
    },
    "http://arxiv.org/abs/2509.16857v1": {
        "id": "http://arxiv.org/abs/2509.16857v1",
        "title": "ShadowServe: Interference-Free KV Cache Fetching for Distributed Prefix Caching",
        "link": "http://arxiv.org/abs/2509.16857v1",
        "tags": [
            "serving",
            "offloading",
            "hardware"
        ],
        "relevant": true,
        "indexed_date": "2025-09-16",
        "tldr": "Addresses KV cache fetch interference in distributed prefix caching for LLM serving. Proposes ShadowServe, a SmartNIC-offloaded system with chunked pipelining and minimal-copy memory management. Achieves up to 2.2x lower TPOT and 1.38x lower TTFT in low-bandwidth scenarios.",
        "abstract": "Distributed prefix caching accelerates long-context LLM serving by reusing KV cache entries for common context prefixes. However, KV cache fetches can become a bottleneck when network bandwidth is limited. Compression mitigates the bandwidth issue, but can degrade overall performance when decompression interferes with model computation.   We present ShadowServe, the first SmartNIC-accelerated, interference-free prefix caching system for LLM serving. ShadowServe separates a control plane on the host and a data plane fully offloaded to the SmartNIC, which eliminates interference to both host GPU and CPU. To overcome the SmartNIC's limited compute and memory resources, we design a chunked pipeline that parallelizes data plane operations across the SmartNIC's compute resources, and a minimal-copy memory management scheme that reduces memory pressure on the SmartNIC. Compared to state-of-the-art solutions, ShadowServe achieves up to 2.2x lower loaded time-per-output-token (TPOT), and reduces time-to-first-token (TTFT) by up to 1.38x in low-bandwidth scenarios (<= 20 Gbps), translating to up to 1.35x higher throughput.",
        "authors": [
            "Xingyu Xiang",
            "Raj Joshi",
            "Yuhan Liu",
            "Jiayi Yao",
            "Chenxingyu Zhao",
            "Junchen Jiang",
            "Yang Zhou",
            "Eddie Kohler",
            "Minlan Yu"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.LG"
        ],
        "submit_date": "2025-09-21"
    },
    "http://arxiv.org/abs/2509.16495v1": {
        "id": "http://arxiv.org/abs/2509.16495v1",
        "title": "Shift Parallelism: Low-Latency, High-Throughput LLM Inference for Dynamic Workloads",
        "link": "http://arxiv.org/abs/2509.16495v1",
        "tags": [
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-09-16",
        "tldr": "Addresses the latency-throughput tradeoff in LLM serving by introducing Shift Parallelism, which dynamically switches between tensor and sequence parallelism to leverage KV cache invariance. Achieves 1.51× lower latency in interactive workloads and 50% higher throughput in batch workloads than tensor parallelism alone.",
        "abstract": "Efficient parallelism is necessary for achieving low-latency, high-throughput inference with large language models (LLMs). Tensor parallelism (TP) is the state-of-the-art method for reducing LLM response latency, however GPU communications reduces combined token throughput. On the other hand, data parallelism (DP) obtains a higher throughput yet is slow in response latency. Best of both worlds does not exist, and it is not possible to combine TP and DP because of the KV cache variance across the parallelisms.   We notice Sequence Parallelism (SP - Ulysses in training) has similar properties as DP but with KV cache invariance. We adapt SP to inference, and combine it with TP to get the best of both worlds. Our solution: Shift Parallelism.   Shift Parallelism dynamically switches across TP and SP, and minimizes latency in low traffic without losing throughput in high traffic. The efficient GPU communications of Shift Parallelism yields up to i) 1.51x faster response in interactive workloads and ii) 50% higher throughput in batch workloads, compared to a TP-only solution.   We evaluate Shift Parallelism with real-world production traces with dynamic traffic patterns as well as synthetic benchmarking patterns across models, context sizes, and arrival rates. All results affirm the same: Shift Parallelism has a better the latency vs. throughput tradeoff than TP or DP, and hence obtains low latency without degrading throughput in dynamic workloads.",
        "authors": [
            "Mert Hidayetoglu",
            "Aurick Qiao",
            "Michael Wyatt",
            "Jeff Rasley",
            "Yuxiong He",
            "Samyam Rajbhandari"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-20"
    },
    "http://arxiv.org/abs/2509.16293v4": {
        "id": "http://arxiv.org/abs/2509.16293v4",
        "title": "Robust LLM Training Infrastructure at ByteDance",
        "link": "http://arxiv.org/abs/2509.16293v4",
        "tags": [
            "training",
            "RL"
        ],
        "relevant": true,
        "indexed_date": "2025-09-16",
        "tldr": "Addresses training instability in large-scale LLM jobs. Proposes ByteRobust, an infrastructure system for fault detection, demarcation, and recovery using data-driven methods tailored to LLM training parallelism. Achieves 97% ETTR (Engineer Troubleshoot and Repair) rate on 9,600 GPUs training job.",
        "abstract": "The training scale of large language models (LLMs) has reached tens of thousands of GPUs and is still continuously expanding, enabling faster learning of larger models. Accompanying the expansion of the resource scale is the prevalence of failures (CUDA error, NaN values, job hang, etc.), which poses significant challenges to training stability. Any large-scale LLM training infrastructure should strive for minimal training interruption, efficient fault diagnosis, and effective failure tolerance to enable highly efficient continuous training. This paper presents ByteRobust, a large-scale GPU infrastructure management system tailored for robust and stable training of LLMs. It exploits the uniqueness of LLM training process and gives top priorities to detecting and recovering failures in a routine manner. Leveraging parallelisms and characteristics of LLM training, ByteRobust enables high-capacity fault tolerance, prompt fault demarcation, and localization with an effective data-driven approach, comprehensively ensuring continuous and efficient training of LLM tasks. ByteRobust is deployed on a production GPU platform and achieves 97% ETTR for a three-month training job on 9,600 GPUs.",
        "authors": [
            "Borui Wan",
            "Gaohong Liu",
            "Zuquan Song",
            "Jun Wang",
            "Yun Zhang",
            "Guangming Sheng",
            "Shuguang Wang",
            "Houmin Wei",
            "Chenyuan Wang",
            "Weiqiang Lou",
            "Xi Yang",
            "Mofan Zhang",
            "Kaihua Jiang",
            "Cheng Ren",
            "Xiaoyun Zhi",
            "Menghan Yu",
            "Zhe Nan",
            "Zhuolin Zheng",
            "Baoquan Zhong",
            "Qinlong Wang",
            "Huan Yu",
            "Jinxin Chi",
            "Wang Zhang",
            "Yuhan Li",
            "Zixian Du",
            "Sida Zhao",
            "Yongqiang Zhang",
            "Jingzhe Tang",
            "Zherui Liu",
            "Chuan Wu",
            "Yanghua Peng",
            "Haibin Lin",
            "Wencong Xiao",
            "Xin Liu",
            "Liang Xiang"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC"
        ],
        "submit_date": "2025-09-19"
    },
    "http://arxiv.org/abs/2509.15965v1": {
        "id": "http://arxiv.org/abs/2509.15965v1",
        "title": "RLinf: Flexible and Efficient Large-scale Reinforcement Learning via Macro-to-Micro Flow Transformation",
        "link": "http://arxiv.org/abs/2509.15965v1",
        "tags": [
            "RL",
            "training"
        ],
        "relevant": true,
        "indexed_date": "2025-09-16",
        "tldr": "Addresses low hardware utilization in RL training by introducing M2Flow, a macro-to-micro flow transformation that optimizes workflow execution via context switching and elastic pipelining. Achieves 1.1x–2.13x speedup in end-to-end training throughput.",
        "abstract": "Reinforcement learning (RL) has demonstrated immense potential in advancing artificial general intelligence, agentic intelligence, and embodied intelligence. However, the inherent heterogeneity and dynamicity of RL workflows often lead to low hardware utilization and slow training on existing systems. In this paper, we present RLinf, a high-performance RL training system based on our key observation that the major roadblock to efficient RL training lies in system flexibility. To maximize flexibility and efficiency, RLinf is built atop a novel RL system design paradigm called macro-to-micro flow transformation (M2Flow), which automatically breaks down high-level, easy-to-compose RL workflows at both the temporal and spatial dimensions, and recomposes them into optimized execution flows. Supported by RLinf worker's adaptive communication capability, we devise context switching and elastic pipelining to realize M2Flow transformation, and a profiling-guided scheduling policy to generate optimal execution plans. Extensive evaluations on both reasoning RL and embodied RL tasks demonstrate that RLinf consistently outperforms state-of-the-art systems, achieving 1.1x-2.13x speedup in end-to-end training throughput.",
        "authors": [
            "Chao Yu",
            "Yuanqing Wang",
            "Zhen Guo",
            "Hao Lin",
            "Si Xu",
            "Hongzhi Zang",
            "Quanlu Zhang",
            "Yongji Wu",
            "Chunyang Zhu",
            "Junhao Hu",
            "Zixiao Huang",
            "Mingjie Wei",
            "Yuqing Xie",
            "Ke Yang",
            "Bo Dai",
            "Zhexuan Xu",
            "Xiangyuan Wang",
            "Xu Fu",
            "Zhihao Liu",
            "Kang Chen",
            "Weilin Liu",
            "Gang Liu",
            "Boxun Li",
            "Jianlei Yang",
            "Zhi Yang",
            "Guohao Dai",
            "Yu Wang"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC"
        ],
        "submit_date": "2025-09-19"
    },
    "http://arxiv.org/abs/2509.15940v1": {
        "id": "http://arxiv.org/abs/2509.15940v1",
        "title": "Efficient Pre-Training of LLMs via Topology-Aware Communication Alignment on More Than 9600 GPUs",
        "link": "http://arxiv.org/abs/2509.15940v1",
        "tags": [
            "networking",
            "training"
        ],
        "relevant": true,
        "indexed_date": "2025-09-16",
        "tldr": "Addresses communication inefficiencies in large-scale LLM training by aligning communication patterns with data center topology. Proposes Arnold, a scheduling system that reduces communication spread and improves end-to-end training throughput by 10.6% on 9600+ GPUs.",
        "abstract": "The scaling law for large language models (LLMs) depicts that the path towards machine intelligence necessitates training at large scale. Thus, companies continuously build large-scale GPU clusters, and launch training jobs that span over thousands of computing nodes. However, LLM pre-training presents unique challenges due to its complex communication patterns, where GPUs exchange data in sparse yet high-volume bursts within specific groups. Inefficient resource scheduling exacerbates bandwidth contention, leading to suboptimal training performance. This paper presents Arnold, a scheduling system summarizing our experience to effectively align LLM communication patterns with data center topology at scale. An in-depth characteristic study is performed to identify the impact of physical network topology to LLM pre-training jobs. Based on the insights, we develop a scheduling algorithm to effectively align communication patterns with the physical network topology in modern data centers. Through simulation experiments, we show the effectiveness of our algorithm in reducing the maximum spread of communication groups by up to $1.67$x. In production training, our scheduling system improves the end-to-end performance by $10.6\\%$ when training with more than $9600$ GPUs, a significant improvement for our training pipeline.",
        "authors": [
            "Guoliang He",
            "Youhe Jiang",
            "Wencong Xiao",
            "Kaihua Jiang",
            "Shuguang Wang",
            "Jun Wang",
            "Zixian Du",
            "Zhuo Jiang",
            "Xinlei Zhang",
            "Binhang Yuan",
            "Eiko Yoneki"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-19"
    },
    "http://arxiv.org/abs/2509.15450v1": {
        "id": "http://arxiv.org/abs/2509.15450v1",
        "title": "PCCL: Photonic circuit-switched collective communication for distributed ML",
        "link": "http://arxiv.org/abs/2509.15450v1",
        "tags": [
            "hardware",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-09-16",
        "tldr": "Addresses communication bottlenecks in distributed ML training by reconfiguring photonic networks to eliminate congestion. Proposes PCCL, a hardware-agnostic system that creates direct circuits for collective operations, achieving up to 3X faster communication and 1.3X end-to-end training speedup.",
        "abstract": "Modern distributed ML suffers from a fundamental gap between the theoretical and realized performance of collective communication algorithms due to congestion and hop-count induced dilation in practical GPU clusters. We present PCCL, a Photonic Collective Communication Library that reconfigures the network topology to match the communication patterns of collective algorithms, thereby eliminating congestion and dilation by creating direct, contention-free circuits between communicating GPUs. Unlike prior approaches that synthesize algorithms for specific network topologies and collectives, PCCL generalizes to any collective primitive and any topology by adapting the network to match each algorithm's communication pattern. PCCL's key innovation lies in its hardware-agnostic optimization framework that intelligently decides when to reconfigure based on the trade-off between network reconfiguration delay and congestion/dilation costs, making it practical across different optical hardware with varying switching speeds. Our evaluation demonstrates that PCCL achieves up to 3X speedup over state-of-the-art algorithms on 128 GPUs across various workloads, buffer sizes, and topologies, translating to a 1.3X speedup in end-to-end training throughput.",
        "authors": [
            "Abhishek Vijaya Kumar",
            "Arjun Devraj",
            "Rachee Singh"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-18"
    },
    "http://arxiv.org/abs/2509.13978v2": {
        "id": "http://arxiv.org/abs/2509.13978v2",
        "title": "LLM Agents for Interactive Workflow Provenance: Reference Architecture and Evaluation Methodology",
        "link": "http://arxiv.org/abs/2509.13978v2",
        "tags": [
            "RAG",
            "agentic"
        ],
        "relevant": true,
        "indexed_date": "2025-09-16",
        "tldr": "Explores using LLM agents to interpret and query scientific workflow provenance via natural language. Combines metadata-driven design and RAG to translate prompts into structured queries, achieving high accuracy on real-world chemistry workflows.",
        "abstract": "Modern scientific discovery increasingly relies on workflows that process data across the Edge, Cloud, and High Performance Computing (HPC) continuum. Comprehensive and in-depth analyses of these data are critical for hypothesis validation, anomaly detection, reproducibility, and impactful findings. Although workflow provenance techniques support such analyses, at large scale, the provenance data become complex and difficult to analyze. Existing systems depend on custom scripts, structured queries, or static dashboards, limiting data interaction. In this work, we introduce an evaluation methodology, reference architecture, and open-source implementation that leverages interactive Large Language Model (LLM) agents for runtime data analysis. Our approach uses a lightweight, metadata-driven design that translates natural language into structured provenance queries. Evaluations across LLaMA, GPT, Gemini, and Claude, covering diverse query classes and a real-world chemistry workflow, show that modular design, prompt tuning, and Retrieval-Augmented Generation (RAG) enable accurate and insightful LLM agent responses beyond recorded provenance.",
        "authors": [
            "Renan Souza",
            "Timothy Poteet",
            "Brian Etz",
            "Daniel Rosendo",
            "Amal Gueroudji",
            "Woong Shin",
            "Prasanna Balaprakash",
            "Rafael Ferreira da Silva"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.DB"
        ],
        "submit_date": "2025-09-17"
    },
    "http://arxiv.org/abs/2509.22681v1": {
        "id": "http://arxiv.org/abs/2509.22681v1",
        "title": "FLAME: A Serving System Optimized for Large-Scale Generative Recommendation with Efficiency",
        "link": "http://arxiv.org/abs/2509.22681v1",
        "tags": [
            "serving",
            "kernel",
            "hardware"
        ],
        "relevant": true,
        "indexed_date": "2025-09-16",
        "tldr": "Designs a production-grade serving system for large-scale generative recommendation models by decoupling pre-processing and computation, optimizing memory with PDA, and accelerating inference via TensorRT-based fused kernels. Achieves up to 6.3x throughput gain and 2.3x latency reduction.",
        "abstract": "Generative recommendation (GR) models possess greater scaling power compared to traditional deep learning recommendation models (DLRMs), yet they also impose a tremendous increase in computational burden. Measured in FLOPs, a typical GR model's workload sits in $10^9 \\sim 10^{11}$ range, roughly four orders of magnitude higher than traditional DLRMs. Delivering accurate results in a few tens of milliseconds while processing billions of such requests per day puts extreme demands on the performance of the online serving system. Therefore, for industry practitioners, the alluring gains of GR models are tempered by the formidable challenge of online deployment at scale in production services. In this work, we introduce a comprehensive solution of online serving system tailored For Large-scale GenerAtive RecoMmendation with Efficiency (FLAME). Specifically, we leveraging CPU-GPU heterogeneous hardware to decouple feature pre-processing and model computation. We encapsulated several memory optimization features as the Proximal Data Accelerator (PDA) module to make full use of limited bandwidth and storage resources, which achieves a 1.9x throughput gain and a 1.7x latency reduction. We implement the Fused Kernel Engine (FKE) module based on the functionality and interface of NVIDIA TensorRT to boost model computation, delivering a speedup ratio of 4.6x-6.1x, throughput gain ratio of 4.7x-6.3x one step further. In addition, we design the Dynamic Stream Orchestrator (DSO) module to coordinate concurrent requests, enhancing the system throughput performance with 1.3x improvement in throughput and 2.3x speed-up under non-uniform distribution of upstream candidates. Comprehensive evaluations demonstrate that our FLAME effectively supports large-scale online deployment of GR models and achieves remarkable improvements in system performance.",
        "authors": [
            "Xianwen Guo",
            "Bin Huang",
            "Xiaomeng Wu",
            "Guanlin Wu",
            "Fangjian Li",
            "Shijia Wang",
            "Qiang Xiao",
            "Chuanjiang Luo",
            "Yong Li"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-17"
    },
    "http://arxiv.org/abs/2509.13523v1": {
        "id": "http://arxiv.org/abs/2509.13523v1",
        "title": "AERIS: Argonne Earth Systems Model for Reliable and Skillful Predictions",
        "link": "http://arxiv.org/abs/2509.13523v1",
        "tags": [
            "diffusion",
            "hardware"
        ],
        "relevant": true,
        "indexed_date": "2025-09-16",
        "tldr": "Designs AERIS, a billion-parameter Swin diffusion transformer for high-resolution weather prediction, using SWiPe to enable efficient window, sequence, and pipeline parallelism; achieves 10.21 ExaFLOPS on Aurora with 95.5% weak scaling efficiency.",
        "abstract": "Generative machine learning offers new opportunities to better understand complex Earth system dynamics. Recent diffusion-based methods address spectral biases and improve ensemble calibration in weather forecasting compared to deterministic methods, yet have so far proven difficult to scale stably at high resolutions. We introduce AERIS, a 1.3 to 80B parameter pixel-level Swin diffusion transformer to address this gap, and SWiPe, a generalizable technique that composes window parallelism with sequence and pipeline parallelism to shard window-based transformers without added communication cost or increased global batch size. On Aurora (10,080 nodes), AERIS sustains 10.21 ExaFLOPS (mixed precision) and a peak performance of 11.21 ExaFLOPS with $1 \\times 1$ patch size on the 0.25° ERA5 dataset, achieving 95.5% weak scaling efficiency, and 81.6% strong scaling efficiency. AERIS outperforms the IFS ENS and remains stable on seasonal scales to 90 days, highlighting the potential of billion-parameter diffusion models for weather and climate prediction.",
        "authors": [
            "Väinö Hatanpää",
            "Eugene Ku",
            "Jason Stock",
            "Murali Emani",
            "Sam Foreman",
            "Chunyong Jung",
            "Sandeep Madireddy",
            "Tung Nguyen",
            "Varuni Sastry",
            "Ray A. O. Sinurat",
            "Sam Wheeler",
            "Huihuo Zheng",
            "Troy Arcomano",
            "Venkatram Vishwanath",
            "Rao Kotamarthi"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "submit_date": "2025-09-16"
    },
    "http://arxiv.org/abs/2509.13201v1": {
        "id": "http://arxiv.org/abs/2509.13201v1",
        "title": "Scaling Up Throughput-oriented LLM Inference Applications on Heterogeneous Opportunistic GPU Clusters with Pervasive Context Management",
        "link": "http://arxiv.org/abs/2509.13201v1",
        "tags": [
            "serving",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-09-16",
        "tldr": "Addresses how to improve throughput of non-latency-sensitive LLM inference on opportunistic GPU clusters. Introduces pervasive context management to reuse computational context across dynamic resources, enabling 98.1% reduction in execution time.",
        "abstract": "The widespread growth in LLM developments increasingly demands more computational power from clusters than what they can supply. Traditional LLM applications inherently require huge static resource allocations, which force users to either wait in a long job queue and accept progress delay, or buy expensive hardware to fulfill their needs and exacerbate the demand-supply problem. However, not all LLM applications are latency-sensitive and can instead be executed in a throughput-oriented way. This throughput orientation allows a dynamic allocation that opportunistically pools available resources over time, avoiding both the long queue and expensive GPU purchases. Effectively utilizing opportunistic resources brings numerous challenges nevertheless. Our solution, pervasive context management, exploits the common computational context in LLM applications and provides mechanisms and policies that allow seamless context reuse on opportunistic resources. Our evaluation shows an LLM application with pervasive context management on opportunistic resources reduces its execution time by 98.1%.",
        "authors": [
            "Thanh Son Phung",
            "Douglas Thain"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-16"
    },
    "http://arxiv.org/abs/2509.10371v2": {
        "id": "http://arxiv.org/abs/2509.10371v2",
        "title": "Characterizing the Efficiency of Distributed Training: A Power, Performance, and Thermal Perspective",
        "link": "http://arxiv.org/abs/2509.10371v2",
        "tags": [
            "training",
            "hardware"
        ],
        "relevant": true,
        "indexed_date": "2025-09-15",
        "tldr": "Characterizes power, thermal, and performance trade-offs in distributed LLM training across GPUs and parallelism strategies. Evaluates hardware utilization, optimizations, and scalability factors. Reveals scale-up vs. scale-out differences: scale-up outperforms in comm-bound cases with tuning, while scale-out excels elsewhere.",
        "abstract": "The rapid scaling of Large Language Models (LLMs) has pushed training workloads far beyond the limits of single-node analysis, demanding a deeper understanding of how these models behave across large-scale, multi-GPU systems. In this paper, we present a comprehensive characterization of LLM training across diverse real-world workloads and hardware platforms, including NVIDIA H100/H200 and AMD MI250 GPUs. We analyze dense and sparse models under various parallelism strategies -- tensor, pipeline, data, and expert -- and evaluate their effects on hardware utilization, power consumption, and thermal behavior. We further evaluate the effectiveness of optimizations such as activation recomputation and compute-communication overlap. Our findings show that performance is not determined solely by scaling hardware capacity. Scale-up systems with fewer, higher-memory GPUs can outperform scale-out systems in communication-bound regimes, but only under carefully tuned configurations; in other cases, scale-out deployments achieve superior throughput. We also show that certain parallelism combinations, such as tensor with pipeline, lead to bandwidth underutilization due to inefficient data chunking, while increasing microbatch sizes beyond a certain point induces bursty execution and peak power excursions that worsen thermal throttling. These insights reveal how training performance is shaped by complex interactions between hardware, system topology, and model execution. We conclude by offering recommendations for system and hardware design to improve the scalability and reliability of future LLM systems and workloads. The source code of this project is available at https://github.com/sitar-lab/CharLLM-PPT.",
        "authors": [
            "Seokjin Go",
            "Joongun Park",
            "Spandan More",
            "Hanjiang Wu",
            "Irene Wang",
            "Aaron Jezghani",
            "Tushar Krishna",
            "Divya Mahajan"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-09-12"
    },
    "http://arxiv.org/abs/2509.09868v1": {
        "id": "http://arxiv.org/abs/2509.09868v1",
        "title": "Ordered Consensus with Equal Opportunity",
        "link": "http://arxiv.org/abs/2509.09868v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-15",
        "tldr": "",
        "abstract": "The specification of state machine replication (SMR) has no requirement on the final total order of commands. In blockchains based on SMR, however, order matters, since different orders could provide their clients with different financial rewards. Ordered consensus augments the specification of SMR to include specific guarantees on such order, with a focus on limiting the influence of Byzantine nodes. Real-world ordering manipulations, however, can and do happen even without Byzantine replicas, typically because of factors, such as faster networks or closer proximity to the blockchain infrastructure, that give some clients an unfair advantage. To address this challenge, this paper proceeds to extend ordered consensus by requiring it to also support equal opportunity, a concrete notion of fairness, widely adopted in social sciences. Informally, equal opportunity requires that two candidates who, according to a set of criteria deemed to be relevant, are equally qualified for a position (in our case, a specific slot in the SMR total order), should have an equal chance of landing it. We show how randomness can be leveraged to keep bias in check, and, to this end, introduce the secret random oracle (SRO), a system component that generates randomness in a fault-tolerant manner. We describe two SRO designs based, respectively, on trusted hardware and threshold verifiable random functions, and instantiate them in Bercow, a new ordered consensus protocol that, by approximating equal opportunity up to within a configurable factor, can effectively mitigate well-known ordering attacks in SMR-based blockchains.",
        "authors": [
            "Yunhao Zhang",
            "Haobin Ni",
            "Soumya Basu",
            "Shir Cohen",
            "Maofan Yin",
            "Lorenzo Alvisi",
            "Robbert van Renesse",
            "Qi Chen",
            "Lidong Zhou"
        ],
        "categories": [
            "cs.DC",
            "cs.CR",
            "cs.MA"
        ],
        "submit_date": "2025-09-11"
    },
    "http://arxiv.org/abs/2509.09915v1": {
        "id": "http://arxiv.org/abs/2509.09915v1",
        "title": "The (R)evolution of Scientific Workflows in the Agentic AI Era: Towards Autonomous Science",
        "link": "http://arxiv.org/abs/2509.09915v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-15",
        "tldr": "",
        "abstract": "Modern scientific discovery increasingly requires coordinating distributed facilities and heterogeneous resources, forcing researchers to act as manual workflow coordinators rather than scientists. Advances in AI leading to AI agents show exciting new opportunities that can accelerate scientific discovery by providing intelligence as a component in the ecosystem. However, it is unclear how this new capability would materialize and integrate in the real world. To address this, we propose a conceptual framework where workflows evolve along two dimensions which are intelligence (from static to intelligent) and composition (from single to swarm) to chart an evolutionary path from current workflow management systems to fully autonomous, distributed scientific laboratories. With these trajectories in mind, we present an architectural blueprint that can help the community take the next steps towards harnessing the opportunities in autonomous science with the potential for 100x discovery acceleration and transformational scientific workflows.",
        "authors": [
            "Woong Shin",
            "Renan Souza",
            "Daniel Rosendo",
            "Frédéric Suter",
            "Feiyi Wang",
            "Prasanna Balaprakash",
            "Rafael Ferreira da Silva"
        ],
        "categories": [
            "cs.AI",
            "cs.DC"
        ],
        "submit_date": "2025-09-12"
    },
    "http://arxiv.org/abs/2509.09898v1": {
        "id": "http://arxiv.org/abs/2509.09898v1",
        "title": "DBOS Network Sensing: A Web Services Approach to Collaborative Awareness",
        "link": "http://arxiv.org/abs/2509.09898v1",
        "tags": [
            "storage",
            "serving",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-09-15",
        "tldr": "Proposes DBOS integrating GraphBLAS hypersparse matrices and PostgreSQL for network-aware web services. Parallelized system scales linearly up to 64 nodes, handling >100K requests/s with negligible overhead in collaborative awareness.",
        "abstract": "DBOS (DataBase Operating System) is a novel capability that integrates web services, operating system functions, and database features to significantly reduce web-deployment effort while increasing resilience. Integration of high performance network sensing enables DBOS web services to collaboratively create a shared awareness of their network environments to enhance their collective resilience and security. Network sensing is added to DBOS using GraphBLAS hypersparse traffic matrices via two approaches: (1) Python-GraphBLAS and (2) OneSparse PostgreSQL. These capabilities are demonstrated using the workflow and analytics from the IEEE/MIT/Amazon Anonymized Network Sensing Graph Challenge. The system was parallelized using pPython and benchmarked using 64 compute nodes on the MIT SuperCloud. The web request rate sustained by a single DBOS instance was ${>}10^5$, well above the required maximum, indicating that network sensing can be added to DBOS with negligible overhead. For collaborative awareness, many DBOS instances were connected to a single DBOS aggregator. The Python-GraphBLAS and OneSparse PostgreSQL implementations scaled linearly up to 64 and 32 nodes respectively. These results suggest that DBOS collaborative network awareness can be achieved with a negligible increase in computing resources.",
        "authors": [
            "Sophia Lockton",
            "Jeremy Kepner",
            "Michael Stonebraker",
            "Hayden Jananthan",
            "LaToya Anderson",
            "William Arcand",
            "David Bestor",
            "William Bergeron",
            "Alex Bonn",
            "Daniel Burrill",
            "Chansup Byun",
            "Timothy Davis",
            "Vijay Gadepally",
            "Michael Houle",
            "Matthew Hubbell",
            "Michael Jones",
            "Piotr Luszczek",
            "Peter Michaleas",
            "Lauren Milechin",
            "Chasen Milner",
            "Guillermo Morales",
            "Julie Mullen",
            "Michel Pelletier",
            "Alex Poliakov",
            "Andrew Prout",
            "Albert Reuther",
            "Antonio Rosa",
            "Charles Yee",
            "Alex Pentland"
        ],
        "categories": [
            "cs.NI",
            "cs.CR",
            "cs.DB",
            "cs.DC",
            "cs.OS"
        ],
        "submit_date": "2025-09-11"
    },
    "http://arxiv.org/abs/2509.10251v1": {
        "id": "http://arxiv.org/abs/2509.10251v1",
        "title": "XBOF: A Cost-Efficient CXL JBOF with Inter-SSD Compute Resource Sharing",
        "link": "http://arxiv.org/abs/2509.10251v1",
        "tags": [
            "storage",
            "kernel",
            "hardware"
        ],
        "relevant": true,
        "indexed_date": "2025-09-15",
        "tldr": "Proposes XBOF, a cost-efficient JBOF design using inter-SSD resource sharing via CXL to mitigate resource underutilization during I/O bursts. Implements disaggregated architecture and decentralized management to harvest idle SSD resources. Reduces costs by 19.0% with 50.4% higher resource utilization and negligible performance loss.",
        "abstract": "Enterprise SSDs integrate numerous computing resources (e.g., ARM processor and onboard DRAM) to satisfy the ever-increasing performance requirements of I/O bursts. While these resources substantially elevate the monetary costs of SSDs, the sporadic nature of I/O bursts causes severe SSD resource underutilization in just a bunch of flash (JBOF) level. Tackling this challenge, we propose XBOF, a cost-efficient JBOF design, which only reserves moderate computing resources in SSDs at low monetary cost, while achieving demanded I/O performance through efficient inter-SSD resource sharing. Specifically, XBOF first disaggregates SSD architecture into multiple disjoint parts based on their functionality, enabling fine-grained SSD internal resource management. XBOF then employs a decentralized scheme to manage these disaggregated resources and harvests the computing resources of idle SSDs to assist busy SSDs in handling I/O bursts. This idea is facilitated by the cache-coherent capability of Compute eXpress Link (CXL), with which the busy SSDs can directly utilize the harvested computing resources to accelerate metadata processing. The evaluation results show that XBOF improves SSD resource utilization by 50.4% and saves 19.0% monetary costs with a negligible performance loss, compared to existing JBOF designs.",
        "authors": [
            "Shushu Yi",
            "Yuda An",
            "Li Peng",
            "Xiurui Pan",
            "Qiao Li",
            "Jieming Yin",
            "Guangyan Zhang",
            "Wenfei Wu",
            "Diyu Zhou",
            "Zhenlin Wang",
            "Xiaolin Wang",
            "Yingwei Luo",
            "Ke Zhou",
            "Jie Zhang"
        ],
        "categories": [
            "cs.OS"
        ],
        "submit_date": "2025-09-12"
    },
    "http://arxiv.org/abs/2509.09525v1": {
        "id": "http://arxiv.org/abs/2509.09525v1",
        "title": "TrEnv: Transparently Share Serverless Execution Environments Across Different Functions and Nodes",
        "link": "http://arxiv.org/abs/2509.09525v1",
        "tags": [
            "serving",
            "offloading",
            "autoscaling"
        ],
        "relevant": true,
        "indexed_date": "2025-09-12",
        "tldr": "Proposes TrEnv, a serverless platform optimizing execution environment reuse for LLM agents. Uses repurposable sandboxes and memory templates to enable fast environment restoration. Reduces P99 latency up to 7× and memory usage by up to 61% compared to state-of-the-art systems.",
        "abstract": "Serverless computing provides dynamic scalability, but its infrastructure overhead becomes a bottleneck for emerging workloads such as LLM agents, which exhibit unpredictable invocation patterns and variable resource demands. Our analysis shows that for these agents, the cost of running on serverless platforms can reach up to 70% of the cost of LLM API calls. This finding motivates the need for a more efficient, high-density serverless platform. We present TrEnv, a co-designed serverless platform that supports both container- and VM-based environments, optimized for the unique demands of LLM agents. TrEnv reduces startup latency and memory usage through repurposable sandboxes and memory templates, which enable fast reuse and restoration of execution environments. To further reduce overhead in VM-based agent workloads, TrEnv leverages browser sharing and a page cache bypassing mechanism. Evaluations show that TrEnv reduces P99 latency by up to 7X and memory usage by 48% in container-based settings, and achieves up to 58% lower P99 latency and 61% memory savings for VM-based agents compared to state-of-the-art systems like E2B.",
        "authors": [
            "Jialiang Huang",
            "Teng Ma",
            "Zheng Liu",
            "Sixing Lin",
            "Kang Chen",
            "Jinlei Jiang",
            "Xia Liao",
            "Yingdi Shan",
            "Yongwei Wu",
            "Ning Zhang",
            "Mengting Lu",
            "Tao Ma",
            "Haifeng Gong",
            "Mingxing Zhang"
        ],
        "categories": [
            "cs.DC",
            "cs.OS"
        ],
        "submit_date": "2025-09-11"
    },
    "http://arxiv.org/abs/2509.09493v1": {
        "id": "http://arxiv.org/abs/2509.09493v1",
        "title": "Weaker Assumptions for Asymmetric Trust",
        "link": "http://arxiv.org/abs/2509.09493v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-12",
        "tldr": "",
        "abstract": "In distributed systems with asymmetric trust, each participant is free to make its own trust assumptions about others, captured by an asymmetric quorum system. This contrasts with ordinary, symmetric quorum systems and threshold models, where trust assumptions are uniformly shared among participants. Fundamental problems like reliable broadcast and consensus are unsolvable in the asymmetric model if quorum systems satisfy only the classical properties of consistency and availability. Existing approaches overcome this by introducing stronger assumptions. We show that some of these assumptions are overly restrictive, so much so that they effectively eliminate the benefits of asymmetric trust. To address this, we propose a new approach to characterize asymmetric problems and, building upon it, present algorithms for reliable broadcast and consensus that require weaker assumptions than previous solutions. Our methods are general and can be extended to other core problems in systems with asymmetric trust.",
        "authors": [
            "Ignacio Amores-Sesar",
            "Christian Cachin",
            "Juan Villacis"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-11"
    },
    "http://arxiv.org/abs/2509.09435v1": {
        "id": "http://arxiv.org/abs/2509.09435v1",
        "title": "Barycentric Coded Distributed Computing with Flexible Recovery Threshold for Collaborative Mobile Edge Computing",
        "link": "http://arxiv.org/abs/2509.09435v1",
        "tags": [
            "edge",
            "RL"
        ],
        "relevant": true,
        "indexed_date": "2025-09-12",
        "tldr": "Proposes a barycentric coded distributed computing scheme to mitigate stragglers in collaborative edge computing. Uses barycentric rational interpolation for flexible decoding with any number of worker results, supporting finite and real fields with numerical stability. Reduces waiting time by 30% and improves accuracy by 2× compared to prior CDC schemes.",
        "abstract": "Collaborative mobile edge computing (MEC) has emerged as a promising paradigm to enable low-capability edge nodes to cooperatively execute computation-intensive tasks. However, straggling edge nodes (stragglers) significantly degrade the performance of MEC systems by prolonging computation latency. While coded distributed computing (CDC) as an effective technique is widely adopted to mitigate straggler effects, existing CDC schemes exhibit two critical limitations: (i) They cannot successfully decode the final result unless the number of received results reaches a fixed recovery threshold, which seriously restricts their flexibility; (ii) They suffer from inherent poles in their encoding/decoding functions, leading to decoding inaccuracies and numerical instability in the computational results. To address these limitations, this paper proposes an approximated CDC scheme based on barycentric rational interpolation. The proposed CDC scheme offers several outstanding advantages. Firstly, it can decode the final result leveraging any returned results from workers. Secondly, it supports computations over both finite and real fields while ensuring numerical stability. Thirdly, its encoding/decoding functions are free of poles, which not only enhances approximation accuracy but also achieves flexible accuracy tuning. Fourthly, it integrates a novel BRI-based gradient coding algorithm accelerating the training process while providing robustness against stragglers. Finally, experimental results reveal that the proposed scheme is superior to existing CDC schemes in both waiting time and approximate accuracy.",
        "authors": [
            "Houming Qiu",
            "Kun Zhu",
            "Dusit Niyato",
            "Nguyen Cong Luong",
            "Changyan Yi",
            "Chen Dai"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-11"
    },
    "http://arxiv.org/abs/2509.09400v1": {
        "id": "http://arxiv.org/abs/2509.09400v1",
        "title": "WebAssembly and Unikernels: A Comparative Study for Serverless at the Edge",
        "link": "http://arxiv.org/abs/2509.09400v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-12",
        "tldr": "",
        "abstract": "Serverless computing at the edge requires lightweight execution environments to minimize cold start latency, especially in Urgent Edge Computing (UEC). This paper compares WebAssembly and unikernel-based MicroVMs for serverless workloads. We present Limes, a WebAssembly runtime built on Wasmtime, and evaluate it against the Firecracker-based environment used in SPARE. Results show that WebAssembly offers lower cold start times for lightweight functions but suffers with complex workloads, while Firecracker provides higher, but stable, cold starts and better execution performance, particularly for I/O-heavy tasks.",
        "authors": [
            "Valerio Besozzi",
            "Enrico Fiasco",
            "Marco Danelutto",
            "Patrizio Dazzi"
        ],
        "categories": [
            "cs.DC",
            "cs.PF"
        ],
        "submit_date": "2025-09-11"
    },
    "http://arxiv.org/abs/2509.09094v1": {
        "id": "http://arxiv.org/abs/2509.09094v1",
        "title": "Coherence-Aware Task Graph Modeling for Realistic Application",
        "link": "http://arxiv.org/abs/2509.09094v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-12",
        "tldr": "",
        "abstract": "As multicore systems continue to scale, cache coherence has emerged as a critical determinant of system performance, with coherence behavior and task execution closely intertwined, reshaping inter-task dependencies. Task graph modeling provides a structured way to capture such dependencies and serves as the foundation for many system-level design strategies. However, these strategies typically rely on predefined task graphs, while many real-world applications lack explicit graphs and exhibit dynamic, data-dependent behavior, limiting the effectiveness of static approaches. To address this, several task graph modeling methods for realistic workloads have been developed. Yet, they either rely on implicit techniques that use application-specific features without producing explicit graphs, or they generate graphs tailored to fixed scheduling models, which limits generality. More importantly, they often overlook coherence interactions, creating a gap between design assumptions and actual runtime behavior. To overcome these limitations, we propose CoTAM, a Coherence-Aware Task Graph Modeling framework for realistic workloads that constructs a unified task graph reflecting runtime behavior. CoTAM analyzes the impact of coherence by decoupling its effects from overall execution, quantifies its influence through a learned weighting scheme, and infers inter-task dependencies for coherence-aware graph generation. Extensive experiments show that CoTAM outperforms implicit methods, bridging the gap between dynamic workload behavior and existing designs while demonstrating the importance of incorporating cache coherence into task graph modeling for accurate and generalizable system-level analysis.",
        "authors": [
            "Guochu Xiong",
            "Xiangzhong Luo",
            "Weichen Liu"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-11"
    },
    "http://arxiv.org/abs/2509.09058v1": {
        "id": "http://arxiv.org/abs/2509.09058v1",
        "title": "Optimizing the Variant Calling Pipeline Execution on Human Genomes Using GPU-Enabled Machines",
        "link": "http://arxiv.org/abs/2509.09058v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-12",
        "tldr": "",
        "abstract": "Variant calling is the first step in analyzing a human genome and aims to detect variants in an individual's genome compared to a reference genome. Due to the computationally-intensive nature of variant calling, genomic data are increasingly processed in cloud environments as large amounts of compute and storage resources can be acquired with the pay-as-you-go pricing model. In this paper, we address the problem of efficiently executing a variant calling pipeline for a workload of human genomes on graphics processing unit (GPU)-enabled machines. We propose a novel machine learning (ML)-based approach for optimizing the workload execution to minimize the total execution time. Our approach encompasses two key techniques: The first technique employs ML to predict the execution times of different stages in a variant calling pipeline based on the characteristics of a genome sequence. Using the predicted times, the second technique generates optimal execution plans for the machines by drawing inspiration from the flexible job shop scheduling problem. The plans are executed via careful synchronization across different machines. We evaluated our approach on a workload of publicly available genome sequences using a testbed with different types of GPU hardware. We observed that our approach was effective in predicting the execution times of variant calling pipeline stages using ML on features such as sequence size, read quality, percentage of duplicate reads, and average read length. In addition, our approach achieved 2X speedup (on an average) over a greedy approach that also used ML for predicting the execution times on the tested workload of sequences. Finally, our approach achieved 1.6X speedup (on an average) over a dynamic approach that executed the workload based on availability of resources without using any ML-based time predictions.",
        "authors": [
            "Ajay Kumar",
            "Praveen Rao",
            "Peter Sanders"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-10"
    },
    "http://arxiv.org/abs/2509.08969v1": {
        "id": "http://arxiv.org/abs/2509.08969v1",
        "title": "A Comparative Analysis of Identifier Schemes: UUIDv4, UUIDv7, and ULID for Distributed Systems",
        "link": "http://arxiv.org/abs/2509.08969v1",
        "tags": [
            "storage",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-09-12",
        "tldr": "Compares identifier schemes (UUIDv4, UUIDv7, ULID) for distributed systems. Measures generation speed, network overhead, and collision risks. ULIDs reduce network overhead by 83.7% and increase generation speed by 97.32% while lowering collision risk by 98.42% versus UUIDv7.",
        "abstract": "Distributed systems require robust, scalable identifier schemes to ensure data uniqueness and efficient indexing across multiple nodes. This paper presents a comprehensive analysis of the evolution of distributed identifiers, comparing traditional auto-increment keys with UUIDv4, UUIDv7, and ULIDs. We combine mathematical calculation of collision probabilities with empirical experiments measuring generation speed and network transmission overhead in a simulated distributed environment. Results demonstrate that ULIDs significantly outperform UUIDv4 and UUIDv7, reducing network overhead by 83.7% and increasing generation speed by 97.32%. statistical analysis further shows ULIDs offer a 98.42% lower collision risk compared to UUIDv7, while maintaining negligible collision probabilities even at high generation rates. These findings highlight ULIDs as an optimal choice for high-performance distributed systems, providing efficient, time-ordered, and lexicographically sortable identifiers suitable for scalable applications. All source code, datasets, and analysis scripts utilized in this research are publicly available in our dedicated repository at https://github.com/nimakarimiank/uids-comparison. This repository contains comprehensive documentation of the experimental setup, including configuration files for the distributed environment, producer and consumer implementations, and message broker integration. Additionally, it provides the data scripts and datasets. Researchers and practitioners are encouraged to explore the repository for full reproducibility of the experiments and to facilitate further investigation or extension of the presented work.",
        "authors": [
            "Nima Karimian Kakolaki"
        ],
        "categories": [
            "cs.DC",
            "cs.DB"
        ],
        "submit_date": "2025-09-10"
    },
    "http://arxiv.org/abs/2509.09653v1": {
        "id": "http://arxiv.org/abs/2509.09653v1",
        "title": "Towards A High-Performance Quantum Data Center Network Architecture",
        "link": "http://arxiv.org/abs/2509.09653v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-12",
        "tldr": "",
        "abstract": "Quantum Data Centers (QDCs) are needed to support large-scale quantum processing for both academic and commercial applications. While large-scale quantum computers are constrained by technological and financial barriers, a modular approach that clusters small quantum computers offers an alternative. This approach, however, introduces new challenges in network scalability, entanglement generation, and quantum memory management. In this paper, we propose a three-layer fat-tree network architecture for QDCs, designed to address these challenges. Our architecture features a unique leaf switch and an advanced swapping spine switch design, optimized to handle high volumes of entanglement requests as well as a queue scheduling mechanism that efficiently manages quantum memory to prevent decoherence. Through queuing-theoretical models and simulations in NetSquid, we demonstrate the proposed architecture's scalability and effectiveness in maintaining high entanglement fidelity, offering a practical path forward for modular QDC networks.",
        "authors": [
            "Yufeng Xin",
            "Liang Zhang"
        ],
        "categories": [
            "quant-ph",
            "cs.DC",
            "cs.NI"
        ],
        "submit_date": "2025-09-11"
    },
    "http://arxiv.org/abs/2509.08971v1": {
        "id": "http://arxiv.org/abs/2509.08971v1",
        "title": "HARD: A Performance Portable Radiation Hydrodynamics Code based on FleCSI Framework",
        "link": "http://arxiv.org/abs/2509.08971v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-12",
        "tldr": "",
        "abstract": "Hydrodynamics And Radiation Diffusion} (HARD) is an open-source application for high-performance simulations of compressible hydrodynamics with radiation-diffusion coupling. Built on the FleCSI (Flexible Computational Science Infrastructure) framework, HARD expresses its computational units as tasks whose execution can be orchestrated by multiple back-end runtimes, including Legion, MPI, and HPX. Node-level parallelism is delegated to Kokkos, providing a single, portable code base that runs efficiently on laptops, small homogeneous clusters, and the largest heterogeneous supercomputers currently available. To ensure scientific reliability, HARD includes a regression-test suite that automatically reproduces canonical verification problems such as the Sod and LeBlanc shock tubes and the Sedov blast wave, comparing numerical solutions against known analytical results. The project is distributed under an OSI-approved license, hosted on GitHub, and accompanied by reproducible build scripts and continuous integration workflows. This combination of performance portability, verification infrastructure, and community-focused development makes HARD a sustainable platform for advancing radiation hydrodynamics research across multiple domains.",
        "authors": [
            "Julien Loiseau",
            "Hyun Lim",
            "Andrés Yagüe López",
            "Mammadbaghir Baghirzade",
            "Shihab Shahriar Khan",
            "Yoonsoo Kim",
            "Sudarshan Neopane",
            "Alexander Strack",
            "Farhana Taiyebah",
            "Benjamin K. Bergen"
        ],
        "categories": [
            "physics.comp-ph",
            "astro-ph.IM",
            "cs.DC"
        ],
        "submit_date": "2025-09-10"
    },
    "http://arxiv.org/abs/2509.09439v2": {
        "id": "http://arxiv.org/abs/2509.09439v2",
        "title": "μFork: Supporting POSIX fork Within a Single-Address-Space OS",
        "link": "http://arxiv.org/abs/2509.09439v2",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-12",
        "tldr": "",
        "abstract": "Single-address-space operating systems have well-known lightweightness benefits that result from their central design idea: the kernel and applications share a unique address space. This model makes these operating systems (OSes) incompatible by design with a large class of software: multiprocess POSIX applications. Indeed, the semantics of the primitive used to create POSIX processes, fork, are inextricably tied to the existence of multiple address spaces. Prior approaches addressing this issue trade off lightweightness, compatibility and/or isolation. We propose μFork, a single-address-space operating system design supporting POSIX fork on modern hardware without compromising on any of these key objectives. μFork emulates POSIX processes (μprocesses) and achieves fork by creating for the child a copy of the parent μprocess' memory at a different location within a single address space. This approach presents two challenges: relocating the child's absolute memory references (pointers), as well as providing user/kernel and μprocesses isolation without impacting lightweightness. We address them using CHERI. We implement μFork and evaluate it upon three real-world use-cases: Redis snapshots, Nginx multi-worker deployments, and Zygote FaaS worker warm-up. μFork outperforms previous work and traditional monolithic OSes on key lightweightness metrics by an order of magnitude, e.g. it can offer a fork-bound FaaS function throughput 24% higher than that of a monolithic OS, and can fork a μprocess in 54μs, 3.7x faster than a traditional fork.",
        "authors": [
            "John Alistair Kressel",
            "Hugo Lefeuvre",
            "Pierre Olivier"
        ],
        "categories": [
            "cs.OS"
        ],
        "submit_date": "2025-09-11"
    },
    "http://arxiv.org/abs/2509.08770v1": {
        "id": "http://arxiv.org/abs/2509.08770v1",
        "title": "Reconfigurable Holographic Surfaces and Near Field Communication for Non-Terrestrial Networks: Potential and Challenges",
        "link": "http://arxiv.org/abs/2509.08770v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-11",
        "tldr": "",
        "abstract": "To overcome the challenges of ultra-low latency, ubiquitous coverage, and soaring data rates, this article presents a combined use of Near Field Communication (NFC) and Reconfigurable Holographic Surfaces (RHS) for Non-Terrestrial Networks (NTN). A system architecture has been presented, which shows that the integration of RHS with NTN platforms such as satellites, High Altitute Platform Stations (HAPS), and Uncrewed Aerial Vehicles (UAV) can achieve precise beamforming and intelligent wavefront control in near-field regions, enhancing Energy Efficiency (EE), spectral utilization, and spatial resolution. Moreover, key applications, challenges, and future directions have been identified to fully adopt this integration. In addition, a use case analysis has been presented to improve the EE of the system in a public safety use case scenario, further strengthening the UAV-RHS fusion.",
        "authors": [
            "Muhammad Ali Jamshed",
            "Muhammad Ahmed Mohsin",
            "Hongliang Zhang",
            "Bushra Haq",
            "Aryan Kaushik",
            "Boya Di",
            "Weiwei Jiang"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-10"
    },
    "http://arxiv.org/abs/2509.08608v1": {
        "id": "http://arxiv.org/abs/2509.08608v1",
        "title": "A 410GFLOP/s, 64 RISC-V Cores, 204.8GBps Shared-Memory Cluster in 12nm FinFET with Systolic Execution Support for Efficient B5G/6G AI-Enhanced O-RAN",
        "link": "http://arxiv.org/abs/2509.08608v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-11",
        "tldr": "",
        "abstract": "We present HeartStream, a 64-RV-core shared-L1-memory cluster (410 GFLOP/s peak performance and 204.8 GBps L1 bandwidth) for energy-efficient AI-enhanced O-RAN. The cores and cluster architecture are customized for baseband processing, supporting complex (16-bit real&imaginary) instructions: multiply&accumulate, division&square-root, SIMD instructions, and hardware-managed systolic queues, improving up to 1.89x the energy efficiency of key baseband kernels. At 800MHz@0.8V, HeartStream delivers up to 243GFLOP/s on complex-valued wireless workloads. Furthermore, the cores also support efficient AI processing on received data at up to 72 GOP/s. HeartStream is fully compatible with base station power and processing latency limits: it achieves leading-edge software-defined PUSCH efficiency (49.6GFLOP/s/W) and consumes just 0.68W (645MHz@0.65V), within the 4 ms end-to-end constraint for B5G/6G uplink.",
        "authors": [
            "Yichao Zhang",
            "Marco Bertuletti",
            "Sergio Mazzola",
            "Samuel Riedel",
            "Luca Benini"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-10"
    },
    "http://arxiv.org/abs/2509.08347v1": {
        "id": "http://arxiv.org/abs/2509.08347v1",
        "title": "An HPC Benchmark Survey and Taxonomy for Characterization",
        "link": "http://arxiv.org/abs/2509.08347v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-11",
        "tldr": "",
        "abstract": "The field of High-Performance Computing (HPC) is defined by providing computing devices with highest performance for a variety of demanding scientific users. The tight co-design relationship between HPC providers and users propels the field forward, paired with technological improvements, achieving continuously higher performance and resource utilization. A key device for system architects, architecture researchers, and scientific users are benchmarks, allowing for well-defined assessment of hardware, software, and algorithms. Many benchmarks exist in the community, from individual niche benchmarks testing specific features, to large-scale benchmark suites for whole procurements. We survey the available HPC benchmarks, summarizing them in table form with key details and concise categorization, also through an interactive website. For categorization, we present a benchmark taxonomy for well-defined characterization of benchmarks.",
        "authors": [
            "Andreas Herten",
            "Olga Pearce",
            "Filipe S. M. Guimarães"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-10"
    },
    "http://arxiv.org/abs/2509.08309v1": {
        "id": "http://arxiv.org/abs/2509.08309v1",
        "title": "Hetis: Serving LLMs in Heterogeneous GPU Clusters with Fine-grained and Dynamic Parallelism",
        "link": "http://arxiv.org/abs/2509.08309v1",
        "tags": [
            "serving",
            "hardware",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-09-11",
        "tldr": "Addresses inefficiencies in heterogeneous GPU clusters for LLM serving. Introduces fine-grained parallelism with dynamic attention distribution and online load dispatching. Achieves up to 2.25× throughput improvement and reduces latency by 1.49×.",
        "abstract": "The significant resource demands in LLM serving prompts production clusters to fully utilize heterogeneous hardware by partitioning LLM models across a mix of high-end and low-end GPUs. However, existing parallelization approaches often struggle to scale efficiently in heterogeneous environments due to their coarse-grained and static parallelization strategies.   In this paper, we introduce Hetis, a new LLM system tailored for heterogeneous GPU clusters. Hetis addresses two critical challenges: (1) memory inefficiency caused by the mismatch between memory capacity and computational power in heterogeneous devices, and (2) computational inefficiency arising from performance gaps across different LLM modules. To tackle these issues, Hetis employs a fine-grained and dynamic parallelism design. Specifically, it selectively parallelizes compute-intensive operations to reduce latency and dynamically distributes Attention computations to low-end GPUs at a head granularity, leveraging the distinct characteristics of each module. Additionally, Hetis features an online load dispatching policy that continuously optimizes serving performance by carefully balancing network latency, computational load, and memory intensity. Evaluation results demonstrate that Hetis can improve serving throughput by up to $2.25\\times$ and reduce latency by $1.49\\times$ compared to existing systems.",
        "authors": [
            "Zizhao Mo",
            "Jianxiong Liao",
            "Huanle Xu",
            "Zhi Zhou",
            "Chengzhong Xu"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-10"
    },
    "http://arxiv.org/abs/2509.08215v1": {
        "id": "http://arxiv.org/abs/2509.08215v1",
        "title": "Design and Implementation of Code Completion System Based on LLM and CodeBERT Hybrid Subsystem",
        "link": "http://arxiv.org/abs/2509.08215v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-11",
        "tldr": "",
        "abstract": "In the rapidly evolving industry of software development, coding efficiency and accuracy play significant roles in delivering high-quality software. Various code suggestion and completion tools, such as CodeBERT from Microsoft and GPT-3.5 from OpenAI, have been developed using deep learning techniques and integrated into IDEs to assist software engineers' development. Research has shown that CodeBERT has outstanding performance in code summarization and capturing code semantics, while GPT-3.5 demonstrated its adept capability at code generation. This study focuses on implementing a hybrid model that integrates CodeBERT and GPT-3.5 models to accomplish code suggestion and autocomplete tasks, leveraging the context-aware effectiveness of CodeBERT and taking advantage of advanced code generation abilities of GPT-3.5. Evaluated in three main metrics: accuracy, quality of generated code and performance efficiency with various software and hardware, the hybrid model outperforms benchmarks, demonstrating its feasibility and effectiveness. Robustness testing further confirms the reliability and stability of the hybrid model. This study not only emphasizes the importance of deep learning in the software development industry, but also reveals the potential of synthesizing complementary deep learning models to fully exploit strengths of each model.",
        "authors": [
            "Bingbing Zhang",
            "Ziyu Lin",
            "Yingxin Su"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-10"
    },
    "http://arxiv.org/abs/2509.08207v2": {
        "id": "http://arxiv.org/abs/2509.08207v2",
        "title": "Aurora: Architecting Argonne's First Exascale Supercomputer for Accelerated Scientific Discovery",
        "link": "http://arxiv.org/abs/2509.08207v2",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-11",
        "tldr": "",
        "abstract": "Aurora is Argonne National Laboratory's pioneering Exascale supercomputer, designed to accelerate scientific discovery with cutting-edge architectural innovations. Key new technologies include the Intel(TM) Xeon(TM) Data Center GPU Max Series (code-named Sapphire Rapids) with support for High Bandwidth Memory (HBM), alongside the Intel(TM) Data Center GPU Max Series (code-named Ponte Vecchio) on each compute node. Aurora also integrates the Distributed Asynchronous Object Storage (DAOS), a novel exascale storage solution, and leverages Intel's oneAPI programming environment. This paper presents an in-depth exploration of Aurora's node architecture, the HPE Slingshot interconnect, the supporting software ecosystem, and DAOS. We provide insights into standard benchmark performance and applications readiness efforts via Aurora's Early Science Program and the Exascale Computing Project.",
        "authors": [
            "William E. Allcock",
            "Benjamin S. Allen",
            "James Anchell",
            "Victor Anisimov",
            "Thomas Applencourt",
            "Abhishek Bagusetty",
            "Ramesh Balakrishnan",
            "Riccardo Balin",
            "Solomon Bekele",
            "Colleen Bertoni",
            "Cyrus Blackworth",
            "Renzo Bustamante",
            "Kevin Canada",
            "John Carrier",
            "Christopher Chan-nui",
            "Lance C. Cheney",
            "Taylor Childers",
            "Paul Coffman",
            "Susan Coghlan",
            "Tanima Dey",
            "Michael D'Mello",
            "Ashok Emani",
            "Murali Emani",
            "Kyle G. Felker",
            "Sam Foreman",
            "Olivier Franza",
            "Longfei Gao",
            "Marta García",
            "María Garzarán",
            "Balazs Gerofi",
            "Yasaman Ghadar",
            "Subrata Goswami",
            "Neha Gupta",
            "Kevin Harms",
            "Väinö Hatanpää",
            "Brian Holland",
            "Carissa Holohan",
            "Brian Homerding",
            "Khalid Hossain",
            "Xue Hu",
            "Louise Huot",
            "Huda Ibeid",
            "Joseph A. Insley",
            "Sai Jayanthi",
            "Hong Jiang",
            "Wei Jiang",
            "Xiao-Yong Jin",
            "Jeongnim Kim",
            "Christopher Knight",
            "Panagiotis Kourdis",
            "Kalyan Kumaran",
            "JaeHyuk Kwack",
            "Janghaeng Lee",
            "Ti Leggett",
            "Ben Lenard",
            "Chris Lewis",
            "Nevin Liber",
            "Johann Lombardi",
            "Raymond M. Loy",
            "Ye Luo",
            "Bethany Lusch",
            "Nilakantan Mahadevan",
            "Beth Markey",
            "Victor A. Mateevitsi",
            "Gordon McPheeters",
            "Ryan Milner",
            "Jerome Mitchell",
            "Vitali A. Morozov",
            "Servesh Muralidharan",
            "Tom Musta",
            "Mrigendra Nagar",
            "Vikram Narayana",
            "Marieme Ngom",
            "Anthony-Trung Nguyen",
            "Nathan Nichols",
            "Aditya Nishtala",
            "James C. Osborn",
            "Michael E. Papka",
            "Scott Parker",
            "Saumil S. Patel",
            "Julia Piotrowska",
            "Adrian C. Pope",
            "Sucheta Raghunanda",
            "Esteban Rangel",
            "Paul M. Rich",
            "Katherine M. Riley",
            "Silvio Rizzi",
            "Kris Rowe",
            "Varuni Sastry",
            "Adam Scovel",
            "Filippo Simini",
            "Haritha Siddabathuni Som",
            "Patrick Steinbrecher",
            "Rick Stevens",
            "Xinmin Tian",
            "Peter Upton",
            "Thomas Uram",
            "Archit K. Vasan",
            "Álvaro Vázquez-Mayagoitia",
            "Kaushik Velusamy",
            "Brice Videau",
            "Venkatram Vishwanath",
            "Brian Whitney",
            "Timothy J. Williams",
            "Michael Woodacre",
            "Sam Zeltner",
            "Chuanjun Zhang",
            "Gengbin Zheng",
            "Huihuo Zheng"
        ],
        "categories": [
            "cs.DC",
            "cs.AR",
            "cs.CE",
            "cs.PF"
        ],
        "submit_date": "2025-09-10"
    },
    "http://arxiv.org/abs/2509.08020v1": {
        "id": "http://arxiv.org/abs/2509.08020v1",
        "title": "Towards Scalable Proteomics: Opportunistic SMC Samplers on HTCondor",
        "link": "http://arxiv.org/abs/2509.08020v1",
        "tags": [
            "training",
            "offline",
            "RL"
        ],
        "relevant": true,
        "indexed_date": "2025-09-11",
        "tldr": "Develops an opportunistic computing framework using HTCondor for scalable Bayesian inference in proteomics via SMC samplers. Proposes a Coordinator-Manager-Follower architecture to reduce synchronization overhead in heterogeneous environments. Achieves weak scaling, increasing samples under fixed time as resources scale.",
        "abstract": "Quantitative proteomics plays a central role in uncovering regulatory mechanisms, identifying disease biomarkers, and guiding the development of precision therapies. These insights are often obtained through complex Bayesian models, whose inference procedures are computationally intensive, especially when applied at scale to biological datasets. This limits the accessibility of advanced modelling techniques needed to fully exploit proteomics data. Although Sequential Monte Carlo (SMC) methods offer a parallelisable alternative to traditional Markov Chain Monte Carlo, their high-performance implementations often rely on specialised hardware, increasing both financial and energy costs. We address these challenges by introducing an opportunistic computing framework for SMC samplers, tailored to the demands of large-scale proteomics inference. Our approach leverages idle compute resources at the University of Liverpool via HTCondor, enabling scalable Bayesian inference without dedicated high-performance computing infrastructure. Central to this framework is a novel Coordinator-Manager-Follower architecture that reduces synchronisation overhead and supports robust operation in heterogeneous, unreliable environments. We evaluate the framework on a realistic proteomics model and show that opportunistic SMC delivers accurate inference with weak scaling, increasing samples generated under a fixed time budget as more resources join. To support adoption, we release CondorSMC, an open-source package for deploying SMC samplers in opportunistic computing environments.",
        "authors": [
            "Matthew Carter",
            "Lee Devlin",
            "Alexander Philips",
            "Edward Pyzer-Knapp",
            "Paul Spirakis",
            "Simon Maskell"
        ],
        "categories": [
            "q-bio.QM",
            "cs.DC",
            "stat.CO"
        ],
        "submit_date": "2025-09-09"
    },
    "http://arxiv.org/abs/2509.07781v1": {
        "id": "http://arxiv.org/abs/2509.07781v1",
        "title": "Scaling atomic ordering in shared memory",
        "link": "http://arxiv.org/abs/2509.07781v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-10",
        "tldr": "",
        "abstract": "Atomic multicast is a communication primitive used in dependable systems to ensure consistent ordering of messages delivered to a set of replica groups. This primitive enables critical services to integrate replication and sharding (i.e., state partitioning) to achieve fault tolerance and scalability. While several atomic multicast protocols have been developed for message-passing systems, only a few are designed for the shared memory system model. This paper introduces TRAM, an atomic multicast protocol specifically designed for shared memory systems, leveraging an overlay tree architecture. Due to its simple and practical design, TRAM delivers exceptional performance, increasing throughput by more than 3$\\times$ and reducing latency by more than 2.3$\\times$ compared to state-of-the-art shared memory-based protocols. Additionally, it significantly outperforms message-passing-based protocols, boosting throughput by up to 5.9$\\times$ and reducing latency by up to 106$\\times$.",
        "authors": [
            "Lorenzo Martignetti",
            "Eliã Batista",
            "Gianpaolo Cugola",
            "Fernando Pedone"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-09"
    },
    "http://arxiv.org/abs/2509.07595v1": {
        "id": "http://arxiv.org/abs/2509.07595v1",
        "title": "AgentX: Towards Orchestrating Robust Agentic Workflow Patterns with FaaS-hosted MCP Services",
        "link": "http://arxiv.org/abs/2509.07595v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-10",
        "tldr": "",
        "abstract": "Generative Artificial Intelligence (GenAI) has rapidly transformed various fields including code generation, text summarization, image generation and so on. Agentic AI is a recent evolution that further advances this by coupling the decision making and generative capabilities of LLMs with actions that can be performed using tools. While seemingly powerful, Agentic systems often struggle when faced with numerous tools, complex multi-step tasks,and long-context management to track history and avoid hallucinations. Workflow patterns such as Chain-of-Thought (CoT) and ReAct help address this. Here, we define a novel agentic workflow pattern, AgentX, composed of stage designer, planner, and executor agents that is competitive or better than the state-of-the-art agentic patterns. We also leverage Model Context Protocol (MCP) tools, and propose two alternative approaches for deploying MCP servers as cloud Functions as a Service (FaaS). We empirically evaluate the success rate, latency and cost for AgentX and two contemporary agentic patterns, ReAct and Magentic One, using these the FaaS and local MCP server alternatives for three practical applications. This highlights the opportunities and challenges of designing and deploying agentic workflows.",
        "authors": [
            "Shiva Sai Krishna Anand Tokal",
            "Vaibhav Jha",
            "Anand Eswaran",
            "Praveen Jayachandran",
            "Yogesh Simmhan"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-09"
    },
    "http://arxiv.org/abs/2509.07567v1": {
        "id": "http://arxiv.org/abs/2509.07567v1",
        "title": "Navigating Energy Doldrums: Modeling the Impact of Energy Price Volatility on HPC Cost of Ownership",
        "link": "http://arxiv.org/abs/2509.07567v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-10",
        "tldr": "",
        "abstract": "Energy costs are a major factor in the total cost of ownership (TCO) for high-performance computing (HPC) systems. The rise of intermittent green energy sources and reduced reliance on fossil fuels have introduced volatility into electricity markets, complicating energy budgeting. This paper explores variable capacity as a strategy for managing HPC energy costs - dynamically adjusting compute resources in response to fluctuating electricity prices. While this approach can lower energy expenses, it risks underutilizing costly hardware. To evaluate this trade-off, we present a simple model that helps operators estimate the TCO impact of variable capacity strategies using key system parameters. We apply this model to real data from a university HPC cluster and assess how different scenarios could affect the cost-effectiveness of this approach in the future.",
        "authors": [
            "Peter Arzt",
            "Felix Wolf"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-09"
    },
    "http://arxiv.org/abs/2509.07506v2": {
        "id": "http://arxiv.org/abs/2509.07506v2",
        "title": "Astra: A Multi-Agent System for GPU Kernel Performance Optimization",
        "link": "http://arxiv.org/abs/2509.07506v2",
        "tags": [
            "serving",
            "kernel",
            "training"
        ],
        "relevant": true,
        "indexed_date": "2025-09-10",
        "tldr": "Proposes Astra, an LLM-based multi-agent system for GPU kernel optimization starting from existing CUDA implementations in SGLang. Agents collaboratively generate, test, profile, and refine kernels for higher efficiency. Achieves average 1.32x speedup on SGLang kernels.",
        "abstract": "GPU kernel optimization has long been a central challenge at the intersection of high-performance computing and machine learning. Efficient kernels are crucial for accelerating large language model (LLM) training and serving, yet attaining high performance typically requires extensive manual tuning. Compiler-based systems reduce some of this burden, but still demand substantial manual design and engineering effort. Recently, researchers have explored using LLMs for GPU kernel generation, though prior work has largely focused on translating high-level PyTorch modules into CUDA code. In this work, we introduce Astra, the first LLM-based multi-agent system for GPU kernel optimization. Unlike previous approaches, Astra starts from existing CUDA implementations extracted from SGLang, a widely deployed framework for serving LLMs, rather than treating PyTorch modules as the specification. Within Astra, specialized LLM agents collaborate through iterative code generation, testing, profiling, and planning to produce kernels that are both correct and high-performance. On kernels from SGLang, Astra achieves an average speedup of 1.32x using zero-shot prompting with OpenAI o4-mini. A detailed case study further demonstrates that LLMs can autonomously apply loop transformations, optimize memory access patterns, exploit CUDA intrinsics, and leverage fast math operations to yield substantial performance gains. Our work highlights multi-agent LLM systems as a promising new paradigm for GPU kernel optimization. Our code is publicly available at https://github.com/Anjiang-Wei/Astra.",
        "authors": [
            "Anjiang Wei",
            "Tianran Sun",
            "Yogesh Seenichamy",
            "Hang Song",
            "Anne Ouyang",
            "Azalia Mirhoseini",
            "Ke Wang",
            "Alex Aiken"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.CL",
            "cs.LG",
            "cs.SE"
        ],
        "submit_date": "2025-09-09"
    },
    "http://arxiv.org/abs/2509.07497v1": {
        "id": "http://arxiv.org/abs/2509.07497v1",
        "title": "DREAMS: Decentralized Resource Allocation and Service Management across the Compute Continuum Using Service Affinity",
        "link": "http://arxiv.org/abs/2509.07497v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-10",
        "tldr": "",
        "abstract": "Modern manufacturing systems require adaptive computing infrastructures that can respond to highly dynamic workloads and increasingly customized production demands. The compute continuum emerges as a promising solution, enabling flexible deployment of microservices across distributed, heterogeneous domains. However, this paradigm also requires a novel approach to resource allocation and service placement, as traditional centralized solutions struggle to scale effectively, suffer from latency bottlenecks, and introduce single points of failure. In this paper, we present DREAMS, a decentralized framework that optimizes microservice placement decisions collaboratively across different computational domains. At its core, DREAMS introduces agents that operate autonomously within each domain while coordinating globally through a Raft-based consensus algorithm and cost-benefit voting. This decentralized architecture enables responsive, privacy-preserving, and fault-tolerant coordination, making it particularly suitable given the growing prevalence of multi-stakeholder scenarios across the compute continuum. In particular, within modern manufacturing environments, DREAMS achieves globally optimized service placements while maintaining high fault tolerance. Further evaluations demonstrate that key coordination operations, such as Local Domain Manager (LDM) registration and migration voting, scale sub-linearly with the number of domains, confirming the efficiency and scalability of our proposal.",
        "authors": [
            "Hai Dinh-Tuan",
            "Tien Hung Nguyen",
            "Sanjeet Raj Pandey"
        ],
        "categories": [
            "cs.DC",
            "cs.PF"
        ],
        "submit_date": "2025-09-09"
    },
    "http://arxiv.org/abs/2509.07425v1": {
        "id": "http://arxiv.org/abs/2509.07425v1",
        "title": "Dependency-Aware Execution Mechanism in Hyperledger Fabric Architecture",
        "link": "http://arxiv.org/abs/2509.07425v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-10",
        "tldr": "",
        "abstract": "Hyperledger Fabric is a leading permissioned blockchain framework for enterprise use, known for its modular design and privacy features. While it strongly supports configurable consensus and access control, Fabric can face challenges in achieving high transaction throughput and low rejection rates under heavy workloads. These performance limitations are often attributed to endorsement, ordering, and validation bottlenecks. Further, optimistic concurrency control and deferred validation in Fabric may lead to resource inefficiencies and contention, as conflicting transactions are identified only during the commit phase. To address these challenges, we propose a dependency-aware execution model for Hyperledger Fabric. Our approach includes: (a) a dependency flagging system during endorsement, marking transactions as independent or dependent using a hashmap; (b) an optimized block construction in the ordering service that prioritizes independent transactions; (c) the incorporation of a Directed Acyclic Graph (DAG) within each block to represent dependencies; and (d) parallel execution of independent transactions at the committer, with dependent transactions processed according to DAG order. Incorporated in Hyperledger Fabric v2.5, our framework was tested on workloads with varying dependency levels and system loads. Results show up to 40% higher throughput and significantly reduced rejection rates in high-contention scenarios. This demonstrates that dependency-aware scheduling and DAG-based execution can substantially enhance Fabric's scalability while remaining compatible with its existing consensus and smart contract layers.",
        "authors": [
            "Sanyam Kaul",
            "Manaswini Piduguralla",
            "Gayathri Shreeya Patnala",
            "Sathya Peri"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-09"
    },
    "http://arxiv.org/abs/2509.07379v1": {
        "id": "http://arxiv.org/abs/2509.07379v1",
        "title": "DuoServe-MoE: Dual-Phase Expert Prefetch and Cache Scheduling for Efficient MoE LLM Inference",
        "link": "http://arxiv.org/abs/2509.07379v1",
        "tags": [
            "MoE",
            "offloading",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-09-10",
        "tldr": "Proposes DuoServe-MoE, a dual-phase GPU scheduling system for MoE LLM inference that separates prefill and decode stages. Uses asynchronous prefetching and layer-level prediction to manage expert weights, reducing latency by 1.42-7.54× and peak memory to 15% of full model size.",
        "abstract": "Large Language Models (LLMs) have demonstrated impressive performance across a wide range of deep learning tasks. Mixture of Experts (MoE) further enhances their capabilities by increasing model width through sparsely activated expert branches, which keeps inference computation efficient. However, the large number of expert weights introduces significant GPU memory pressure, especially in resource-constrained environments such as single-GPU servers. More importantly, MoE inference consists of two fundamentally different stages: a prefill stage where most experts are activated densely, and a decode stage where only a few experts are triggered sparsely. Treating these stages with a uniform scheduling strategy often leads to suboptimal latency and memory usage. To address this, we propose DuoServe-MoE, an inference serving system that explicitly separates prefill and decode stages and applies tailored expert scheduling strategies to each. In the prefill stage, DuoServe-MoE uses a two-stream CUDA pipeline that overlaps expert weight prefetching with the computation of non-MoE layers, limiting expert residency in GPU memory. In the decode stage, a lightweight layer-level predictor trained offline from activation traces is used to prefetch only the most likely activated experts, without requiring any changes to the model. Experiments on 4-bit Mixtral-8x7B and 8x22B models show that DuoServe-MoE improves end-to-end latency by 1.42 to 7.54 times while keeping peak memory usage at only 15 percent of the full model size.",
        "authors": [
            "Yuning Zhang",
            "Grant Pinkert",
            "Nan Yang",
            "Yanli Li",
            "Dong Yuan"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-09"
    },
    "http://arxiv.org/abs/2509.07378v2": {
        "id": "http://arxiv.org/abs/2509.07378v2",
        "title": "Optimizing Task Scheduling in Fog Computing with Deadline Awareness",
        "link": "http://arxiv.org/abs/2509.07378v2",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-10",
        "tldr": "",
        "abstract": "The rise of Internet of Things (IoT) devices has led to the development of numerous time-sensitive applications that require quick responses and low latency. Fog computing has emerged as a solution for processing these IoT applications, but it faces challenges such as resource allocation and job scheduling. Therefore, it is crucial to determine how to assign and schedule tasks on Fog nodes. This work aims to schedule tasks in IoT while minimizing the total energy consumption of nodes and enhancing the Quality of Service (QoS) requirements of IoT tasks, taking into account task deadlines. This paper classifies Fog nodes into two categories based on their traffic level: low and high. It schedules short-deadline tasks on low-traffic nodes using an Improved Golden Eagle Optimization (IGEO) algorithm, an enhancement that utilizes genetic operators for discretization. Long-deadline tasks are processed on high-traffic nodes using reinforcement learning (RL). This combined approach is called the Reinforcement Improved Golden Eagle Optimization (RIGEO) algorithm. Experimental results demonstrate that RIGEO achieves up to a 29% reduction in energy consumption, up to an 86% improvement in response time, and up to a 19% reduction in deadline violations compared to state-of-the-art algorithms.",
        "authors": [
            "Mohammad Sadegh Sirjani",
            "Mohammad Ahmad",
            "Somayeh Sobati-Moghadam"
        ],
        "categories": [
            "cs.DC",
            "cs.AR"
        ],
        "submit_date": "2025-09-09"
    },
    "http://arxiv.org/abs/2509.07199v1": {
        "id": "http://arxiv.org/abs/2509.07199v1",
        "title": "A Study on Messaging Trade-offs in Data Streaming for Scientific Workflows",
        "link": "http://arxiv.org/abs/2509.07199v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-10",
        "tldr": "",
        "abstract": "Memory-to-memory data streaming is essential for modern scientific workflows that require near real-time data analysis, experimental steering, and informed decision-making during experiment execution. It eliminates the latency bottlenecks associated with file-based transfers to parallel storage, enabling rapid data movement between experimental facilities and HPC systems. These tightly coupled experimental-HPC workflows demand low latency, high throughput, and reliable data delivery to support on-the-fly analysis and timely feedback for experimental control. Off-the-shelf messaging frameworks are increasingly considered viable solutions for enabling such direct memory streaming due to their maturity, broad adoption, and ability to abstract core messaging and reliability functionalities from the application layer. However, effectively meeting the workflows' requirements depends on utilizing the framework's capabilities and carefully tuning its configurations.   In this paper, we present a study that investigates the messaging parameters, and their configuration choices that impact the streaming requirements of two representative scientific workflows. We specifically characterize throughput trade-offs associated with reliable message transmission for these workflows. Our study is conducted through streaming simulations using synthetic workloads derived from the Deleria and LCLS workflows, employing the RabbitMQ messaging framework within the context of the Data Streaming to HPC infrastructure at OLCF. Our simulations reveal several key observations and practical insights that help users understand which configurations best meet the needs of their streaming workloads.",
        "authors": [
            "Anjus George",
            "Michael J. Brim",
            "Christopher Zimmer",
            "Tyler J. Skluzacek",
            "A. J. Ruckman",
            "Gustav R. Jansen",
            "Sarp Oral"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-08"
    },
    "http://arxiv.org/abs/2509.07158v1": {
        "id": "http://arxiv.org/abs/2509.07158v1",
        "title": "Bodega: Serving Linearizable Reads Locally from Anywhere at Anytime via Roster Leases",
        "link": "http://arxiv.org/abs/2509.07158v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-10",
        "tldr": "",
        "abstract": "We present Bodega, the first consensus protocol that serves linearizable reads locally from any desired node, regardless of interfering writes. Bodega achieves this via a novel roster leases algorithm that safeguards the roster, a new notion of cluster metadata. The roster is a generalization of leadership; it tracks arbitrary subsets of replicas as responder nodes for local reads. A consistent agreement on the roster is established through roster leases, an all-to-all leasing mechanism that generalizes existing all-to-one leasing approaches (Leader Leases, Quorum Leases), unlocking a new point in the protocol design space. Bodega further employs optimistic holding and early accept notifications to minimize interruption from interfering writes, and incorporates smart roster coverage and lightweight heartbeats to maximize practicality. Bodega is a non-intrusive extension to classic consensus; it imposes no special requirements on writes other than a responder-covering quorum. We implement Bodega and related works in Vineyard, a protocol-generic replicated key-value store written in async Rust. We compare it to previous protocols (Leader Leases, EPaxos, PQR, and Quorum Leases) and two production coordination services (etcd and ZooKeeper). Bodega speeds up average client read requests by 5.6x-13.1x on real WAN clusters versus previous approaches under moderate write interference, delivers comparable write performance, supports fast proactive roster changes as well as fault tolerance via leases, and closely matches the performance of sequentially-consistent etcd and ZooKeeper deployments across all YCSB workloads. We will open-source Vineyard upon publication.",
        "authors": [
            "Guanzhou Hu",
            "Andrea Arpaci-Dusseau",
            "Remzi Arpaci-Dusseau"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-08"
    },
    "http://arxiv.org/abs/2509.07157v1": {
        "id": "http://arxiv.org/abs/2509.07157v1",
        "title": "Crossword: Adaptive Consensus for Dynamic Data-Heavy Workloads",
        "link": "http://arxiv.org/abs/2509.07157v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-10",
        "tldr": "",
        "abstract": "We present Crossword, a flexible consensus protocol for dynamic data-heavy workloads, a rising challenge in the cloud where replication payload sizes span a wide spectrum and introduce sporadic bandwidth stress. Crossword applies per-instance erasure coding and distributes coded shards intelligently to reduce critical-path data transfer significantly when desirable. Unlike previous approaches that statically assign shards to servers, Crossword enables an adaptive tradeoff between the assignment of shards and quorum size in reaction to dynamic workloads and network conditions, while always retaining the availability guarantee of classic protocols. Crossword handles leader failover gracefully by employing a lazy follower gossiping mechanism that incurs minimal impact on critical-path performance. We implement Crossword (along with relevant protocols) in Gazette, a distributed, replicated, and protocol-generic key-value store written in async Rust. We evaluate Crossword comprehensively to show that it matches the best performance among previous protocols (MultiPaxos, Raft, RSPaxos, and CRaft) in static scenarios, and outperforms them by up to 2.3x under dynamic workloads and network conditions. Our integration of Crossword with CockroachDB brings 1.32x higher aggregate throughput to TPC-C under 5-way replication. We will open-source Gazette upon publication.",
        "authors": [
            "Guanzhou Hu",
            "Yiwei Chen",
            "Andrea Arpaci-Dusseau",
            "Remzi Arpaci-Dusseau"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-08"
    },
    "http://arxiv.org/abs/2509.07727v1": {
        "id": "http://arxiv.org/abs/2509.07727v1",
        "title": "MoE-Compression: How the Compression Error of Experts Affects the Inference Accuracy of MoE Model?",
        "link": "http://arxiv.org/abs/2509.07727v1",
        "tags": [
            "MoE",
            "offloading",
            "compression"
        ],
        "relevant": true,
        "indexed_date": "2025-09-10",
        "tldr": "Investigates how expert compression in MoE model offloading affects inference accuracy. Proposes error-bounded lossy compression for non-activated experts to reduce data transfer, analyzing layer-wise error impact. Shows deep-layer compression can improve accuracy while middle-layer errors degrade it.",
        "abstract": "With the widespread application of Mixture of Experts (MoE) reasoning models in the field of LLM learning, efficiently serving MoE models under limited GPU memory constraints has emerged as a significant challenge. Offloading the non-activated experts to main memory has been identified as an efficient approach to address such a problem, while it brings the challenges of transferring the expert between the GPU memory and main memory. We need to explore an efficient approach to compress the expert and analyze how the compression error affects the inference performance.   To bridge this gap, we propose employing error-bounded lossy compression algorithms (such as SZ3 and CuSZp) to compress non-activated experts, thereby reducing data transfer overhead during MoE inference. We conduct extensive experiments across various benchmarks and present a comprehensive analysis of how compression-induced errors in different experts affect overall inference accuracy. The results indicate that experts in the shallow layers, which are primarily responsible for the attention mechanism and the transformation of input tokens into vector representations, exhibit minimal degradation in inference accuracy when subjected to bounded errors. In contrast, errors in the middle-layer experts, which are central to model reasoning, significantly impair inference accuracy. Interestingly, introducing bounded errors in the deep-layer experts, which are mainly responsible for instruction following and output integration, can sometimes lead to improvements in inference accuracy.",
        "authors": [
            "Songkai Ma",
            "Zhaorui Zhang",
            "Sheng Di",
            "Benben Liu",
            "Xiaodong Yu",
            "Xiaoyi Lu",
            "Dan Wang"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "submit_date": "2025-09-09"
    },
    "http://arxiv.org/abs/2509.07690v4": {
        "id": "http://arxiv.org/abs/2509.07690v4",
        "title": "HYLU: Hybrid Parallel Sparse LU Factorization",
        "link": "http://arxiv.org/abs/2509.07690v4",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-10",
        "tldr": "",
        "abstract": "This article introduces HYLU, a hybrid parallel LU factorization-based general-purpose solver designed for efficiently solving sparse linear systems (Ax=b) on multi-core shared-memory architectures. The key technical feature of HYLU is the integration of hybrid numerical kernels so that it can adapt to various sparsity patterns of coefficient matrices. Tests on 34 sparse matrices from SuiteSparse Matrix Collection reveal that HYLU outperforms Intel MKL PARDISO in the numerical factorization phase by geometric means of 1.95X (for one-time solving) and 2.40X (for repeated solving). HYLU can be downloaded from https://github.com/chenxm1986/hylu.",
        "authors": [
            "Xiaoming Chen"
        ],
        "categories": [
            "cs.AR",
            "cs.DC",
            "cs.MS",
            "math.NA"
        ],
        "submit_date": "2025-09-09"
    },
    "http://arxiv.org/abs/2509.07003v1": {
        "id": "http://arxiv.org/abs/2509.07003v1",
        "title": "veScale: Consistent and Efficient Tensor Programming with Eager-Mode SPMD",
        "link": "http://arxiv.org/abs/2509.07003v1",
        "tags": [
            "training",
            "kernel",
            "sparse"
        ],
        "relevant": true,
        "indexed_date": "2025-09-10",
        "tldr": "Proposes veScale, an eager-mode SPMD training system for LLMs that ensures consistency with single-device execution and optimizes performance. Introduces distributed RNG for correctness and reduces PyTorch overhead for efficiency. Achieves 2.2× speedup over TorchTitan with 78.4% less code complexity.",
        "abstract": "Large Language Models (LLMs) have scaled rapidly in size and complexity, requiring increasingly intricate parallelism for distributed training, such as 3D parallelism. This sophistication motivates a shift toward simpler, more debuggable programming paradigm like Single Program Multiple Data (SPMD). However, SPMD in eager execution introduces two key challenges: ensuring consistency with single-device execution and achieving high performance at scale. In this paper, we introduce veScale, an eager-mode training system that fully embraces SPMD paradigm to democratize distributed tensor programming. veScale addresses the prevalent issue of inconsistent results in systems like PyTorch by introducing a novel algorithm of distributed Random Number Generation (RNG) compatible with arbitrary sharded operators. veScale also significantly boosts training performance by reducing PyTorch primitive's overhead and improving communication efficiency. Evaluations show that veScale delivers up to 2.2x speedup over the state-of-the-art training systems, like TorchTitan, and cuts code complexity by 78.4%, while preserving single-device-equivalent results.",
        "authors": [
            "Youjie Li",
            "Cheng Wan",
            "Zhiqi Lin",
            "Hongyu Zhu",
            "Jiacheng Yang",
            "Ziang Song",
            "Xinyi Di",
            "Jiawei Wu",
            "Huiyao Shu",
            "Wenlei Bao",
            "Yanghua Peng",
            "Haibin Lin",
            "Li-Wen Chang"
        ],
        "categories": [
            "cs.PL",
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-09-05"
    },
    "http://arxiv.org/abs/2509.06514v2": {
        "id": "http://arxiv.org/abs/2509.06514v2",
        "title": "IM-PIR: In-Memory Private Information Retrieval",
        "link": "http://arxiv.org/abs/2509.06514v2",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-09",
        "tldr": "",
        "abstract": "Private information retrieval (PIR) is a cryptographic primitive that allows a client to securely query one or multiple servers without revealing their specific interests. In spite of their strong security guarantees, current PIR constructions are computationally costly. Specifically, most PIR implementations are memory-bound due to the need to scan extensive databases (in the order of GB), making them inherently constrained by the limited memory bandwidth in traditional processor-centric computing architectures. Processing-in-memory (PIM) is an emerging computing paradigm that augments memory with compute capabilities, addressing the memory bandwidth bottleneck while simultaneously providing extensive parallelism. Recent research has demonstrated PIM's potential to significantly improve performance across a range of data-intensive workloads, including graph processing, genome analysis, and machine learning.   In this work, we propose the first PIM-based architecture for multi-server PIR. We discuss the algorithmic foundations of the latter and show how its operations align with the core strengths of PIM architectures: extensive parallelism and high memory bandwidth. Based on this observation, we design and implement IM-PIR, a PIM-based multi-server PIR approach on top of UPMEM PIM, the first openly commercialized PIM architecture. Our evaluation demonstrates that a PIM-based multi-server PIR implementation significantly improves query throughput by more than 3.7x when compared to a standard CPU-based PIR approach.",
        "authors": [
            "Mpoki Mwaisela",
            "Peterson Yuhala",
            "Pascal Felber",
            "Valerio Schiavoni"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-08"
    },
    "http://arxiv.org/abs/2509.06362v1": {
        "id": "http://arxiv.org/abs/2509.06362v1",
        "title": "MaaSO: SLO-aware Orchestration of Heterogeneous Model Instances for MaaS",
        "link": "http://arxiv.org/abs/2509.06362v1",
        "tags": [
            "serving",
            "autoscaling"
        ],
        "relevant": true,
        "indexed_date": "2025-09-09",
        "tldr": "Addresses SLO-aware orchestration for heterogeneous LLM instances in MaaS. Proposes MaaSO with profiler, placer, and distributor modules to optimize instance configurations and request distribution. Improves SLO satisfaction by 15-30% and reduces latency by 40-60%.",
        "abstract": "Model-as-a-Service (MaaS) platforms face diverse Service Level Objective (SLO) requirements stemming from various large language model (LLM) applications, manifested in contextual complexity, first-token latency, and between-token latency. On the other hand, an LLM instance, when configured with different parallelism strategies and inference batch sizes, exhibits distinct performance characteristics and can thus be used to serve different SLO requirements. However, current LLM inference systems typically deploy instances of the same model with identical configurations, lacking mechanisms to leverage such heterogeneity. To fill this research gap, we propose MaaSO, the first MaaS Orchestrator, which comprises three modules: (1) a profiler characterizing instance performance under diverse parallelism strategies and inference batch sizes; (2) a placer optimizing heterogeneous instance configurations; (3) a distributor enabling SLO-aware request distribution and preventing cascaded timeouts in continuous batching. Experiments show that MaaSO improves the SLO satisfaction ratio by 15 to 30% and reduces response latency by 40 to 60% compared to existing approaches, and significantly lowers overall orchestration overhead.",
        "authors": [
            "Mo Xuan",
            "Zhang yue",
            "Wu Weigang"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-08"
    },
    "http://arxiv.org/abs/2509.06261v2": {
        "id": "http://arxiv.org/abs/2509.06261v2",
        "title": "FineServe: Precision-Aware KV Slab and Two-Level Scheduling for Heterogeneous Precision LLM Serving",
        "link": "http://arxiv.org/abs/2509.06261v2",
        "tags": [
            "serving",
            "quantization",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-09-09",
        "tldr": "Proposes FineServe for efficient serving of mixed-precision LLMs. Introduces KV Slab for adaptive KV cache management and a two-level scheduler for placement and batch adjustments. Achieves 2.2x higher SLO attainment and 1.8x higher throughput compared to state-of-the-art.",
        "abstract": "Recent advances in Post-Training Quantization (PTQ) techniques have significantly increased demand for serving quantized large language models (LLMs), enabling higher throughput and substantially reduced memory usage with minimal accuracy loss. Quantized models address memory constraints in LLMs and enhance GPU resource utilization through efficient GPU sharing. However, quantized models have smaller KV block sizes than non-quantized models, causing limited memory efficiency due to memory fragmentation. Also, distinct resource usage patterns between quantized and non-quantized models require efficient scheduling to maximize throughput. To address these challenges, we propose FineServe, an inference serving framework for mixed-precision LLMs. FineServe's key contributions include: (1) KV Slab, a precision-aware adaptive memory management technique dynamically allocating KV cache based on model quantization characteristics, significantly reducing GPU memory fragmentation, and (2) a two-level scheduling framework comprising a global scheduler that places models to GPUs based on request rates, latency SLOs, and memory constraints and efficiency, and a local scheduler that adaptively adjusts batch sizes according to real-time request fluctuations. Experimental results demonstrate that FineServe achieves up to 2.2x higher SLO attainment and 1.8x higher token generation throughput compared to the state-of-the-art GPU sharing systems.",
        "authors": [
            "Kyungmin Bin",
            "Seungbeom Choi",
            "Jimyoung Son",
            "Jieun Choi",
            "Daseul Bae",
            "Daehyeon Baek",
            "Kihyo Moon",
            "Minsung Jang",
            "Hyojung Lee"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-09-08"
    },
    "http://arxiv.org/abs/2509.06229v1": {
        "id": "http://arxiv.org/abs/2509.06229v1",
        "title": "20 Years in Life of a Smart Building: A retrospective",
        "link": "http://arxiv.org/abs/2509.06229v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-09",
        "tldr": "",
        "abstract": "Operating an intelligent smart building automation system in 2025 is met with many challenges: hardware failures, vendor obsolescence, evolving security threats and more. None of these have been comprehensibly addressed by the industrial building nor home automation industries, limiting feasibility of operating large, truly smart automation deployments. This paper introduces KaOS, a distributed control platform for constructing robust and evolvable smart building automation systems using affordable, off-the-shelf IoT hardware. Supporting control applications and distributed system operations by leveraging containerisation and managed resource access, KaOS seeks to achieve flexibility, security, and fault tolerance without sacrificing cost-effectiveness. Initial evaluation confirms the practical feasibility of our approach, highlighting its potential to sustainably maintain and incrementally evolve building control functionalities over extended timeframes.",
        "authors": [
            "Karolina Skrivankova",
            "Mark Handley",
            "Stephen Hailes"
        ],
        "categories": [
            "cs.DC",
            "eess.SY"
        ],
        "submit_date": "2025-09-07"
    },
    "http://arxiv.org/abs/2509.06064v1": {
        "id": "http://arxiv.org/abs/2509.06064v1",
        "title": "Gathering in Non-Vertex-Transitive Graphs Under Round Robin",
        "link": "http://arxiv.org/abs/2509.06064v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-09",
        "tldr": "",
        "abstract": "The Gathering problem for a swarm of robots asks for a distributed algorithm that brings such entities to a common place, not known in advance. We consider the well-known OBLOT model with robots constrained to move along the edges of a graph, hence gathering in one vertex, eventually. Despite the classical setting under which the problem has been usually approached, we consider the `hostile' case where: i) the initial configuration may contain multiplicities, i.e. more than one robot may occupy the same vertex; ii) robots cannot detect multiplicities. As a scheduler for robot activation, we consider the \"favorable\" round-robin case, where robots are activated one at a time.   Our objective is to achieve a complete characterization of the problem in the broad context of non-vertex-transitive graphs, i.e., graphs where the vertices are partitioned into at least two different classes of equivalence. We provide a resolution algorithm for any configuration of robots moving on such graphs, along with its correctness. Furthermore, we analyze its time complexity.",
        "authors": [
            "Serafino Cicerone",
            "Alessia Di Fonso",
            "Gabriele Di Stefano",
            "Alfredo Navarra"
        ],
        "categories": [
            "cs.DC",
            "math.CO"
        ],
        "submit_date": "2025-09-07"
    },
    "http://arxiv.org/abs/2509.06046v1": {
        "id": "http://arxiv.org/abs/2509.06046v1",
        "title": "DISTRIBUTEDANN: Efficient Scaling of a Single DISKANN Graph Across Thousands of Computers",
        "link": "http://arxiv.org/abs/2509.06046v1",
        "tags": [
            "serving",
            "offline",
            "storage"
        ],
        "relevant": true,
        "indexed_date": "2025-09-09",
        "tldr": "Proposes DISTRIBUTEDANN for scaling large vector indexes across thousands of machines. Combines distributed KV store and in-memory ANN index for efficient query routing. Achieves 26ms median latency and 100k QPS, 6x more efficient than conventional approaches.",
        "abstract": "We present DISTRIBUTEDANN, a distributed vector search service that makes it possible to search over a single 50 billion vector graph index spread across over a thousand machines that offers 26ms median query latency and processes over 100,000 queries per second. This is 6x more efficient than existing partitioning and routing strategies that route the vector query to a subset of partitions in a scale out vector search system. DISTRIBUTEDANN is built using two well-understood components: a distributed key-value store and an in-memory ANN index. DISTRIBUTEDANN has replaced conventional scale-out architectures for serving the Bing search engine, and we share our experience from making this transition.",
        "authors": [
            "Philip Adams",
            "Menghao Li",
            "Shi Zhang",
            "Li Tan",
            "Qi Chen",
            "Mingqin Li",
            "Zengzhong Li",
            "Knut Risvik",
            "Harsha Vardhan Simhadri"
        ],
        "categories": [
            "cs.DC",
            "cs.DS",
            "cs.IR"
        ],
        "submit_date": "2025-09-07"
    },
    "http://arxiv.org/abs/2509.05870v1": {
        "id": "http://arxiv.org/abs/2509.05870v1",
        "title": "A Simple and Robust Protocol for Distributed Counting",
        "link": "http://arxiv.org/abs/2509.05870v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-09",
        "tldr": "",
        "abstract": "We revisit the distributed counting problem, where a server must continuously approximate the total number of events occurring across $k$ sites while minimizing communication. The communication complexity of this problem is known to be $Θ(\\frac{k}ε\\log N)$ for deterministic protocols. Huang, Yi, and Zhang (2012) showed that randomization can reduce this to $Θ(\\frac{\\sqrt{k}}ε\\log N)$, but their analysis is restricted to the {\\em oblivious setting}, where the stream of events is independent of the protocol's outputs.   Xiong, Zhu, and Huang (2023) presented a robust protocol for distributed counting that removes the oblivious assumption. However, their communication complexity is suboptimal by a $polylog(k)$ factor and their protocol is substantially more complex than the oblivious protocol of Huang et al. (2012). This left open a natural question: could it be that the simple protocol of Huang et al. (2012) is already robust?   We resolve this question with two main contributions. First, we show that the protocol of Huang et al. (2012) is itself not robust by constructing an explicit adaptive attack that forces it to lose its accuracy. Second, we present a new, surprisingly simple, robust protocol for distributed counting that achieves the optimal communication complexity of $O(\\frac{\\sqrt{k}}ε \\log N)$. Our protocol is simpler than that of Xiong et al. (2023), perhaps even simpler than that of Huang et al. (2012), and is the first to match the optimal oblivious complexity in the adaptive setting.",
        "authors": [
            "Edith Cohen",
            "Moshe Shechner",
            "Uri Stemmer"
        ],
        "categories": [
            "cs.DC",
            "cs.DS"
        ],
        "submit_date": "2025-09-06"
    },
    "http://arxiv.org/abs/2509.05303v1": {
        "id": "http://arxiv.org/abs/2509.05303v1",
        "title": "Multi-IaC-Eval: Benchmarking Cloud Infrastructure as Code Across Multiple Formats",
        "link": "http://arxiv.org/abs/2509.05303v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-09",
        "tldr": "",
        "abstract": "Infrastructure as Code (IaC) is fundamental to modern cloud computing, enabling teams to define and manage infrastructure through machine-readable configuration files. However, different cloud service providers utilize diverse IaC formats. The lack of a standardized format requires cloud architects to be proficient in multiple IaC languages, adding complexity to cloud deployment. While Large Language Models (LLMs) show promise in automating IaC creation and maintenance, progress has been limited by the lack of comprehensive benchmarks across multiple IaC formats. We present Multi-IaC-Bench, a novel benchmark dataset for evaluating LLM-based IaC generation and mutation across AWS CloudFormation, Terraform, and Cloud Development Kit (CDK) formats. The dataset consists of triplets containing initial IaC templates, natural language modification requests, and corresponding updated templates, created through a synthetic data generation pipeline with rigorous validation. We evaluate several state-of-the-art LLMs on Multi-IaC-Bench, demonstrating that while modern LLMs can achieve high success rates (>95%) in generating syntactically valid IaC across formats, significant challenges remain in semantic alignment and handling complex infrastructure patterns. Our ablation studies highlight the importance of prompt engineering and retry mechanisms in successful IaC generation. We release Multi-IaC-Bench to facilitate further research in AI-assisted infrastructure management and establish standardized evaluation metrics for this crucial domain.",
        "authors": [
            "Sam Davidson",
            "Li Sun",
            "Bhavana Bhasker",
            "Laurent Callot",
            "Anoop Deoras"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-08-21"
    },
    "http://arxiv.org/abs/2509.06588v1": {
        "id": "http://arxiv.org/abs/2509.06588v1",
        "title": "Distributed Automatic Generation Control subject to Ramp-Rate-Limits: Anytime Feasibility and Uniform Network-Connectivity",
        "link": "http://arxiv.org/abs/2509.06588v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-09",
        "tldr": "",
        "abstract": "This paper considers automatic generation control over an information-sharing network of communicating generators as a multi-agent system. The optimization solution is distributed among the agents based on information consensus algorithms, while addressing the generators' ramp-rate-limits (RRL). This is typically ignored in the existing linear/nonlinear optimization solutions but they exist in real-time power generation scenarios. Without addressing the RRL, the generators cannot follow the assigned rate of generating power by the optimization algorithm; therefore, the existing solutions may not necessarily converge to the exact optimal cost or may lose feasibility in practice. The proposed solution in this work addresses the ramp-rate-limit constraint along with the box constraint (limits on the generated powers) and the coupling-constraint (generation-demand balance) at all iteration times of the algorithm. The latter is referred to as the anytime feasibility and implies that at every termination point of the algorithm, the balance between the demand and generated power holds. To improve the convergence rate of the algorithm we further consider internal signum-based nonlinearity. We also show that our solution can tolerate communication link removal. This follows from the uniform-connectivity assumption on the communication network.",
        "authors": [
            "Mohammadreza Doostmohammadian",
            "Hamid R. Rabiee"
        ],
        "categories": [
            "eess.SY",
            "cs.DC",
            "eess.SP",
            "math.OC"
        ],
        "submit_date": "2025-09-08"
    },
    "http://arxiv.org/abs/2509.06552v1": {
        "id": "http://arxiv.org/abs/2509.06552v1",
        "title": "Tackling Device Data Distribution Real-time Shift via Prototype-based Parameter Editing",
        "link": "http://arxiv.org/abs/2509.06552v1",
        "tags": [
            "offline",
            "edge",
            "recommendation"
        ],
        "relevant": true,
        "indexed_date": "2025-09-09",
        "tldr": "Proposes Persona, a prototype-based parameter editing framework for on-device models to adapt to real-time data distribution shifts without retraining. Uses a cloud-based neural adapter to generate editing matrices that cluster on-device models into dynamically refined prototypes. Achieves improved generalization on vision and recommendation tasks across multiple datasets.",
        "abstract": "The on-device real-time data distribution shift on devices challenges the generalization of lightweight on-device models. This critical issue is often overlooked in current research, which predominantly relies on data-intensive and computationally expensive fine-tuning approaches. To tackle this, we introduce Persona, a novel personalized method using a prototype-based, backpropagation-free parameter editing framework to enhance model generalization without post-deployment retraining. Persona employs a neural adapter in the cloud to generate a parameter editing matrix based on real-time device data. This matrix adeptly adapts on-device models to the prevailing data distributions, efficiently clustering them into prototype models. The prototypes are dynamically refined via the parameter editing matrix, facilitating efficient evolution. Furthermore, the integration of cross-layer knowledge transfer ensures consistent and context-aware multi-layer parameter changes and prototype assignment. Extensive experiments on vision task and recommendation task on multiple datasets confirm Persona's effectiveness and generality.",
        "authors": [
            "Zheqi Lv",
            "Wenqiao Zhang",
            "Kairui Fu",
            "Qi Tian",
            "Shengyu Zhang",
            "Jiajie Su",
            "Jingyuan Chen",
            "Kun Kuang",
            "Fei Wu"
        ],
        "categories": [
            "cs.LG",
            "cs.CV",
            "cs.DC",
            "cs.IR"
        ],
        "submit_date": "2025-09-08"
    },
    "http://arxiv.org/abs/2509.06466v1": {
        "id": "http://arxiv.org/abs/2509.06466v1",
        "title": "Several Performance Bounds on Decentralized Online Optimization are Highly Conservative and Potentially Misleading",
        "link": "http://arxiv.org/abs/2509.06466v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-09",
        "tldr": "",
        "abstract": "We analyze Decentralized Online Optimization algorithms using the Performance Estimation Problem approach which allows, to automatically compute exact worst-case performance of optimization algorithms. Our analysis shows that several available performance guarantees are very conservative, sometimes by multiple orders of magnitude, and can lead to misguided choices of algorithm. Moreover, at least in terms of worst-case performance, some algorithms appear not to benefit from inter-agent communications for a significant period of time. We show how to improve classical methods by tuning their step-sizes, and find that we can save up to 20% on their actual worst-case performance regret.",
        "authors": [
            "Erwan Meunier",
            "Julien M. Hendrickx"
        ],
        "categories": [
            "math.OC",
            "cs.AI",
            "cs.DC",
            "cs.MA"
        ],
        "submit_date": "2025-09-08"
    },
    "http://arxiv.org/abs/2509.06163v1": {
        "id": "http://arxiv.org/abs/2509.06163v1",
        "title": "Social Dynamics of DAOs: Power, Onboarding, and Inclusivity",
        "link": "http://arxiv.org/abs/2509.06163v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-09",
        "tldr": "",
        "abstract": "This report explores the often-overlooked cultural and social dynamics shaping participation and power in DAOs. Drawing on qualitative interviews and ethnographic observations, it shows how factors such as financial privilege, informal gatekeeping, visibility bias, and onboarding structures create barriers to meaningful inclusion. While DAOs are frequently framed as permissionless and egalitarian, the lived experiences of contributors reveal a more complex reality, one in which soft power and implicit norms determine people's position within DAOs. Instead of offering solutionist prescriptions, this report argues for a deeper cultural reflection within the DAO ecosystem. It highlights that decentralisation is not solely a protocol-level feature, but an ongoing social process that requires intentional cultivation of trust, belonging, and epistemic plurality. With this report, we want to sharpen the collective awareness of structural blind spots and call for building more inclusive and culturally conscious decentralised systems.",
        "authors": [
            "Victoria Kozlova",
            "Ben Biedermann"
        ],
        "categories": [
            "cs.CY",
            "cs.DC"
        ],
        "submit_date": "2025-09-07"
    },
    "http://arxiv.org/abs/2509.05884v1": {
        "id": "http://arxiv.org/abs/2509.05884v1",
        "title": "Introduction to Number Theoretic Transform",
        "link": "http://arxiv.org/abs/2509.05884v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-09",
        "tldr": "",
        "abstract": "The Number Theoretic Transform (NTT) can be regarded as a variant of the Discrete Fourier Transform. NTT has been quite a powerful mathematical tool in developing Post-Quantum Cryptography and Homomorphic Encryption. The Fourier Transform essentially decomposes a signal into its frequencies. They are traditionally sine or cosine waves. NTT works more over groups or finite fields rather than on a continuous signal and polynomials work as the analog of sine waves in case of NTT. Fast Fourier Trnasform (FFT) style NTT or fast NTT has been proven to be useful in lattice-based cryptography due to its ability to reduce the complexity of polynomial multiplication from quadratic to quasilinear. We have introduced the concepts of cyclic, negacyclic convolutions along with NTT and its inverse and their fast versions.",
        "authors": [
            "Banhirup Sengupta",
            "Peenal Gupta",
            "Souvik Sengupta"
        ],
        "categories": [
            "cs.CR",
            "cs.DC"
        ],
        "submit_date": "2025-09-07"
    },
    "http://arxiv.org/abs/2509.05759v1": {
        "id": "http://arxiv.org/abs/2509.05759v1",
        "title": "Tiga: Accelerating Geo-Distributed Transactions with Synchronized Clocks [Technical Report]",
        "link": "http://arxiv.org/abs/2509.05759v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-09",
        "tldr": "",
        "abstract": "This paper presents Tiga, a new design for geo-replicated and scalable transactional databases such as Google Spanner. Tiga aims to commit transactions within 1 wide-area roundtrip time, or 1 WRTT, for a wide range of scenarios, while maintaining high throughput with minimal computational overhead. Tiga consolidates concurrency control and consensus, completing both strictly serializable execution and consistent replication in a single round. It uses synchronized clocks to proactively order transactions by assigning each a future timestamp at submission. In most cases, transactions arrive at servers before their future timestamps and are serialized according to the designated timestamp, requiring 1 WRTT to commit. In rare cases, transactions are delayed and proactive ordering fails, in which case Tiga falls back to a slow path, committing in 1.5--2 WRTTs. Compared to state-of-the-art solutions, Tiga can commit more transactions at 1-WRTT latency, and incurs much less throughput overhead. Evaluation results show that Tiga outperforms all baselines, achieving 1.3--7.2$\\times$ higher throughput and 1.4--4.6$\\times$ lower latency. Tiga is open-sourced at https://github.com/New-Consensus-Concurrency-Control/Tiga.",
        "authors": [
            "Jinkun Geng",
            "Shuai Mu",
            "Anirudh Sivaraman",
            "Balaji Prabhakar"
        ],
        "categories": [
            "cs.NI",
            "cs.DB",
            "cs.DC"
        ],
        "submit_date": "2025-09-06"
    },
    "http://arxiv.org/abs/2509.05679v1": {
        "id": "http://arxiv.org/abs/2509.05679v1",
        "title": "Distributed Deep Learning using Stochastic Gradient Staleness",
        "link": "http://arxiv.org/abs/2509.05679v1",
        "tags": [
            "training"
        ],
        "relevant": true,
        "indexed_date": "2025-09-09",
        "tldr": "Proposes a distributed training method combining data parallelism and fully decoupled parallel backpropagation to accelerate deep learning. Mitigates locking issues and processes more data per iteration, enhancing training efficiency. Achieves convergence to critical points and demonstrates effectiveness on CIFAR-10 classification tasks.",
        "abstract": "Despite the notable success of deep neural networks (DNNs) in solving complex tasks, the training process still remains considerable challenges. A primary obstacle is the substantial time required for training, particularly as high performing DNNs tend to become increasingly deep (characterized by a larger number of hidden layers) and require extensive training datasets. To address these challenges, this paper introduces a distributed training method that integrates two prominent strategies for accelerating deep learning: data parallelism and fully decoupled parallel backpropagation algorithm. By utilizing multiple computational units operating in parallel, the proposed approach enhances the amount of training data processed in each iteration while mitigating locking issues commonly associated with the backpropagation algorithm. These features collectively contribute to significant improvements in training efficiency. The proposed distributed training method is rigorously proven to converge to critical points under certain conditions. Its effectiveness is further demonstrated through empirical evaluations, wherein an DNN is trained to perform classification tasks on the CIFAR-10 dataset.",
        "authors": [
            "Viet Hoang Pham",
            "Hyo-Sung Ahn"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "submit_date": "2025-09-06"
    },
    "http://arxiv.org/abs/2509.05675v1": {
        "id": "http://arxiv.org/abs/2509.05675v1",
        "title": "Workflow for High-Fidelity Dynamic Analysis of Structures with Pile Foundation",
        "link": "http://arxiv.org/abs/2509.05675v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-09",
        "tldr": "",
        "abstract": "The demand for high-fidelity numerical simulations in soil-structure interaction analysis is on the rise, yet a standardized workflow to guide the creation of such simulations remains elusive. This paper aims to bridge this gap by presenting a step-by-step guideline proposing a workflow for dynamic analysis of structures with pile foundations. The proposed workflow encompasses instructions on how to use Domain Reduction Method for loading, Perfectly Matched Layer elements for wave absorption, soil-structure interaction modeling using Embedded interface elements, and domain decomposition for efficient use of processing units. Through a series of numerical simulations, we showcase the practical application of this workflow. Our results reveal the efficacy of the Domain Reduction Method in reducing simulation size without compromising model fidelity, show the precision of Perfectly Matched Layer elements in modeling infinite domains, highlight the efficiency of Embedded Interface elements in establishing connections between structures and the soil domain, and demonstrate the overall effectiveness of the proposed workflow in conducting high-fidelity simulations. While our study focuses on simplified geometries and loading scenarios, it serves as a foundational framework for future research endeavors aimed at exploring more intricate structural configurations and dynamic loading conditions",
        "authors": [
            "Amin Pakzad",
            "Pedro Arduino",
            "Wenyang Zhang",
            "Ertugrul Tacirouglu"
        ],
        "categories": [
            "math.NA",
            "cs.DC"
        ],
        "submit_date": "2025-09-06"
    },
    "http://arxiv.org/abs/2509.05511v1": {
        "id": "http://arxiv.org/abs/2509.05511v1",
        "title": "Efficient Fault Localization in a Cloud Stack Using End-to-End Application Service Topology",
        "link": "http://arxiv.org/abs/2509.05511v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-09",
        "tldr": "",
        "abstract": "Cloud application services are distributed in nature and have components across the stack working together to deliver the experience to end users. The wide adoption of microservice architecture exacerbates failure management due to increased service components. To be effective, the strategies to enhance the application service resilience need to be autonomous and developed at the service's granularity, considering its end-to-end components. However, the massive amount of observability data generated by all these components across the service stack poses a significant challenge in reacting to anomalies and restoring the service quality in real time. Identifying the most informative observability data from across the cloud service stack and timely localization of root causes of anomalies thus becomes crucial to ensure service resilience. This article presents a novel approach that considers the application service topology to select the most informative metrics across the cloud stack to support efficient, explainable, and accurate root cause identifications in case of performance anomalies. The usefulness of the selected metrics is then evaluated using the state-of-the-art Root Cause Detection (RCD) algorithm for localizing the root cause of performance anomalies. As a step towards improving the accuracy and efficiency of RCD, this article then proposes the Topology-Aware-RCD (TA-RCD) that incorporates the end-to-end application service topology in RCD. The evaluation of the failure injection studies shows that the proposed approach performs at least 2X times better on average than the state-of-the-art RCD algorithm regarding Top-3 and Top-5 recall.",
        "authors": [
            "Dhanya R Mathews",
            "Mudit Verma",
            "Pooja Aggarwal",
            "J. Lakshmi"
        ],
        "categories": [
            "cs.PF",
            "cs.DC"
        ],
        "submit_date": "2025-09-05"
    },
    "http://arxiv.org/abs/2509.05488v1": {
        "id": "http://arxiv.org/abs/2509.05488v1",
        "title": "MambaLite-Micro: Memory-Optimized Mamba Inference on MCUs",
        "link": "http://arxiv.org/abs/2509.05488v1",
        "tags": [
            "edge",
            "kernel",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-09-09",
        "tldr": "Introduces MambaLite-Micro, a memory-optimized inference engine for Mamba models on MCUs, leveraging operator fusion and memory layout optimization. Achieves 83% peak memory reduction while maintaining numerical accuracy comparable to PyTorch.",
        "abstract": "Deploying Mamba models on microcontrollers (MCUs) remains challenging due to limited memory, the lack of native operator support, and the absence of embedded-friendly toolchains. We present, to our knowledge, the first deployment of a Mamba-based neural architecture on a resource-constrained MCU, a fully C-based runtime-free inference engine: MambaLite-Micro. Our pipeline maps a trained PyTorch Mamba model to on-device execution by (1) exporting model weights into a lightweight format, and (2) implementing a handcrafted Mamba layer and supporting operators in C with operator fusion and memory layout optimization. MambaLite-Micro eliminates large intermediate tensors, reducing 83.0% peak memory, while maintaining an average numerical error of only 1.7x10-5 relative to the PyTorch Mamba implementation. When evaluated on keyword spotting(KWS) and human activity recognition (HAR) tasks, MambaLite-Micro achieved 100% consistency with the PyTorch baselines, fully preserving classification accuracy. We further validated portability by deploying on both ESP32S3 and STM32H7 microcontrollers, demonstrating consistent operation across heterogeneous embedded platforms and paving the way for bringing advanced sequence models like Mamba to real-world resource-constrained applications.",
        "authors": [
            "Hongjun Xu",
            "Junxi Xia",
            "Weisi Yang",
            "Yueyuan Sui",
            "Stephen Xia"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.OS"
        ],
        "submit_date": "2025-09-05"
    },
    "http://arxiv.org/abs/2509.05258v2": {
        "id": "http://arxiv.org/abs/2509.05258v2",
        "title": "Scaling Performance of Large Language Model Pretraining",
        "link": "http://arxiv.org/abs/2509.05258v2",
        "tags": [
            "training",
            "scaling"
        ],
        "relevant": true,
        "indexed_date": "2025-09-08",
        "tldr": "Explores how to scale LLM pretraining efficiently. Focuses on distributed training, large dataset management across nodes, and optimizing data parallelism for full GPU utilization. Reports performance gains in compute efficiency and resource scaling.",
        "abstract": "Large language models (LLMs) show best-in-class performance across a wide range of natural language processing applications. Training these models is an extremely computationally expensive task; frontier Artificial Intelligence (AI) research companies are investing billions of dollars into supercomputing infrastructure to train progressively larger models on increasingly massive datasets. Unfortunately, very little information about the scaling performance and training considerations of these large training pipelines is released publicly. Working with very large datasets and models can be complex and practical recommendations are scarce in the public literature for tuning training performance when scaling up large language models. In this paper, we aim to demystify the large language model pretraining pipeline somewhat - in particular with respect to distributed training, managing large datasets across hundreds of nodes, and scaling up data parallelism with an emphasis on fully leveraging available GPU compute capacity.",
        "authors": [
            "Alexander Interrante-Grant",
            "Carla Varela-Rosa",
            "Suhaas Narayan",
            "Chris Connelly",
            "Albert Reuther"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-09-05"
    },
    "http://arxiv.org/abs/2509.05248v1": {
        "id": "http://arxiv.org/abs/2509.05248v1",
        "title": "Dynamic reconfiguration for malleable applications using RMA",
        "link": "http://arxiv.org/abs/2509.05248v1",
        "tags": [
            "networking",
            "training"
        ],
        "relevant": true,
        "indexed_date": "2025-09-08",
        "tldr": "Proposes dynamic application resizing using RMA communication for MPI-based malleable applications. Extends the Wait Drains strategy to enable efficient background reconfiguration. Results show comparable performance but high initialization costs limit advantages.",
        "abstract": "This paper investigates the novel one-sided communication methods based on remote memory access (RMA) operations in MPI for dynamic resizing of malleable applications, enabling data redistribution with minimal impact on application execution. After their integration into the MaM library, these methods are compared with traditional collective-based approaches. In addition, the existing strategy Wait Drains is extended to support efficient background reconfiguration. Results show comparable performance, though high initialization costs currently limit their advantage.",
        "authors": [
            "Iker Martín-Álvarez",
            "José I. Aliaga",
            "Maribel Castillo"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-05"
    },
    "http://arxiv.org/abs/2509.05216v1": {
        "id": "http://arxiv.org/abs/2509.05216v1",
        "title": "Toward Distributed 3D Gaussian Splatting for High-Resolution Isosurface Visualization",
        "link": "http://arxiv.org/abs/2509.05216v1",
        "tags": [
            "training",
            "multi-modal",
            "storage"
        ],
        "relevant": true,
        "indexed_date": "2025-09-08",
        "tldr": "Proposes a multi-GPU extension for 3D Gaussian Splatting to enable scalable, high-resolution scientific visualization. Distributes optimization across GPUs for improved throughput and capacity. Achieves 5.6× speedup training the Kingsnake dataset (4M Gaussians) on four GPUs vs. single-GPU.",
        "abstract": "We present a multi-GPU extension of the 3D Gaussian Splatting (3D-GS) pipeline for scientific visualization. Building on previous work that demonstrated high-fidelity isosurface reconstruction using Gaussian primitives, we incorporate a multi-GPU training backend adapted from Grendel-GS to enable scalable processing of large datasets. By distributing optimization across GPUs, our method improves training throughput and supports high-resolution reconstructions that exceed single-GPU capacity. In our experiments, the system achieves a 5.6X speedup on the Kingsnake dataset (4M Gaussians) using four GPUs compared to a single-GPU baseline, and successfully trains the Miranda dataset (18M Gaussians) that is an infeasible task on a single A100 GPU. This work lays the groundwork for integrating 3D-GS into HPC-based scientific workflows, enabling real-time post hoc and in situ visualization of complex simulations.",
        "authors": [
            "Mengjiao Han",
            "Andres Sewell",
            "Joseph Insley",
            "Janet Knowles",
            "Victor A. Mateevitsi",
            "Michael E. Papka",
            "Steve Petruzza",
            "Silvio Rizzi"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-05"
    },
    "http://arxiv.org/abs/2509.04827v2": {
        "id": "http://arxiv.org/abs/2509.04827v2",
        "title": "VoltanaLLM: Feedback-Driven Frequency Control and State-Space Routing for Energy-Efficient LLM Serving",
        "link": "http://arxiv.org/abs/2509.04827v2",
        "tags": [
            "serving",
            "offloading",
            "disaggregation"
        ],
        "relevant": true,
        "indexed_date": "2025-09-08",
        "tldr": "Reduces energy costs in LLM serving while maintaining SLOs. Combines feedback-driven GPU frequency scaling with state-space routing in prefill/decode disaggregated systems. Achieves 36.3% energy savings with near-perfect SLO attainment.",
        "abstract": "Modern Large Language Model (LLM) serving systems increasingly support interactive applications, like real-time chat assistants, code generation tools, and agentic workflows. However, the soaring energy cost of LLM inference presents a growing challenge for sustainable and cost-effective deployment. This paper introduces VoltanaLLM, a system for SLO-aware, energy-efficient LLM serving, built from a control theory perspective. VoltanaLLM co-designs frequency scaling and request routing in emerging prefill/decode disaggregated architectures, leveraging their decoupled execution to enable fine-grained phase-specific control. It consists of a feedback-driven frequency controller that dynamically adapts GPU frequency for prefill and decode phases, and a state-space router that explores routing decisions across frequency-scaled instances to minimize energy under latency constraints. We implement VoltanaLLM in SGLang and evaluate its performance over multiple state-of-the-art LLMs and real-world datasets. The results demonstrate that VoltanaLLM achieves up to 36.3% energy savings while maintaining near-perfect SLO attainment rate, paving the way for sustainable and intelligent LLM serving. Code of VoltanaLLM is open-sourced on GitHub: https://github.com/Supercomputing-System-AI-Lab/VoltanaLLM.",
        "authors": [
            "Jiahuan Yu",
            "Aryan Taneja",
            "Junfeng Lin",
            "Minjia Zhang"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.LG"
        ],
        "submit_date": "2025-09-05"
    },
    "http://arxiv.org/abs/2509.04719v2": {
        "id": "http://arxiv.org/abs/2509.04719v2",
        "title": "STADI: Fine-Grained Step-Patch Diffusion Parallelism for Heterogeneous GPUs",
        "link": "http://arxiv.org/abs/2509.04719v2",
        "tags": [
            "diffusion",
            "serving",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-09-08",
        "tldr": "Proposes STADI, a fine-grained spatio-temporal parallelism framework for diffusion model inference on heterogeneous GPUs. Uses computation-aware step quantization and elastic patch allocation to balance workload. Reduces latency by up to 45% compared to patch parallelism.",
        "abstract": "The escalating adoption of diffusion models for applications such as image generation demands efficient parallel inference techniques to manage their substantial computational cost. However, existing diffusion parallelism inference schemes often underutilize resources in heterogeneous multi-GPU environments, where varying hardware capabilities or background tasks cause workload imbalance. This paper introduces Spatio-Temporal Adaptive Diffusion Inference (STADI), a novel framework to accelerate diffusion model inference in such settings. At its core is a hybrid scheduler that orchestrates fine-grained parallelism across both temporal and spatial dimensions. Temporally, STADI introduces a novel computation-aware step allocator applied after warmup phases, using a least-common-multiple-minimizing quantization technique to reduce denoising steps on slower GPUs and execution synchronization. To further minimize GPU idle periods, STADI executes an elastic patch parallelism mechanism that allocates variably sized image patches to GPUs according to their computational capability, ensuring balanced workload distribution through a complementary spatial mechanism. Extensive experiments on both load-imbalanced and heterogeneous multi-GPU clusters validate STADI's efficacy, demonstrating improved load balancing and mitigation of performance bottlenecks. Compared to patch parallelism, a state-of-the-art diffusion inference framework, our method significantly reduces end-to-end inference latency by up to 45% and significantly improves resource utilization on heterogeneous GPUs.",
        "authors": [
            "Han Liang",
            "Jiahui Zhou",
            "Zicheng Zhou",
            "Xiaoxi Zhang",
            "Xu Chen"
        ],
        "categories": [
            "cs.DC",
            "cs.CV"
        ],
        "submit_date": "2025-09-05"
    },
    "http://arxiv.org/abs/2509.04383v1": {
        "id": "http://arxiv.org/abs/2509.04383v1",
        "title": "On the impact of unlimited computational power in OBLOT: consequences for synchronous robots on graphs",
        "link": "http://arxiv.org/abs/2509.04383v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-05",
        "tldr": "",
        "abstract": "The OBLOT model has been extensively studied in theoretical swarm robotics. It assumes weak capabilities for the involved mobile robots, such as they are anonymous, disoriented, no memory of past events (oblivious), and silent. Their only means of (implicit) communication is transferred to their positioning, i.e., stigmergic information. These limited capabilities make the design of distributed algorithms a challenging task. Over the last two decades, numerous research papers have addressed the question of which tasks can be accomplished within this model. Nevertheless, as it usually happens in distributed computing, also in OBLOT the computational power available to the robots is neglected as the main cost measures for the designed algorithms refer to the number of movements or the number of rounds required. In this paper, we prove that for synchronous robots moving on finite graphs, the unlimited computational power (other than finite time) has a significant impact. In fact, by exploiting it, we provide a definitive resolution algorithm that applies to a wide class of problems while guaranteeing the minimum number of moves and rounds.",
        "authors": [
            "Serafino Cicerone",
            "Alessia Di Fonso",
            "Gabriele Di Stefano",
            "Alfredo Navarra"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-04"
    },
    "http://arxiv.org/abs/2509.04085v1": {
        "id": "http://arxiv.org/abs/2509.04085v1",
        "title": "Trustworthy Second-hand Marketplace for Built Environment",
        "link": "http://arxiv.org/abs/2509.04085v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-05",
        "tldr": "",
        "abstract": "The construction industry faces significant challenges regarding material waste and sustainable practices, necessitating innovative solutions that integrate automation, traceability, and decentralised decision-making to enable efficient material reuse. This paper presents a blockchain-enabled digital marketplace for sustainable construction material reuse, ensuring transparency and traceability using InterPlanetary File System (IPFS). The proposed framework enhances trust and accountability in material exchange, addressing key challenges in industrial automation and circular supply chains. A framework has been developed to demonstrate the operational processes of the marketplace, illustrating its practical application and effectiveness. Our contributions show how the marketplace can facilitate the efficient and trustworthy exchange of reusable materials, representing a substantial step towards more sustainable construction practices.",
        "authors": [
            "Stanly Wilson",
            "Kwabena Adu-Duodu",
            "Yinhao Li",
            "Ringo Sham",
            "Yingli Wang",
            "Ellis Solaiman",
            "Charith Perera",
            "Rajiv Ranjan",
            "Omer Rana"
        ],
        "categories": [
            "cs.DC",
            "cs.ET"
        ],
        "submit_date": "2025-09-04"
    },
    "http://arxiv.org/abs/2509.04084v2": {
        "id": "http://arxiv.org/abs/2509.04084v2",
        "title": "Optimizing Frequent Checkpointing via Low-Cost Differential for Distributed Training Systems",
        "link": "http://arxiv.org/abs/2509.04084v2",
        "tags": [
            "training",
            "storage"
        ],
        "relevant": true,
        "indexed_date": "2025-09-05",
        "tldr": "Proposes differential checkpointing using compressed gradients to reduce costs in distributed training systems. Introduces gradient reuse and batched writes, dynamically tuning frequency and batch size. Achieves per-iteration checkpointing with <3.1% runtime overhead in experiments.",
        "abstract": "Distributed training of large deep-learning models often leads to failures, so checkpointing is commonly employed for recovery. State-of-the-art studies focus on frequent checkpointing for fast recovery from failures. However, it generates numerous checkpoints, incurring substantial costs and thus degrading training performance. Recently, differential checkpointing has been proposed to reduce costs, but it is limited to recommendation systems, so its application to general distributed training systems remains unexplored.   We proposes \\sysname, an efficient frequent checkpointing framework that \\textit{reuses} compressed gradients, serving as differential checkpoints to reduce cost. Furthermore, \\sysname incorporates a batched gradient write optimization to persist these differentials to storage efficiently. It also dynamically tunes both the checkpoint frequency and the batching size to maximize performance. In non-compression scenario, We further proposes \\sysnameplus with a layer-wise gradient reusing and snapshotting approach and a CPU-based asynchronous persistence strategy, enabling frequent checkpointing without gradient compression. Experiments on various workloads show that \\sysname can achieve checkpointing frequency up to per iteration with less than 3.1\\% runtime overhead.",
        "authors": [
            "Chenxuan Yao",
            "Yuchong Hu",
            "Feifan Liu",
            "Zhengyu Liu",
            "Dan Feng"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-04"
    },
    "http://arxiv.org/abs/2509.04038v1": {
        "id": "http://arxiv.org/abs/2509.04038v1",
        "title": "Counterfactual simulations for large scale systems with burnout variables",
        "link": "http://arxiv.org/abs/2509.04038v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-05",
        "tldr": "",
        "abstract": "We consider large-scale systems influenced by burnout variables - state variables that start active, shape dynamics, and irreversibly deactivate once certain conditions are met. Simulating what-if scenarios in such systems is computationally demanding, as alternative trajectories often require sequential processing, which does not scale very well. This challenge arises in settings like online advertising, because of campaigns budgets, complicating counterfactual analysis despite rich data availability. We introduce a new type of algorithms based on what we refer to as uncertainty relaxation, that enables efficient parallel computation, significantly improving scalability for counterfactual estimation in systems with burnout variables.",
        "authors": [
            "Benjamin Heymann"
        ],
        "categories": [
            "cs.DC",
            "math.OC",
            "stat.ME"
        ],
        "submit_date": "2025-09-04"
    },
    "http://arxiv.org/abs/2509.04004v1": {
        "id": "http://arxiv.org/abs/2509.04004v1",
        "title": "Gathering of asynchronous robots on circle with limited visibility using finite communication",
        "link": "http://arxiv.org/abs/2509.04004v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-05",
        "tldr": "",
        "abstract": "This work addresses the gathering problem for a set of autonomous, anonymous, and homogeneous robots with limited visibility operating in a continuous circle. The robots are initially placed at distinct positions, forming a rotationally asymmetric configuration. The robots agree on the clockwise direction. In the $θ$-visibility model, a robot can only see those robots on the circle that are at an angular distance $<θ$ from it. Di Luna \\textit{et. al.} [DISC'20] have shown that, in $π/2$ visibility, gathering is impossible. In addition, they provided an algorithm for robots with $π$ visibility, operating under a semi-synchronous scheduler. In the $π$ visibility model, only one point, the point at the angular distance $π$ is removed from the visibility. Ghosh \\textit{et. al.} [SSS'23] provided a gathering algorithm for $π$ visibility model with robot having finite memory ($\\mathcal{FSTA}$), operating under a special asynchronous scheduler.   If the robots can see all points on the circle, then the gathering can be done by electing a leader in the weakest robot model under a fully asynchronous scheduler. However, previous works have shown that even the removal of one point from the visibility makes gathering difficult. In both works, the robots had rigid movement. In this work, we propose an algorithm that solves the gathering problem under the $π$-visibility model for robots that have finite communication ability ($\\mathcal{FCOM}$). In this work the robot movement is non-rigid and the robots work under a fully asynchronous scheduler.",
        "authors": [
            "Avisek Sharma",
            "Satakshi Ghosh",
            "Buddhadeb Sau"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-04"
    },
    "http://arxiv.org/abs/2509.03755v1": {
        "id": "http://arxiv.org/abs/2509.03755v1",
        "title": "Distributed Download from an External Data Source in Asynchronous Faulty Settings",
        "link": "http://arxiv.org/abs/2509.03755v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-05",
        "tldr": "",
        "abstract": "The distributedData Retrieval (DR) model consists of $k$ peers connected by a complete peer-to-peer communication network, and a trusted external data source that stores an array $\\textbf{X}$ of $n$ bits ($n \\gg k$). Up to $βk$ of the peers might fail in any execution (for $β\\in [0, 1)$). Peers can obtain the information either by inexpensive messages passed among themselves or through expensive queries to the source array $\\textbf{X}$. In the DR model, we focus on designing protocols that minimize the number of queries performed by any nonfaulty peer (a measure referred to as query complexity) while maximizing the resilience parameter $β$.   The Download problem requires each nonfaulty peer to correctly learn the entire array $\\textbf{X}$. Earlier work on this problem focused on synchronous communication networks and established several deterministic and randomized upper and lower bounds. Our work is the first to extend the study of distributed data retrieval to asynchronous communication networks. We address the Download problem under both the Byzantine and crash failure models. We present query-optimal deterministic solutions in an asynchronous model that can tolerate any fixed fraction $β<1$ of crash faults. In the Byzantine failure model, it is known that deterministic protocols incur a query complexity of $Ω(n)$ per peer, even under synchrony. We extend this lower bound to randomized protocols in the asynchronous model for $β\\geq 1/2$, and further show that for $β< 1/2$, a randomized protocol exists with near-optimal query complexity. To the best of our knowledge, this is the first work to address the Download problem in asynchronous communication networks.",
        "authors": [
            "John Augustine",
            "Soumyottam Chatterjee",
            "Valerie King",
            "Manish Kumar",
            "Shachar Meir",
            "David Peleg"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-03"
    },
    "http://arxiv.org/abs/2509.03653v1": {
        "id": "http://arxiv.org/abs/2509.03653v1",
        "title": "Combining Performance and Productivity: Accelerating the Network Sensing Graph Challenge with GPUs and Commodity Data Science Software",
        "link": "http://arxiv.org/abs/2509.03653v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-05",
        "tldr": "",
        "abstract": "The HPEC Graph Challenge is a collection of benchmarks representing complex workloads that test the hardware and software components of HPC systems, which traditional benchmarks, such as LINPACK, do not. The first benchmark, Subgraph Isomorphism, focused on several compute-bound and memory-bound kernels. The most recent of the challenges, the Anonymized Network Sensing Graph Challenge, represents a shift in direction, as it represents a longer end-to-end workload that requires many more software components, including, but not limited to, data I/O, data structures for representing graph data, and a wide range of functions for data preparation and network analysis. A notable feature of this new graph challenge is the use of GraphBLAS to represent the computational aspects of the problem statement. In this paper, we show an alternative interpretation of the GraphBLAS formulations using the language of data science. With this formulation, we show that the new graph challenge can be implemented using off-the-shelf ETL tools available in open-source, enterprise software such as NVIDIA's RAPIDS ecosystem. Using off-the-shelf software, RAPIDS cuDF and cupy, we enable significant software acceleration without requiring any specific HPC code and show speedups, over the same code running with Pandas on the CPU, of 147x-509x on an NVIDIA A100 GPU, 243x-1269X for an NVIDIA H100 GPU, and 332X-2185X for an NVIDIA H200 GPU.",
        "authors": [
            "Siddharth Samsi",
            "Dan Campbell",
            "Emanuel Scoullos",
            "Oded Green"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-03"
    },
    "http://arxiv.org/abs/2509.04277v1": {
        "id": "http://arxiv.org/abs/2509.04277v1",
        "title": "Massively-Parallel Implementation of Inextensible Elastic Rods Using Inter-block GPU Synchronization",
        "link": "http://arxiv.org/abs/2509.04277v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-05",
        "tldr": "",
        "abstract": "An elastic rod is a long and thin body able to sustain large global deformations, even if local strains are small. The Cosserat rod is a non-linear elastic rod with an oriented centreline, which enables modelling of bending, stretching and twisting deformations. It can be used for physically-based computer simulation of threads, wires, ropes, as well as flexible surgical instruments such as catheters, guidewires or sutures. We present a massively-parallel implementation of the original CoRdE model as well as our inextensible variation. By superseding the CUDA Scalable Programming Model and using inter-block synchronization, we managed to simulate multiple physics time-steps per single kernel launch utilizing all the GPU's streaming multiprocessors. Under some constraints, this results in nearly constant computation time, regardless of the number of Cosserat elements simulated. When executing 10 time-steps per single kernel launch, our implementation of the original, extensible CoRdE was x40.0 faster. In a number of tests, the GPU implementation of our inextensible CoRdE modification achieved an average speed-up of x15.11 over the corresponding CPU version. Simulating a catheter/guidewire pair (2x512 Cosserat elements) in a cardiovascular application resulted in a 13.5 fold performance boost, enabling for accurate real-time simulation at haptic interactive rates (0.5-1kHz).",
        "authors": [
            "Przemyslaw Korzeniowski",
            "Niels Hald",
            "Fernando Bello"
        ],
        "categories": [
            "cs.GR",
            "cs.DC"
        ],
        "submit_date": "2025-09-04"
    },
    "http://arxiv.org/abs/2509.04095v1": {
        "id": "http://arxiv.org/abs/2509.04095v1",
        "title": "Cloud-Assisted Remote Control for Aerial Robots: From Theory to Proof-of-Concept Implementation",
        "link": "http://arxiv.org/abs/2509.04095v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-05",
        "tldr": "",
        "abstract": "Cloud robotics has emerged as a promising technology for robotics applications due to its advantages of offloading computationally intensive tasks, facilitating data sharing, and enhancing robot coordination. However, integrating cloud computing with robotics remains a complex challenge due to network latency, security concerns, and the need for efficient resource management. In this work, we present a scalable and intuitive framework for testing cloud and edge robotic systems. The framework consists of two main components enabled by containerized technology: (a) a containerized cloud cluster and (b) the containerized robot simulation environment. The system incorporates two endpoints of a User Datagram Protocol (UDP) tunnel, enabling bidirectional communication between the cloud cluster container and the robot simulation environment, while simulating realistic network conditions. To achieve this, we consider the use case of cloud-assisted remote control for aerial robots, while utilizing Linux-based traffic control to introduce artificial delay and jitter, replicating variable network conditions encountered in practical cloud-robot deployments.",
        "authors": [
            "Achilleas Santi Seisa",
            "Viswa Narayanan Sankaranarayanan",
            "Gerasimos Damigos",
            "Sumeet Gajanan Satpute",
            "George Nikolakopoulos"
        ],
        "categories": [
            "cs.RO",
            "cs.DC"
        ],
        "submit_date": "2025-09-04"
    },
    "http://arxiv.org/abs/2509.03945v1": {
        "id": "http://arxiv.org/abs/2509.03945v1",
        "title": "Prob-GParareal: A Probabilistic Numerical Parallel-in-Time Solver for Differential Equations",
        "link": "http://arxiv.org/abs/2509.03945v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-05",
        "tldr": "",
        "abstract": "We introduce Prob-GParareal, a probabilistic extension of the GParareal algorithm designed to provide uncertainty quantification for the Parallel-in-Time (PinT) solution of (ordinary and partial) differential equations (ODEs, PDEs). The method employs Gaussian processes (GPs) to model the Parareal correction function, as GParareal does, further enabling the propagation of numerical uncertainty across time and yielding probabilistic forecasts of system's evolution. Furthermore, Prob-GParareal accommodates probabilistic initial conditions and maintains compatibility with classical numerical solvers, ensuring its straightforward integration into existing Parareal frameworks. Here, we first conduct a theoretical analysis of the computational complexity and derive error bounds of Prob-GParareal. Then, we numerically demonstrate the accuracy and robustness of the proposed algorithm on five benchmark ODE systems, including chaotic, stiff, and bifurcation problems. To showcase the flexibility and potential scalability of the proposed algorithm, we also consider Prob-nnGParareal, a variant obtained by replacing the GPs in Parareal with the nearest-neighbors GPs, illustrating its increased performance on an additional PDE example. This work bridges a critical gap in the development of probabilistic counterparts to established PinT methods.",
        "authors": [
            "Guglielmo Gattiglio",
            "Lyudmila Grigoryeva",
            "Massimiliano Tamborrino"
        ],
        "categories": [
            "stat.CO",
            "cs.DC",
            "math.NA",
            "stat.ML"
        ],
        "submit_date": "2025-09-04"
    },
    "http://arxiv.org/abs/2509.03855v7": {
        "id": "http://arxiv.org/abs/2509.03855v7",
        "title": "Towards Deterministic Sub-0.5 us Response on Linux through Interrupt Isolation",
        "link": "http://arxiv.org/abs/2509.03855v7",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-05",
        "tldr": "",
        "abstract": "Real-time responsiveness in Linux is often constrained by interrupt contention and timer handling overhead, making it challenging to achieve sub-microsecond latency. This work introduces an interrupt isolation approach that centralizes and minimizes timer interrupt interference across CPU cores. By enabling a dedicated API to selectively invoke timer handling routines and suppress non-critical inter-processor interrupts, our design significantly reduces jitter and response latency. Experiments conducted on an ARM-based multicore platform demonstrate that the proposed mechanism consistently achieves sub-0.5 us response times, outperforming conventional Linux PREEMPT-RT configurations. These results highlight the potential of interrupt isolation as a lightweight and effective strategy for deterministic real-time workloads in general-purpose operating systems.",
        "authors": [
            "Zhouyi Zhou",
            "Zhili Liu",
            "Shancong Zhang",
            "Jiemin Li",
            "Dengke Du",
            "Mengke Sun",
            "Zhiqiang Wang",
            "Hongyan Liu",
            "Guokai Xu"
        ],
        "categories": [
            "cs.OS"
        ],
        "submit_date": "2025-09-04"
    },
    "http://arxiv.org/abs/2509.03394v1": {
        "id": "http://arxiv.org/abs/2509.03394v1",
        "title": "CloudFormer: An Attention-based Performance Prediction for Public Clouds with Unknown Workload",
        "link": "http://arxiv.org/abs/2509.03394v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-04",
        "tldr": "",
        "abstract": "Cloud platforms are increasingly relied upon to host diverse, resource-intensive workloads due to their scalability, flexibility, and cost-efficiency. In multi-tenant cloud environments, virtual machines are consolidated on shared physical servers to improve resource utilization. While virtualization guarantees resource partitioning for CPU, memory, and storage, it cannot ensure performance isolation. Competition for shared resources such as last-level cache, memory bandwidth, and network interfaces often leads to severe performance degradation. Existing management techniques, including VM scheduling and resource provisioning, require accurate performance prediction to mitigate interference. However, this remains challenging in public clouds due to the black-box nature of VMs and the highly dynamic nature of workloads. To address these limitations, we propose CloudFormer, a dual-branch Transformer-based model designed to predict VM performance degradation in black-box environments. CloudFormer jointly models temporal dynamics and system-level interactions, leveraging 206 system metrics at one-second resolution across both static and dynamic scenarios. This design enables the model to capture transient interference effects and adapt to varying workload conditions without scenario-specific tuning. Complementing the methodology, we provide a fine-grained dataset that significantly expands the temporal resolution and metric diversity compared to existing benchmarks. Experimental results demonstrate that CloudFormer consistently outperforms state-of-the-art baselines across multiple evaluation metrics, achieving robust generalization across diverse and previously unseen workloads. Notably, CloudFormer attains a mean absolute error (MAE) of just 7.8%, representing a substantial improvement in predictive accuracy and outperforming existing methods at least by 28%.",
        "authors": [
            "Amirhossein Shahbazinia",
            "Darong Huang",
            "Luis Costero",
            "David Atienza"
        ],
        "categories": [
            "cs.DC",
            "cs.LG",
            "cs.PF"
        ],
        "submit_date": "2025-09-03"
    },
    "http://arxiv.org/abs/2509.03145v1": {
        "id": "http://arxiv.org/abs/2509.03145v1",
        "title": "Efficient and Secure Sleepy Model for BFT Consensus",
        "link": "http://arxiv.org/abs/2509.03145v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-04",
        "tldr": "",
        "abstract": "Byzantine Fault Tolerant (BFT) consensus protocols for dynamically available systems face a critical challenge: balancing latency and security in fluctuating node participation. Existing solutions often require multiple rounds of voting per decision, leading to high latency or limited resilience to adversarial behavior. This paper presents a BFT protocol integrating a pre-commit mechanism with publicly verifiable secret sharing (PVSS) into message transmission. By binding users' identities to their messages through PVSS, our approach reduces communication rounds. Compared to other state-of-the-art methods, our protocol typically requires only four network delays (4$Δ$) in common scenarios while being resilient to up to 1/2 adversarial participants. This integration enhances the efficiency and security of the protocol without compromising integrity. Theoretical analysis demonstrates the robustness of the protocol against Byzantine attacks. Experimental evaluations show that, compared to traditional BFT protocols, our protocol significantly prevents fork occurrences and improves chain stability. Furthermore, compared to longest-chain protocol, our protocol maintains stability and lower latency in scenarios with moderate participation fluctuations.",
        "authors": [
            "Pengkun Ren",
            "Hai Dong",
            "Zahir Tari",
            "Pengcheng Zhang"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-03"
    },
    "http://arxiv.org/abs/2509.03104v1": {
        "id": "http://arxiv.org/abs/2509.03104v1",
        "title": "The High Cost of Keeping Warm: Characterizing Overhead in Serverless Autoscaling Policies",
        "link": "http://arxiv.org/abs/2509.03104v1",
        "tags": [
            "autoscaling"
        ],
        "relevant": true,
        "indexed_date": "2025-09-04",
        "tldr": "Investigates overheads in serverless autoscaling policies for cloud platforms. Analyzes synchronous/asynchronous autoscalers via real workload replay and hybrid simulation, highlighting 10-40% CPU churn overhead and 2-10× excess memory usage. Demonstrates trade-offs between overhead reduction and performance degradation.",
        "abstract": "Serverless computing is transforming cloud application development, but the performance-cost trade-offs of control plane designs remain poorly understood due to a lack of open, cross-platform benchmarks and detailed system analyses. In this work, we address these gaps by designing a serverless system that approximates the scaling behaviors of commercial providers, including AWS Lambda and Google Cloud Run. We systematically compare the performance and cost-efficiency of both synchronous and asynchronous autoscaling policies by replaying real-world workloads and varying key autoscaling parameters.   We demonstrate that our open-source systems can closely replicate the operational characteristics of commercial platforms, enabling reproducible and transparent experimentation. By evaluating how autoscaling parameters affect latency, memory usage, and CPU overhead, we reveal several key findings. First, we find that serverless systems exhibit significant computational overhead due to instance churn equivalent to 10-40% of the CPU cycles spent on request handling, primarily originating from worker nodes. Second, we observe high memory allocation due to scaling policy: 2-10 times more than actively used. Finally, we demonstrate that reducing these overheads typically results in significant performance degradation in the current systems, underscoring the need for new, cost-efficient autoscaling strategies. Additionally, we employ a hybrid methodology that combines real control plane deployments with large-scale simulation to extend our evaluation closer to a production scale, thereby bridging the gap between small research clusters and real-world environments.",
        "authors": [
            "Leonid Kondrashov",
            "Boxi Zhou",
            "Hancheng Wang",
            "Dmitrii Ustiugov"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-03"
    },
    "http://arxiv.org/abs/2509.03047v1": {
        "id": "http://arxiv.org/abs/2509.03047v1",
        "title": "FlashRecovery: Fast and Low-Cost Recovery from Failures for Large-Scale Training of LLMs",
        "link": "http://arxiv.org/abs/2509.03047v1",
        "tags": [
            "training",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-09-04",
        "tldr": "Addresses high training time loss from failures in large-scale LLM clusters. Introduces FlashRecovery with real-time detection, scale-independent restart, and single-step recovery without checkpoints. Achieves recovery in 150 seconds on 4800-device clusters, with near-constant time across scales.",
        "abstract": "Large language models (LLMs) have made a profound impact across various fields due to their advanced capabilities. However, training these models at unprecedented scales requires extensive AI accelerator clusters and sophisticated parallelism strategies, which pose significant challenges in maintaining system reliability over prolonged training periods. A major concern is the substantial loss of training time caused by inevitable hardware and software failures. To address these challenges, we present FlashRecovery, a fast and low-cost failure recovery system comprising three core modules: (1) Active and real-time failure detection. This module performs continuous training state monitoring, enabling immediate identification of hardware and software failures within seconds, thus ensuring rapid incident response; (2) Scale-independent task restart. By employing different recovery strategies for normal and faulty nodes, combined with an optimized communication group reconstruction protocol, our approach ensures that the recovery time remains nearly constant, regardless of cluster scale; (3) Checkpoint-free recovery within one step. Our novel recovery mechanism enables single-step restoration, completely eliminating dependence on traditional checkpointing methods and their associated overhead. Collectively, these innovations enable FlashRecovery to achieve optimal Recovery Time Objective (RTO) and Recovery Point Objective (RPO), substantially improving the reliability and efficiency of long-duration LLM training. Experimental results demonstrate that FlashRecovery system can achieve training restoration on training cluster with 4, 800 devices in 150 seconds. We also verify that the time required for failure recovery is nearly consistent for different scales of training tasks.",
        "authors": [
            "Haijun Zhang",
            "Jinxiang Wang",
            "Zhenhua Yu",
            "Yanyong Zhang",
            "Xuejie Ji",
            "Kaining Mao",
            "Jun Zhang",
            "Yaqing Zhang",
            "Ting Wu",
            "Fei Jie",
            "Xiemin Huang",
            "Zhifang Cai",
            "Junhua Cheng",
            "Shuwei Wang",
            "Wei Li",
            "Xiaoming Bao",
            "Hua Xu",
            "Shixiong Zhao",
            "Jun Li",
            "Hongwei Sun",
            "Ziyang Zhang",
            "Yi Xiong",
            "Chunsheng Li"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-09-03"
    },
    "http://arxiv.org/abs/2509.03018v1": {
        "id": "http://arxiv.org/abs/2509.03018v1",
        "title": "Mycroft: Tracing Dependencies in Collective Communication Towards Reliable LLM Training",
        "link": "http://arxiv.org/abs/2509.03018v1",
        "tags": [
            "training",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-09-04",
        "tldr": "Addresses collective communication reliability issues in distributed LLM training. Proposes Mycroft, a distributed tracing system using state and dependency analysis for root cause detection. Detects anomalies in 15 seconds (90% cases) and identifies root cause in 20 seconds (60% cases).",
        "abstract": "Reliability is essential for ensuring efficiency in LLM training. However, many real-world reliability issues remain difficult to resolve, resulting in wasted resources and degraded model performance. Unfortunately, today's collective communication libraries operate as black boxes, hiding critical information needed for effective root cause analysis. We propose Mycroft, a lightweight distributed tracing and root cause analysis system designed to address previously hidden reliability issues in collective communication. Mycroft's key idea is to trace collective communication states and leverage internal control and data dependencies to resolve reliability problems in LLM training. Mycroft has been deployed at ByteDance for over six months to debug collective communication related issues at runtime. It detected anomalies within 15 seconds in 90% of cases and identified the root cause within 20 seconds in 60% of cases. We also conducted extensive fault injection experiments to demonstrate Mycroft's capability and efficiency.",
        "authors": [
            "Yangtao Deng",
            "Lei Zhang",
            "Qinlong Wang",
            "Xiaoyun Zhi",
            "Xinlei Zhang",
            "Zhuo Jiang",
            "Haohan Xu",
            "Lei Wang",
            "Zuquan Song",
            "Gaohong Liu",
            "Yang Bai",
            "Shuguang Wang",
            "Wencong Xiao",
            "Jianxi Ye",
            "Minlan Yu",
            "Hong Xu"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-09-03"
    },
    "http://arxiv.org/abs/2509.02767v1": {
        "id": "http://arxiv.org/abs/2509.02767v1",
        "title": "A Novel IaaS Tax Model as Leverage Towards Green Cloud Computing",
        "link": "http://arxiv.org/abs/2509.02767v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-04",
        "tldr": "",
        "abstract": "The cloud computing technology uses datacenters, which require energy. Recent trends show that the required energy for these datacenters will rise over time, or at least remain constant. Hence, the scientific community developed different algorithms, architectures, and approaches for improving the energy efficiency of cloud datacenters, which are summarized under the umbrella term Green Cloud computing. In this paper, we use an economic approach - taxes - for reducing the energy consumption of datacenters. We developed a tax model called GreenCloud tax, which penalizes energy-inefficient datacenters while fostering datacenters that are energy-efficient. Hence, providers running energy-efficient datacenters are able to offer cheaper prices to consumers, which consequently leads to a shift of workloads from energy-inefficient datacenters to energy-efficient datacenters. The GreenCloud tax approach was implemented using the simulation environment CloudSim. We applied real data sets published in the SPEC benchmark for the executed simulation scenarios, which we used for evaluating the GreenCloud tax.",
        "authors": [
            "Benedikt Pittl",
            "Werner Mach",
            "Erich Schikuta"
        ],
        "categories": [
            "cs.DC",
            "cs.CY"
        ],
        "submit_date": "2025-09-02"
    },
    "http://arxiv.org/abs/2509.03472v1": {
        "id": "http://arxiv.org/abs/2509.03472v1",
        "title": "DPQuant: Efficient and Differentially-Private Model Training via Dynamic Quantization Scheduling",
        "link": "http://arxiv.org/abs/2509.03472v1",
        "tags": [
            "training",
            "quantization",
            "sparse"
        ],
        "relevant": true,
        "indexed_date": "2025-09-04",
        "tldr": "Proposes DPQuant, a dynamic quantization scheduling method for differentially private SGD training. Uses probabilistic layer sampling and loss-aware prioritization to reduce quantization variance while preserving privacy. Achieves 2.21x theoretical throughput improvement with <2% accuracy drop.",
        "abstract": "Differentially-Private SGD (DP-SGD) is a powerful technique to protect user privacy when using sensitive data to train neural networks. During training, converting model weights and activations into low-precision formats, i.e., quantization, can drastically reduce training times, energy consumption, and cost, and is thus a widely used technique. In this work, we demonstrate that quantization causes significantly higher accuracy degradation in DP-SGD compared to regular SGD. We observe that this is caused by noise injection in DP-SGD, which amplifies quantization variance, leading to disproportionately large accuracy degradation. To address this challenge, we present QPQuant, a dynamic quantization framework that adaptively selects a changing subset of layers to quantize at each epoch. Our method combines two key ideas that effectively reduce quantization variance: (i) probabilistic sampling of the layers that rotates which layers are quantized every epoch, and (ii) loss-aware layer prioritization, which uses a differentially private loss sensitivity estimator to identify layers that can be quantized with minimal impact on model quality. This estimator consumes a negligible fraction of the overall privacy budget, preserving DP guarantees. Empirical evaluations on ResNet18, ResNet50, and DenseNet121 across a range of datasets demonstrate that DPQuant consistently outperforms static quantization baselines, achieving near Pareto-optimal accuracy-compute trade-offs and up to 2.21x theoretical throughput improvements on low-precision hardware, with less than 2% drop in validation accuracy.",
        "authors": [
            "Yubo Gao",
            "Renbo Tu",
            "Gennady Pekhimenko",
            "Nandita Vijaykumar"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC"
        ],
        "submit_date": "2025-09-03"
    },
    "http://arxiv.org/abs/2509.03075v1": {
        "id": "http://arxiv.org/abs/2509.03075v1",
        "title": "A description of the radio astronomy data processing tool DDF Pipeline",
        "link": "http://arxiv.org/abs/2509.03075v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-04",
        "tldr": "",
        "abstract": "This paper presents the DDF Pipeline, a radio astronomy data processing tool initially designed for the LOw-Frequency ARray (LO- FAR) radio-telescope and a candidate for processing data from the Square Kilometre Array (SKA). This work describes the DDF Pipeline software and presents a coarse-grain profiling execution to characterize its performance.",
        "authors": [
            "Mathis Certenais",
            "François Bodin",
            "Laurent Morin"
        ],
        "categories": [
            "astro-ph.IM",
            "cs.DC"
        ],
        "submit_date": "2025-09-03"
    },
    "http://arxiv.org/abs/2509.02909v1": {
        "id": "http://arxiv.org/abs/2509.02909v1",
        "title": "Treasure Hunt in Anonymous Graphs with Quantum Pebbles by Oblivious Agents",
        "link": "http://arxiv.org/abs/2509.02909v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-04",
        "tldr": "",
        "abstract": "We investigate the problem of finding a static treasure in anonymous graphs using oblivious agents and introduce a novel approach that leverages quantum information. In anonymous graphs, vertices are unlabelled, indistinguishable, and edges are locally labelled with port numbers. Agents typically rely on stationary classical pebbles placed by an oracle to guide their search. However, this classical approach is constrained by limited information transmission and high traversal complexity. Classical pebbles are not sufficient for search if the agents are oblivious. We propose the first use of quantum pebbles for search in anonymous graphs. Quantum pebbles periodically emit qubits in a fixed quantum state. Each pebble encodes the port number to the next node using a unique quantum state. The agent determines the correct path by performing measurements in multiple bases, exploiting the probabilistic nature of quantum measurement to distinguish states. We show that this strategy enables an oblivious agent to locate the treasure in $D$ steps using $D$ quantum pebbles, where $D$ is the length of the shortest path between the starting point and the treasure. Moreover, only $O((\\log D + \\log Δ)/(\\log 1/δ))$ measurements per node are required to ensure high success probability in a graph with maximum degree $Δ$ where $δ= \\cos^2(\\fracπ{2Δ})$. We propose the use of quantum information as a guidance mechanism in anonymous graph search. We demonstrate that quantum pebbles can not only emulate the functionality of classical pebbles but can do so with improved efficiency, offering a promising direction for future quantum-enhanced distributed algorithms.",
        "authors": [
            "Gaurav Gaur",
            "Barun Gorain",
            "Rishi Ranjan Singh",
            "Daya Gaur"
        ],
        "categories": [
            "quant-ph",
            "cs.DC",
            "cs.DS",
            "cs.ET"
        ],
        "submit_date": "2025-09-03"
    },
    "http://arxiv.org/abs/2509.02629v1": {
        "id": "http://arxiv.org/abs/2509.02629v1",
        "title": "\\textit{In Silico} Benchmarking of Detectable Byzantine Agreement in Noisy Quantum Networks",
        "link": "http://arxiv.org/abs/2509.02629v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-04",
        "tldr": "",
        "abstract": "Quantum communication resources offer significant advantages for fault-tolerant distributed protocols, particularly in Byzantine Agreement (BA), where reliability against adversarial interference is essential. Quantum Detectable Byzantine Agreement (QDBA) enables consensus protocols that surpass classical limitations by leveraging entangled quantum states. In this work, we focus on the practical realization of QDBA using Einstein-Podolsky-Rosen (EPR) pairs, the simplest maximally entangled quantum resources, making the protocol experimentally accessible across current quantum hardware platforms. We present a comprehensive computational study of the EPRQDBA protocol under realistic quantum network conditions, utilizing the Aliro Quantum Network Simulator to evaluate the performance and robustness of the protocol. Our simulations systematically explore the protocol's parameter space --including variations in network size, traitorous node count, the amount of entanglement consumed in the protocol, and physically motivated noise models tailored specifically for superconducting and photonic qubit technologies. Through extensive numerical experiments, we provide insights into how these physically realistic parameters impact protocol performance, establishing critical thresholds and optimal operational regimes for experimental implementations. This work bridges theoretical advances in quantum consensus protocols with practical network implementations, offering a concrete reference for experimentalists. Our findings serve as a guideline for evaluating and optimizing QDBA implementations in realistic, noisy environments.",
        "authors": [
            "Mayank Bhatia",
            "Shaan Doshi",
            "Daniel Winton",
            "Brian Doolittle",
            "Bruno Abreu",
            "Santiago Núñez-Corrales"
        ],
        "categories": [
            "quant-ph",
            "cs.DC",
            "cs.NI"
        ],
        "submit_date": "2025-09-01"
    },
    "http://arxiv.org/abs/2509.02590v1": {
        "id": "http://arxiv.org/abs/2509.02590v1",
        "title": "On the Optimization of Methods for Establishing Well-Connected Communities",
        "link": "http://arxiv.org/abs/2509.02590v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-04",
        "tldr": "",
        "abstract": "Community detection plays a central role in uncovering meso scale structures in networks. However, existing methods often suffer from disconnected or weakly connected clusters, undermining interpretability and robustness. Well-Connected Clusters (WCC) and Connectivity Modifier (CM) algorithms are post-processing techniques that improve the accuracy of many clustering methods. However, they are computationally prohibitive on massive graphs. In this work, we present optimized parallel implementations of WCC and CM using the HPE Chapel programming language. First, we design fast and efficient parallel algorithms that leverage Chapel's parallel constructs to achieve substantial performance improvements and scalability on modern multicore architectures. Second, we integrate this software into Arkouda/Arachne, an open-source, high-performance framework for large-scale graph analytics. Our implementations uniquely enable well-connected community detection on massive graphs with more than 2 billion edges, providing a practical solution for connectivity-preserving clustering at web scale. For example, our implementations of WCC and CM enable community detection of the over 2-billion edge Open-Alex dataset in minutes using 128 cores, a result infeasible to compute previously.",
        "authors": [
            "Mohammad Dindoost",
            "Oliver Alvarado Rodriguez",
            "Bartosz Bryg",
            "Minhyuk Park",
            "George Chacko",
            "Tandy Warnow",
            "David A. Bader"
        ],
        "categories": [
            "cs.SI",
            "cs.DC"
        ],
        "submit_date": "2025-08-29"
    },
    "http://arxiv.org/abs/2509.02899v1": {
        "id": "http://arxiv.org/abs/2509.02899v1",
        "title": "Safe Sharing of Fast Kernel-Bypass I/O Among Nontrusting Applications",
        "link": "http://arxiv.org/abs/2509.02899v1",
        "tags": [
            "kernel",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-09-04",
        "tldr": "Addresses safe sharing of kernel-bypass I/O among untrusted applications. Proposes a protected library model with bounded-time operations, buffer unmapping attack prevention, and dynamic work division. Achieves 50% lower latency and 7x throughput improvement over FastDDS with lower CPU usage.",
        "abstract": "Protected user-level libraries have been proposed as a way to allow mutually distrusting applications to safely share kernel-bypass services. In this paper, we identify and solve several previously unaddressed obstacles to realizing this design and identify several optimization opportunities. First, to preserve the kernel's ability to reclaim failed processes, protected library functions must complete in modest, bounded time. We show how to move unbounded waits outside the library itself, enabling synchronous interaction among processes without the need for polling. Second, we show how the bounded time requirement can be leveraged to achieve lower and more stable latency for inter-process interactions. Third, we observe that prior work on protected libraries is vulnerable to a buffer unmapping attack; we prevent this attack by preventing applications from removing pages that they share with the protected library. Fourth, we show how a trusted daemon can respond to asynchronous events and dynamically divide work with application threads in a protected library.   By extending and improving the protected library model, our work provides a new way to structure OS services, combining the advantages of kernel bypass and microkernels. We present a set of safety and performance guidelines for developers of protected libraries, and a set of recommendations for developers of future protected library operating systems. We demonstrate the convenience and performance of our approach with a prototype version of the DDS communication service. To the best of our knowledge, this prototype represents the first successful sharing of a kernel-bypass NIC among mutually untrusting applications. Relative to the commercial FastDDS implementation, we achieve approximately 50\\% lower latency and up to 7x throughput, with lower CPU utilization.",
        "authors": [
            "Alan Beadle",
            "Michael L. Scott",
            "John Criswell"
        ],
        "categories": [
            "cs.OS",
            "cs.CR"
        ],
        "submit_date": "2025-09-02"
    },
    "http://arxiv.org/abs/2509.02549v1": {
        "id": "http://arxiv.org/abs/2509.02549v1",
        "title": "Energy-Efficient Split Learning for Resource-Constrained Environments: A Smart Farming Solution",
        "link": "http://arxiv.org/abs/2509.02549v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-03",
        "tldr": "",
        "abstract": "Smart farming systems encounter significant challenges, including limited resources, the need for data privacy, and poor connectivity in rural areas. To address these issues, we present eEnergy-Split, an energy-efficient framework that utilizes split learning (SL) to enable collaborative model training without direct data sharing or heavy computation on edge devices. By distributing the model between edge devices and a central server, eEnergy-Split reduces on-device energy usage by up to 86 percent compared to federated learning (FL) while safeguarding data privacy. Moreover, SL improves classification accuracy by up to 6.2 percent over FL on ResNet-18 and by more modest amounts on GoogleNet and MobileNetV2. We propose an optimal edge deployment algorithm and a UAV trajectory planning strategy that solves the Traveling Salesman Problem (TSP) exactly to minimize flight cost and extend and maximize communication rounds. Comprehensive evaluations on agricultural pest datasets reveal that eEnergy-Split lowers UAV energy consumption compared to baseline methods and boosts overall accuracy by up to 17 percent. Notably, the energy efficiency of SL is shown to be model-dependent-yielding substantial savings in lightweight models like MobileNet, while communication and memory overheads may reduce efficiency gains in deeper networks. These results highlight the potential of combining SL with energy-aware design to deliver a scalable, privacy-preserving solution for resource-constrained smart farming environments.",
        "authors": [
            "Keiwan Soltani",
            "Vishesh Kumar Tanwar",
            "Ashish Gupta",
            "Sajal K. Das"
        ],
        "categories": [
            "cs.DC",
            "cs.ET"
        ],
        "submit_date": "2025-09-02"
    },
    "http://arxiv.org/abs/2509.02480v1": {
        "id": "http://arxiv.org/abs/2509.02480v1",
        "title": "MLP-Offload: Multi-Level, Multi-Path Offloading for LLM Pre-training to Break the GPU Memory Wall",
        "link": "http://arxiv.org/abs/2509.02480v1",
        "tags": [
            "offloading",
            "training"
        ],
        "relevant": true,
        "indexed_date": "2025-09-03",
        "tldr": "Proposes MLP-Offload, a multi-level multi-path offloading engine for LLM training to reduce I/O bottlenecks during backward and update phases. It offloads optimizer states across tiers with cache efficiency and concurrency control. Achieves 2.5× faster iterations compared to state-of-the-art.",
        "abstract": "Training LLMs larger than the aggregated memory of multiple GPUs is increasingly necessary due to the faster growth of LLM sizes compared to GPU memory. To this end, multi-tier host memory or disk offloading techniques are proposed by state of art. Despite advanced asynchronous multi-tier read/write strategies, such offloading strategies result in significant I/O overheads in the critical path of training, resulting in slower iterations. To this end, we propose MLP-Offload, a novel multi-level, multi-path offloading engine specifically designed for optimizing LLM training on resource-constrained setups by mitigating I/O bottlenecks. We make several key observations that drive the design of MLP-Offload, such as I/O overheads during the update dominate the iteration time; I/O bandwidth of the third-level remote storage tier remains unutilized; and, contention due to concurrent offloading amplifies I/O bottlenecks. Driven by these insights, we design and implement MLP-Offload to offload the optimizer states across multiple tiers in a cache-efficient and concurrency-controlled fashion to mitigate I/O bottlenecks during the backward and update phases. Evaluations on models up to 280B parameters shows that MLP-Offload achieves 2.5$\\times$ faster iterations compared to the state-of-the-art LLM training runtimes.",
        "authors": [
            "Avinash Maurya",
            "M. Mustafa Rafique",
            "Franck Cappello",
            "Bogdan Nicolae"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.LG"
        ],
        "submit_date": "2025-09-02"
    },
    "http://arxiv.org/abs/2509.02457v1": {
        "id": "http://arxiv.org/abs/2509.02457v1",
        "title": "Safe Memory Reclamation Techniques",
        "link": "http://arxiv.org/abs/2509.02457v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-03",
        "tldr": "",
        "abstract": "Safe memory reclamation is crucial to memory safety for optimistic and lock-free concurrent data structures in non garbage collected programming languages. However, several challenges arise in designing an ideal safe memory reclamation algorithm, including achieving high speed and scalability, easy of use for programmers, applicability to wide class of data structures, managing the large memory footprint caused by delayed freeing of memory for safety and performance, and avoiding asymmetric overhead on data structure operations. Several approaches to designing safe memory reclamation algorithms are studied by blending ideas and tools from across the hardware-software stack. These solutions cross traditional boundaries and exploit features exposed at different layers.",
        "authors": [
            "Ajay Singh"
        ],
        "categories": [
            "cs.DC",
            "cs.DS",
            "cs.PF",
            "cs.PL"
        ],
        "submit_date": "2025-09-02"
    },
    "http://arxiv.org/abs/2509.02449v1": {
        "id": "http://arxiv.org/abs/2509.02449v1",
        "title": "KubeIntellect: A Modular LLM-Orchestrated Agent Framework for End-to-End Kubernetes Management",
        "link": "http://arxiv.org/abs/2509.02449v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-03",
        "tldr": "",
        "abstract": "Kubernetes has become the foundation of modern cloud-native infrastructure, yet its management remains complex and fragmented. Administrators must navigate a vast API surface, manage heterogeneous workloads, and coordinate tasks across disconnected tools - often requiring precise commands, YAML configuration, and contextual expertise.   This paper presents KubeIntellect, a Large Language Model (LLM)-powered system for intelligent, end-to-end Kubernetes control. Unlike existing tools that focus on observability or static automation, KubeIntellect supports natural language interaction across the full spectrum of Kubernetes API operations, including read, write, delete, exec, access control, lifecycle, and advanced verbs. The system uses modular agents aligned with functional domains (e.g., logs, metrics, RBAC), orchestrated by a supervisor that interprets user queries, maintains workflow memory, invokes reusable tools, or synthesizes new ones via a secure Code Generator Agent.   KubeIntellect integrates memory checkpoints, human-in-the-loop clarification, and dynamic task sequencing into a structured orchestration framework. Evaluation results show a 93% tool synthesis success rate and 100% reliability across 200 natural language queries, demonstrating the system's ability to operate efficiently under diverse workloads. An automated demo environment is provided on Azure, with additional support for local testing via kind. This work introduces a new class of interpretable, extensible, and LLM-driven systems for managing complex infrastructure.",
        "authors": [
            "Mohsen Seyedkazemi Ardebili",
            "Andrea Bartolini"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-02"
    },
    "http://arxiv.org/abs/2509.02447v2": {
        "id": "http://arxiv.org/abs/2509.02447v2",
        "title": "An Efficient and Adaptive Watermark Detection System with Tile-based Error Correction",
        "link": "http://arxiv.org/abs/2509.02447v2",
        "tags": [
            "offline",
            "video",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-09-03",
        "tldr": "Proposes QRMark for efficient watermark detection in generated images, combining QR code error correction with tiling and resource-aware GPU scheduling. Achieves 2.43x inference speedup over baseline while maintaining accuracy against transformations.",
        "abstract": "Efficient and reliable detection of generated images is critical for the responsible deployment of generative models. Existing approaches primarily focus on improving detection accuracy and robustness under various image transformations and adversarial manipulations, yet they largely overlook the efficiency challenges of watermark detection across large-scale image collections. To address this gap, we propose QRMark, an efficient and adaptive end-to-end method for detecting embedded image watermarks. The core idea of QRMark is to combine QR Code-inspired error correction with tailored tiling techniques to improve detection efficiency while preserving accuracy and robustness. At the algorithmic level, QRMark employs a Reed-Solomon error correction mechanism to mitigate the accuracy degradation introduced by tiling. At the system level, QRMark implements a resource-aware multi-channel horizontal fusion policy that adaptively assigns more streams to GPU-intensive stages of the detection pipeline. It further employs a tile-based workload interleaving strategy to overlap data-loading overhead with computation and schedules kernels across stages to maximize efficiency. End-to-end evaluations show that QRMark achieves an average 2.43x inference speedup over the sequential baseline.",
        "authors": [
            "Xinrui Zhong",
            "Xinze Feng",
            "Jingwei Zuo",
            "Fanjiang Ye",
            "Yi Mu",
            "Junfeng Guo",
            "Heng Huang",
            "Myungjin Lee",
            "Yuke Wang"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-02"
    },
    "http://arxiv.org/abs/2509.02440v1": {
        "id": "http://arxiv.org/abs/2509.02440v1",
        "title": "Efficient Pyramidal Analysis of Gigapixel Images on a Decentralized Modest Computer Cluster",
        "link": "http://arxiv.org/abs/2509.02440v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-03",
        "tldr": "",
        "abstract": "Analyzing gigapixel images is recognized as computationally demanding. In this paper, we introduce PyramidAI, a technique for analyzing gigapixel images with reduced computational cost. The proposed approach adopts a gradual analysis of the image, beginning with lower resolutions and progressively concentrating on regions of interest for detailed examination at higher resolutions. We investigated two strategies for tuning the accuracy-computation performance trade-off when implementing the adaptive resolution selection, validated against the Camelyon16 dataset of biomedical images. Our results demonstrate that PyramidAI substantially decreases the amount of processed data required for analysis by up to 2.65x, while preserving the accuracy in identifying relevant sections on a single computer. To ensure democratization of gigapixel image analysis, we evaluated the potential to use mainstream computers to perform the computation by exploiting the parallelism potential of the approach. Using a simulator, we estimated the best data distribution and load balancing algorithm according to the number of workers. The selected algorithms were implemented and highlighted the same conclusions in a real-world setting. Analysis time is reduced from more than an hour to a few minutes using 12 modest workers, offering a practical solution for efficient large-scale image analysis.",
        "authors": [
            "Marie Reinbigler",
            "Rishi Sharma",
            "Rafael Pires",
            "Elisabeth Brunet",
            "Anne-Marie Kermarrec",
            "Catalin Fetita"
        ],
        "categories": [
            "cs.DC",
            "cs.CV"
        ],
        "submit_date": "2025-09-02"
    },
    "http://arxiv.org/abs/2509.01928v2": {
        "id": "http://arxiv.org/abs/2509.01928v2",
        "title": "A Continuous Energy Ising Machine Leveraging Difference-of-Convex Programming",
        "link": "http://arxiv.org/abs/2509.01928v2",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-03",
        "tldr": "",
        "abstract": "Many combinatorial optimization problems can be reformulated as finding the ground state of the Ising model. Existing Ising solvers are mostly inspired by simulated annealing. Although annealing techniques offer scalability, they lack convergence guarantees and are sensitive to the cooling schedule. We propose solving the Ising problem by relaxing the binary spins to continuous variables and introducing an attraction potential that steers the solution toward binary spin configurations. A key property of this potential is that its combination with the Ising energy produces a Hamiltonian that can be written as a difference of convex polynomials. This enables us to design efficient iterative algorithms that require a single matrix-vector multiplication per iteration and provide convergence guarantees. We implement our Ising solver on a wide range of GPU platforms, from edge devices to high-performance computing clusters, and demonstrate that it consistently outperforms existing solvers across problem sizes ranging from small ($10^3$ spins) to ultra-large ($10^8$ spins).",
        "authors": [
            "Debraj Banerjee",
            "Santanu Mahapatra",
            "Kunal Narayan Chaudhury"
        ],
        "categories": [
            "cs.DC",
            "math-ph",
            "math.OC",
            "quant-ph"
        ],
        "submit_date": "2025-09-02"
    },
    "http://arxiv.org/abs/2509.01811v1": {
        "id": "http://arxiv.org/abs/2509.01811v1",
        "title": "Optimal Parallel Scheduling under Concave Speedup Functions",
        "link": "http://arxiv.org/abs/2509.01811v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-03",
        "tldr": "",
        "abstract": "Efficient scheduling of parallel computation resources across multiple jobs is a fundamental problem in modern cloud/edge computing systems for many AI-based applications. Allocating more resources to a job accelerates its completion, but with diminishing returns. Prior work (heSRPT) solved this problem only for some specific speedup functions with an exponential form, providing a closed-form solution. However, the general case with arbitrary concave speedup functions -- which more accurately capture real-world workloads -- has remained open.   In this paper, we solve this open problem by developing optimal scheduling algorithms for parallel jobs under general concave speedup functions. We first discover a fundamental and broadly-applicable rule for optimal parallel scheduling, namely the Consistent Derivative Ratio (CDR) Rule, which states that the ratio of the derivatives of the speedup functions across active jobs remains constant over time. To efficiently compute the optimal allocations that satisfy the CDR Rule, we propose the General Water-Filling (GWF) method, a more general version of classical water-filling in wireless communications. Combining these insights, we design the SmartFill Algorithm to solve the general scheduling problem. Unlike heSRPT, which always allocates resources to all active jobs, SmartFill selectively determines which jobs should receive resources and how much they should be allocated. For a broad class of so-called \\emph{regular} speedup functions, SmartFill yields closed-form optimal solutions, while for non-regular functions it efficiently computes the optimum with low complexity. Numerical evaluations show that SmartFill can substantially outperform heSRPT across a wide range of concave speedup functions.",
        "authors": [
            "Chengzhang Li",
            "Peizhong Ju",
            "Atilla Eryilmaz",
            "Ness Shroff"
        ],
        "categories": [
            "cs.DC",
            "cs.PF"
        ],
        "submit_date": "2025-09-01"
    },
    "http://arxiv.org/abs/2509.01626v1": {
        "id": "http://arxiv.org/abs/2509.01626v1",
        "title": "STZ: A High Quality and High Speed Streaming Lossy Compression Framework for Scientific Data",
        "link": "http://arxiv.org/abs/2509.01626v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-03",
        "tldr": "",
        "abstract": "Error-bounded lossy compression is one of the most efficient solutions to reduce the volume of scientific data. For lossy compression, progressive decompression and random-access decompression are critical features that enable on-demand data access and flexible analysis workflows. However, these features can severely degrade compression quality and speed. To address these limitations, we propose a novel streaming compression framework that supports both progressive decompression and random-access decompression while maintaining high compression quality and speed. Our contributions are three-fold: (1) we design the first compression framework that simultaneously enables both progressive decompression and random-access decompression; (2) we introduce a hierarchical partitioning strategy to enable both streaming features, along with a hierarchical prediction mechanism that mitigates the impact of partitioning and achieves high compression quality -- even comparable to state-of-the-art (SOTA) non-streaming compressor SZ3; and (3) our framework delivers high compression and decompression speed, up to 6.7$\\times$ faster than SZ3.",
        "authors": [
            "Daoce Wang",
            "Pascal Grosset",
            "Jesus Pulido",
            "Jiannan Tian",
            "Tushar M. Athawale",
            "Jinda Jia",
            "Baixi Sun",
            "Boyuan Zhang",
            "Sian Jin",
            "Kai Zhao",
            "James Ahrens",
            "Fengguang Song"
        ],
        "categories": [
            "cs.DC",
            "cs.MM"
        ],
        "submit_date": "2025-09-01"
    },
    "http://arxiv.org/abs/2509.01425v1": {
        "id": "http://arxiv.org/abs/2509.01425v1",
        "title": "HiCR, an Abstract Model for Distributed Heterogeneous Programming",
        "link": "http://arxiv.org/abs/2509.01425v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-03",
        "tldr": "",
        "abstract": "We present HiCR, a model to represent the semantics of distributed heterogeneous applications and runtime systems. The model describes a minimal set of abstract operations to enable hardware topology discovery, kernel execution, memory management, communication, and instance management, without prescribing any implementation decisions. The goal of the model is to enable execution in current and future systems without the need for significant refactoring, while also being able to serve any governing parallel programming paradigm. In terms of software abstraction, HiCR is naturally located between distributed heterogeneous systems and runtime systems. We coin the phrase \\emph{Runtime Support Layer} for this level of abstraction. We explain how the model's components and operations are realized by a plugin-based approach that takes care of device-specific implementation details, and present examples of HiCR-based applications that operate equally on a diversity of platforms.",
        "authors": [
            "Sergio Miguel Martin",
            "Luca Terracciano",
            "Kiril Dichev",
            "Noah Baumann",
            "Jiashu Lin",
            "Albert-Jan Yzelman"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-01"
    },
    "http://arxiv.org/abs/2509.01229v1": {
        "id": "http://arxiv.org/abs/2509.01229v1",
        "title": "LiquidGEMM: Hardware-Efficient W4A8 GEMM Kernel for High-Performance LLM Serving",
        "link": "http://arxiv.org/abs/2509.01229v1",
        "tags": [
            "serving",
            "kernel",
            "quantization"
        ],
        "relevant": true,
        "indexed_date": "2025-09-03",
        "tldr": "Proposes LiquidGEMM, a GEMM kernel for W4A8 quantized LLM inference, featuring LiquidQuant for efficient dequantization and implicit pipelining. Achieves up to 4.94× end-to-end system speedup and 2.90× kernel speedup over baselines.",
        "abstract": "Quantization is a critical technique for accelerating LLM inference by reducing memory footprint and improving computational efficiency. Among various schemes, 4-bit weight and 8-bit activation quantization (W4A8) offers a strong balance between accuracy and performance. However, existing W4A8 GEMM kernels fall short in practice due to inefficient dequantization on CUDA Cores, which cannot keep pace with the high throughput of Tensor Cores. In this paper, we present LiquidGEMM, a hardware-efficient W4A8 GEMM kernel for efficient LLM serving. LiquidGEMM designs two key techniques: LiquidQuant, a hardware-efficient quantization method that enables fast, overflow-safe dequantization using just two arithmetic instructions per four elements; and an implicit fine-grained pipeline that fully overlaps weight loading, dequantization, and MMA across warp groups without software synchronization or redundant memory traffic. Experimental results show that LiquidGEMM achieves up to 2.90x speedup over state-of-the-art W4A8 kernels and up to 4.94x end-to-end system-level speedup. Compared to various quantized GEMM kernels in NVIDIA TensorRT-LLM, LiquidGEMM delivers 1.12-1.63x performance gains, and achieves up to 1.63x system-level speedup.",
        "authors": [
            "Huanqi Hu",
            "Bowen Xiao",
            "Shixuan Sun",
            "Jianian Yin",
            "Zhexi Zhang",
            "Xiang Luo",
            "Chengquan Jiang",
            "Weiqi Xu",
            "Xiaoying Jia",
            "Xin Liu",
            "Minyi Guo"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.LG"
        ],
        "submit_date": "2025-09-01"
    },
    "http://arxiv.org/abs/2509.01193v1": {
        "id": "http://arxiv.org/abs/2509.01193v1",
        "title": "LobRA: Multi-tenant Fine-tuning over Heterogeneous Data",
        "link": "http://arxiv.org/abs/2509.01193v1",
        "tags": [
            "training",
            "LoRA",
            "storage"
        ],
        "relevant": true,
        "indexed_date": "2025-09-03",
        "tldr": "Optimizes multi-tenant LoRA fine-tuning cost by addressing data heterogeneity. Proposes LobRA framework with heterogeneous replica configurations and workload-aware data dispatch. Reduces GPU seconds by 45.03%-60.67% for joint fine-tuning.",
        "abstract": "With the breakthrough of Transformer-based pre-trained models, the demand for fine-tuning (FT) to adapt the base pre-trained models to downstream applications continues to grow, so it is essential for service providers to reduce the cost of processing FT requests. Low-rank adaption (LoRA) is a widely used FT technique that only trains small-scale adapters and keeps the base model unaltered, conveying the possibility of processing multiple FT tasks by jointly training different LoRA adapters with a shared base model.   Nevertheless, through in-depth analysis, we reveal the efficiency of joint FT is dampened by two heterogeneity issues in the training data -- the sequence length variation and skewness. To tackle these issues, we develop LobRA, a brand new framework that supports processing multiple FT tasks by jointly training LoRA adapters. Two innovative designs are introduced. Firstly, LobRA deploys the FT replicas (i.e., model replicas for FT) with heterogeneous resource usages and parallel configurations, matching the diverse workloads caused by the sequence length variation. Secondly, for each training step, LobRA takes account of the sequence length skewness and dispatches the training data among the heterogeneous FT replicas to achieve workload balance. We conduct experiments to assess the performance of LobRA, validating that it significantly reduces the GPU seconds required for joint FT by 45.03%-60.67%.",
        "authors": [
            "Sheng Lin",
            "Fangcheng Fu",
            "Haoyang Li",
            "Hao Ge",
            "Xuanyu Wang",
            "Jiawen Niu",
            "Yaofeng Tu",
            "Bin Cui"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-09-01"
    },
    "http://arxiv.org/abs/2509.01083v3": {
        "id": "http://arxiv.org/abs/2509.01083v3",
        "title": "DSDE: Dynamic Speculative Decoding with KLD Stability for Real-World Serving",
        "link": "http://arxiv.org/abs/2509.01083v3",
        "tags": [
            "serving",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-09-03",
        "tldr": "Proposes DSDE, a speculative decoding framework with dynamic adaptation using KLD variance signals and adaptive speculation length cap. Achieves competitive latency and superior robustness, especially in low-acceptance-rate regimes, for LLM inference serving.",
        "abstract": "Speculative decoding accelerates large language model inference, but its reliance on a fixed speculation length is suboptimal in large-batch serving environments with diverse requests. This paper explores a new direction for dynamic adaptation by investigating a novel class of post-hoc, diagnostic signals. We propose Dynamic Speculative Decoding Engine (DSDE), a training-free framework built on two primary components: (1) a predictive signal based on the variance of the Kullback-Leibler (KLD) divergence, which diagnoses the generation's regional stability, and (2) an adaptive speculation length cap to mitigate the straggler problem in per-sequence decoding. Experiments demonstrate the potential of using KLD-based stability signals for dynamic adaptation. An algorithm guided by these signals achieves end-to-end latency competitive with leading baselines and exhibits superior robustness across diverse workloads. This robustness is particularly valuable in challenging low-acceptance-rate regimes, where the proposed signal maintains its diagnostic utility. Collectively, these findings validate post-hoc signals as a valuable component for building more robust and intelligent LLM inference systems, and highlight a promising direction for future research on dynamic speculation length adaptation.",
        "authors": [
            "Mingyu Yang",
            "Jae-Young Choi",
            "Kihyo Moon",
            "Minsung Jang",
            "Eunjoo Jeon"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.IT"
        ],
        "submit_date": "2025-09-01"
    },
    "http://arxiv.org/abs/2509.00937v1": {
        "id": "http://arxiv.org/abs/2509.00937v1",
        "title": "Parallelizing Drug Discovery: HPC Pipelines for Alzheimer's Molecular Docking and Simulation",
        "link": "http://arxiv.org/abs/2509.00937v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-03",
        "tldr": "",
        "abstract": "High-performance computing (HPC) is reshaping computational drug discovery by enabling large-scale, time-efficient molecular simulations. In this work, we explore HPC-driven pipelines for Alzheimer's disease drug discovery, focusing on virtual screening, molecular docking, and molecular dynamics simulations. We implemented a parallelised workflow using GROMACS with hybrid MPI-OpenMP strategies, benchmarking scaling performance across energy minimisation, equilibration, and production stages. Additionally, we developed a docking prototype that demonstrates significant runtime gains when moving from sequential execution to process-based parallelism using Python's multiprocessing library. Case studies on prolinamide derivatives and baicalein highlight the biological relevance of these workflows in targeting amyloid-beta and tau proteins. While limitations remain in data management, computational costs, and scaling efficiency, our results underline the potential of HPC to accelerate neurodegenerative drug discovery.",
        "authors": [
            "Paul Ruiz Alliata",
            "Diana Rubaga",
            "Daniel Kumlin",
            "Alberto Puliga"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-31"
    },
    "http://arxiv.org/abs/2509.00883v1": {
        "id": "http://arxiv.org/abs/2509.00883v1",
        "title": "Accelerating Latency-Critical Applications with AI-Powered Semi-Automatic Fine-Grained Parallelization on SMT Processors",
        "link": "http://arxiv.org/abs/2509.00883v1",
        "tags": [
            "serving",
            "kernel",
            "offline"
        ],
        "relevant": true,
        "indexed_date": "2025-09-03",
        "tldr": "Proposes Aira, an AI-driven tool for fine-grained parallelization of latency-critical apps on SMT processors. Uses LLM-guided hotspot analysis and performance simulation to auto-parallelize tasks via Relic framework. Achieves 17% geomean performance gain on industrial benchmarks.",
        "abstract": "Latency-critical applications tend to show low utilization of functional units due to frequent cache misses and mispredictions during speculative execution in high-performance superscalar processors. However, due to significant impact on single-thread performance, Simultaneous Multithreading (SMT) technology is rarely used with heavy threads of latency-critical applications. In this paper, we explore utilization of SMT technology to support fine-grained parallelization of latency-critical applications. Following the advancements in the development of Large Language Models (LLMs), we introduce Aira, an AI-powered Parallelization Adviser. To implement Aira, we extend AI Coding Agent in Cursor IDE with additional tools connected through Model Context Protocol, enabling end-to-end AI Agent for parallelization. Additional connected tools enable LLM-guided hotspot detection, collection of dynamic dependencies with Dynamic Binary Instrumentation, SMT-aware performance simulation to estimate performance gains. We apply Aira with Relic parallel framework for fine-grained task parallelism on SMT cores to parallelize latency-critical benchmarks representing real-world applications used in industry. We show 17% geomean performance gain from parallelization of latency-critical benchmarks using Aira with Relic framework.",
        "authors": [
            "Denis Los",
            "Igor Petushkov"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-08-31"
    },
    "http://arxiv.org/abs/2509.00642v1": {
        "id": "http://arxiv.org/abs/2509.00642v1",
        "title": "HADIS: Hybrid Adaptive Diffusion Model Serving for Efficient Text-to-Image Generation",
        "link": "http://arxiv.org/abs/2509.00642v1",
        "tags": [
            "serving",
            "diffusion",
            "quantization"
        ],
        "relevant": true,
        "indexed_date": "2025-09-03",
        "tldr": "Proposes HADIS, a hybrid adaptive serving system for diffusion-based text-to-image generation that dynamically routes prompts and allocates resources using offline-profiled Pareto-optimal configurations. Reduces latency violations by 2.7-45× while improving response quality by 35%.",
        "abstract": "Text-to-image diffusion models have achieved remarkable visual quality but incur high computational costs, making real-time, scalable deployment challenging. Existing query-aware serving systems mitigate the cost by cascading lightweight and heavyweight models, but most rely on a fixed cascade configuration and route all prompts through an initial lightweight stage, wasting resources on complex queries. We present HADIS, a hybrid adaptive diffusion model serving system that jointly optimizes cascade model selection, query routing, and resource allocation. HADIS employs a rule-based prompt router to send clearly hard queries directly to heavyweight models, bypassing the overhead of the lightweight stage. To reduce the complexity of resource management, HADIS uses an offline profiling phase to produce a Pareto-optimal cascade configuration table. At runtime, HADIS selects the best cascade configuration and GPU allocation given latency and workload constraints. Empirical evaluations on real-world traces demonstrate that HADIS improves response quality by up to 35% while reducing latency violation rates by 2.7-45$\\times$ compared to state-of-the-art model serving systems.",
        "authors": [
            "Qizheng Yang",
            "Tung-I Chen",
            "Siyu Zhao",
            "Ramesh K. Sitaraman",
            "Hui Guan"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-31"
    },
    "http://arxiv.org/abs/2509.00579v1": {
        "id": "http://arxiv.org/abs/2509.00579v1",
        "title": "KVComp: A High-Performance, LLM-Aware, Lossy Compression Framework for KV Cache",
        "link": "http://arxiv.org/abs/2509.00579v1",
        "tags": [
            "serving",
            "offloading",
            "quantization"
        ],
        "relevant": true,
        "indexed_date": "2025-09-03",
        "tldr": "Addresses KV cache memory bottleneck in long-context LLM inference. Proposes KVComp, a lossy compression framework with algorithm-hardware co-design. Achieves 47-83% higher memory reduction and reduces decompression overhead.",
        "abstract": "Transformer-based large language models (LLMs) demonstrate impressive potential in various practical applications. However, long context inference poses a significant challenge due to the enormous memory requirements of the key-value (KV) cache, which can scale to multiple gigabytes as sequence length and batch size increase. In this paper, we present KVComp, a generic and efficient KV cache management framework optimized for long-text generation that synergistically works with both latency-critical and throughput-critical inference systems. KVComp employs novel lossy compression techniques specifically designed for KV cache data characteristics, featuring careful co-design of compression algorithms and system architecture. Our approach maintains compatibility with the growing nature of KV cache while preserving high computational efficiency. Experimental results show that KVComp achieves on average 47\\% and up to 83\\% higher memory reduction rate compared to existing methods with little/no model accuracy degradation. Furthermore, KVComp achieves extremely high execution throughput, effectively reducing decompression overhead and, in some cases, even accelerating the matrix-vector multiplication operation and outperform cuBLAS-based attention kernels with less data movement.",
        "authors": [
            "Bo Jiang",
            "Taolue Yang",
            "Youyuan Liu",
            "Chengming Zhang",
            "Xubin He",
            "Sian Jin"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-08-30"
    },
    "http://arxiv.org/abs/2509.02481v1": {
        "id": "http://arxiv.org/abs/2509.02481v1",
        "title": "HydroGAT: Distributed Heterogeneous Graph Attention Transformer for Spatiotemporal Flood Prediction",
        "link": "http://arxiv.org/abs/2509.02481v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-03",
        "tldr": "",
        "abstract": "Accurate flood forecasting remains a challenge for water-resource management, as it demands modeling of local, time-varying runoff drivers (e.g., rainfall-induced peaks, baseflow trends) and complex spatial interactions across a river network. Traditional data-driven approaches, such as convolutional networks and sequence-based models, ignore topological information about the region. Graph Neural Networks (GNNs) propagate information exactly along the river network, which is ideal for learning hydrological routing. However, state-of-the-art GNN-based flood prediction models collapse pixels to coarse catchment polygons as the cost of training explodes with graph size and higher resolution. Furthermore, most existing methods treat spatial and temporal dependencies separately, either applying GNNs solely on spatial graphs or transformers purely on temporal sequences, thus failing to simultaneously capture spatiotemporal interactions critical for accurate flood prediction. We introduce a heterogenous basin graph where every land and river pixel is a node connected by physical hydrological flow directions and inter-catchment relationships. We propose HydroGAT, a spatiotemporal network that adaptively learns local temporal importance and the most influential upstream locations. Evaluated in two Midwestern US basins and across five baseline architectures, our model achieves higher NSE (up to 0.97), improved KGE (up to 0.96), and low bias (PBIAS within $\\pm$5%) in hourly discharge prediction, while offering interpretable attention maps that reveal sparse, structured intercatchment influences. To support high-resolution basin-scale training, we develop a distributed data-parallel pipeline that scales efficiently up to 64 NVIDIA A100 GPUs on NERSC Perlmutter supercomputer, demonstrating up to 15x speedup across machines. Our code is available at https://github.com/swapp-lab/HydroGAT.",
        "authors": [
            "Aishwarya Sarkar",
            "Autrin Hakimi",
            "Xiaoqiong Chen",
            "Hai Huang",
            "Chaoqun Lu",
            "Ibrahim Demir",
            "Ali Jannesari"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "submit_date": "2025-09-02"
    },
    "http://arxiv.org/abs/2509.02130v2": {
        "id": "http://arxiv.org/abs/2509.02130v2",
        "title": "Online Identification of IT Systems through Active Causal Learning",
        "link": "http://arxiv.org/abs/2509.02130v2",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-03",
        "tldr": "",
        "abstract": "Identifying a causal model of an IT system is fundamental to many branches of systems engineering and operation. Such a model can be used to predict the effects of control actions, optimize operations, diagnose failures, detect intrusions, etc., which is central to achieving the longstanding goal of automating network and system management tasks. Traditionally, causal models have been designed and maintained by domain experts. This, however, proves increasingly challenging with the growing complexity and dynamism of modern IT systems. In this paper, we present the first principled method for online, data-driven identification of an IT system in the form of a causal model. The method, which we call active causal learning, estimates causal functions that capture the dependencies among system variables in an iterative fashion using Gaussian process regression based on system measurements, which are collected through a rollout-based intervention policy. We prove that this method is optimal in the Bayesian sense and that it produces effective interventions. Experimental validation on a testbed shows that our method enables accurate identification of a causal system model while inducing low interference with system operations.",
        "authors": [
            "Kim Hammar",
            "Rolf Stadler"
        ],
        "categories": [
            "cs.LG",
            "cs.DC",
            "cs.NI"
        ],
        "submit_date": "2025-09-02"
    },
    "http://arxiv.org/abs/2509.02121v1": {
        "id": "http://arxiv.org/abs/2509.02121v1",
        "title": "Batch Query Processing and Optimization for Agentic Workflows",
        "link": "http://arxiv.org/abs/2509.02121v1",
        "tags": [
            "serving",
            "multi-modal",
            "RL"
        ],
        "relevant": true,
        "indexed_date": "2025-09-03",
        "tldr": "Proposes Halo, a system for optimizing batch processing in agentic LLM workflows by representing them as DAGs and applying cost-aware plan optimization. Introduces adaptive batching, KV-cache sharing, and compute-communication overlap. Achieves 18.6x batch inference speedup and 4.7x online throughput improvement.",
        "abstract": "Large Language Models (LLMs) in agentic workflows combine multi-step reasoning, tool use, and collaboration across multiple specialized agents. Existing LLM serving engines optimize individual calls in isolation, while multi-agent frameworks focus on orchestration without system-level performance planning. As a result, repeated prompts, overlapping contexts, and concurrent executions create substantial redundancy and poor GPU utilization, especially in batch analytics scenarios. We introduce Halo, a system that brings batch query processing and optimization into agentic LLM workflows. Halo represents each workflow as a structured query plan DAG and constructs a consolidated graph for batched queries that exposes shared computation. Guided by a cost model that jointly considers prefill and decode costs, cache reuse, and GPU placement, Halo performs plan-level optimization to minimize redundant execution. Its runtime integrates adaptive batching, KV-cache sharing and migration, along with compute-communication overlap to maximize hardware efficiency. Evaluation across six benchmarks shows that Halo achieves up to 18.6x speedup for batch inference and 4.7x throughput improvement under online serving, scaling to workloads of tens of thousands of queries and complex graphs. These gains are achieved without compromising output quality. By unifying query optimization with LLM serving, Halo enables efficient agentic workflows in data analytics and decision-making applications.",
        "authors": [
            "Junyi Shen",
            "Noppanat Wadlom",
            "Yao Lu"
        ],
        "categories": [
            "cs.DB",
            "cs.DC"
        ],
        "submit_date": "2025-09-02"
    },
    "http://arxiv.org/abs/2509.01966v1": {
        "id": "http://arxiv.org/abs/2509.01966v1",
        "title": "OASIS: Object-based Analytics Storage for Intelligent SQL Query Offloading in Scientific Tabular Workloads",
        "link": "http://arxiv.org/abs/2509.01966v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-03",
        "tldr": "",
        "abstract": "Computation-Enabled Object Storage (COS) systems, such as MinIO and Ceph, have recently emerged as promising storage solutions for post hoc, SQL-based analysis on large-scale datasets in High-Performance Computing (HPC) environments. By supporting object-granular layouts, COS facilitates column-oriented access and supports in-storage execution of data reduction operators, such as filters, close to where the data resides. Despite growing interest and adoption, existing COS systems exhibit several fundamental limitations that hinder their effectiveness. First, they impose rigid constraints on output data formats, limiting flexibility and interoperability. Second, they support offloading for only a narrow set of operators and expressions, restricting their applicability to more complex analytical tasks. Third--and perhaps most critically--they fail to incorporate design strategies that enable compute offloading optimized for the characteristics of deep storage hierarchies. To address these challenges, this paper proposes OASIS, a novel COS system that features: (i) flexible and interoperable output delivery through diverse formats, including columnar layouts such as Arrow; (ii) broad support for complex operators (e.g., aggregate, sort) and array-aware expressions, including element-wise predicates over array structures; and (iii) dynamic selection of optimal execution paths across internal storage layers, guided by operator characteristics and data movement costs. We implemented a prototype of OASIS and integrated it into the Spark analytics framework. Through extensive evaluation using real-world scientific queries from HPC workflows, OASIS achieves up to a 32.7% performance improvement over Spark configured with existing COS-based storage systems.",
        "authors": [
            "Soon Hwang",
            "Junhyeok Park",
            "Junghyun Ryu",
            "Seonghoon Ahn",
            "Jeoungahn Park",
            "Jeongjin Lee",
            "Soonyeal Yang",
            "Jungki Noh",
            "Woosuk Chung",
            "Hoshik Kim",
            "Youngjae Kim"
        ],
        "categories": [
            "cs.DB",
            "cs.DC"
        ],
        "submit_date": "2025-09-02"
    },
    "http://arxiv.org/abs/2509.00105v1": {
        "id": "http://arxiv.org/abs/2509.00105v1",
        "title": "AdaptCache: KV Cache Native Storage Hierarchy for Low-Delay and High-Quality Language Model Serving",
        "link": "http://arxiv.org/abs/2509.00105v1",
        "tags": [
            "serving",
            "offloading",
            "quantization"
        ],
        "relevant": true,
        "indexed_date": "2025-09-03",
        "tldr": "Addresses high loading delays in LLM serving due to KV cache storage on SSD. Proposes AdaptCache, a lossy compression system optimizing algorithm, rate, and device placement for KV cache entries to increase DRAM hits. Achieves 1.43-2.4x delay savings at same quality and 6-55% quality improvements at same delay.",
        "abstract": "Large language model (LLM) applications often reuse previously processed context, such as chat history and documents, which introduces significant redundant computation. Existing LLM serving systems address such redundant computation by storing the KV caches of processed context and loading the corresponding KV cache when a new request reuses the context. Further, as these LLM applications scale, the total size of KV caches becomes excessively large and requires both DRAM and SSD for full storage.   However, prior work that stores KV caches in DRAM and SSD suffers from high loading delays, as most KV cache hits come from SSD, which is slow to load. To increase the KV cache hit rate on DRAM, we identify lossy KV cache compression as a promising approach. We design a lossy compression system that decides the compression algorithm, compression rate and device placement for each KV cache entry to maximise DRAM hits and minimise loading delay without significantly degrading generation quality. Compared to various static compression baselines across three tasks, our system AdaptCache achieves 1.43--2.4 x delay savings at the same quality and 6--55% quality improvements at the same delay.",
        "authors": [
            "Shaoting Feng",
            "Hanchen Li",
            "Kuntai Du",
            "Zhuohan Gu",
            "Yuhan Liu",
            "Jiayi Yao",
            "Siddhant Ray",
            "Samuel Shen",
            "Yihua Cheng",
            "Ganesh Ananthanarayanan",
            "Junchen Jiang"
        ],
        "categories": [
            "cs.OS",
            "cs.AI",
            "cs.LG"
        ],
        "submit_date": "2025-08-28"
    },
    "http://arxiv.org/abs/2509.01245v4": {
        "id": "http://arxiv.org/abs/2509.01245v4",
        "title": "Towards Agentic OS: An LLM Agent Framework for Linux Schedulers",
        "link": "http://arxiv.org/abs/2509.01245v4",
        "tags": [
            "serving",
            "edge",
            "agentic"
        ],
        "relevant": true,
        "indexed_date": "2025-09-03",
        "tldr": "Addresses suboptimal OS scheduler performance due to semantic gap. Proposes SchedCP, a framework using autonomous LLM agents for analysis and policy generation in Linux. Achieves up to 1.79x performance improvement and 13x cost reduction compared to naive approaches.",
        "abstract": "Operating system schedulers suffer from a fundamental semantic gap, where kernel policies fail to understand application-specific needs, leading to suboptimal performance. We introduce SchedCP, the first framework that enables fully autonomous Large Language Model (LLM) agents to safely and efficiently optimize Linux schedulers without human involvement. Our core insight is that the challenge is not merely to apply a better LLM, but to architect a decoupled control plane that separates the AI's role of semantic reasoning (\"what to optimize\") from the system's role of execution (\"how to observe and act\"), thereby separating the optimization problem into two stages: goal-inference and policy-synthesis. Implemented as Model Context Protocol(MCP) server, SchedCP provides a stable interface with three key services: a Workload Analysis Engine, an evolving Scheduler Policy Repository, and an Execution Verifier that validates all AI-generated code and configure before deployment with static and dynamic analysis.   We demonstrate this architecture's power with sched-agent, a multi-agent system that autonomously analyzes workloads, synthesizes custom eBPF scheduling policies, and deploys them via the sched\\_ext infrastructure. Our evaluation shows that SchedCP achieves up to an 1.79x performance improvement, and a 13x cost reduction compared to naive agentic approaches, all while maintaining high success rate. By bridging the semantic gap, SchedCP democratizes expert-level system optimization and represents a step towards creating truly self-optimizing, application-aware operating systems. The code is open-sourced in https://github.com/eunomia-bpf/schedcp",
        "authors": [
            "Yusheng Zheng",
            "Yanpeng Hu",
            "Wei Zhang",
            "Andi Quinn"
        ],
        "categories": [
            "cs.AI",
            "cs.MA",
            "cs.OS"
        ],
        "submit_date": "2025-09-01"
    },
    "http://arxiv.org/abs/2508.21706v2": {
        "id": "http://arxiv.org/abs/2508.21706v2",
        "title": "Accelerating Mixture-of-Experts Inference by Hiding Offloading Latency with Speculative Decoding",
        "link": "http://arxiv.org/abs/2508.21706v2",
        "tags": [
            "MoE",
            "offloading",
            "speculative"
        ],
        "relevant": true,
        "indexed_date": "2025-09-01",
        "tldr": "Accelerates Mixture-of-Experts inference by hiding offloading latency with speculative decoding. Uses GPU-CPU orchestration, CPU chunked attention verification, and hyperparameter autotuning. Achieves up to 2.5× decode throughput improvement over state-of-the-art offloading techniques.",
        "abstract": "Recent advancements in Mixture of Experts (MoE) models have significantly increased their parameter scale as well as model performance. Extensive offloading techniques have been proposed to address the GPU memory limitations of MoE inference. However, due to the I/O bottleneck and sparse computation of MoE models, existing offloading techniques still suffer from low hardware utilization. To fully utilize the hardware resources, we propose SpecMoEOff, which employs the speculative decoding technique to enlarge the workload of each expert. SpecMoEOff orchestrates the GPU and CPU by both theoretical and empirical roofline analysis. In addition, we develop a dedicated CPU chunked attention verification kernel to fit the speculative decoding in offloading scenarios as well as minimizing the additional overhead led by draft models. SpecMoEOff further integrates an optimizer to automatically tune the hyperparameters of speculative decoding under given hardware and workload. Experimental results show that SpecMoEOff achieves up to 2.5x decode throughput improvement over the state-of-the-art MoE offloading techniques.",
        "authors": [
            "Zhibin Wang",
            "Zhonghui Zhang",
            "Yuhang Zhou",
            "Zibo Wang",
            "Mo Zhou",
            "Peng Jiang",
            "Weilin Cai",
            "Chengying Huan",
            "Rong Gu",
            "Sheng Zhong",
            "Chen Tian"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-29"
    },
    "http://arxiv.org/abs/2508.21613v3": {
        "id": "http://arxiv.org/abs/2508.21613v3",
        "title": "Odyssey: Adaptive Policy Selection for Resilient Distributed Training",
        "link": "http://arxiv.org/abs/2508.21613v3",
        "tags": [
            "training"
        ],
        "relevant": true,
        "indexed_date": "2025-09-01",
        "tldr": "Addresses frequent training interruptions in LLM training via adaptive fault-tolerant policy selection. Proposes Odyssey, which uses a unified performance model and expedient plan search to select optimal recovery strategies. Achieves within 11.00% performance gap of failure-free training, with up to 1.355× higher throughput than baselines.",
        "abstract": "Training large language models faces frequent interruptions due to various faults, demanding robust fault-tolerance. Existing backup-free methods, such as redundant computation, dynamic parallelism, and data rerouting, each incur performance penalties, whether from ongoing overhead, lengthy reconfigurations, or post-recovery inefficiencies. We propose Odyssey, an adaptive fault-tolerant system that intelligently selects optimal recovery strategies when a failure occurs. Odyssey achieves this through a unified performance model, expedient execution plan search, accurate performance estimation, and efficient communication optimizations. Experiments on a 32-card cluster show that Odyssey maintains a performance gap of within 11.00% between post-recovery and failure-free training, while preserving model convergence and efficient memory usage. Compared to state-of-the-art methods, Odyssey achieves up to 1.229x and 1.355x higher average throughput than Oobleck and Recycle, respectively.",
        "authors": [
            "Yuhang Zhou",
            "Zhibin Wang",
            "Peng Jiang",
            "Haoran Xia",
            "Junhe Lu",
            "Qianyu Jiang",
            "Rong Gu",
            "Hengxi Xu",
            "Xinjing Huang",
            "Guanghuan Fang",
            "Zhiheng Hu",
            "Jingyi Zhang",
            "Yongjin Cai",
            "Jian He",
            "Chen Tian"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-29"
    },
    "http://arxiv.org/abs/2508.21473v1": {
        "id": "http://arxiv.org/abs/2508.21473v1",
        "title": "Unpacking Maximum Extractable Value on Polygon: A Study on Atomic Arbitrage",
        "link": "http://arxiv.org/abs/2508.21473v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-01",
        "tldr": "",
        "abstract": "The evolution of blockchain technology, from its origins as a decentralized ledger for cryptocurrencies to its broader applications in areas like decentralized finance (DeFi), has significantly transformed financial ecosystems while introducing new challenges such as Maximum Extractable Value (MEV). This paper explores MEV on the Polygon blockchain, with a particular focus on Atomic Arbitrage (AA) transactions. We establish criteria for identifying AA transactions and analyze key factors such as searcher behavior, bidding dynamics, and token usage. Utilizing a dataset spanning 22 months and covering 23 million blocks, we examine MEV dynamics with a focus on Spam-based and Auction-based backrunning strategies. Our findings reveal that while Spam-based transactions are more prevalent, Auction-based transactions demonstrate greater profitability. Through detailed examples and analysis, we investigate the interactions between network architecture, transaction sequencing, and MEV extraction, offering comprehensive insights into the evolution and challenges of MEV in decentralized ecosystems. These results emphasize the need for robust transaction ordering mechanisms and highlight the implications of emerging MEV strategies for blockchain networks.",
        "authors": [
            "Daniil Vostrikov",
            "Yash Madhwal",
            "Andrey Seoev",
            "Anastasiia Smirnova",
            "Yury Yanovich",
            "Alexey Smirnov",
            "Vladimir Gorgadze"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-29"
    },
    "http://arxiv.org/abs/2508.21289v1": {
        "id": "http://arxiv.org/abs/2508.21289v1",
        "title": "Addressing Reproducibility Challenges in HPC with Continuous Integration",
        "link": "http://arxiv.org/abs/2508.21289v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-01",
        "tldr": "",
        "abstract": "The high-performance computing (HPC) community has adopted incentive structures to motivate reproducible research, with major conferences awarding badges to papers that meet reproducibility requirements. Yet, many papers do not meet such requirements. The uniqueness of HPC infrastructure and software, coupled with strict access requirements, may limit opportunities for reproducibility. In the absence of resource access, we believe that regular documented testing, through continuous integration (CI), coupled with complete provenance information, can be used as a substitute. Here, we argue that better HPC-compliant CI solutions will improve reproducibility of applications. We present a survey of reproducibility initiatives and describe the barriers to reproducibility in HPC. To address existing limitations, we present a GitHub Action, CORRECT, that enables secure execution of tests on remote HPC resources. We evaluate CORRECT's usability across three different types of HPC applications, demonstrating the effectiveness of using CORRECT for automating and documenting reproducibility evaluations.",
        "authors": [
            "Valérie Hayot-Sasson",
            "Nathaniel Hudson",
            "André Bauer",
            "Maxime Gonthier",
            "Ian Foster",
            "Kyle Chard"
        ],
        "categories": [
            "cs.DC",
            "cs.SE"
        ],
        "submit_date": "2025-08-29"
    },
    "http://arxiv.org/abs/2508.21230v1": {
        "id": "http://arxiv.org/abs/2508.21230v1",
        "title": "Fast and Scalable Mixed Precision Euclidean Distance Calculations Using GPU Tensor Cores",
        "link": "http://arxiv.org/abs/2508.21230v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-01",
        "tldr": "",
        "abstract": "Modern GPUs are equipped with tensor cores (TCs) that are commonly used for matrix multiplication in artificial intelligence workloads. However, because they have high computational throughput, they can lead to significant performance gains in other algorithms if they can be successfully exploited. We examine using TCs to compute Euclidean distance calculations, which are used in many data analytics applications. Prior work has only investigated using 64 bit floating point (FP64) data for computation; however, TCs can operate on lower precision floating point data (i.e., 16 bit matrix multiplication and 32 bit accumulation), which we refer to as FP16-32. FP16-32 TC peak throughput is so high that TCs are easily starved of data. We propose a Fast and Scalable Tensor core Euclidean Distance (FaSTED) algorithm. To achieve high computational throughput, we design FaSTED for significant hierarchical reuse of data and maximize memory utilization at every level (global memory, shared memory, and registers). We apply FaSTED to the application of similarity searches, which typically employ an indexing data structure to eliminate superfluous Euclidean distance calculations. We compare to the state-of-the-art (SOTA) TC Euclidean distance algorithm in the literature that employs FP64, as well as to two single precision (FP32) CUDA core algorithms that both employ an index. We find that across four real-world high-dimensional datasets spanning 128-960 dimensions, the mixed-precision brute force approach achieves a speedup over the SOTA algorithms of 2.5-51x. We also quantify the accuracy loss of our mixed precision algorithm to be less than <0.06% when compared to the FP64 baseline.",
        "authors": [
            "Brian Curless",
            "Michael Gowanlock"
        ],
        "categories": [
            "cs.DC",
            "cs.PF"
        ],
        "submit_date": "2025-08-28"
    },
    "http://arxiv.org/abs/2508.21431v1": {
        "id": "http://arxiv.org/abs/2508.21431v1",
        "title": "An Optimistic Gradient Tracking Method for Distributed Minimax Optimization",
        "link": "http://arxiv.org/abs/2508.21431v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-09-01",
        "tldr": "",
        "abstract": "This paper studies the distributed minimax optimization problem over networks. To enhance convergence performance, we propose a distributed optimistic gradient tracking method, termed DOGT, which solves a surrogate function that captures the similarity between local objective functions to approximate a centralized optimistic approach locally. Leveraging a Lyapunov-based analysis, we prove that DOGT achieves linear convergence to the optimal solution for strongly convex-strongly concave objective functions while remaining robust to the heterogeneity among them. Moreover, by integrating an accelerated consensus protocol, the accelerated DOGT (ADOGT) algorithm achieves an optimal convergence rate of $\\mathcal{O} \\left( κ\\log \\left( ε^{-1} \\right) \\right)$ and communication complexity of $\\mathcal{O} \\left( κ\\log \\left( ε^{-1} \\right) /\\sqrt{1-\\sqrt{ρ_W}} \\right)$ for a suboptimality level of $ε>0$, where $κ$ is the condition number of the objective function and $ρ_W$ is the spectrum gap of the network. Numerical experiments illustrate the effectiveness of the proposed algorithms.",
        "authors": [
            "Yan Huang",
            "Jinming Xu",
            "Jiming Chen",
            "Karl Henrik Johansson"
        ],
        "categories": [
            "math.OC",
            "cs.DC"
        ],
        "submit_date": "2025-08-29"
    },
    "http://arxiv.org/abs/2508.20508v1": {
        "id": "http://arxiv.org/abs/2508.20508v1",
        "title": "Collaborative Evolution of Intelligent Agents in Large-Scale Microservice Systems",
        "link": "http://arxiv.org/abs/2508.20508v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-29",
        "tldr": "",
        "abstract": "This paper proposes an intelligent service optimization method based on a multi-agent collaborative evolution mechanism to address governance challenges in large-scale microservice architectures. These challenges include complex service dependencies, dynamic topology structures, and fluctuating workloads. The method models each service as an agent and introduces graph representation learning to construct a service dependency graph. This enables agents to perceive and embed structural changes within the system. Each agent learns its policy based on a Markov Decision Process. A centralized training and decentralized execution framework is used to integrate local autonomy with global coordination. To enhance overall system performance and adaptability, a game-driven policy optimization mechanism is designed. Through a selection-mutation process, agent strategy distributions are dynamically adjusted. This supports adaptive collaboration and behavioral evolution among services. Under this mechanism, the system can quickly respond and achieve stable policy convergence when facing scenarios such as sudden workload spikes, topology reconfigurations, or resource conflicts. To evaluate the effectiveness of the proposed method, experiments are conducted on a representative microservice simulation platform. Comparative analyses are performed against several advanced approaches, focusing on coordination efficiency, adaptability, and policy convergence performance. Experimental results show that the proposed method outperforms others in several key metrics. It significantly improves governance efficiency and operational stability in large-scale microservice systems. The method demonstrates strong practical value and engineering feasibility.",
        "authors": [
            "Yilin Li",
            "Song Han",
            "Sibo Wang",
            "Ming Wang",
            "Renzi Meng"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-28"
    },
    "http://arxiv.org/abs/2508.20403v1": {
        "id": "http://arxiv.org/abs/2508.20403v1",
        "title": "pdGRASS: A Fast Parallel Density-Aware Algorithm for Graph Spectral Sparsification",
        "link": "http://arxiv.org/abs/2508.20403v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-29",
        "tldr": "",
        "abstract": "Graph Spectral Sparsification (GSS) identifies an ultra-sparse subgraph, or sparsifier, whose Laplacian matrix closely approximates the spectral properties of the original graph, enabling substantial reductions in computational complexity for computationally intensive problems in scientific computing. The state-of-the-art method for efficient GSS is feGRASS, consisting of two steps: 1) spanning tree generation and 2) off-tree edge recovery. However, feGRASS suffers from two main issues: 1) difficulties in parallelizing the recovery step for strict data dependencies, and 2) performance degradation on skewed inputs, often requiring multiple passes to recover sufficient edges. To address these challenges, we propose parallel density-aware Graph Spectral Sparsification (pdGRASS), a parallel algorithm that organizes edges into disjoint subtasks without data dependencies between them, enabling efficient parallelization and sufficient edge recovery in a single pass. We empirically evaluate feGRASS and pdGRASS based on 1) off-tree edge-recovery runtime and 2) sparsifier quality, measured by the iteration count required for convergence in a preconditioned conjugate gradient (PCG) application. The evaluation demonstrates that, depending on the number of edges recovered, pdGRASS achieves average speedups ranging from 3.9x to 8.8x. The resulting sparsifiers also show between 1.2x higher and 1.8x lower PCG iteration counts, with further improvements as more edges are recovered. Additionally, pdGRASS mitigates the worst-case runtimes of feGRASS with over 1000x speedup. These results highlight pdGRASS's significant improvements in scalability and performance for the graph spectral sparsification problem.",
        "authors": [
            "Tiancheng Zhao",
            "Zekun Yin",
            "Huihai An",
            "Xiaoyu Yang",
            "Zhou Jin",
            "Jiasi Shen",
            "Helen Xu"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-28"
    },
    "http://arxiv.org/abs/2508.20375v1": {
        "id": "http://arxiv.org/abs/2508.20375v1",
        "title": "CoFormer: Collaborating with Heterogeneous Edge Devices for Scalable Transformer Inference",
        "link": "http://arxiv.org/abs/2508.20375v1",
        "tags": [
            "edge",
            "serving",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-08-29",
        "tldr": "Addresses scalable Transformer inference on resource-constrained edge devices. Proposes collaborative system (CoFormer) by decomposing large models into smaller ones for distributed execution with aggregation. Achieves 3.1× speedup and 76.3% memory reduction for GPT2-XL inference.",
        "abstract": "The impressive performance of transformer models has sparked the deployment of intelligent applications on resource-constrained edge devices. However, ensuring high-quality service for real-time edge systems is a significant challenge due to the considerable computational demands and resource requirements of these models. Existing strategies typically either offload transformer computations to other devices or directly deploy compressed models on individual edge devices. These strategies, however, result in either considerable communication overhead or suboptimal trade-offs between accuracy and efficiency. To tackle these challenges, we propose a collaborative inference system for general transformer models, termed CoFormer. The central idea behind CoFormer is to exploit the divisibility and integrability of transformer. An off-the-shelf large transformer can be decomposed into multiple smaller models for distributed inference, and their intermediate results are aggregated to generate the final output. We formulate an optimization problem to minimize both inference latency and accuracy degradation under heterogeneous hardware constraints. DeBo algorithm is proposed to first solve the optimization problem to derive the decomposition policy, and then progressively calibrate decomposed models to restore performance. We demonstrate the capability to support a wide range of transformer models on heterogeneous edge devices, achieving up to 3.1$\\times$ inference speedup with large transformer models. Notably, CoFormer enables the efficient inference of GPT2-XL with 1.6 billion parameters on edge devices, reducing memory requirements by 76.3\\%. CoFormer can also reduce energy consumption by approximately 40\\% while maintaining satisfactory inference performance.",
        "authors": [
            "Guanyu Xu",
            "Zhiwei Hao",
            "Li Shen",
            "Yong Luo",
            "Fuhui Sun",
            "Xiaoyan Wang",
            "Han Hu",
            "Yonggang Wen"
        ],
        "categories": [
            "cs.DC",
            "cs.LG",
            "cs.PF"
        ],
        "submit_date": "2025-08-28"
    },
    "http://arxiv.org/abs/2508.20274v1": {
        "id": "http://arxiv.org/abs/2508.20274v1",
        "title": "Predictable LLM Serving on GPU Clusters",
        "link": "http://arxiv.org/abs/2508.20274v1",
        "tags": [
            "serving",
            "offloading",
            "hardware"
        ],
        "relevant": true,
        "indexed_date": "2025-08-29",
        "tldr": "Addresses latency interference in LLM serving due to PCIe contention on multi-tenant GPUs. Proposes a dynamic host controller with MIG reconfiguration, PCIe-aware placement, and guardrails. Reduces SLO miss-rate by ~32% and improves p99 latency by ~15% with ≤5% throughput cost.",
        "abstract": "Latency-sensitive inference on shared A100 clusters often suffers noisy-neighbor interference on the PCIe fabric, inflating tail latency and SLO violations. We present a fabric-agnostic, VM-deployable host-level controller that combines dynamic Multi-Instance GPU (MIG) reconfiguration, PCIe-aware placement, and lightweight guardrails (MPS quotas, cgroup I/O). It samples per-tenant tails and system signals, uses topology hints to avoid PCIe hot spots, and gates actions with dwell/cool-down to avoid thrash. On a single host and a 2-node (16-GPU) cluster, SLO miss-rate is reduced by \\(\\approx\\)32\\% (\\(\\approx\\)1.5) and p99 latency improves \\(\\approx\\)15\\% with \\(\\leq\\)5\\% throughput cost versus static MIG and naive placement; ablations show MIG and placement contribute comparably. We also evaluate LLM serving with vLLM on OLMo 2 7B Instruct: TTFT p99 improves \\(\\approx\\)10--15\\% at \\(\\leq\\)5\\% cost without changing the controller.",
        "authors": [
            "Erfan Darzi",
            "Shreeanant Bharadwaj",
            "Sree Bhargavi Balija"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-27"
    },
    "http://arxiv.org/abs/2508.20258v1": {
        "id": "http://arxiv.org/abs/2508.20258v1",
        "title": "SwizzlePerf: Hardware-Aware LLMs for GPU Kernel Performance Optimization",
        "link": "http://arxiv.org/abs/2508.20258v1",
        "tags": [
            "kernel",
            "hardware",
            "training"
        ],
        "relevant": true,
        "indexed_date": "2025-08-29",
        "tldr": "Addresses inefficient hardware-unaware optimization for GPU kernels. Proposes SwizzlePerf, an LLM-based approach with explicit hardware-awareness using memory patterns, architecture specs, and profiling. Achieves up to 2.06x speedup and 70% L2 hit rate improvement on ML/Science kernels.",
        "abstract": "Large language models (LLMs) have shown progress in GPU kernel performance engineering using inefficient search-based methods that optimize around runtime. Any existing approach lacks a key characteristic that human performance engineers rely on for near-optimal utilization -- hardware-awareness. By leveraging the workload's specific memory access patterns, architecture specifications, filtered profiling logs, and reflections on historical performance, we can make software-level optimizations that are tailored to the underlying hardware. SwizzlePerf automatically generates spatial optimizations for GPU kernels on disaggregated architectures by giving LLMs explicit hardware-awareness.   For a GEMM kernel, SwizzlePerf takes less than 5 minutes to generate the same hardware-specific optimal swizzling pattern that took expert performance engineers 2 weeks to find. On a suite of 10 diverse ML and Science kernels, SwizzlePerf can generate swizzling patterns for 9 of the kernels that achieve up to a 2.06x speedup and 70% improvement in L2 hit rate. This work is the first of many steps toward systematically creating hardware-aware LLM performance engineering agents.",
        "authors": [
            "Arya Tschand",
            "Muhammad Awad",
            "Ryan Swann",
            "Kesavan Ramakrishnan",
            "Jeffrey Ma",
            "Keith Lowery",
            "Ganesh Dasika",
            "Vijay Janapa Reddi"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-08-27"
    },
    "http://arxiv.org/abs/2508.20253v1": {
        "id": "http://arxiv.org/abs/2508.20253v1",
        "title": "SpeedMalloc: Improving Multi-threaded Applications via a Lightweight Core for Memory Allocation",
        "link": "http://arxiv.org/abs/2508.20253v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-29",
        "tldr": "",
        "abstract": "Memory allocation, though constituting only a small portion of the executed code, can have a \"butterfly effect\" on overall program performance, leading to significant and far-reaching impacts. Despite accounting for just approximately 5% of total instructions, memory allocation can result in up to a 2.7x performance variation depending on the allocator used. This effect arises from the complexity of memory allocation in modern multi-threaded multi-core systems, where allocator metadata becomes intertwined with user data, leading to cache pollution or increased cross-thread synchronization overhead. Offloading memory allocators to accelerators, e.g., Mallacc and Memento, is a potential direction to improve the allocator performance and mitigate cache pollution. However, these accelerators currently have limited support for multi-threaded applications, and synchronization between cores and accelerators remains a significant challenge.   We present SpeedMalloc, using a lightweight support-core to process memory allocation tasks in multi-threaded applications. The support-core is a lightweight programmable processor with efficient cross-core data synchronization and houses all allocator metadata in its own caches. This design minimizes cache conflicts with user data and eliminates the need for cross-core metadata synchronization. In addition, using a general-purpose core instead of domain-specific accelerators makes SpeedMalloc capable of adopting new allocator designs. We compare SpeedMalloc with state-of-the-art software and hardware allocators, including Jemalloc, TCMalloc, Mimalloc, Mallacc, and Memento. SpeedMalloc achieves 1.75x, 1.18x, 1.15x, 1.23x, and 1.18x speedups on multithreaded workloads over these five allocators, respectively.",
        "authors": [
            "Ruihao Li",
            "Qinzhe Wu",
            "Krishna Kavi",
            "Gayatri Mehta",
            "Jonathan C. Beard",
            "Neeraja J. Yadwadkar",
            "Lizy K. John"
        ],
        "categories": [
            "cs.DC",
            "cs.AR"
        ],
        "submit_date": "2025-08-27"
    },
    "http://arxiv.org/abs/2508.20645v1": {
        "id": "http://arxiv.org/abs/2508.20645v1",
        "title": "A Hybrid Stochastic Gradient Tracking Method for Distributed Online Optimization Over Time-Varying Directed Networks",
        "link": "http://arxiv.org/abs/2508.20645v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-29",
        "tldr": "",
        "abstract": "With the increasing scale and dynamics of data, distributed online optimization has become essential for real-time decision-making in various applications. However, existing algorithms often rely on bounded gradient assumptions and overlook the impact of stochastic gradients, especially in time-varying directed networks. This study proposes a novel Time-Varying Hybrid Stochastic Gradient Tracking algorithm named TV-HSGT, based on hybrid stochastic gradient tracking and variance reduction mechanisms. Specifically, TV-HSGT integrates row-stochastic and column-stochastic communication schemes over time-varying digraphs, eliminating the need for Perron vector estimation or out-degree information. By combining current and recursive stochastic gradients, it effectively reduces gradient variance while accurately tracking global descent directions. Theoretical analysis demonstrates that TV-HSGT can achieve improved bounds on dynamic regret without assuming gradient boundedness. Experimental results on logistic regression tasks confirm the effectiveness of TV-HSGT in dynamic and resource-constrained environments.",
        "authors": [
            "Xinli Shi",
            "Xingxing Yuan",
            "Longkang Zhu",
            "Guanghui Wen"
        ],
        "categories": [
            "cs.LG",
            "cs.DC",
            "math.OC"
        ],
        "submit_date": "2025-08-28"
    },
    "http://arxiv.org/abs/2508.20603v1": {
        "id": "http://arxiv.org/abs/2508.20603v1",
        "title": "High performance visualization for Astronomy and Cosmology: the VisIVO's pathway toward Exascale systems",
        "link": "http://arxiv.org/abs/2508.20603v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-29",
        "tldr": "",
        "abstract": "Petabyte-scale data volumes are generated by observations and simulations in modern astronomy and astrophysics. Storage, access, and data analysis are significantly hampered by such data volumes and are leading to the development of a new generation of software tools. The Visualization Interface for the Virtual Observatory (VisIVO) has been designed, developed and maintained by INAF since 2005 to perform multi-dimensional data analysis and knowledge discovery in multivariate astrophysical datasets. Utilizing containerization and virtualization technologies, VisIVO has already been used to exploit distributed computing infrastructures including the European Open Science Cloud (EOSC).   We intend to adapt VisIVO solutions for high performance visualization of data generated on the (pre-)Exascale systems by HPC applications in Astrophysics and Cosmology (A\\&C), including GADGET (GAlaxies with Dark matter and Gas) and PLUTO simulations, thanks to the collaboration within the SPACE Center of Excellence, the H2020 EUPEX Project, and the ICSC National Research Centre. In this work, we outline the evolution's course as well as the execution strategies designed to achieve the following goals: enhance the portability of the VisIVO modular applications and their resource requirements; foster reproducibility and maintainability; take advantage of a more flexible resource exploitation over heterogeneous HPC facilities; and, finally, minimize data-movement overheads and improve I/O performances.",
        "authors": [
            "Eva Sciacca",
            "Nicola Tuccari",
            "Fabio Vitello",
            "Valentina Cesare"
        ],
        "categories": [
            "astro-ph.IM",
            "cs.DC"
        ],
        "submit_date": "2025-08-28"
    },
    "http://arxiv.org/abs/2508.20333v1": {
        "id": "http://arxiv.org/abs/2508.20333v1",
        "title": "Poison Once, Refuse Forever: Weaponizing Alignment for Injecting Bias in LLMs",
        "link": "http://arxiv.org/abs/2508.20333v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-29",
        "tldr": "",
        "abstract": "Large Language Models (LLMs) are aligned to meet ethical standards and safety requirements by training them to refuse answering harmful or unsafe prompts. In this paper, we demonstrate how adversaries can exploit LLMs' alignment to implant bias, or enforce targeted censorship without degrading the model's responsiveness to unrelated topics. Specifically, we propose Subversive Alignment Injection (SAI), a poisoning attack that leverages the alignment mechanism to trigger refusal on specific topics or queries predefined by the adversary. Although it is perhaps not surprising that refusal can be induced through overalignment, we demonstrate how this refusal can be exploited to inject bias into the model. Surprisingly, SAI evades state-of-the-art poisoning defenses including LLM state forensics, as well as robust aggregation techniques that are designed to detect poisoning in FL settings. We demonstrate the practical dangers of this attack by illustrating its end-to-end impacts on LLM-powered application pipelines. For chat based applications such as ChatDoctor, with 1% data poisoning, the system refuses to answer healthcare questions to targeted racial category leading to high bias ($ΔDP$ of 23%). We also show that bias can be induced in other NLP tasks: for a resume selection pipeline aligned to refuse to summarize CVs from a selected university, high bias in selection ($ΔDP$ of 27%) results. Even higher bias ($ΔDP$~38%) results on 9 other chat based downstream applications.",
        "authors": [
            "Md Abdullah Al Mamun",
            "Ihsen Alouani",
            "Nael Abu-Ghazaleh"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "cs.DC"
        ],
        "submit_date": "2025-08-28"
    },
    "http://arxiv.org/abs/2508.20016v2": {
        "id": "http://arxiv.org/abs/2508.20016v2",
        "title": "HPC Digital Twins for Evaluating Scheduling Policies, Incentive Structures and their Impact on Power and Cooling",
        "link": "http://arxiv.org/abs/2508.20016v2",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-28",
        "tldr": "",
        "abstract": "Schedulers are critical for optimal resource utilization in high-performance computing. Traditional methods to evaluate schedulers are limited to post-deployment analysis, or simulators, which do not model associated infrastructure. In this work, we present the first-of-its-kind integration of scheduling and digital twins in HPC. This enables what-if studies to understand the impact of parameter configurations and scheduling decisions on the physical assets, even before deployment, or regarching changes not easily realizable in production. We (1) provide the first digital twin framework extended with scheduling capabilities, (2) integrate various top-tier HPC systems given their publicly available datasets, (3) implement extensions to integrate external scheduling simulators. Finally, we show how to (4) implement and evaluate incentive structures, as-well-as (5) evaluate machine learning based scheduling, in such novel digital-twin based meta-framework to prototype scheduling. Our work enables what-if scenarios of HPC systems to evaluate sustainability, and the impact on the simulated system.",
        "authors": [
            "Matthias Maiterth",
            "Wesley H. Brewer",
            "Jaya S. Kuruvella",
            "Arunavo Dey",
            "Tanzima Z. Islam",
            "Kevin Menear",
            "Dmitry Duplyakin",
            "Rashadul Kabir",
            "Tapasya Patki",
            "Terry Jones",
            "Feiyi Wang"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.ET",
            "eess.SY"
        ],
        "submit_date": "2025-08-27"
    },
    "http://arxiv.org/abs/2508.19805v3": {
        "id": "http://arxiv.org/abs/2508.19805v3",
        "title": "Beyond Pairwise Comparisons: Unveiling Structural Landscape of Mobile Robot Models",
        "link": "http://arxiv.org/abs/2508.19805v3",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-28",
        "tldr": "",
        "abstract": "Understanding the computational power of mobile robot systems is a fundamental challenge in distributed computing. While prior work has focused on pairwise separations between models, we explore how robot capabilities, light observability, and scheduler synchrony interact in more complex ways.   We first show that the Exponential Times Expansion (ETE) problem is solvable only in the strongest model -- fully-synchronous robots with full mutual lights ($\\mathcal{LUMT}^F$). We then introduce the Hexagonal Edge Traversal (HET) and TAR(d)* problems to demonstrate how internal memory and lights interact with synchrony: under weak synchrony, internal memory alone is insufficient, while full synchrony can substitute for both lights and memory.   In the asynchronous setting, we classify problems such as LP-MLCv, VEC, and ZCC to show fine-grained separations between $\\mathcal{FSTA}$ and $\\mathcal{FCOM}$ robots. We also analyze Vertex Traversal Rendezvous (VTR) and Leave Place Convergence (LP-Cv), illustrating the limitations of internal memory in symmetric settings.   These results extend the known separation map of 14 canonical robot models, revealing structural phenomena only visible through higher-order comparisons. Our work provides new impossibility criteria and deepens the understanding of how observability, memory, and synchrony collectively shape the computational power of mobile robots.",
        "authors": [
            "Shota Naito",
            "Tsukasa Ninomiya",
            "Koichi Wada"
        ],
        "categories": [
            "cs.DC",
            "cs.RO"
        ],
        "submit_date": "2025-08-27"
    },
    "http://arxiv.org/abs/2508.19670v1": {
        "id": "http://arxiv.org/abs/2508.19670v1",
        "title": "Beyond the Bermuda Triangle of Contention: IOMMU Interference in Mixed Criticality Systems",
        "link": "http://arxiv.org/abs/2508.19670v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-28",
        "tldr": "",
        "abstract": "As Mixed Criticality Systems (MCSs) evolve, they increasingly integrate heterogeneous computing platforms, combining general-purpose processors with specialized accelerators such as AI engines, GPUs, and high-speed networking interfaces. This heterogeneity introduces challenges, as these accelerators and DMA-capable devices act as independent bus masters, directly accessing memory. Consequently, ensuring both security and timing predictability in such environments becomes critical. To address these concerns, the Input-Output Memory Management Unit (IOMMU) plays a key role in mediating and regulating memory access, preventing unauthorized transactions while enforcing isolation and access control policies. While prior work has explored IOMMU-related side-channel vulnerabilities from a security standpoint, its role in performance interference remains largely unexplored. Moreover, many of the same architectural properties that enable side-channel leakage, such as shared TLBs, caching effects, and translation overheads, can also introduce timing unpredictability. In this work, we analyze the contention effects within IOMMU structures using the Xilinx UltraScale+ ZCU104 platform, demonstrating how their shared nature introduce unpredictable delays. Our findings reveal that IOMMU-induced interference primarily affects small memory transactions, where translation overheads significantly impact execution time. Additionally, we hypothesize that contention effects arising from IOTLBs exhibit similar behavior across architectures due to shared caching principles, such as prefetching and hierarchical TLB structures. Notably, our experiments show that IOMMU interference can delay DMA transactions by up to 1.79x for lower-size transfers on the Arm SMMUv2 implementation.",
        "authors": [
            "Diogo Costa",
            "Jose Martins",
            "Sandro Pinto"
        ],
        "categories": [
            "cs.DC",
            "eess.SY"
        ],
        "submit_date": "2025-08-27"
    },
    "http://arxiv.org/abs/2508.19559v1": {
        "id": "http://arxiv.org/abs/2508.19559v1",
        "title": "Taming the Chaos: Coordinated Autoscaling for Heterogeneous and Disaggregated LLM Inference",
        "link": "http://arxiv.org/abs/2508.19559v1",
        "tags": [
            "serving",
            "autoscaling",
            "disaggregation"
        ],
        "relevant": true,
        "indexed_date": "2025-08-28",
        "tldr": "Introduces HeteroScale, a coordinated autoscaling framework for disaggregated Prefill-Decode LLM serving. Combines topology-aware scheduling and a novel metric-driven policy for joint scaling of prefill/decode pools. Achieves 26.6 percentage-point increase in GPU utilization, saving hundreds of thousands of GPU-hours daily.",
        "abstract": "Serving Large Language Models (LLMs) is a GPU-intensive task where traditional autoscalers fall short, particularly for modern Prefill-Decode (P/D) disaggregated architectures. This architectural shift, while powerful, introduces significant operational challenges, including inefficient use of heterogeneous hardware, network bottlenecks, and critical imbalances between prefill and decode stages. We introduce HeteroScale, a coordinated autoscaling framework that addresses the core challenges of P/D disaggregated serving. HeteroScale combines a topology-aware scheduler that adapts to heterogeneous hardware and network constraints with a novel metric-driven policy derived from the first large-scale empirical study of autoscaling signals in production. By leveraging a single, robust metric to jointly scale prefill and decode pools, HeteroScale maintains architectural balance while ensuring efficient, adaptive resource management. Deployed in a massive production environment on tens of thousands of GPUs, HeteroScale has proven its effectiveness, increasing average GPU utilization by a significant 26.6 percentage points and saving hundreds of thousands of GPU-hours daily, all while upholding stringent service level objectives.",
        "authors": [
            "Rongzhi Li",
            "Ruogu Du",
            "Zefang Chu",
            "Sida Zhao",
            "Chunlei Han",
            "Zuocheng Shi",
            "Yiwen Shao",
            "Huanle Han",
            "Long Huang",
            "Zherui Liu",
            "Shufan Liu"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-08-27"
    },
    "http://arxiv.org/abs/2508.19452v5": {
        "id": "http://arxiv.org/abs/2508.19452v5",
        "title": "Formal Modeling and Verification of the Algorand Consensus Protocol in CADP",
        "link": "http://arxiv.org/abs/2508.19452v5",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-28",
        "tldr": "",
        "abstract": "Algorand is a scalable and secure permissionless blockchain that achieves proof-of-stake consensus via cryptographic self-sortition and binary Byzantine agreement. In this paper we present a process algebraic model of the Algorand consensus protocol with the aim of enabling formal verification. Our model captures the behavior of participants in terms of the structured alternation of consensus steps toward a committee-based agreement. We validate the correctness of the protocol in the absence of adversaries and then extend our model to assess the influence of coordinated malicious nodes that can force the commit of an empty block instead of the proposed one. The adversarial scenario is analyzed through an equivalence-checking-based noninterference framework that we have implemented in the CADP verification toolkit. In addition to highlighting both the robustness and the limitations of the Algorand protocol under adversarial assumptions, this work illustrates the added value of using formal methods for the analysis of consensus algorithms within blockchains.",
        "authors": [
            "Andrea Esposito",
            "Francesco P. Rossi",
            "Marco Bernardo",
            "Francesco Fabris",
            "Hubert Garavel"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-26"
    },
    "http://arxiv.org/abs/2508.19373v1": {
        "id": "http://arxiv.org/abs/2508.19373v1",
        "title": "HAP: Hybrid Adaptive Parallelism for Efficient Mixture-of-Experts Inference",
        "link": "http://arxiv.org/abs/2508.19373v1",
        "tags": [
            "MoE",
            "serving",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-08-28",
        "tldr": "Proposes HAP, a hybrid adaptive parallelism framework for MoE inference, which uses hierarchical decomposition and ILP optimization to dynamically select parallel strategies. Achieves up to 1.77x speedup over TP-based inference on GPU platforms.",
        "abstract": "Current inference systems for Mixture-of-Experts (MoE) models primarily employ static parallelization strategies. However, these static approaches cannot consistently achieve optimal performance across different inference scenarios, as they lack the flexibility to adapt to varying computational requirements. In this work, we propose HAP (Hybrid Adaptive Parallelism), a novel method that dynamically selects hybrid parallel strategies to enhance MoE inference efficiency. The fundamental innovation of HAP lies in hierarchically decomposing MoE architectures into two distinct computational modules: the Attention module and the Expert module, each augmented with a specialized inference latency simulation model. This decomposition promotes the construction of a comprehensive search space for seeking model parallel strategies. By leveraging Integer Linear Programming (ILP), HAP could solve the optimal hybrid parallel configurations to maximize inference efficiency under varying computational constraints. Our experiments demonstrate that HAP consistently determines parallel configurations that achieve comparable or superior performance to the TP strategy prevalent in mainstream inference systems. Compared to the TP-based inference, HAP-based inference achieves speedups of 1.68x, 1.77x, and 1.57x on A100, A6000, and V100 GPU platforms, respectively. Furthermore, HAP showcases remarkable generalization capability, maintaining performance effectiveness across diverse MoE model configurations, including Mixtral and Qwen series models.",
        "authors": [
            "Haoran Lin",
            "Xianzhi Yu",
            "Kang Zhao",
            "Han Bao",
            "Zongyuan Zhan",
            "Ting Hu",
            "Wulong Liu",
            "Zekun Yin",
            "Xin Li",
            "Weiguo Liu"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-26"
    },
    "http://arxiv.org/abs/2508.19868v1": {
        "id": "http://arxiv.org/abs/2508.19868v1",
        "title": "New Tools, Programming Models, and System Support for Processing-in-Memory Architectures",
        "link": "http://arxiv.org/abs/2508.19868v1",
        "tags": [
            "hardware",
            "kernel",
            "quantization"
        ],
        "relevant": true,
        "indexed_date": "2025-08-28",
        "tldr": "Develops tools and systems for processing-in-memory (PIM) architectures. Proposes DAMOV benchmark, MIMDRAM co-design, Proteus for latency reduction via parallelism and dynamic quantization, and DaPPA programming model. Proteus reduces latency by dynamically adjusting bit-precision, achieving speedup in PUD operations.",
        "abstract": "Our goal in this dissertation is to provide tools, programming models, and system support for PIM architectures (with a focus on DRAM-based solutions), to ease the adoption of PIM in current and future systems. To this end, we make at least four new major contributions.   First, we introduce DAMOV, the first rigorous methodology to characterize memory-related data movement bottlenecks in modern workloads, and the first data movement benchmark suite. Second, we introduce MIMDRAM, a new hardware/software co-designed substrate that addresses the major current programmability and flexibility limitations of the bulk bitwise execution model of processing-using-DRAM (PUD) architectures. MIMDRAM enables the allocation and control of only the needed computing resources inside DRAM for PUD computing. Third, we introduce Proteus, the first hardware framework that addresses the high execution latency of bulk bitwise PUD operations in state-of-the-art PUD architectures by implementing a data-aware runtime engine for PUD. Proteus reduces the latency of PUD operations in three different ways: (i) Proteus concurrently executes independent in-DRAM primitives belong to a single PUD operation across DRAM arrays. (ii) Proteus dynamically reduces the bit-precision (and consequentially the latency and energy consumption) of PUD operations by exploiting narrow values (i.e., values with many leading zeros or ones). (iii) Proteus chooses and uses the most appropriate data representation and arithmetic algorithm implementation for a given PUD instruction transparently to the programmer. Fourth, we introduce DaPPA (data-parallel processing-in-memory architecture), a new programming framework that eases programmability for general-purpose PNM architectures by allowing the programmer to write efficient PIM-friendly code without the need to manage hardware resources explicitly.",
        "authors": [
            "Geraldo F. Oliveira"
        ],
        "categories": [
            "cs.AR",
            "cs.DC"
        ],
        "submit_date": "2025-08-27"
    },
    "http://arxiv.org/abs/2508.19504v1": {
        "id": "http://arxiv.org/abs/2508.19504v1",
        "title": "Aegis: Taxonomy and Optimizations for Overcoming Agent-Environment Failures in LLM Agents",
        "link": "http://arxiv.org/abs/2508.19504v1",
        "tags": [
            "agentic",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-08-28",
        "tldr": "Addresses how to improve LLM agent success rates by optimizing the system environment. Proposes Aegis with environment observability enhancement, common computation offloading, and speculative actions. Boosts agent success rates by 6.7-12.5% without modifying the agent or underlying LLM.",
        "abstract": "Large Language Models (LLMs) agents augmented with domain tools promise to autonomously execute complex tasks requiring human-level intelligence, such as customer service and digital assistance. However, their practical deployment is often limited by their low success rates under complex real-world environments. To tackle this, prior research has primarily focused on improving the agents themselves, such as developing strong agentic LLMs, while overlooking the role of the system environment in which the agent operates.   In this paper, we study a complementary direction: improving agent success rates by optimizing the system environment in which the agent operates. We collect 142 agent traces (3,656 turns of agent-environment interactions) across 5 state-of-the-art agentic benchmarks. By analyzing these agent failures, we propose a taxonomy for agent-environment interaction failures that includes 6 failure modes. Guided by these findings, we design Aegis, a set of targeted environment optimizations: 1) environment observability enhancement, 2) common computation offloading, and 3) speculative agentic actions. These techniques improve agent success rates on average by 6.7-12.5%, without any modifications to the agent and underlying LLM.",
        "authors": [
            "Kevin Song",
            "Anand Jayarajan",
            "Yaoyao Ding",
            "Qidong Su",
            "Zhanda Zhu",
            "Sihang Liu",
            "Gennady Pekhimenko"
        ],
        "categories": [
            "cs.MA",
            "cs.DC"
        ],
        "submit_date": "2025-08-27"
    },
    "http://arxiv.org/abs/2508.19138v1": {
        "id": "http://arxiv.org/abs/2508.19138v1",
        "title": "Ab-initio Quantum Transport with the GW Approximation, 42,240 Atoms, and Sustained Exascale Performance",
        "link": "http://arxiv.org/abs/2508.19138v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-27",
        "tldr": "",
        "abstract": "Designing nanoscale electronic devices such as the currently manufactured nanoribbon field-effect transistors (NRFETs) requires advanced modeling tools capturing all relevant quantum mechanical effects. State-of-the-art approaches combine the non-equilibrium Green's function (NEGF) formalism and density functional theory (DFT). However, as device dimensions do not exceed a few nanometers anymore, electrons are confined in ultra-small volumes, giving rise to strong electron-electron interactions. To account for these critical effects, DFT+NEGF solvers should be extended with the GW approximation, which massively increases their computational intensity. Here, we present the first implementation of the NEGF+GW scheme capable of handling NRFET geometries with dimensions comparable to experiments. This package, called QuaTrEx, makes use of a novel spatial domain decomposition scheme, can treat devices made of up to 84,480 atoms, scales very well on the Alps and Frontier supercomputers (>80% weak scaling efficiency), and sustains an exascale FP64 performance on 42,240 atoms (1.15 Eflop/s).",
        "authors": [
            "Nicolas Vetsch",
            "Alexander Maeder",
            "Vincent Maillou",
            "Anders Winka",
            "Jiang Cao",
            "Grzegorz Kwasniewski",
            "Leonard Deuschle",
            "Torsten Hoefler",
            "Alexandros Nikolaos Ziogas",
            "Mathieu Luisier"
        ],
        "categories": [
            "cs.DC",
            "cond-mat.mes-hall",
            "cs.CE"
        ],
        "submit_date": "2025-08-26"
    },
    "http://arxiv.org/abs/2508.19073v2": {
        "id": "http://arxiv.org/abs/2508.19073v2",
        "title": "CARMA: Collocation-Aware Resource Manager",
        "link": "http://arxiv.org/abs/2508.19073v2",
        "tags": [
            "training"
        ],
        "relevant": true,
        "indexed_date": "2025-08-27",
        "tldr": "Addresses GPU underutilization during deep learning training by collocating tasks without OOM crashes or interference. Proposes CARMA, a resource management system with risk analysis, placement policies, memory estimators, and recovery methods. Achieves 54% higher SM utilization and 35% lower makespan.",
        "abstract": "GPUs running deep learning (DL) workloads are frequently underutilized. Collocating multiple DL training tasks on the same GPU can improve utilization but introduces two key risks: (1) out-of-memory (OOM) crashes for newly scheduled tasks, and (2) severe performance interference among co-running tasks, which can negate any throughput gains. These issues reduce system robustness, quality of service, and energy efficiency. We present CARMA, a task-level, collocation-aware resource management system for the server-scale. CARMA addresses collocation challenges via (1) fine-grained monitoring and bookkeeping of GPUs and a collocation risk analysis that filters out the high-risk GPUs; (2) task placement policies that cap GPU utilization to avoid OOMs and limit interference; (3) integration of GPU memory need estimators for DL tasks to minimize OOMs during collocation; and (4) a lightweight recovery method that relaunches jobs crashed due to OOMs. Our evaluation on a DL training workload derived from real-world traces shows that CARMA uses GPUs more efficiently by making more informed collocation decisions: for the best-performing collocation policy, CARMA increases GPU streaming multiprocessor (SM) utilization by 54%, the parallelism achieved per SM by 61%, and memory use by 62%. This results in a $\\sim$35% and $\\sim$15% reduction in the end-to-end execution time (makespan) and GPU energy consumption, respectively, for this workload.",
        "authors": [
            "Ehsan Yousefzadeh-Asl-Miandoab",
            "Reza Karimzadeh",
            "Bulat Ibragimov",
            "Florina M. Ciorba",
            "Pınar Tözün"
        ],
        "categories": [
            "cs.DC",
            "cs.LG",
            "cs.PF"
        ],
        "submit_date": "2025-08-26"
    },
    "http://arxiv.org/abs/2508.18969v1": {
        "id": "http://arxiv.org/abs/2508.18969v1",
        "title": "Deep Learning-Enabled Supercritical Flame Simulation at Detailed Chemistry and Real-Fluid Accuracy Towards Trillion-Cell Scale",
        "link": "http://arxiv.org/abs/2508.18969v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-27",
        "tldr": "",
        "abstract": "For decades, supercritical flame simulations incorporating detailed chemistry and real-fluid transport have been limited to millions of cells, constraining the resolved spatial and temporal scales of the physical system. We optimize the supercritical flame simulation software DeepFlame -- which incorporates deep neural networks while retaining the real-fluid mechanical and chemical accuracy -- from three perspectives: parallel computing, computational efficiency, and I/O performance. Our highly optimized DeepFlame achieves supercritical liquid oxygen/methane (LOX/\\ce{CH4}) turbulent combustion simulation of up to 618 and 154 billion cells with unprecedented time-to-solution, attaining 439/1186 and 187/316 PFlop/s (32.3\\%/21.8\\% and 37.4\\%/31.8\\% of the peak) in FP32/mixed-FP16 precision on Sunway (98,304 nodes) and Fugaku (73,728 nodes) supercomputers, respectively. This computational capability surpasses existing capacities by three orders of magnitude, enabling the first practical simulation of rocket engine combustion with >100 LOX/\\ce{CH4} injectors. This breakthrough establishes high-fidelity supercritical flame modeling as a critical design tool for next-generation rocket propulsion and ultra-high energy density systems.",
        "authors": [
            "Zhuoqiang Guo",
            "Runze Mao",
            "Lijun Liu",
            "Guangming Tan",
            "Weile Jia",
            "Zhi X. Chen"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-26"
    },
    "http://arxiv.org/abs/2508.18950v1": {
        "id": "http://arxiv.org/abs/2508.18950v1",
        "title": "SIREN: Software Identification and Recognition in HPC Systems",
        "link": "http://arxiv.org/abs/2508.18950v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-27",
        "tldr": "",
        "abstract": "HPC systems use monitoring and operational data analytics to ensure efficiency, performance, and orderly operations. Application-specific insights are crucial for analyzing the increasing complexity and diversity of HPC workloads, particularly through the identification of unknown software and recognition of repeated executions, which facilitate system optimization and security improvements. However, traditional identification methods using job or file names are unreliable for arbitrary user-provided names (a.out). Fuzzy hashing of executables detects similarities despite changes in executable version or compilation approach while preserving privacy and file integrity, overcoming these limitations. We introduce SIREN, a process-level data collection framework for software identification and recognition. SIREN improves observability in HPC by enabling analysis of process metadata, environment information, and executable fuzzy hashes. Findings from a first opt-in deployment campaign on LUMI show SIREN's ability to provide insights into software usage, recognition of repeated executions of known applications, and similarity-based identification of unknown applications.",
        "authors": [
            "Thomas Jakobsche",
            "Fredrik Robertsén",
            "Jessica R. Jones",
            "Utz-Uwe Haus",
            "Florina M. Ciorba"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-26"
    },
    "http://arxiv.org/abs/2508.18850v1": {
        "id": "http://arxiv.org/abs/2508.18850v1",
        "title": "ClusterFusion: Expanding Operator Fusion Scope for LLM Inference via Cluster-Level Collective Primitive",
        "link": "http://arxiv.org/abs/2508.18850v1",
        "tags": [
            "kernel",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-08-27",
        "tldr": "Proposes ClusterFusion, a framework using cluster-level communication primitives (ClusterReduce/ClusterGather) to fuse LLM decoding operators into single kernels. Leverages Hopper architecture for on-chip data exchange, reducing off-chip traffic. Achieves 1.61x lower latency on H100 GPUs vs. state-of-the-art.",
        "abstract": "Large language model (LLM) decoding suffers from high latency due to fragmented execution across operators and heavy reliance on off-chip memory for data exchange and reduction. This execution model limits opportunities for fusion and incurs significant memory traffic and kernel launch overhead. While modern architectures such as NVIDIA Hopper provide distributed shared memory and low-latency intra-cluster interconnects, they expose only low-level data movement instructions, lacking structured abstractions for collective on-chip communication. To bridge this software-hardware gap, we introduce two cluster-level communication primitives, ClusterReduce and ClusterGather, which abstract common communication patterns and enable structured, high-speed data exchange and reduction between thread blocks within a cluster, allowing intermediate results to be on-chip without involving off-chip memory. Building on these abstractions, we design ClusterFusion, an execution framework that schedules communication and computation jointly to expand operator fusion scope by composing decoding stages such as QKV Projection, Attention, and Output Projection into a single fused kernels. Evaluations on H100 GPUs show that ClusterFusion outperforms state-of-the-art inference frameworks by 1.61x on average in end-to-end latency across different models and configurations. The source code is available at https://github.com/xinhao-luo/ClusterFusion.",
        "authors": [
            "Xinhao Luo",
            "Zihan Liu",
            "Yangjie Zhou",
            "Shihan Fang",
            "Ziyu Huang",
            "Yu Feng",
            "Chen Zhang",
            "Shixuan Sun",
            "Zhenzhe Zheng",
            "Jingwen Leng",
            "Minyi Guo"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-08-26"
    },
    "http://arxiv.org/abs/2508.18667v1": {
        "id": "http://arxiv.org/abs/2508.18667v1",
        "title": "Examining MPI and its Extensions for Asynchronous Multithreaded Communication",
        "link": "http://arxiv.org/abs/2508.18667v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-27",
        "tldr": "",
        "abstract": "The increasing complexity of HPC architectures and the growing adoption of irregular scientific algorithms demand efficient support for asynchronous, multithreaded communication. This need is especially pronounced with Asynchronous Many-Task (AMT) systems. This communication pattern was not a consideration during the design of the original MPI specification. The MPI community has recently introduced several extensions to address these evolving requirements. This work evaluates two such extensions, the Virtual Communication Interface (VCI) and the Continuation extensions, in the context of an established AMT runtime HPX. We begin by using an MPI-level microbenchmark, modeled from HPX's low-level communication mechanism, to measure the peak performance potential of these extensions. We then integrate them into HPX to evaluate their effectiveness in real-world scenarios. Our results show that while these extensions can enhance performance compared to standard MPI, areas for improvement remain. The current continuation proposal limits the maximum multithreaded message rate achievable in the multi-VCI setting. Furthermore, the recommended one-VCI-per-thread mode proves ineffective in real-world systems due to the attentiveness problem. These findings underscore the importance of improving intra-VCI threading efficiency to achieve scalable multithreaded communication and fully realize the benefits of recent MPI extensions.",
        "authors": [
            "Jiakun Yan",
            "Marc Snir",
            "Yanfei Guo"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-26"
    },
    "http://arxiv.org/abs/2508.18572v1": {
        "id": "http://arxiv.org/abs/2508.18572v1",
        "title": "Strata: Hierarchical Context Caching for Long Context Language Model Serving",
        "link": "http://arxiv.org/abs/2508.18572v1",
        "tags": [
            "serving",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-08-27",
        "tldr": "Addresses performance bottlenecks in long-context LLM serving due to inefficient hierarchical KV cache storage and transfer. Proposes Strata with GPU-assisted I/O for fragmentation mitigation and cache-aware scheduling to overlap tasks. Achieves 5x lower TTFT vs vLLM on long contexts.",
        "abstract": "Large Language Models (LLMs) with expanding context windows face significant performance hurdles. While caching key-value (KV) states is critical for avoiding redundant computation, the storage footprint of long-context caches quickly exceeds GPU memory capacity, forcing production systems to adopt hierarchical caching across memory hierarchies. However, transferring large cached contexts back to the GPU introduces severe performance bottlenecks: fragmented I/O from paged layouts prevents full bandwidth utilization, and existing schedulers fail to account for cache-loading delays, leaving systems loading-bound rather than compute-bound. We present Strata, a hierarchical context caching framework designed for efficient long context LLM serving. Strata introduces GPU-assisted I/O to combat KV cache fragmentation, decoupling GPU and CPU memory layouts and employs cache-aware request scheduling to balance compute with I/O latency and overlapping unavoidable stalls with complementary tasks. Built on SGLang and deployed in production, Strata achieves up to 5x lower Time-To-First-Token (TTFT) compared to vLLM + LMCache and 3.75x speedup over NVIDIA TensorRT-LLM on long-context benchmarks, without degrading short-context performance.",
        "authors": [
            "Zhiqiang Xie",
            "Ziyi Xu",
            "Mark Zhao",
            "Yuwei An",
            "Vikram Sharma Mailthody",
            "Scott Mahlke",
            "Michael Garland",
            "Christos Kozyrakis"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-26"
    },
    "http://arxiv.org/abs/2508.18556v1": {
        "id": "http://arxiv.org/abs/2508.18556v1",
        "title": "Managing Multi Instance GPUs for High Throughput and Energy Savings",
        "link": "http://arxiv.org/abs/2508.18556v1",
        "tags": [
            "serving",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-08-27",
        "tldr": "Explores GPU partitioning and scheduling to boost throughput and save energy for diverse workloads, including LLMs. Proposes dynamic memory estimation, partition fusion/fission, and process restart. Achieves up to 1.43x higher throughput and 1.11x energy savings for LLM workloads on A100.",
        "abstract": "Modern GPUs such as the Ampere series (A30, A100) as well as the Hopper series (H100, H200) offer performance as well as security isolation features. They also support a good amount of concurrency, but taking advantage of it can be quite challenging due to the complex constraints on partitioning the chip.   In this work, we develop partitioning and scheduling schemes for a variety of workloads, ranging from scientific to modern ML workloads, including LLMs. We develop several schemes involving dynamic memory estimation, partition fusion and partition fission. We also support process restart to recover from out-of-memory errors for workloads and early restart as an optimization. This approach yields up to 6.20x throughput and 5.93x energy improvements for general workloads; and we see 1.59x and 1.12x improvement to throughput and energy, respectively, for ML workloads on an A100 GPU. We leverage this technique on LLM workloads and show good improvements, including up to 1.43x throughput improvement and 1.11x energy savings.",
        "authors": [
            "Abhijeet Saraha",
            "Yuanbo Li",
            "Chris Porter",
            "Santosh Pande"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-25"
    },
    "http://arxiv.org/abs/2508.18489v1": {
        "id": "http://arxiv.org/abs/2508.18489v1",
        "title": "Experiences with Model Context Protocol Servers for Science and High Performance Computing",
        "link": "http://arxiv.org/abs/2508.18489v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-27",
        "tldr": "",
        "abstract": "Large language model (LLM)-powered agents are increasingly used to plan and execute scientific workflows, yet most research cyberinfrastructure (CI) exposes heterogeneous APIs and implements security models that present barriers for use by agents. We report on our experience using the Model Context Protocol (MCP) as a unifying interface that makes research capabilities discoverable, invokable, and composable. Our approach is pragmatic: we implement thin MCP servers over mature services, including Globus Transfer, Compute, and Search; status APIs exposed by computing facilities; Octopus event fabric; and domain-specific tools such as Garden and Galaxy. We use case studies in computational chemistry, bioinformatics, quantum chemistry, and filesystem monitoring to illustrate how this MCP-oriented architecture can be used in practice. We distill lessons learned and outline open challenges in evaluation and trust for agent-led science.",
        "authors": [
            "Haochen Pan",
            "Ryan Chard",
            "Reid Mello",
            "Christopher Grams",
            "Tanjin He",
            "Alexander Brace",
            "Owen Price Skelly",
            "Will Engler",
            "Hayden Holbrook",
            "Song Young Oh",
            "Maxime Gonthier",
            "Michael Papka",
            "Ben Blaiszik",
            "Kyle Chard",
            "Ian Foster"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-25"
    },
    "http://arxiv.org/abs/2508.19160v1": {
        "id": "http://arxiv.org/abs/2508.19160v1",
        "title": "Architecting Distributed Quantum Computers: Design Insights from Resource Estimation",
        "link": "http://arxiv.org/abs/2508.19160v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-27",
        "tldr": "",
        "abstract": "To enable practically useful quantum computing, we require hundreds to thousands of logical qubits (collections of physical qubits with error correction). Current monolithic device architectures have scaling limits beyond few tens of logical qubits. To scale up, we require architectures that orchestrate several monolithic devices into a distributed quantum computing system. Currently, resource estimation, which is crucial for determining hardware needs and bottlenecks, focuses exclusively on monolithic systems. Our work fills this gap and answers key architectural design questions about distributed systems, including the impact of distribution on application resource needs, the organization of qubits across nodes and the requirements of entanglement distillation (quantum network). To answer these questions, we develop a novel resource estimation framework that models the key components of the distributed execution stack. We analyse the performance of practical quantum algorithms on various hardware configurations, spanning different qubit speeds, entanglement generation rates and distillation protocols. We show that distributed architectures have practically feasible resource requirements; for a node size of 45K qubits, distributed systems need on average 1.4X higher number of physical qubits and 4X higher execution time compared to monolithic architectures, but with more favourable hardware implementation prospects. Our insights on entanglement generation rates, node sizes and architecture have the potential to inform system designs in the coming years.",
        "authors": [
            "Dmitry Filippov",
            "Peter Yang",
            "Prakash Murali"
        ],
        "categories": [
            "quant-ph",
            "cs.AR",
            "cs.DC",
            "cs.ET"
        ],
        "submit_date": "2025-08-26"
    },
    "http://arxiv.org/abs/2508.18588v1": {
        "id": "http://arxiv.org/abs/2508.18588v1",
        "title": "History Rhymes: Accelerating LLM Reinforcement Learning with RhymeRL",
        "link": "http://arxiv.org/abs/2508.18588v1",
        "tags": [
            "training",
            "RL",
            "quantization"
        ],
        "relevant": true,
        "indexed_date": "2025-08-27",
        "tldr": "Addresses low GPU utilization in LLM RL training due to slow rollouts and imbalances. Proposes RhymeRL with HistoSpec for speculative decoding using historical rollouts and HistoPipe for workload balancing. Achieves 2.6x speedup without accuracy loss.",
        "abstract": "With the rapid advancement of large language models (LLMs), reinforcement learning (RL) has emerged as a pivotal methodology for enhancing the reasoning capabilities of LLMs. Unlike traditional pre-training approaches, RL encompasses multiple stages: rollout, reward, and training, which necessitates collaboration among various worker types. However, current RL systems continue to grapple with substantial GPU underutilization, due to two primary factors: (1) The rollout stage dominates the overall RL process due to test-time scaling; (2) Imbalances in rollout lengths (within the same batch) result in GPU bubbles. While prior solutions like asynchronous execution and truncation offer partial relief, they may compromise training accuracy for efficiency.   Our key insight stems from a previously overlooked observation: rollout responses exhibit remarkable similarity across adjacent training epochs. Based on the insight, we introduce RhymeRL, an LLM RL system designed to accelerate RL training with two key innovations. First, to enhance rollout generation, we present HistoSpec, a speculative decoding inference engine that utilizes the similarity of historical rollout token sequences to obtain accurate drafts. Second, to tackle rollout bubbles, we introduce HistoPipe, a two-tier scheduling strategy that leverages the similarity of historical rollout distributions to balance workload among rollout workers. We have evaluated RhymeRL within a real production environment, demonstrating scalability from dozens to thousands of GPUs. Experimental results demonstrate that RhymeRL achieves a 2.6x performance improvement over existing methods, without compromising accuracy or modifying the RL paradigm.",
        "authors": [
            "Jingkai He",
            "Tianjian Li",
            "Erhu Feng",
            "Dong Du",
            "Qian Liu",
            "Tao Liu",
            "Yubin Xia",
            "Haibo Chen"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "submit_date": "2025-08-26"
    },
    "http://arxiv.org/abs/2508.18376v1": {
        "id": "http://arxiv.org/abs/2508.18376v1",
        "title": "DualSparse-MoE: Coordinating Tensor/Neuron-Level Sparsity with Expert Partition and Reconstruction",
        "link": "http://arxiv.org/abs/2508.18376v1",
        "tags": [
            "MoE",
            "offline",
            "sparse"
        ],
        "relevant": true,
        "indexed_date": "2025-08-27",
        "tldr": "Proposes DualSparse-MoE, an inference system leveraging post-training tensor-level expert partitioning and neuron-level reconstruction for MoE models. Integrates dynamic tensor-level computation dropping and static neuron-level reconstruction to reduce computation. Achieves 1.41x MoE module speedup with 0.5% accuracy degradation.",
        "abstract": "Mixture of Experts (MoE) has become a mainstream architecture for building Large Language Models (LLMs) by reducing per-token computation while enabling model scaling. It can be viewed as partitioning a large Feed-Forward Network (FFN) at the tensor level into fine-grained sub-FFNs, or experts, and activating only a sparse subset for each input. While this sparsity improves efficiency, MoE still faces substantial challenges due to their massive computational scale and unpredictable activation patterns.   To enable efficient MoE deployment, we identify dual sparsity at the tensor and neuron levels in pre-trained MoE modules as a key factor for both accuracy and efficiency. Unlike prior work that increases tensor-level sparsity through finer-grained expert design during pre-training, we introduce post-training expert partitioning to induce such sparsity without retraining. This preserves the mathematical consistency of model transformations and enhances both efficiency and accuracy in subsequent fine-tuning and inference. Building upon this, we propose DualSparse-MoE, an inference system that integrates dynamic tensor-level computation dropping with static neuron-level reconstruction to deliver significant efficiency gains with minimal accuracy loss.   Experimental results show that enforcing an approximate 25% drop rate with our approach reduces average accuracy by only 0.08%-0.28% across three prevailing MoE models, while nearly all degrees of computation dropping consistently yield proportional computational speedups. Furthermore, incorporating load-imbalance awareness into expert parallelism achieves a 1.41x MoE module speedup with just 0.5% average accuracy degradation.",
        "authors": [
            "Weilin Cai",
            "Le Qin",
            "Shwai He",
            "Junwei Cui",
            "Ang Li",
            "Jiayi Huang"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "submit_date": "2025-08-25"
    },
    "http://arxiv.org/abs/2508.18224v2": {
        "id": "http://arxiv.org/abs/2508.18224v2",
        "title": "FSA: An Alternative Efficient Implementation of Native Sparse Attention Kernel",
        "link": "http://arxiv.org/abs/2508.18224v2",
        "tags": [
            "sparse",
            "kernel",
            "training"
        ],
        "relevant": true,
        "indexed_date": "2025-08-26",
        "tldr": "Addresses inefficient kernel implementation for native sparse attention when applied to LLMs with few query heads per GQA group. Proposes Flash Sparse Attention (FSA), a reordered kernel loop design compatible with popular LLM configurations. Achieves up to 3.5x kernel latency reduction and 1.25x end-to-end training speedup.",
        "abstract": "Recent advance in sparse attention mechanisms has demonstrated strong potential for reducing the computational cost of long-context training and inference in large language models (LLMs). Native Sparse Attention (NSA), one state-of-the-art approach, introduces natively trainable, hardware-aligned sparse attention that delivers substantial system-level performance boost while maintaining accuracy comparable to full attention. However, the kernel implementation of NSA forces a loop order that is only efficient with a relatively large number of query heads in each Grouped Query Attention (GQA) group, whereas existing LLMs widely adopt much smaller number of query heads in each GQA group -- such an inconsistency significantly limits the applicability of this sparse algorithmic advance. In this work, we propose Flash Sparse Attention (FSA), an alternative kernel implementation that enables efficient NSA computation across a wide range of popular LLMs with varied smaller number of heads in each GQA group on modern GPUs. Compared to vanilla NSA kernel implementation, our empirical evaluation demonstrates that FSA achieves (i) up to 3.5x and on average 1.6x kernel-level latency reduction, (ii) up to 1.25x and 1.09x on average end-to-end training speedup on state-of-the-art LLMs, and (iii) up to 1.36x and 1.11x on average for prefill-phase speedup in LLM generative inference. Github Repo at https://github.com/Relaxed-System-Lab/Flash-Sparse-Attention.",
        "authors": [
            "Ran Yan",
            "Youhe Jiang",
            "Zhuoming Chen",
            "Haohui Mai",
            "Beidi Chen",
            "Binhang Yuan"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-08-25"
    },
    "http://arxiv.org/abs/2508.18206v1": {
        "id": "http://arxiv.org/abs/2508.18206v1",
        "title": "Practical GPU Choices for Earth Observation: ResNet-50 Training Throughput on Integrated, Laptop, and Cloud Accelerators",
        "link": "http://arxiv.org/abs/2508.18206v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-26",
        "tldr": "",
        "abstract": "This project implements a ResNet-based pipeline for land use and land cover (LULC) classification on Sentinel-2 imagery, benchmarked across three heterogeneous GPUs. The workflow automates data acquisition, geospatial preprocessing, tiling, model training, and visualization, and is fully containerized for reproducibility. Performance evaluation reveals up to a 2x training speed-up on an NVIDIA RTX 3060 and a Tesla T4 compared to the Apple M3 Pro baseline, while maintaining high classification accuracy on the EuroSAT dataset. These results demonstrate the feasibility of deploying deep learning LULC models on consumer and free cloud GPUs for scalable geospatial analytics.",
        "authors": [
            "Ritvik Chaturvedi"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-08-25"
    },
    "http://arxiv.org/abs/2508.18193v2": {
        "id": "http://arxiv.org/abs/2508.18193v2",
        "title": "Wait-free Replicated Data Types and Fair Reconciliation",
        "link": "http://arxiv.org/abs/2508.18193v2",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-26",
        "tldr": "",
        "abstract": "Replication ensures data availability in fault-prone distributed systems. The celebrated CAP theorem stipulates that replicas cannot guarantee both strong consistency and availability under network partitions. A popular alternative, adopted by CRDTs, is to relax consistency to be eventual. It enables progress to be wait-free, as replicas can serve requests immediately. Yet, wait-free replication faces a key challenge: due to asynchrony and concurrency, operations may be constantly reordered, leading to results inconsistent with their original contexts and preventing them from stabilizing over time. Moreover, a particular client may experience starvation if, from some point on, each of its operations is reordered at least once.   We make two contributions. First, we formalize the problem addressed by wait-free replicated data types (e.g., CRDTs) as eventual state-machine replication. We then augment it with stability and fairness ensuring, respectively, that (1)~all replicas share a growing stable prefix of operations, and (2)~no client starves. Second, we present a generic DAG-based framework to achieve eventual state-machine replication for any replicated data type, where replicas exchange their local views and merge them using a reconciliation function. We then propose reconciliation functions ensuring stability and fairness.",
        "authors": [
            "Petr Kuznetsov",
            "Maxence Perion",
            "Sara Tucci-Piergiovanni"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-25"
    },
    "http://arxiv.org/abs/2508.17814v1": {
        "id": "http://arxiv.org/abs/2508.17814v1",
        "title": "Scalable Engine and the Performance of Different LLM Models in a SLURM based HPC architecture",
        "link": "http://arxiv.org/abs/2508.17814v1",
        "tags": [
            "serving",
            "offline",
            "training"
        ],
        "relevant": true,
        "indexed_date": "2025-08-26",
        "tldr": "Proposes SLURM-based HPC architecture for scalable LLM inference with dynamic scheduling and containerization. Evaluates throughput and latency for models like Llama 3.1 and 3.2. Achieves sub-50 ms latency for 128 concurrent requests with small models and minimal overhead.",
        "abstract": "This work elaborates on a High performance computing (HPC) architecture based on Simple Linux Utility for Resource Management (SLURM) [1] for deploying heterogeneous Large Language Models (LLMs) into a scalable inference engine. Dynamic resource scheduling and seamless integration of containerized microservices have been leveraged herein to manage CPU, GPU, and memory allocations efficiently in multi-node clusters. Extensive experiments, using Llama 3.2 (1B and 3B parameters) [2] and Llama 3.1 (8B and 70B) [3], probe throughput, latency, and concurrency and show that small models can handle up to 128 concurrent requests at sub-50 ms latency, while for larger models, saturation happens with as few as two concurrent users, with a latency of more than 2 seconds. This architecture includes Representational State Transfer Application Programming Interfaces (REST APIs) [4] endpoints for single and bulk inferences, as well as advanced workflows such as multi-step \"tribunal\" refinement. Experimental results confirm minimal overhead from container and scheduling activities and show that the approach scales reliably both for batch and interactive settings. We further illustrate real-world scenarios, including the deployment of chatbots with retrievalaugmented generation, which helps to demonstrate the flexibility and robustness of the architecture. The obtained results pave ways for significantly more efficient, responsive, and fault-tolerant LLM inference on large-scale HPC infrastructures.",
        "authors": [
            "Anderson de Lima Luiz",
            "Shubham Vijay Kurlekar",
            "Munir Georges"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-08-25"
    },
    "http://arxiv.org/abs/2508.17624v1": {
        "id": "http://arxiv.org/abs/2508.17624v1",
        "title": "ExpertWeave: Efficiently Serving Expert-Specialized Fine-Tuned Adapters at Scale",
        "link": "http://arxiv.org/abs/2508.17624v1",
        "tags": [
            "MoE",
            "serving",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-08-26",
        "tldr": "Addresses the challenge of serving multiple expert-specialized fine-tuned adapters efficiently. Proposes ExpertWeave system with virtual-memory-assisted expert weight manager and fused kernel for batched rerouting. Achieves up to 18% higher throughput and 94x more KV cache capacity compared to baselines.",
        "abstract": "Expert-Specialized Fine-Tuning (ESFT) adapts Mixture-of-Experts (MoE) large language models to enhance their task-specific performance by selectively tuning the top-activated experts for the task. Serving these fine-tuned models at scale is challenging: deploying merged models in isolation is prohibitively resource-hungry, while existing multi-adapter serving systems with LoRA-style additive updates are incompatible with ESFT's expert-oriented paradigm. We present ExpertWeave, a system that serves multiple ESFT adapters concurrently over a single shared MoE base model, drastically reducing the memory footprint and improving resource utilization. To seamlessly integrate into existing inference pipelines for MoE models with non-intrusive modifications and minimal latency overhead, ExpertWeave introduces a virtual-memory-assisted expert weight manager that co-locates base-model and adapter experts without incurring memory overhead from fragmentation, and a fused kernel for batched rerouting to enable lightweight redirection of tokens to the appropriate experts at runtime. Our evaluations show that ExpertWeave can simultaneously serve multiple adapters of a 16B MoE model on a single accelerator where the baseline runs out of memory, or provides up to 94x more KV cache capacity and achieves up to 18% higher throughput while using comparable resources, all without compromising model accuracy. ExpertWeave maintains low overhead even when scaling to 20 adapters, with a 4-11% latency increase compared with serving the base model alone. Source code will be released soon.",
        "authors": [
            "Ge Shi",
            "Hanieh Sadri",
            "Qian Wang",
            "Yu Zhang",
            "Ying Xiong",
            "Yong Zhang",
            "Zhenan Fan"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-25"
    },
    "http://arxiv.org/abs/2508.17593v1": {
        "id": "http://arxiv.org/abs/2508.17593v1",
        "title": "Zen-Attention: A Compiler Framework for Dynamic Attention Folding on AMD NPUs",
        "link": "http://arxiv.org/abs/2508.17593v1",
        "tags": [
            "kernel",
            "hardware"
        ],
        "relevant": true,
        "indexed_date": "2025-08-26",
        "tldr": "Optimizes attention layer execution on AMD NPUs for reduced latency and DRAM bandwidth. Proposes Zen-Attention, a compiler framework for dynamic attention folding with optimized tiling, buffer allocation, and data movement. Achieves up to 4x attention latency improvement and 32% end-to-end network latency reduction.",
        "abstract": "Transformer-based deep learning models are increasingly deployed on energy, and DRAM bandwidth constrained devices such as laptops and gaming consoles, which presents significant challenges in meeting the latency requirements of the models. The industry is turning to neural processing units (NPUs) for superior performance-per-watt (perf/watt); however, efficiently mapping dynamic attention layers to the NPUs remains a challenging task. For optimizing perf/watt, AMD XDNA NPUs employ software managed caches and share system memory with host. This requires substantial engineering effort to unlock efficient tiling, buffer allocation, and data movement to extract the maximum efficiency from the device. This paper introduces Zen-Attention, a framework that optimizes DRAM bandwidth utilization in the attention layer of models by systematically exploring the complex design space of layer folding, tiling, and data-movement on the interconnect, and the tensor layouts to come up with an optimal solution. Our evaluation includes comparative analysis of end-to-end model latency and specific attention latency in each model. We demonstrate how the framework enhances mapping capabilities by varying input dimensions, which require padding and masking in the attention block. For representative transformer models, the Zen-Attention Framework achieves up to 4x improvement in the latency of the attention block and up to 32% improvement in end-to-end network latency compared to the baseline Unfolded- approaches.",
        "authors": [
            "Aadesh Deshmukh",
            "Venkata Yaswanth Raparti",
            "Samuel Hsu"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-25"
    },
    "http://arxiv.org/abs/2508.17493v1": {
        "id": "http://arxiv.org/abs/2508.17493v1",
        "title": "Easy Acceleration with Distributed Arrays",
        "link": "http://arxiv.org/abs/2508.17493v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-26",
        "tldr": "",
        "abstract": "High level programming languages and GPU accelerators are powerful enablers for a wide range of applications. Achieving scalable vertical (within a compute node), horizontal (across compute nodes), and temporal (over different generations of hardware) performance while retaining productivity requires effective abstractions. Distributed arrays are one such abstraction that enables high level programming to achieve highly scalable performance. Distributed arrays achieve this performance by deriving parallelism from data locality, which naturally leads to high memory bandwidth efficiency. This paper explores distributed array performance using the STREAM memory bandwidth benchmark on a variety of hardware. Scalable performance is demonstrated within and across CPU cores, CPU nodes, and GPU nodes. Horizontal scaling across multiple nodes was linear. The hardware used spans decades and allows a direct comparison of hardware improvements for memory bandwidth over this time range; showing a 10x increase in CPU core bandwidth over 20 years, 100x increase in CPU node bandwidth over 20 years, and 5x increase in GPU node bandwidth over 5 years. Running on hundreds of MIT SuperCloud nodes simultaneously achieved a sustained bandwidth $>$1 PB/s.",
        "authors": [
            "Jeremy Kepner",
            "Chansup Byun",
            "LaToya Anderson",
            "William Arcand",
            "David Bestor",
            "William Bergeron",
            "Alex Bonn",
            "Daniel Burrill",
            "Vijay Gadepally",
            "Ryan Haney",
            "Michael Houle",
            "Matthew Hubbell",
            "Hayden Jananthan",
            "Michael Jones",
            "Piotr Luszczek",
            "Lauren Milechin",
            "Guillermo Morales",
            "Julie Mullen",
            "Andrew Prout",
            "Albert Reuther",
            "Antonio Rosa",
            "Charles Yee",
            "Peter Michaleas"
        ],
        "categories": [
            "cs.DC",
            "cs.CE",
            "cs.MS",
            "cs.PF"
        ],
        "submit_date": "2025-08-24"
    },
    "http://arxiv.org/abs/2508.17311v2": {
        "id": "http://arxiv.org/abs/2508.17311v2",
        "title": "Bine Trees: Enhancing Collective Operations by Optimizing Communication Locality",
        "link": "http://arxiv.org/abs/2508.17311v2",
        "tags": [
            "training"
        ],
        "relevant": true,
        "indexed_date": "2025-08-26",
        "tldr": "Addresses performance of collective operations in HPC systems via communication locality. Proposes Bine trees, a family of algorithms optimizing traffic patterns. Achieves up to 5x speedups and 33% reduction in global-link traffic on supercomputers with various topologies.",
        "abstract": "Communication locality plays a key role in the performance of collective operations on large HPC systems, especially on oversubscribed networks where groups of nodes are fully connected internally but sparsely linked through global connections. We present Bine (binomial negabinary) trees, a family of collective algorithms that improve communication locality. Bine trees maintain the generality of binomial trees and butterflies while cutting global-link traffic by up to 33%. We implement eight Bine-based collectives and evaluate them on four large-scale supercomputers with Dragonfly, Dragonfly+, oversubscribed fat-tree, and torus topologies, achieving up to 5x speedups and consistent reductions in global-link traffic across different vector sizes and node counts.",
        "authors": [
            "Daniele De Sensi",
            "Saverio Pasqualoni",
            "Lorenzo Piarulli",
            "Tommaso Bonato",
            "Seydou Ba",
            "Matteo Turisini",
            "Jens Domke",
            "Torsten Hoefler"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.PF"
        ],
        "submit_date": "2025-08-24"
    },
    "http://arxiv.org/abs/2508.17219v1": {
        "id": "http://arxiv.org/abs/2508.17219v1",
        "title": "TokenLake: A Unified Segment-level Prefix Cache Pool for Fine-grained Elastic Long-Context LLM Serving",
        "link": "http://arxiv.org/abs/2508.17219v1",
        "tags": [
            "serving",
            "offloading",
            "autoscaling"
        ],
        "relevant": true,
        "indexed_date": "2025-08-26",
        "tldr": "Proposes TokenLake, a unified segment-level prefix cache pool for efficient LLM serving. It provides a declarative interface for cache management and uses heavy-hitter-aware load balancing to deduplicate caches and minimize communication. Achieves up to 2.6x higher throughput than cache-aware routing.",
        "abstract": "Prefix caching is crucial to accelerate multi-turn interactions and requests with shared prefixes. At the cluster level, existing prefix caching systems are tightly coupled with request scheduling to optimize cache efficiency and computation performance together, leading to load imbalance, data redundancy, and memory fragmentation of caching systems across instances. To address these issues, memory pooling is promising to shield the scheduler from the underlying cache management so that it can focus on the computation optimization. However, because existing prefix caching systems only transfer increasingly longer prefix caches between instances, they cannot achieve low-latency memory pooling.   To address these problems, we propose a unified segment-level prefix cache pool, TokenLake. It uses a declarative cache interface to expose requests' query tensors, prefix caches, and cache-aware operations to TokenLake for efficient pooling. Powered by this abstraction, TokenLake can manage prefix cache at the segment level with a heavy-hitter-aware load balancing algorithm to achieve better cache load balance, deduplication, and defragmentation. TokenLake also transparently minimizes the communication volume of query tensors and new caches. Based on TokenLake, the scheduler can schedule requests elastically by using existing techniques without considering prefix cache management. Evaluations on real-world workloads show that TokenLake can improve throughput by up to 2.6$\\times$ and 2.0$\\times$ and boost hit rate by 2.0$\\times$ and 2.1$\\times$, compared to state-of-the-art cache-aware routing and cache-centric PD-disaggregation solutions, respectively.",
        "authors": [
            "Bingyang Wu",
            "Zili Zhang",
            "Yinmin Zhong",
            "Guanzhe Huang",
            "Yibo Zhu",
            "Xuanzhe Liu",
            "Xin Jin"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-08-24"
    },
    "http://arxiv.org/abs/2508.16809v1": {
        "id": "http://arxiv.org/abs/2508.16809v1",
        "title": "PICO: Performance Insights for Collective Operations",
        "link": "http://arxiv.org/abs/2508.16809v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-26",
        "tldr": "",
        "abstract": "Collective operations are cornerstones of both HPC application and large-scale AI training and inference. Yet, comprehensive, systematic and reproducible performance evaluation and benchmarking of said operations is not straightforward. Existing frameworks do not provide sufficiently detailed profiling information, nor they ensure reproducibility and extensibility. In this paper, we present PICO (Performance Insights for Collective Operations), a novel lightweight, extensible framework built with the aim of simplifying collective operations benchmarking.",
        "authors": [
            "Saverio Pasqualoni",
            "Lorenzo Piarulli",
            "Daniele De Sensi"
        ],
        "categories": [
            "cs.DC",
            "cs.PF"
        ],
        "submit_date": "2025-08-22"
    },
    "http://arxiv.org/abs/2508.16792v1": {
        "id": "http://arxiv.org/abs/2508.16792v1",
        "title": "Neuromorphic Simulation of Drosophila Melanogaster Brain Connectome on Loihi 2",
        "link": "http://arxiv.org/abs/2508.16792v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-26",
        "tldr": "",
        "abstract": "We demonstrate the first-ever nontrivial, biologically realistic connectome simulated on neuromorphic computing hardware. Specifically, we implement the whole-brain connectome of the adult Drosophila melanogaster (fruit fly) from the FlyWire Consortium containing 140K neurons and 50M synapses on the Intel Loihi 2 neuromorphic platform. This task is particularly challenging due to the characteristic connectivity structure of biological networks. Unlike artificial neural networks and most abstracted neural models, real biological circuits exhibit sparse, recurrent, and irregular connectivity that is poorly suited to conventional computing methods intended for dense linear algebra. Though neuromorphic hardware is architecturally better suited to discrete event-based biological communication, mapping the connectivity structure to frontier systems still faces challenges from low-level hardware constraints, such as fan-in and fan-out memory limitations. We describe solutions to these challenges that allow for the full FlyWire connectome to fit onto 12 Loihi 2 chips. We statistically validate our implementation by comparing network behavior across multiple reference simulations. Significantly, we achieve a neuromorphic implementation that is orders of magnitude faster than numerical simulations on conventional hardware, and we also find that performance advantages increase with sparser activity. These results affirm that today's scalable neuromorphic platforms are capable of implementing and accelerating biologically realistic models -- a key enabling technology for advancing neuro-inspired AI and computational neuroscience.",
        "authors": [
            "Felix Wang",
            "Bradley H. Theilman",
            "Fred Rothganger",
            "William Severa",
            "Craig M. Vineyard",
            "James B. Aimone"
        ],
        "categories": [
            "cs.DC",
            "cs.NE"
        ],
        "submit_date": "2025-08-22"
    },
    "http://arxiv.org/abs/2508.16646v1": {
        "id": "http://arxiv.org/abs/2508.16646v1",
        "title": "Equinox: Holistic Fair Scheduling in Serving Large Language Models",
        "link": "http://arxiv.org/abs/2508.16646v1",
        "tags": [
            "serving",
            "autoscaling",
            "quantization"
        ],
        "relevant": true,
        "indexed_date": "2025-08-26",
        "tldr": "Addresses LLM serving fairness by predicting user-perceived latency and resource efficiency. Proposes Equinox with Mixture of Prediction Experts for holistic fairness scoring and adaptive batching. Achieves 1.3× throughput, 60% lower time-to-first-token latency, and 13% higher fairness versus baseline.",
        "abstract": "We address the limitations of current LLM serving with a dual-counter framework separating user and operator perspectives. The User Fairness Counter measures quality of service via weighted tokens and latency; the Resource Fairness Counter measures operational efficiency through throughput and GPU utilization. Since these metrics are only available post-execution, creating a scheduling paradox, we introduce a deterministic Mixture of Prediction Experts (MoPE) framework to predict user-perceived latency, output tokens, throughput, and GPU utilization. These predictions enable calculation of a unified Holistic Fairness score that balances both counters through tunable parameters for proactive fairness-aware scheduling. We implement this in Equinox, an open-source system with other optimizations like adaptive batching, and stall-free scheduling. Evaluations on production traces (ShareGPT, LMSYS) and synthetic workloads demonstrate Equinox achieves up to $1.3\\times$ higher throughput, 60\\% lower time-to-first-token latency, and 13\\% higher fairness versus VTC while maintaining 94\\% GPU utilization, proving fairness under bounded discrepancy across heterogeneous platforms.",
        "authors": [
            "Zhixiang Wei",
            "James Yen",
            "Jingyi Chen",
            "Ziyang Zhang",
            "Zhibai Huang",
            "Chen Chen",
            "Xingzi Yu",
            "Yicheng Gu",
            "Chenggang Wu",
            "Yun Wang",
            "Mingyuan Xia",
            "Jie Wu",
            "Hao Wang",
            "Zhengwei Qi"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-08-19"
    },
    "http://arxiv.org/abs/2508.16639v1": {
        "id": "http://arxiv.org/abs/2508.16639v1",
        "title": "GPU Acceleration for Faster Evolutionary Spatial Cyclic Game Systems",
        "link": "http://arxiv.org/abs/2508.16639v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-26",
        "tldr": "",
        "abstract": "This dissertation presents the design, implementation and evaluation of GPU-accelerated simulation frameworks for Evolutionary Spatial Cyclic Games (ESCGs), a class of agent-based models used to study ecological and evolutionary dynamics. Traditional single-threaded ESCG simulations are computationally expensive and scale poorly. To address this, high-performance implementations were developed using Apple's Metal and Nvidia's CUDA, with a validated single-threaded C++ version serving as a baseline comparison point.   Benchmarking results show that GPU acceleration delivers significant speedups, with the CUDA maxStep implementation achieving up to a 28x improvement. Larger system sizes, up to 3200x3200, became tractable, while Metal faced scalability limits. The GPU frameworks also enabled replication and critical extension of recent ESCG studies, revealing sensitivities to system size and runtime not fully explored in prior work.   Overall, this project provides a configurable ESCG simulation platform that advances the computational toolkit for this field of research. This dissertation forms the basis for a paper accepted for publication and presentation at the European Modelling and Simulation Symposium.",
        "authors": [
            "Louie Sinadjan"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-17"
    },
    "http://arxiv.org/abs/2508.16592v2": {
        "id": "http://arxiv.org/abs/2508.16592v2",
        "title": "Performance measurements of modern Fortran MPI applications with Score-P",
        "link": "http://arxiv.org/abs/2508.16592v2",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-26",
        "tldr": "",
        "abstract": "Version 3.0 of the Message-Passing Interface (MPI) standard, released in 2012, introduced a new set of language bindings for Fortran 2008. By making use of modern language features and the enhanced interoperability with C, there was finally a type safe and standard conforming method to call MPI from Fortran. This highly recommended use mpi_f08 language binding has since then been widely adopted among developers of modern Fortran applications. However, tool support for the F08 bindings is still lacking almost a decade later, forcing users to recede to the less safe and convenient interfaces. Full support for the F08 bindings was added to the performance measurement infrastructure Score-P by implementing MPI wrappers in Fortran. Wrappers cover the latest MPI standard version 4.1 in its entirety, matching the features of the C wrappers. By implementing the wrappers in modern Fortran, we can provide full support for MPI procedures passing attributes, info objects, or callbacks. The implementation is regularly tested under the MPICH test suite. The new F08 wrappers were already used by two fluid dynamics simulation codes -- Neko, a spectral finite-element code derived from Nek5000, and EPIC (Elliptical Parcel-In-Cell) -- to successfully generate performance measurements. In this work, we additionally present our design considerations and sketch out the implementation, discussing the challenges we faced in the process. The key component of the implementation is a code generator that produces approximately 50k lines of MPI wrapper code to be used by Score-P, relying on the Python pympistandard module to provide programmatic access to the extracted data from the MPI standard.",
        "authors": [
            "Gregor Corbin"
        ],
        "categories": [
            "cs.DC",
            "cs.MS",
            "cs.PF"
        ],
        "submit_date": "2025-08-08"
    },
    "http://arxiv.org/abs/2508.18123v2": {
        "id": "http://arxiv.org/abs/2508.18123v2",
        "title": "Views: a hardware-friendly graph database model for storing semantic information",
        "link": "http://arxiv.org/abs/2508.18123v2",
        "tags": [
            "RAG",
            "storage",
            "hardware"
        ],
        "relevant": true,
        "indexed_date": "2025-08-26",
        "tldr": "Proposes a hardware-friendly graph database model (Views) optimized for storage and retrieval efficiency in semantic AI and RAG applications. Demonstrates functional equivalence with traditional representations and improved storage performance.",
        "abstract": "The graph database (GDB) is an increasingly common storage model for data involving relationships between entries. Beyond its widespread usage in database industries, the advantages of GDBs indicate a strong potential in constructing symbolic artificial intelligences (AIs) and retrieval-augmented generation (RAG), where knowledge of data inter-relationships takes a critical role in implementation. However, current GDB models are not optimised for hardware acceleration, leading to bottlenecks in storage capacity and computational efficiency. In this paper, we propose a hardware-friendly GDB model, called Views. We show its data structure and organisation tailored for efficient storage and retrieval of graph data and demonstrate its functional equivalence and storage performance advantage compared to represent traditional graph representations. We further demonstrate its symbolic processing abilities in semantic reasoning and cognitive modelling with practical examples and provide a short perspective on future developments.",
        "authors": [
            "Yanjun Yang",
            "Adrian Wheeldon",
            "Yihan Pan",
            "Themis Prodromakis",
            "Alex Serb"
        ],
        "categories": [
            "cs.DB",
            "cs.AR",
            "cs.DC",
            "cs.SC"
        ],
        "submit_date": "2025-08-25"
    },
    "http://arxiv.org/abs/2508.16712v1": {
        "id": "http://arxiv.org/abs/2508.16712v1",
        "title": "Systematic Characterization of LLM Quantization: A Performance, Energy, and Quality Perspective",
        "link": "http://arxiv.org/abs/2508.16712v1",
        "tags": [
            "quantization",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-08-26",
        "tldr": "Characterizes tradeoffs in post-training LLM quantization for efficient serving using automated qMeter framework. Evaluates 11 methods across model sizes and GPUs under serving conditions, revealing performance, energy, and quality dependencies. Enables better capacity planning and scheduling.",
        "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across diverse domains, but their heavy resource demands make quantization-reducing precision to lower-bit formats-critical for efficient serving. While many quantization methods exist, a systematic understanding of their performance, energy, and quality tradeoffs in realistic serving conditions remains a gap. In this work, we first develop a fully automated online characterization framework qMeter, and then conduct an in-depth characterization of 11 post-training LLM quantization methods across 4 model sizes (7B-70B) and two GPU architectures (A100, H100). We evaluate quantization at the application, workload, parallelism, and hardware levels under online serving conditions. Our study reveals highly task- and method-dependent tradeoffs, strong sensitivity to workload characteristics, and complex interactions with parallelism and GPU architecture. We further present three optimization case studies illustrating deployment challenges in capacity planning, energy-efficient scheduling, and multi-objective tuning. To the best of our knowledge, this is one of the first comprehensive application-, system-, and hardware-level characterization of LLM quantization from a joint performance, energy, and quality perspective.",
        "authors": [
            "Tianyao Shi",
            "Yi Ding"
        ],
        "categories": [
            "cs.PF",
            "cs.AI",
            "cs.AR",
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-08-22"
    },
    "http://arxiv.org/abs/2508.16700v2": {
        "id": "http://arxiv.org/abs/2508.16700v2",
        "title": "GPT-OSS-20B: A Comprehensive Deployment-Centric Analysis of OpenAI's Open-Weight Mixture of Experts Model",
        "link": "http://arxiv.org/abs/2508.16700v2",
        "tags": [
            "serving",
            "MoE",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-08-26",
        "tldr": "Evaluates the deployment efficiency of a MoE model (GPT-OSS-20B) against dense models on a single GPU. Focuses on metrics like throughput, latency, VRAM usage, and energy consumption. Achieves 31.8% higher decode throughput, 31.7% lower peak VRAM, and 25.8% lower energy per token compared to baselines.",
        "abstract": "We present a single-GPU (H100, bf16) evaluation of GPT-OSS-20B (Mixture-of-Experts; 20.9B total, approx. 3.61B active) against dense baselines Qwen3-32B and Yi-34B across multiple dimensions. We measure true time-to-first-token (TTFT), full-decode throughput (TPOT), end-to-end latency percentiles, peak VRAM with past key values (PKV) held, and energy via a consistent nvidia-smi-based sampler. At a 2048-token context with 64-token decode, GPT-OSS-20B delivers higher decode throughput and tokens per Joule than dense baselines Qwen3-32B and Yi-34B, while substantially reducing peak VRAM and energy per 1000 generated tokens; its TTFT is higher due to MoE routing overhead. With only 17.3% of parameters active (3.61B of 20.9B), GPT-OSS-20B provides about 31.8% higher decode throughput and 25.8% lower energy per 1000 generated tokens than Qwen3-32B at 2048/64, while using 31.7% less peak VRAM. Normalized by active parameters, GPT-OSS-20B shows markedly stronger per-active-parameter efficiency (APE), underscoring MoE's deployment advantages. We do not evaluate accuracy; this is a deployment-focused study. We release code and consolidated results to enable replication and extension.",
        "authors": [
            "Deepak Kumar",
            "Divakar Yadav",
            "Yash Patel"
        ],
        "categories": [
            "cs.AR",
            "cs.AI",
            "cs.DC",
            "cs.PF"
        ],
        "submit_date": "2025-08-22"
    },
    "http://arxiv.org/abs/2508.16298v3": {
        "id": "http://arxiv.org/abs/2508.16298v3",
        "title": "Scalable hybrid quantum Monte Carlo simulation of U(1) gauge field coupled to fermions on GPU",
        "link": "http://arxiv.org/abs/2508.16298v3",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-26",
        "tldr": "",
        "abstract": "We develop a GPU-accelerated hybrid quantum Monte Carlo (QMC) algorithm to solve the fundamental yet difficult problem of $U(1)$ gauge field coupled to fermions, which gives rise to a $U(1)$ Dirac spin liquid state under the description of (2+1)d quantum electrodynamics QED$_3$. The algorithm renders a good acceptance rate and, more importantly, nearly linear space-time volume scaling in computational complexity $O(N_τ V_s)$, where $N_τ$ is the imaginary time dimension and $V_s$ is spatial volume, which is much more efficient than determinant QMC with scaling behavior of $O(N_τV_s^3)$. Such acceleration is achieved via a collection of technical improvements, including (i) the design of the efficient problem-specific preconditioner, (ii) customized CUDA kernel for matrix-vector multiplication, and (iii) CUDA Graph implementation on the GPU. These advances allow us to simulate the $U(1)$ Dirac spin liquid state with unprecedentedly large system sizes, which is up to $N_τ\\times L\\times L = 660\\times66\\times66$, and reveal its novel properties. With these technical improvements, we see the asymptotic convergence in the scaling dimensions of various fermion bilinear operators and the conserved current operator when approaching the thermodynamic limit. The scaling dimensions find good agreement with field-theoretical expectation, which provides supporting evidence for the conformal nature of the $U(1)$ Dirac spin liquid state in the \\qed. Our technical advancements open an avenue to study the Dirac spin liquid state and its transition towards symmetry-breaking phases at larger system sizes and with less computational burden.",
        "authors": [
            "Kexin Feng",
            "Chuang Chen",
            "Zi Yang Meng"
        ],
        "categories": [
            "cond-mat.str-el",
            "cs.DC",
            "hep-th"
        ],
        "submit_date": "2025-08-22"
    },
    "http://arxiv.org/abs/2508.16690v2": {
        "id": "http://arxiv.org/abs/2508.16690v2",
        "title": "Iridescent: A Framework Enabling Online System Implementation Specialization",
        "link": "http://arxiv.org/abs/2508.16690v2",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-26",
        "tldr": "",
        "abstract": "Specializing systems to specifics of the workload they serve and platform they are running on often significantly improves performance. However, specializing systems is difficult in practice because of compounding challenges: i) complexity for the developers to determine and implement optimal specialization; ii) inherent loss of generality of the resulting implementation, and iii) difficulty in identifying and implementing a single optimal specialized configuration for the messy reality of modern systems. To address this, we introduce Iridescent, a framework for automated online system specialization guided by observed overall system performance. Iridescent lets developers specify a space of possible specialization choices, and then at runtime generates and runs different specialization choices through JIT compilation as the system runs. By using overall system performance metrics to guide this search, developers can use Iridescent to find optimal system specializations for the hardware and workload conditions at a given time. We demonstrate feasibility, effectivity, and ease of use.",
        "authors": [
            "Vaastav Anand",
            "Deepak Garg",
            "Antoine Kaufmann"
        ],
        "categories": [
            "cs.OS"
        ],
        "submit_date": "2025-08-21"
    },
    "http://arxiv.org/abs/2508.17764v1": {
        "id": "http://arxiv.org/abs/2508.17764v1",
        "title": "Puzzle: Scheduling Multiple Deep Learning Models on Mobile Device with Heterogeneous Processors",
        "link": "http://arxiv.org/abs/2508.17764v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-26",
        "tldr": "",
        "abstract": "As deep learning models are increasingly deployed on mobile devices, modern mobile devices incorporate deep learning-specific accelerators to handle the growing computational demands, thus increasing their hardware heterogeneity. However, existing works on scheduling deep learning workloads across these processors have significant limitations: most studies focus on single-model scenarios rather than realistic multi-model scenarios, overlook performance variations from different hardware/software configurations, and struggle with accurate execution time estimation. To address these challenges, we propose a novel genetic algorithm-based methodology for scheduling multiple deep learning networks on heterogeneous processors by partitioning the networks into multiple subgraphs. Our approach incorporates three different types of chromosomes for partition/mapping/priority exploration, and leverages device-in-the-loop profiling and evaluation for accurate execution time estimation. Based on this methodology, our system, Puzzle, demonstrates superior performance in extensive evaluations with randomly generated scenarios involving nine state-of-the-art networks. The results demonstrate Puzzle can support 3.7 and 2.2 times higher request frequency on average compared to the two heuristic baselines, NPU Only and Best Mapping, respectively, while satisfying the equivalent level of real-time requirements.",
        "authors": [
            "Duseok Kang",
            "Yunseong Lee",
            "Junghoon Kim"
        ],
        "categories": [
            "cs.LG",
            "cs.OS"
        ],
        "submit_date": "2025-08-25"
    },
    "http://arxiv.org/abs/2508.16308v1": {
        "id": "http://arxiv.org/abs/2508.16308v1",
        "title": "Generalizing Brooks' theorem via Partial Coloring is Hard Classically and Locally",
        "link": "http://arxiv.org/abs/2508.16308v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-25",
        "tldr": "",
        "abstract": "We investigate the classical and distributed complexity of \\emph{$k$-partial $c$-coloring} where $c=k$, a natural generalization of Brooks' theorem where each vertex should be colored from the palette $\\{1,\\ldots,c\\} = \\{1,\\ldots,k\\}$ such that it must have at least $\\min\\{k, °(v)\\}$ neighbors colored differently. Das, Fraigniaud, and Ros{é}n~[OPODIS 2023] showed that the problem of $k$-partial $(k+1)$-coloring admits efficient centralized and distributed algorithms and posed an open problem about the status of the distributed complexity of $k$-partial $k$-coloring. We show that the problem becomes significantly harder when the number of colors is reduced from $k+1$ to $k$ for every constant $k\\geq 3$.   In the classical setting, we prove that deciding whether a graph admits a $k$-partial $k$-coloring is NP-complete for every constant $k \\geq 3$, revealing a sharp contrast with the linear-time solvable $(k+1)$-color case. For the distributed LOCAL model, we establish an $Ω(n)$-round lower bound for computing $k$-partial $k$-colorings, even when the graph is guaranteed to be $k$-partial $k$-colorable. This demonstrates an exponential separation from the $O(\\log^2 k \\cdot \\log n)$-round algorithms known for $(k+1)$-colorings.   Our results leverage novel structural characterizations of ``hard instances'' where partial coloring reduces to proper coloring, and we construct intricate graph gadgets to prove lower bounds via indistinguishability arguments.",
        "authors": [
            "Jan Bok",
            "Avinandan Das",
            "Anna Gujgiczer",
            "Nikola Jedličková"
        ],
        "categories": [
            "cs.DC",
            "cs.CC",
            "cs.DM"
        ],
        "submit_date": "2025-08-22"
    },
    "http://arxiv.org/abs/2508.15919v2": {
        "id": "http://arxiv.org/abs/2508.15919v2",
        "title": "HyperFlexis: Joint Design of Algorithms and Systems for Multi-SLO Serving and Fast Scaling",
        "link": "http://arxiv.org/abs/2508.15919v2",
        "tags": [
            "serving",
            "disaggregation",
            "autoscaling"
        ],
        "relevant": true,
        "indexed_date": "2025-08-25",
        "tldr": "Proposes HyperFlexis, an LLM serving system combining scheduling and scaling optimizations for multi-SLO compliance. Features a multi-SLO scheduler, KV cache transfers, D2D weight transfer for faster scaling, and P/D role transitions. Achieves 4.44× higher SLO attainment and 65.82% lower latency.",
        "abstract": "Modern large language model (LLM) serving systems face challenges from highly variable requests with diverse lengths, priorities, and stage-specific service-level objectives (SLOs). Meeting these requires real-time scheduling, rapid and cost-effective scaling, and support for both collocated and disaggregated Prefill/Decode (P/D) architectures. We present HyperFlexis, a unified LLM serving system that integrates algorithmic and system-level innovations to jointly optimize scheduling and scaling under multiple SLOs. It features a multi-SLO-aware scheduler that leverages budget estimation and request prioritization to ensure proactive SLO compliance for both new and ongoing requests. The system supports prefill- and decode-stage multi-SLO scheduling for P/D-disaggregated architectures and KV cache transfers. It also enables cost-effective scaling decisions, prefill-decode instance linking during scaling, and rapid P/D role transitions. To accelerate scaling and reduce cold-start latency, a device-to-device (D2D) weight transfer mechanism is proposed that lowers weight loading overhead by up to 19.39$\\times$. These optimizations allow the system to achieve up to 4.44$\\times$ higher SLO attainment, 65.82% lower request latency, and cost parity with state-of-the-art baselines. The code will be released soon.",
        "authors": [
            "Zahra Yousefijamarani",
            "Xinglu Wang",
            "Qian Wang",
            "Morgan Lindsay Heisler",
            "Taha Shabani",
            "Niloofar Gholipour",
            "Parham Yassini",
            "Hong Chang",
            "Kan Chen",
            "Qiantao Zhang",
            "Xiaolong Bai",
            "Jiannan Wang",
            "Ying Xiong",
            "Yong Zhang",
            "Zhenan Fan"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-08-21"
    },
    "http://arxiv.org/abs/2508.16522v1": {
        "id": "http://arxiv.org/abs/2508.16522v1",
        "title": "On the Duality of Task and Actor Programming Models",
        "link": "http://arxiv.org/abs/2508.16522v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-25",
        "tldr": "",
        "abstract": "Programming models for distributed and heterogeneous machines are rapidly growing in popularity to meet the demands of modern workloads. Task and actor models are common choices that offer different trade-offs between development productivity and achieved performance. Task-based models offer better productivity and composition of software, whereas actor-based models routinely deliver better peak performance due to lower overheads. While task-based and actor-based models appear to be different superficially, we demonstrate these programming models are duals of each other. Importantly, we show that this duality extends beyond functionality to performance, and elucidate techniques that let task-based systems deliver performance competitive with actor-based systems without compromising productivity. We apply these techniques to both Realm, an explicitly parallel task-based runtime, as well as Legion, an implicitly parallel task-based runtime. We show these techniques reduce Realm's overheads by between 1.7-5.3x, coming within a factor of two of the overheads imposed by heavily optimized actor-based systems like Charm++ and MPI. We further show that our techniques enable between 1.3-5.0x improved strong scaling of unmodified Legion applications.",
        "authors": [
            "Rohan Yadav",
            "Joseph Guman",
            "Sean Treichler",
            "Michael Garland",
            "Alex Aiken",
            "Fredrik Kjolstad",
            "Michael Bauer"
        ],
        "categories": [
            "cs.PL",
            "cs.DC"
        ],
        "submit_date": "2025-08-22"
    },
    "http://arxiv.org/abs/2508.16297v2": {
        "id": "http://arxiv.org/abs/2508.16297v2",
        "title": "Hybrid Classical-Quantum Supercomputing: A demonstration of a multi-user, multi-QPU and multi-GPU environment",
        "link": "http://arxiv.org/abs/2508.16297v2",
        "tags": [
            "training",
            "hardware",
            "recommendation"
        ],
        "relevant": true,
        "indexed_date": "2025-08-25",
        "tldr": "Demonstrates a multi-user hybrid classical-quantum computing environment for HPC centers. Integrates quantum processing units (QPUs) and GPUs using standard HPC infrastructure with Slurm workload management and NVIDIA CUDA-Q APIs. Achieves practical deployment for optimizing hybrid machine learning and optimization tasks.",
        "abstract": "Achieving a practical quantum advantage for near-term applications is widely expected to rely on hybrid classical-quantum algorithms. To deliver this practical advantage to users, high performance computing (HPC) centers need to provide a suitable software and hardware stack that supports algorithms of this type. In this paper, we describe the world's first implementation of a classical-quantum environment in an HPC center that allows multiple users to execute hybrid algorithms on multiple quantum processing units (QPUs) and GPUs. Our setup at the Poznan Supercomputing and Networking Center (PCSS) aligns with current HPC norms: the computing hardware including QPUs is installed in an active data center room with standard facilities; there are no special considerations for networking, power, and cooling; we use Slurm for workload management as well as the NVIDIA CUDA-Q extension API for classical-quantum interactions. We demonstrate applications of this environment for hybrid classical-quantum machine learning and optimisation. The aim of this work is to provide the community with an experimental example for further research and development on how quantum computing can practically enhance and extend HPC capabilities.",
        "authors": [
            "Mateusz Slysz",
            "Piotr Rydlichowski",
            "Krzysztof Kurowski",
            "Omar Bacarreza",
            "Esperanza Cuenca Gomez",
            "Zohim Chandani",
            "Bettina Heim",
            "Pradnya Khalate",
            "William R. Clements",
            "James Fletcher"
        ],
        "categories": [
            "quant-ph",
            "cs.DC",
            "cs.ET"
        ],
        "submit_date": "2025-08-22"
    },
    "http://arxiv.org/abs/2508.16268v1": {
        "id": "http://arxiv.org/abs/2508.16268v1",
        "title": "Self-Healing Network of Interconnected Edge Devices Empowered by Infrastructure-as-Code and LoRa Communication",
        "link": "http://arxiv.org/abs/2508.16268v1",
        "tags": [
            "edge",
            "networking",
            "offline"
        ],
        "relevant": true,
        "indexed_date": "2025-08-25",
        "tldr": "Proposes self-healing network for edge devices using LoRa and IaC to handle node failures and limited bandwidth. Design includes containerized architecture, data packet fragmentation, and automated failover redeployment. Achieves sub-second service recovery after node failure.",
        "abstract": "This Paper proposes a self-healing, automated network of Raspberry Pi devices designed for deployment in scenarios where traditional networking is unavailable. Leveraging the low-power, long-range capabilities of the LoRa (Long Range) protocol alongside Infrastructure as Code (IaC) methodologies, the research addresses challenges such as limited bandwidth, data collisions, and node failures. Given that LoRa's packet-based system is incompatible with conventional IaC tools like Ansible and Terraform, which rely on TCP/IP networking, the research adapts IaC principles within a containerised architecture deployed across a Raspberry Pi cluster. Evaluation experiments indicate that fragmenting data packets and retransmitting any missed fragments can mitigate LoRa's inherent throughput and packet size limitations, although issues such as collisions and line-of-sight interference persist. An automated failover mechanism was integrated into the architecture, enabling unresponsive services to be redeployed to alternative nodes within one second, demonstrating the system's resilience in maintaining operational continuity despite node or service failures. The paper also identifies practical challenges, including the necessity for time-slotting transmissions to prevent data packet overlap and collisions. Future research should explore the integration of mesh networking to enhance range, develop more advanced scheduling algorithms, and adopt cutting-edge low-power wide-area network (LPWAN) techniques.",
        "authors": [
            "Rob Carson",
            "Mohamed Chahine Ghanem",
            "Feriel Bouakkaz"
        ],
        "categories": [
            "cs.NI",
            "cs.DC"
        ],
        "submit_date": "2025-08-22"
    },
    "http://arxiv.org/abs/2508.15833v1": {
        "id": "http://arxiv.org/abs/2508.15833v1",
        "title": "Towards Integrated Energy-Communication-Transportation Hub: A Base-Station-Centric Design in 5G and Beyond",
        "link": "http://arxiv.org/abs/2508.15833v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-25",
        "tldr": "",
        "abstract": "The rise of 5G communication has transformed the telecom industry for critical applications. With the widespread deployment of 5G base stations comes a significant concern about energy consumption. Key industrial players have recently shown strong interest in incorporating energy storage systems to store excess energy during off-peak hours, reducing costs and participating in demand response. The fast development of batteries opens up new possibilities, such as the transportation area. An effective method is needed to maximize base station battery utilization and reduce operating costs. In this trend towards next-generation smart and integrated energy-communication-transportation (ECT) infrastructure, base stations are believed to play a key role as service hubs. By exploring the overlap between base station distribution and electric vehicle charging infrastructure, we demonstrate the feasibility of efficiently charging EVs using base station batteries and renewable power plants at the Hub. Our model considers various factors, including base station traffic conditions, weather, and EV charging behavior. This paper introduces an incentive mechanism for setting charging prices and employs a deep reinforcement learning-based method for battery scheduling. Experimental results demonstrate the effectiveness of our proposed ECT-Hub in optimizing surplus energy utilization and reducing operating costs, particularly through revenue-generating EV charging.",
        "authors": [
            "Linfeng Shen",
            "Guanzhen Wu",
            "Cong Zhang",
            "Xiaoyi Fan",
            "Jiangchuan Liu"
        ],
        "categories": [
            "cs.NI",
            "cs.DC"
        ],
        "submit_date": "2025-08-18"
    },
    "http://arxiv.org/abs/2508.15980v1": {
        "id": "http://arxiv.org/abs/2508.15980v1",
        "title": "CXLAimPod: CXL Memory is all you need in AI era",
        "link": "http://arxiv.org/abs/2508.15980v1",
        "tags": [
            "networking",
            "hardware"
        ],
        "relevant": true,
        "indexed_date": "2025-08-25",
        "tldr": "Proposes CXLAimPod, a full-duplex aware scheduling framework leveraging CXL memory to optimize mixed read-write patterns. Integrates cgroup-based hints and eBPF for application-aware optimization. Achieves up to 71.6% improvement in LLM text generation and 150% for specific Redis patterns.",
        "abstract": "The proliferation of data-intensive applications, ranging from large language models to key-value stores, increasingly stresses memory systems with mixed read-write access patterns. Traditional half-duplex architectures such as DDR5 are ill-suited for such workloads, suffering bus turnaround penalties that reduce their effective bandwidth under mixed read-write patterns. Compute Express Link (CXL) offers a breakthrough with its full-duplex channels, yet this architectural potential remains untapped as existing software stacks are oblivious to this capability. This paper introduces CXLAimPod, an adaptive scheduling framework designed to bridge this software-hardware gap through system support, including cgroup-based hints for application-aware optimization. Our characterization quantifies the opportunity, revealing that CXL systems achieve 55-61% bandwidth improvement at balanced read-write ratios compared to flat DDR5 performance, demonstrating the benefits of full-duplex architecture. To realize this potential, the CXLAimPod framework integrates multiple scheduling strategies with a cgroup-based hint mechanism to navigate the trade-offs between throughput, latency, and overhead. Implemented efficiently within the Linux kernel via eBPF, CXLAimPod delivers significant performance improvements over default CXL configurations. Evaluation on diverse workloads shows 7.4% average improvement for Redis (with up to 150% for specific sequential patterns), 71.6% improvement for LLM text generation, and 9.1% for vector databases, demon-strating that duplex-aware scheduling can effectively exploit CXL's architectural advantages.",
        "authors": [
            "Yiwei Yang",
            "Yusheng Zheng",
            "Yiqi Chen",
            "Zheng Liang",
            "Kexin Chu",
            "Zhe Zhou",
            "Andi Quinn",
            "Wei Zhang"
        ],
        "categories": [
            "cs.OS"
        ],
        "submit_date": "2025-08-21"
    },
    "http://arxiv.org/abs/2508.15647v1": {
        "id": "http://arxiv.org/abs/2508.15647v1",
        "title": "CausalMesh: A Formally Verified Causal Cache for Stateful Serverless Computing",
        "link": "http://arxiv.org/abs/2508.15647v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-22",
        "tldr": "",
        "abstract": "Stateful serverless workflows consist of multiple serverless functions that access state on a remote database. Developers sometimes add a cache layer between the serverless runtime and the database to improve I/O latency. However, in a serverless environment, functions in the same workflow may be scheduled to different nodes with different caches, which can cause non-intuitive anomalies. This paper presents CausalMesh, a novel approach to causally consistent caching in environments where a computation may migrate from one machine to another, such as in serverless computing. CausalMesh is the first cache system that supports coordination-free and abort-free read/write operations and read transactions when clients roam among multiple servers. CausalMesh also supports read-write transactional causal consistency in the presence of client roaming, but at the cost of abort-freedom.   We have formally verified CausalMesh's protocol in Dafny, and our experimental evaluation shows that CausalMesh has lower latency and higher throughput than existing proposals",
        "authors": [
            "Haoran Zhang",
            "Zihao Zhang",
            "Shuai Mu",
            "Sebastian Angel",
            "Vincent Liu"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-21"
    },
    "http://arxiv.org/abs/2508.15601v1": {
        "id": "http://arxiv.org/abs/2508.15601v1",
        "title": "Efficient Mixed-Precision Large Language Model Inference with TurboMind",
        "link": "http://arxiv.org/abs/2508.15601v1",
        "tags": [
            "serving",
            "quantization",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-08-22",
        "tldr": "Proposes mixed-precision inference techniques for LLMs, including GEMM and attention pipelines with hardware-aware optimizations. Achieves up to 61% lower latency and 156% higher throughput compared to existing frameworks, integrated into TurboMind engine.",
        "abstract": "Mixed-precision inference techniques reduce the memory and computational demands of Large Language Models (LLMs) by applying hybrid precision formats to model weights, activations, and KV caches. This work introduces mixed-precision LLM inference techniques that encompass (i) systematic memory and compute optimization across hierarchical storage and tensor core architectures, and (ii) comprehensive end-to-end mixed-precision optimization across diverse precision formats and hardware configurations. Our approach features two novel mixed-precision pipelines designed for optimal hardware utilization: a General Matrix Multiply (GEMM) pipeline that optimizes matrix operations through offline weight packing and online acceleration, and an attention pipeline that enables efficient attention computation with arbitrary Query, Key, and Value precision combinations. The key implementation of the pipelines includes (i) hardware-aware weight packing for automatic format optimization, (ii) adaptive head alignment for efficient attention computation, (iii) instruction-level parallelism for memory hierarchy exploitation, and (iv) KV memory loading pipeline for enhanced inference efficiency. We conduct comprehensive evaluations across 16 popular LLMs and 4 representative GPU architectures. Results demonstrate that our approach achieves up to 61% lower serving latency (30% on average) and up to 156% higher throughput (58% on average) in mixed-precision workloads compared to existing mixed-precision frameworks, establishing consistent performance improvements across all tested configurations and hardware types. This work is integrated into TurboMind, a high-performance inference engine of the LMDeploy project, which is open-sourced and publicly available at https://github.com/InternLM/lmdeploy.",
        "authors": [
            "Li Zhang",
            "Youhe Jiang",
            "Guoliang He",
            "Xin Chen",
            "Han Lv",
            "Qian Yao",
            "Fangcheng Fu",
            "Kai Chen"
        ],
        "categories": [
            "cs.DC",
            "cs.PF"
        ],
        "submit_date": "2025-08-21"
    },
    "http://arxiv.org/abs/2508.15562v2": {
        "id": "http://arxiv.org/abs/2508.15562v2",
        "title": "Lower Bounds for $k$-Set Agreement in Fault-Prone Networks",
        "link": "http://arxiv.org/abs/2508.15562v2",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-22",
        "tldr": "",
        "abstract": "We develop a new lower bound for k-set agreement in synchronous message-passing systems connected by an arbitrary directed communication network, where up to t processes may crash. Our result thus generalizes the t/k+1 lower bound for complete networks in the t-resilient model by Chaudhuri, Herlihy, Lynch, and Tuttle [JACM'00]. Moreover, it generalizes two lower bounds for oblivious algorithms in synchronous systems connected by an arbitrary undirected communication network known to the processes, namely, the domination number-based lower bound by Castaneda, Fraigniaud, Paz, Rajsbaum, Roy, and Travers [TCS'21] for failure-free processes, and the radius-based lower bound in the t-resilient model by Fraigniaud, Nguyen, and Paz [STACS'24].   Our topological proof non-trivially generalizes and extends the connectivity-based approach for the complete network, as presented in the book by Herlihy, Kozlov, and Rajsbaum (2013). It is based on a sequence of shellable carrier maps that, starting from a shellable input complex, determine the evolution of the protocol complex: During the first t/k rounds, carrier maps that crash exactly k processes per round are used, ensuring high connectivity of their images. A Sperner's lemma style argument is used to prove that k-set agreement is still impossible by that round. From round t/k+1 up to our lower bound, we employ a novel carrier map that maintains high connectivity. Our proof also provides a strikingly simple lower bound for k-set agreement in synchronous systems with an arbitrary communication network with initial crashes. We express the resulting additional agreement overhead via an appropriately defined radius of the communication graphs. Finally, we prove that the usual input pseudosphere complex for k-set agreement can be replaced by an exponentially smaller input complex based on Kuhn triangulations, which we prove to be also shellable.",
        "authors": [
            "Pierre Fraigniaud",
            "Minh Hang Nguyen",
            "Ami Paz",
            "Ulrich Schmid",
            "Hugo Rincon Galeana"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-21"
    },
    "http://arxiv.org/abs/2508.15484v1": {
        "id": "http://arxiv.org/abs/2508.15484v1",
        "title": "Universal Dancing by Luminous Robots under Sequential Schedulers",
        "link": "http://arxiv.org/abs/2508.15484v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-22",
        "tldr": "",
        "abstract": "The Dancing problem requires a swarm of $n$ autonomous mobile robots to form a sequence of patterns, aka perform a choreography. Existing work has proven that some crucial restrictions on choreographies and initial configurations (e.g., on repetitions of patterns, periodicity, symmetries, contractions/expansions) must hold so that the Dancing problem can be solved under certain robot models. Here, we prove that these necessary constraints can be dropped by considering the LUMI model (i.e., where robots are endowed with a light whose color can be chosen from a constant-size palette) under the quite unexplored sequential scheduler. We formalize the class of Universal Dancing problems which require a swarm of $n$ robots starting from any initial configuration to perform a (periodic or finite) sequence of arbitrary patterns, only provided that each pattern consists of $n$ vertices (including multiplicities). However, we prove that, to be solvable under LUMI, the length of the feasible choreographies is bounded by the compositions of $n$ into the number of colors available to the robots. We provide an algorithm solving the Universal Dancing problem by exploiting the peculiar capability of sequential robots to implement a distributed counter mechanism. Even assuming non-rigid movements, our algorithm ensures spatial homogeneity of the performed choreography.",
        "authors": [
            "Caterina Feletti",
            "Paola Flocchini",
            "Debasish Pattanayak",
            "Giuseppe Prencipe",
            "Nicola Santoro"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-21"
    },
    "http://arxiv.org/abs/2508.15351v2": {
        "id": "http://arxiv.org/abs/2508.15351v2",
        "title": "Databelt: A Continuous Data Path for Serverless Workflows in the 3D Compute Continuum",
        "link": "http://arxiv.org/abs/2508.15351v2",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-22",
        "tldr": "",
        "abstract": "Typically, serverless functions rely on remote storage services for managing state, which can result in increased latency and network communication overhead. In a dynamic environment such as the 3D (Edge-Cloud-Space) Compute Continuum, serverless functions face additional challenges due to frequent changes in network topology. As satellites move in and out of the range of ground stations, functions must make multiple hops to access cloud services, leading to high-latency state access and unnecessary data transfers. In this paper, we present Databelt, a state management framework for serverless workflows designed for the dynamic environment of the 3D Compute Continuum. Databelt introduces an SLO-aware state propagation mechanism that enables the function state to move continuously in orbit. Databelt proactively offloads function states to the most suitable node, such that when functions execute, the data is already present on the execution node or nearby, thus minimizing state access latency and reducing the number of network hops. Additionally, Databelt introduces a function state fusion mechanism that abstracts state management for functions sharing the same serverless runtime. When functions are fused, Databelt seamlessly retrieves their state as a group, reducing redundant network and storage operations and improving overall workflow efficiency. Our experimental results show that Databelt reduces workflow execution time by up to 66% and increases throughput by 50% compared to the baselines. Furthermore, our results show that Databelt function state fusion reduces storage operations latency by up to 20%, by reducing repetitive storage requests for functions within the same runtime, ensuring efficient execution of serverless workflows in highly dynamic network environments such as the 3D Continuum.",
        "authors": [
            "Cynthia Marcelino",
            "Leonard Guelmino",
            "Thomas Pusztai",
            "Stefan Nastic"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-21"
    },
    "http://arxiv.org/abs/2508.15105v4": {
        "id": "http://arxiv.org/abs/2508.15105v4",
        "title": "Declarative Data Pipeline for Large Scale ML Services",
        "link": "http://arxiv.org/abs/2508.15105v4",
        "tags": [
            "training",
            "offline",
            "storage"
        ],
        "relevant": true,
        "indexed_date": "2025-08-22",
        "tldr": "Addresses scaling challenges for ML data processing in large collaborative environments. Proposes Declarative Data Pipeline (DDP) with modular Pipes integrated into Apache Spark for optimized data flows. Achieves 500x scalability improvement and 10x throughput gains on enterprise-scale datasets.",
        "abstract": "Modern distributed data processing systems struggle to balance performance, maintainability, and developer productivity when integrating machine learning at scale. These challenges intensify in large collaborative environments due to high communication overhead and coordination complexity. We present a \"Declarative Data Pipeline\" (DDP) architecture that addresses these challenges while processing billions of records efficiently. Our modular framework seamlessly integrates machine learning within Apache Spark using logical computation units called Pipes, departing from traditional microservice approaches. By establishing clear component boundaries and standardized interfaces, we achieve modularity and optimization without sacrificing maintainability. Enterprise case studies demonstrate substantial improvements: 50% better development efficiency, collaboration efforts compressed from weeks to days, 500x scalability improvement, and 10x throughput gains.",
        "authors": [
            "Yunzhao Yang",
            "Runhui Wang",
            "Xuanqing Liu",
            "Adit Krishnan",
            "Yefan Tao",
            "Yuqian Deng",
            "Kuangyou Yao",
            "Peiyuan Sun",
            "Henrik Johnson",
            "Aditi sinha",
            "Davor Golac",
            "Gerald Friedland",
            "Usman Shakeel",
            "Daryl Cooke",
            "Joe Sullivan",
            "Madhusudhanan Chandrasekaran",
            "Chris Kong"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-20"
    },
    "http://arxiv.org/abs/2508.15703v1": {
        "id": "http://arxiv.org/abs/2508.15703v1",
        "title": "Mitigating context switching in densely packed Linux clusters with Latency-Aware Group Scheduling",
        "link": "http://arxiv.org/abs/2508.15703v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-22",
        "tldr": "",
        "abstract": "Cluster orchestrators such as Kubernetes depend on accurate estimates of node capacity and job requirements. Inaccuracies in either lead to poor placement decisions and degraded cluster performance. In this paper, we show that in densely packed workloads, such as serverless applications, CPU context switching overheads can become so significant that a node's performance is severely degraded, even when the orchestrator placement is theoretically sound. In practice this issue is typically mitigated by over-provisioning the cluster, leading to wasted resources.   We show that these context switching overhead arise from both an increase in the average cost of an individual context switch and a higher rate of context switching, which together amplify overhead multiplicatively when managing large numbers of concurrent cgroups, Linux's group scheduling mechanism for managing multi-threaded colocated workloads. We propose and evaluate modifications to the standard Linux kernel scheduler that mitigate these effects, achieving the same effective performance with a 28% smaller cluster size. The key insight behind our approach is to prioritise task completion over low-level per-task fairness, enabling the scheduler to drain contended CPU run queues more rapidly and thereby reduce time spent on context switching.",
        "authors": [
            "Al Amjad Tawfiq Isstaif",
            "Evangelia Kalyvianaki",
            "Richard Mortier"
        ],
        "categories": [
            "cs.OS",
            "cs.DC"
        ],
        "submit_date": "2025-08-21"
    },
    "http://arxiv.org/abs/2508.15436v1": {
        "id": "http://arxiv.org/abs/2508.15436v1",
        "title": "On the Effectiveness of Graph Reordering for Accelerating Approximate Nearest Neighbor Search on GPU",
        "link": "http://arxiv.org/abs/2508.15436v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-22",
        "tldr": "",
        "abstract": "We present the first systematic investigation of graph reordering effects for graph-based Approximate Nearest Neighbor Search (ANNS) on a GPU. While graph-based ANNS has become the dominant paradigm for modern AI applications, recent approaches focus on algorithmic innovations while neglecting memory layout considerations that significantly affect execution time. Our unified evaluation framework enables comprehensive evaluation of diverse reordering strategies across different graph indices through a graph adapter that converts arbitrary graph topologies into a common representation and a GPU-optimized graph traversal engine. We conduct a comprehensive analysis across diverse datasets and state-of-the-art graph indices, introducing analysis metrics that quantify the relationship between structural properties and memory layout effectiveness. Our GPU-targeted reordering achieves up to 15$\\%$ QPS improvements while preserving search accuracy, demonstrating that memory layout optimization operates orthogonally to existing algorithmic innovations. We will release all code upon publication to facilitate reproducibility and foster further research.",
        "authors": [
            "Yutaro Oguri",
            "Mai Nishimura",
            "Yusuke Matsui"
        ],
        "categories": [
            "cs.IR",
            "cs.CV",
            "cs.DB",
            "cs.DC",
            "cs.DS"
        ],
        "submit_date": "2025-08-21"
    },
    "http://arxiv.org/abs/2508.15267v1": {
        "id": "http://arxiv.org/abs/2508.15267v1",
        "title": "Optimizing Compilation for Distributed Quantum Computing via Clustering and Annealing",
        "link": "http://arxiv.org/abs/2508.15267v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-22",
        "tldr": "",
        "abstract": "Efficiently mapping quantum programs onto Distributed quantum computing (DQC) are challenging, particularly when considering the heterogeneous quantum processing units (QPUs) with different structures. In this paper, we present a comprehensive compilation framework that addresses these challenges with three key insights: exploiting structural patterns within quantum circuits, using clustering for initial qubit placement, and adjusting qubit mapping with annealing algorithms. Experimental results demonstrate the effectiveness of our methods and the capability to handle complex heterogeneous distributed quantum systems. Our evaluation shows that our method reduces the objective value at most 88.40\\% compared to the baseline.",
        "authors": [
            "Ruilin Zhou",
            "Jinglei Cheng",
            "Yuhang Gan",
            "Junyu Liu",
            "Chen Qian"
        ],
        "categories": [
            "quant-ph",
            "cs.DC"
        ],
        "submit_date": "2025-08-21"
    },
    "http://arxiv.org/abs/2508.15158v1": {
        "id": "http://arxiv.org/abs/2508.15158v1",
        "title": "Reliable Multi-view 3D Reconstruction for `Just-in-time' Edge Environments",
        "link": "http://arxiv.org/abs/2508.15158v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-22",
        "tldr": "",
        "abstract": "Multi-view 3D reconstruction applications are revolutionizing critical use cases that require rapid situational-awareness, such as emergency response, tactical scenarios, and public safety. In many cases, their near-real-time latency requirements and ad-hoc needs for compute resources necessitate adoption of `Just-in-time' edge environments where the system is set up on the fly to support the applications during the mission lifetime. However, reliability issues can arise from the inherent dynamism and operational adversities of such edge environments, resulting in spatiotemporally correlated disruptions that impact the camera operations, which can lead to sustained degradation of reconstruction quality. In this paper, we propose a novel portfolio theory inspired edge resource management strategy for reliable multi-view 3D reconstruction against possible system disruptions. Our proposed methodology can guarantee reconstruction quality satisfaction even when the cameras are prone to spatiotemporally correlated disruptions. The portfolio theoretic optimization problem is solved using a genetic algorithm that converges quickly for realistic system settings. Using publicly available and customized 3D datasets, we demonstrate the proposed camera selection strategy's benefits in guaranteeing reliable 3D reconstruction against traditional baseline strategies, under spatiotemporal disruptions.",
        "authors": [
            "Md. Nurul Absur",
            "Abhinav Kumar",
            "Swastik Brahma",
            "Saptarshi Debroy"
        ],
        "categories": [
            "cs.CV",
            "cs.DC"
        ],
        "submit_date": "2025-08-21"
    },
    "http://arxiv.org/abs/2508.15010v2": {
        "id": "http://arxiv.org/abs/2508.15010v2",
        "title": "TOAST: Fast and scalable auto-partitioning based on principled static analysis",
        "link": "http://arxiv.org/abs/2508.15010v2",
        "tags": [
            "training",
            "kernel",
            "sparse"
        ],
        "relevant": true,
        "indexed_date": "2025-08-22",
        "tldr": "Proposes TOAST, a system for auto-partitioning large models using static compiler analysis and Monte Carlo Tree Search. Identifies tensor sharding constraints to prune search space, avoiding out-of-memory errors. Achieves superior partitioning plans 8.5x faster than industrial methods.",
        "abstract": "Partitioning large machine learning models across distributed accelerator systems is a complex process, requiring a series of interdependent decisions that are further complicated by internal sharding ambiguities. Consequently, existing auto-partitioners often suffer from out-of-memory errors or are prohibitively slow when exploring the exponentially large space of possible partitionings. To mitigate this, they artificially restrict the search space, but this approach frequently yields infeasible solutions that violate device memory constraints or lead to sub-optimal performance.   We propose a system that combines a novel static compiler analysis with a Monte Carlo Tree Search. Our analysis constructs an efficient decision space by identifying (i) tensor dimensions requiring identical sharding, and (ii) partitioning \"conflicts\" that require resolution.   Our system significantly outperforms state-of-the-art industrial methods across diverse hardware platforms and model architectures, discovering previously unknown, superior solutions, and the process is fully automated even for complex and large models.",
        "authors": [
            "Sami Alabed",
            "Dominik Grewe",
            "Norman Alexander Rink",
            "Masha Samsikova",
            "Timur Sitdikov",
            "Agnieszka Swietlik",
            "Dimitrios Vytiniotis",
            "Daniel Belov"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "submit_date": "2025-08-20"
    },
    "http://arxiv.org/abs/2508.14917v2": {
        "id": "http://arxiv.org/abs/2508.14917v2",
        "title": "Scalable FPGA Framework for Real-Time Denoising in High-Throughput Imaging: A DRAM-Optimized Pipeline using High-Level Synthesis",
        "link": "http://arxiv.org/abs/2508.14917v2",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-22",
        "tldr": "",
        "abstract": "High-throughput imaging workflows, such as Parallel Rapid Imaging with Spectroscopic Mapping (PRISM), generate data at rates that exceed conventional real-time processing capabilities. We present a scalable FPGA-based preprocessing pipeline for real-time denoising, implemented via High-Level Synthesis (HLS) and optimized for DRAM-backed buffering. Our architecture performs frame subtraction and averaging directly on streamed image data, minimizing latency through burst-mode AXI4 interfaces. The resulting kernel operates below the inter-frame interval, enabling inline denoising and reducing dataset size for downstream CPU/GPU analysis. Validated under PRISM-scale acquisition, this modular FPGA framework offers a practical solution for latency-sensitive imaging workflows in spectroscopy and microscopy.",
        "authors": [
            "Weichien Liao"
        ],
        "categories": [
            "cs.AR",
            "cs.CV",
            "cs.DC",
            "eess.IV",
            "eess.SP",
            "physics.ins-det"
        ],
        "submit_date": "2025-08-15"
    },
    "http://arxiv.org/abs/2508.14883v1": {
        "id": "http://arxiv.org/abs/2508.14883v1",
        "title": "The Cost Advantage of Virtual Machine Migrations: Empirical Insights into Amazon's EC2 Marketspace",
        "link": "http://arxiv.org/abs/2508.14883v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-21",
        "tldr": "",
        "abstract": "In recent years, cloud providers have introduced novel approaches for trading virtual machines. For example, Virtustream introduced so-called muVMs to charge cloud computing resources while other providers such as Google, Microsoft, or Amazon re-invented their marketspaces. Today, the market leader Amazon runs six marketspaces for trading virtual machines. Consumers can purchase bundles of virtual machines, which are called cloud-portfolios, from multiple marketspaces and providers. An industry-relevant field of research is to identify best practices and guidelines on how such optimal portfolios are created. In the paper at hand, a cost analysis of cloud portfolios is presented. Therefore, pricing data from Amazon was used as well as a real virtual machine utilization dataset from the Bitbrains datacenter. The results show that a cost optimum can only be reached if heterogeneous portfolios are created where virtual machines are purchased from different marketspaces. Additionally, the cost-benefit of migrating virtual machines to different marketplaces during runtime is presented. Such migrations are especially cost-effective for virtual machines of cloud-portfolios which run between 6 hours and 1 year. The paper further shows that most of the resources of virtual machines are never utilized by consumers, which represents a significant future potential for cost optimization. For the validation of the results, a second dataset of the Bitbrains datacenter was used, which contains utility data of virtual machines from a different domain of application.",
        "authors": [
            "Benedikt Pittl",
            "Werner Mach",
            "Erich Schikuta"
        ],
        "categories": [
            "cs.DC",
            "cs.GT"
        ],
        "submit_date": "2025-08-20"
    },
    "http://arxiv.org/abs/2508.14848v1": {
        "id": "http://arxiv.org/abs/2508.14848v1",
        "title": "Leveraging Hardware-Aware Computation in Mixed-Precision Matrix Multiply: A Tile-Centric Approach",
        "link": "http://arxiv.org/abs/2508.14848v1",
        "tags": [
            "kernel",
            "quantization",
            "hardware"
        ],
        "relevant": true,
        "indexed_date": "2025-08-21",
        "tldr": "Proposes an adaptive mixed-precision GEMM framework with fine-grained tile-level precision control. Utilizes PaRSEC runtime for workload balancing across CPUs and GPUs. Achieves scalable performance on Fugaku (ARM), NVIDIA A100, and Frontier (AMD) systems, enhancing compute efficiency.",
        "abstract": "General Matrix Multiplication (GEMM) is a critical operation underpinning a wide range of applications in high-performance computing (HPC) and artificial intelligence (AI). The emergence of hardware optimized for low-precision arithmetic necessitates a reevaluation of numerical algorithms to leverage mixed-precision computations, achieving improved performance and energy efficiency. This research introduces an adaptive mixed-precision GEMM framework that supports different precision formats at fine-grained tile/block levels. We utilize the PaRSEC runtime system to balance workloads across various architectures. The performance scales well on ARM CPU-based Fugaku supercomputer, Nvidia GPU-based A100 DGX, and AMD GPU-based Frontier supercomputer. This research aims to enhance computational efficiency and accuracy by bridging algorithmic advancements and hardware innovations, driving transformative progress in various applications.",
        "authors": [
            "Qiao Zhang",
            "Rabab Alomairy",
            "Dali Wang",
            "Zhuowei Gu",
            "Qinglei Cao"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-20"
    },
    "http://arxiv.org/abs/2508.14830v1": {
        "id": "http://arxiv.org/abs/2508.14830v1",
        "title": "MOHAF: A Multi-Objective Hierarchical Auction Framework for Scalable and Fair Resource Allocation in IoT Ecosystems",
        "link": "http://arxiv.org/abs/2508.14830v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-21",
        "tldr": "",
        "abstract": "The rapid growth of Internet of Things (IoT) ecosystems has intensified the challenge of efficiently allocating heterogeneous resources in highly dynamic, distributed environments. Conventional centralized mechanisms and single-objective auction models, focusing solely on metrics such as cost minimization or revenue maximization, struggle to deliver balanced system performance. This paper proposes the Multi-Objective Hierarchical Auction Framework (MOHAF), a distributed resource allocation mechanism that jointly optimizes cost, Quality of Service (QoS), energy efficiency, and fairness. MOHAF integrates hierarchical clustering to reduce computational complexity with a greedy, submodular optimization strategy that guarantees a (1-1/e) approximation ratio. A dynamic pricing mechanism adapts in real time to resource utilization, enhancing market stability and allocation quality. Extensive experiments on the Google Cluster Data trace, comprising 3,553 requests and 888 resources, demonstrate MOHAF's superior allocation efficiency (0.263) compared to Greedy (0.185), First-Price (0.138), and Random (0.101) auctions, while achieving perfect fairness (Jain's index = 1.000). Ablation studies reveal the critical influence of cost and QoS components in sustaining balanced multi-objective outcomes. With near-linear scalability, theoretical guarantees, and robust empirical performance, MOHAF offers a practical and adaptable solution for large-scale IoT deployments, effectively reconciling efficiency, equity, and sustainability in distributed resource coordination.",
        "authors": [
            "Kushagra Agrawal",
            "Polat Goktas",
            "Anjan Bandopadhyay",
            "Debolina Ghosh",
            "Junali Jasmine Jena",
            "Mahendra Kumar Gourisaria"
        ],
        "categories": [
            "cs.DC",
            "cs.GT",
            "cs.NE",
            "cs.NI"
        ],
        "submit_date": "2025-08-20"
    },
    "http://arxiv.org/abs/2508.14716v3": {
        "id": "http://arxiv.org/abs/2508.14716v3",
        "title": "DAG it off: Latency Prefers No Common Coins",
        "link": "http://arxiv.org/abs/2508.14716v3",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-21",
        "tldr": "",
        "abstract": "We introduce Black Marlin, the first Directed Acyclic Graph (DAG)-based Byzantine atomic broadcast protocol in a partially synchronous setting that successfully forgoes the reliable broadcast and common coin primitives while delivering transactions every round. Black Marlin achieves the optimal latency of 3 rounds of communication (4.25 with Byzantine faults) while maintaining optimal communication and amortized communication complexities. We present a formal security analysis of the protocol, accompanied by empirical evidence that Black Marlin outperforms state-of-the-art DAG-based protocols in both throughput and latency.",
        "authors": [
            "Ignacio Amores-Sesar",
            "Viktor Grøndal",
            "Adam Holmgård",
            "Mads Ottendal"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-20"
    },
    "http://arxiv.org/abs/2508.14625v1": {
        "id": "http://arxiv.org/abs/2508.14625v1",
        "title": "A Systematic Evaluation of the Potential of Carbon-Aware Execution for Scientific Workflows",
        "link": "http://arxiv.org/abs/2508.14625v1",
        "tags": [
            "offline",
            "training"
        ],
        "relevant": true,
        "indexed_date": "2025-08-21",
        "tldr": "Investigates reducing carbon emissions for scientific workflows via carbon-aware execution. Proposes temporal shifting, pausing/resuming, and resource scaling methods. Achieves up to 80% emission reduction with temporal shifting and 67% with resource scaling.",
        "abstract": "Scientific workflows are widely used to automate scientific data analysis and often involve computationally intensive processing of large datasets on compute clusters. As such, their execution tends to be long-running and resource-intensive, resulting in substantial energy consumption and, depending on the energy mix, carbon emissions. Meanwhile, a wealth of carbon-aware computing methods have been proposed, yet little work has focused specifically on scientific workflows, even though they present a substantial opportunity for carbon-aware computing because they are often significantly delay tolerant, efficiently interruptible, highly scalable and widely heterogeneous. In this study, we first exemplify the problem of carbon emissions associated with running scientific workflows, and then show the potential for carbon-aware workflow execution. For this, we estimate the carbon footprint of seven real-world Nextflow workflows executed on different cluster infrastructures using both average and marginal carbon intensity data. Furthermore, we systematically evaluate the impact of carbon-aware temporal shifting, and the pausing and resuming of the workflow. Moreover, we apply resource scaling to workflows and workflow tasks. Finally, we report the potential reduction in overall carbon emissions, with temporal shifting capable of decreasing emissions by over 80%, and resource scaling capable of decreasing emissions by 67%.",
        "authors": [
            "Kathleen West",
            "Youssef Moawad",
            "Fabian Lehmann",
            "Vasilis Bountris",
            "Ulf Leser",
            "Yehia Elkhatib",
            "Lauritz Thamsen"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-20"
    },
    "http://arxiv.org/abs/2508.14524v1": {
        "id": "http://arxiv.org/abs/2508.14524v1",
        "title": "Boosting Payment Channel Network Liquidity with Topology Optimization and Transaction Selection",
        "link": "http://arxiv.org/abs/2508.14524v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-21",
        "tldr": "",
        "abstract": "Payment channel networks (PCNs) are a promising technology that alleviates blockchain scalability by shifting the transaction load from the blockchain to the PCN. Nevertheless, the network topology has to be carefully designed to maximise the transaction throughput in PCNs. Additionally, users in PCNs also have to make optimal decisions on which transactions to forward and which to reject to prolong the lifetime of their channels. In this work, we consider an input sequence of transactions over $p$ parties. Each transaction consists of a transaction size, source, and target, and can be either accepted or rejected (entailing a cost). The goal is to design a PCN topology among the $p$ cooperating parties, along with the channel capacities, and then output a decision for each transaction in the sequence to minimise the cost of creating and augmenting channels, as well as the cost of rejecting transactions. Our main contribution is an $\\mathcal{O}(p)$ approximation algorithm for the problem with $p$ parties. We further show that with some assumptions on the distribution of transactions, we can reduce the approximation ratio to $\\mathcal{O}(\\sqrt{p})$. We complement our theoretical analysis with an empirical study of our assumptions and approach in the context of the Lightning Network.",
        "authors": [
            "Krishnendu Chatterjee",
            "Jan Matyáš Křišťan",
            "Stefan Schmid",
            "Jakub Svoboda",
            "Michelle Yeo"
        ],
        "categories": [
            "cs.DC",
            "cs.CR"
        ],
        "submit_date": "2025-08-20"
    },
    "http://arxiv.org/abs/2508.14506v1": {
        "id": "http://arxiv.org/abs/2508.14506v1",
        "title": "Auditable Shared Objects: From Registers to Synchronization Primitives",
        "link": "http://arxiv.org/abs/2508.14506v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-21",
        "tldr": "",
        "abstract": "Auditability allows to track operations performed on a shared object, recording who accessed which information. This gives data owners more control on their data. Initially studied in the context of single-writer registers, this work extends the notion of auditability to other shared objects, and studies their properties.   We start by moving from single-writer to multi-writer registers, and provide an implementation of an auditable $n$-writer $m$-reader read / write register, with $O(n+m)$ step complexity. This implementation uses $(m+n)$-sliding registers, which have consensus number $m+n$. We show that this consensus number is necessary. The implementation extends naturally to support an auditable load-linked / store-conditional (LL/SC) shared object. LL/SC is a primitive that supports efficient implementation of many shared objects. Finally, we relate auditable registers to other access control objects, by implementing an anti-flickering deny list from auditable registers.",
        "authors": [
            "Hagit Attiya",
            "Antonio Fernández Anta",
            "Alessia Milani",
            "Alexandre Rapetti",
            "Corentin Travers"
        ],
        "categories": [
            "cs.DC",
            "cs.DB",
            "cs.DS"
        ],
        "submit_date": "2025-08-20"
    },
    "http://arxiv.org/abs/2508.14319v2": {
        "id": "http://arxiv.org/abs/2508.14319v2",
        "title": "SSSP-Del: Fully Dynamic Distributed Algorithm for Single-Source Shortest Path",
        "link": "http://arxiv.org/abs/2508.14319v2",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-21",
        "tldr": "",
        "abstract": "Modern graphs are both large and dynamic, presenting significant challenges for fundamental queries, such as the Single-Source Shortest Path (SSSP) problem. Naively recomputing the SSSP tree after each topology change is prohibitively expensive, causing on-demand computation to suffer from high latency. Existing dynamic SSSP algorithms often cannot simultaneously handle both edge additions and deletions, operate in distributed memory, and provide low-latency query results. To address these challenges, this paper presents SSSP-Del, a new vertex-centric, asynchronous, and fully distributed algorithm for dynamic SSSP. Operating in a shared-nothing architecture, our algorithm processes streams of both edge insertions and deletions. We conduct a comprehensive evaluation on large real-world and synthetic graphs with millions of vertices, and provide a thorough analysis by evaluating result latency, solution stability, and throughput.",
        "authors": [
            "Parshan Javanrood",
            "Matei Ripeanu"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-20"
    },
    "http://arxiv.org/abs/2508.14271v1": {
        "id": "http://arxiv.org/abs/2508.14271v1",
        "title": "Pure Data Spaces",
        "link": "http://arxiv.org/abs/2508.14271v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-21",
        "tldr": "",
        "abstract": "In a previous work, \"pure data\" is proposed as an axiomatic foundation for mathematics and computing, based on \"finite sequence\" as the foundational concept rather than based on logic or type. Within this framework, objects with mathematical meaning are \"data\" and collections of mathematical objects must then be associative data, called a \"space.\" A space is then the basic collection in this framework analogous to sets in Set Theory or objects in Category Theory. A theory of spaces is developed,where spaces are studied via their semiring of endomorphisms. To illustrate these concepts, and as a way of exploring the implications of the framework, pure data spaces are \"grown organically\" from the substrate of pure data with minimal combinatoric definitions. Familiar objects from classical mathematics emerge this way, including natural numbers, integers, rational numbers, boolean spaces, matrix algebras, Gaussian Integers, Quaternions, and non-associative algebras like the Integer Octonions. Insights from these examples are discussed with a view towards new directions in theory and new exploration.",
        "authors": [
            "Saul Youssef"
        ],
        "categories": [
            "cs.DC",
            "math.LO"
        ],
        "submit_date": "2025-08-19"
    },
    "http://arxiv.org/abs/2508.14247v1": {
        "id": "http://arxiv.org/abs/2508.14247v1",
        "title": "Time-optimal Asynchronous Minimal Vertex Covering by Myopic Robots",
        "link": "http://arxiv.org/abs/2508.14247v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-21",
        "tldr": "",
        "abstract": "In a connected graph with an autonomous robot swarm with limited visibility, it is natural to ask whether the robots can be deployed to certain vertices satisfying a given property using only local knowledge. This paper affirmatively answers the question with a set of \\emph{myopic} (finite visibility range) luminous robots with the aim of \\emph{filling a minimal vertex cover} (MVC) of a given graph $G = (V, E)$. The graph has special vertices, called \\emph{doors}, through which robots enter sequentially. Starting from the doors, the goal of the robots is to settle on a set of vertices that forms a minimal vertex cover of $G$ under the asynchronous ($\\mathcal{ASYNC}$) scheduler. We are also interested in achieving the \\emph{minimum vertex cover} (MinVC, which is NP-hard \\cite{Karp1972} for general graphs) for a specific graph class using the myopic robots. We establish lower bounds on the visibility range for the robots and on the time complexity (which is $Ω(|E|)$). We present two algorithms for trees: one for single door, which is both time and memory-optimal, and the other for multiple doors, which is memory-optimal and achieves time-optimality when the number of doors is a constant. Interestingly, our technique achieves MinVC on trees with a single door. We then move to the general graph, where we present two algorithms, one for the single door and the other for the multiple doors with an extra memory of $O(\\log Δ)$ for the robots, where $Δ$ is the maximum degree of $G$. All our algorithms run in $O(|E|)$ epochs.",
        "authors": [
            "Saswata Jana",
            "Subhajit Pramanick",
            "Adri Bhattacharya",
            "Partha Sarathi Mandal"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-19"
    },
    "http://arxiv.org/abs/2508.14565v2": {
        "id": "http://arxiv.org/abs/2508.14565v2",
        "title": "Cooperative SGD with Dynamic Mixing Matrices",
        "link": "http://arxiv.org/abs/2508.14565v2",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-21",
        "tldr": "",
        "abstract": "One of the most common methods to train machine learning algorithms today is the stochastic gradient descent (SGD). In a distributed setting, SGD-based algorithms have been shown to converge theoretically under specific circumstances. A substantial number of works in the distributed SGD setting assume a fixed topology for the edge devices. These papers also assume that the contribution of nodes to the global model is uniform. However, experiments have shown that such assumptions are suboptimal and a non uniform aggregation strategy coupled with a dynamically shifting topology and client selection can significantly improve the performance of such models. This paper details a unified framework that covers several Local-Update SGD-based distributed algorithms with dynamic topologies and provides improved or matching theoretical guarantees on convergence compared to existing work.",
        "authors": [
            "Soumya Sarkar",
            "Shweta Jain"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "submit_date": "2025-08-20"
    },
    "http://arxiv.org/abs/2508.14339v1": {
        "id": "http://arxiv.org/abs/2508.14339v1",
        "title": "Lagrangian Simulation Volume-Based Contour Tree Simplification",
        "link": "http://arxiv.org/abs/2508.14339v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-21",
        "tldr": "",
        "abstract": "Many scientific and engineering problems are modelled by simulating scalar fields defined either on space-filling meshes (Eulerian) or as particles (Lagrangian). For analysis and visualization, topological primitives such as contour trees can be used, but these often need simplification to filter out small-scale features. For parcel-based convective cloud simulations, simplification of the contour tree requires a volumetric measure rather than persistence. Unlike for cubic meshes, volume cannot be approximated by counting regular vertices. Typically, this is addressed by resampling irregular data onto a uniform grid. Unfortunately, the spatial proximity of parcels requires a high sampling frequency, resulting in a massive increase in data size for processing. We therefore extend volume-based contour tree simplification to parcel-in-cell simulations with a graph adaptor in Viskores (VTK-m), using Delaunay tetrahedralization of the parcel centroids as input. Instead of relying on a volume approximation by counting regular vertices -- as was done for cubic meshes -- we adapt the 2D area splines reported by Bajaj et al. 10.1145/259081.259279, and Zhou et al. 10.1109/TVCG.2018.2796555. We implement this in Viskores (formerly called VTK-m) as prefix-sum style hypersweeps for parallel efficiency and show how it can be generalized to compute any integrable property. Finally, our results reveal that contour trees computed directly on the parcels are orders of magnitude faster than computing them on a resampled grid, while also arguably offering better quality segmentation, avoiding interpolation artifacts.",
        "authors": [
            "Domantas Dilys",
            "Hamish Carr",
            "Steven Boeing"
        ],
        "categories": [
            "cs.CG",
            "cs.DC",
            "cs.DS"
        ],
        "submit_date": "2025-08-20"
    },
    "http://arxiv.org/abs/2508.14318v2": {
        "id": "http://arxiv.org/abs/2508.14318v2",
        "title": "Power Stabilization for AI Training Datacenters",
        "link": "http://arxiv.org/abs/2508.14318v2",
        "tags": [
            "training",
            "hardware",
            "autoscaling"
        ],
        "relevant": true,
        "indexed_date": "2025-08-21",
        "tldr": "Addresses power stability challenges in large-scale AI training datacenters due to alternating compute/communication phases. Proposes multi-pronged solutions across software, GPU hardware, and infrastructure layers. Achieves stabilized power profiles validated via hardware experiments and cloud power simulator.",
        "abstract": "Large Artificial Intelligence (AI) training workloads spanning several tens of thousands of GPUs present unique power management challenges. These arise due to the high variability in power consumption during the training. Given the synchronous nature of these jobs, during every iteration there is a computation-heavy phase, where each GPU works on the local data, and a communication-heavy phase where all the GPUs synchronize on the data. Because compute-heavy phases require much more power than communication phases, large power swings occur. The amplitude of these power swings is ever increasing with the increase in the size of training jobs. An even bigger challenge arises from the frequency spectrum of these power swings which, if harmonized with critical frequencies of utilities, can cause physical damage to the power grid infrastructure. Therefore, to continue scaling AI training workloads safely, we need to stabilize the power of such workloads. This paper introduces the challenge with production data and explores innovative solutions across the stack: software, GPU hardware, and datacenter infrastructure. We present the pros and cons of each of these approaches and finally present a multi-pronged approach to solving the challenge. The proposed solutions are rigorously tested using a combination of real hardware and Microsoft's in-house cloud power simulator, providing critical insights into the efficacy of these interventions under real-world conditions.",
        "authors": [
            "Esha Choukse",
            "Brijesh Warrier",
            "Scot Heath",
            "Luz Belmont",
            "April Zhao",
            "Hassan Ali Khan",
            "Brian Harry",
            "Matthew Kappel",
            "Russell J. Hewett",
            "Kushal Datta",
            "Yu Pei",
            "Caroline Lichtenberger",
            "John Siegler",
            "David Lukofsky",
            "Zaid Kahn",
            "Gurpreet Sahota",
            "Andy Sullivan",
            "Charles Frederick",
            "Hien Thai",
            "Rebecca Naughton",
            "Daniel Jurnove",
            "Justin Harp",
            "Reid Carper",
            "Nithish Mahalingam",
            "Srini Varkala",
            "Alok Gautam Kumbhare",
            "Satyajit Desai",
            "Venkatesh Ramamurthy",
            "Praneeth Gottumukkala",
            "Girish Bhatia",
            "Kelsey Wildstone",
            "Laurentiu Olariu",
            "Ileana Incorvaia",
            "Alex Wetmore",
            "Prabhat Ram",
            "Melur Raghuraman",
            "Mohammed Ayna",
            "Mike Kendrick",
            "Ricardo Bianchini",
            "Aaron Hurst",
            "Reza Zamani",
            "Xin Li",
            "Michael Petrov",
            "Gene Oden",
            "Rory Carmichael",
            "Tom Li",
            "Apoorv Gupta",
            "Pratikkumar Patel",
            "Nilesh Dattani",
            "Lawrence Marwong",
            "Rob Nertney",
            "Hirofumi Kobayashi",
            "Jeff Liott",
            "Miro Enev",
            "Divya Ramakrishnan",
            "Ian Buck",
            "Jonah Alben"
        ],
        "categories": [
            "cs.AR",
            "cs.AI",
            "cs.DC"
        ],
        "submit_date": "2025-08-20"
    },
    "http://arxiv.org/abs/2508.14209v1": {
        "id": "http://arxiv.org/abs/2508.14209v1",
        "title": "A High Performance GPU CountSketch Implementation and Its Application to Multisketching and Least Squares Problems",
        "link": "http://arxiv.org/abs/2508.14209v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-21",
        "tldr": "",
        "abstract": "Random sketching is a dimensionality reduction technique that approximately preserves norms and singular values up to some $O(1)$ distortion factor with high probability. The most popular sketches in literature are the Gaussian sketch and the subsampled randomized Hadamard transform, while the CountSketch has lower complexity. Combining two sketches, known as multisketching, offers an inexpensive means of quickly reducing the dimension of a matrix by combining a CountSketch and Gaussian sketch.   However, there has been little investigation into high performance CountSketch implementations. In this work, we develop an efficient GPU implementation of the CountSketch, and demonstrate the performance of multisketching using this technique. We also demonstrate the potential for using this implementation within a multisketched least squares solver that is up to $77\\%$ faster than the normal equations with significantly better numerical stability, at the cost of an $O(1)$ multiplicative factor introduced into the relative residual norm.",
        "authors": [
            "Andrew J. Higgins",
            "Erik G. Boman",
            "Ichitaro Yamazaki"
        ],
        "categories": [
            "math.NA",
            "cs.DC",
            "cs.PF"
        ],
        "submit_date": "2025-08-19"
    },
    "http://arxiv.org/abs/2508.13840v1": {
        "id": "http://arxiv.org/abs/2508.13840v1",
        "title": "Is RISC-V ready for High Performance Computing? An evaluation of the Sophon SG2044",
        "link": "http://arxiv.org/abs/2508.13840v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-20",
        "tldr": "",
        "abstract": "The pace of RISC-V adoption continues to grow rapidly, yet for the successes enjoyed in areas such as embedded computing, RISC-V is yet to gain ubiquity in High Performance Computing (HPC). The Sophon SG2044 is SOPHGO's next generation 64-core high performance CPU that has been designed for workstation and server grade workloads. Building upon the SG2042, subsystems that were a bottleneck in the previous generation have been upgraded.   In this paper we undertake the first performance study of the SG2044 for HPC. Comparing against the SG2042 and other architectures, we find that the SG2044 is most advantageous when running at higher core counts, delivering up to 4.91 greater performance than the SG2042 over 64-cores. Two of the most important upgrades in the SG2044 are support for RVV v1.0 and an enhanced memory subsystem. This results in the SG2044 significantly closing the performance gap with other architectures, especially for compute-bound workloads.",
        "authors": [
            "Nick Brown"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-19"
    },
    "http://arxiv.org/abs/2508.13693v1": {
        "id": "http://arxiv.org/abs/2508.13693v1",
        "title": "Estimating CO$_2$ emissions of distributed applications and platforms with SimGrid/Batsim",
        "link": "http://arxiv.org/abs/2508.13693v1",
        "tags": [
            "training",
            "RL",
            "recommendation"
        ],
        "relevant": true,
        "indexed_date": "2025-08-20",
        "tldr": "Introduces a framework to generate synthetic datasets representing multi-modal human activity for RL training. Uses multi-objective optimization to create realistic data, improving RL agent performance by up to 47.3% on human-AI collaboration tasks.",
        "abstract": "This work presents a carbon footprint plugin designed to extend the capabilities of the Batsim simulator by allowing the calculation of CO$_2$ emissions during simulation runs. The goal is to comprehensively assess the environmental impact associated with task and resource management strategies in data centers. The plugin is developed within SimGrid -- the underlying simulation framework of Batsim -- and computes carbon emissions based on the simulated platform's energy consumption and carbon intensity factor of the simulated machines. Once implemented, it is integrated into Batsim, ensuring compatibility with existing simulation workflows and enabling researchers to assess the carbon efficiency of their scheduling strategies.",
        "authors": [
            "Gabriella Saraiva",
            "Miguel Vasconcelos",
            "Sarita Mazzini Bruschi",
            "Danilo Carastan-Santos",
            "Daniel Cordeiro"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-19"
    },
    "http://arxiv.org/abs/2508.13636v1": {
        "id": "http://arxiv.org/abs/2508.13636v1",
        "title": "LUNDIsim: model meshes for flow simulation and scientific data compression benchmarks",
        "link": "http://arxiv.org/abs/2508.13636v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-20",
        "tldr": "",
        "abstract": "The volume of scientific data produced for and by numerical simulation workflows is increasing at an incredible rate. This raises concerns either in computability, interpretability, and sustainability. This is especially noticeable in earth science (geology, meteorology, oceanography, and astronomy), notably with climate studies.    We highlight five main evaluation issues: efficiency, discrepancy, diversity, interpretability, availability.   Among remedies, lossless and lossy compression techniques are becoming popular to better manage dataset volumes. Performance assessment -- with comparative benchmarks -- require open datasets shared under FAIR principles (Findable, Accessible, Interoperable, Reusable), with MRE (Minimal Reproducible Example) ancillary data for reuse. We share LUNDIsim, an exemplary faulted geological mesh. It is inspired by SPE10 comparative Challenge. Enhanced by porosity/permeability datasets, this dataset proposes four distinct subsurface environments. They were primarily designed for flow simulation in porous media. Several consistent resolutions (with HexaShrink multiscale representations) are proposed for each model. We also provide a set of reservoir features for reproducing typical two-phase flow simulations on all LUNDIsim models in a reservoir engineering context. This dataset is chiefly meant for benchmarking and evaluating data size reduction (upscaling) or genuine composite mesh compression algorithms. It is also suitable for other advanced mesh processing workflows in geology and reservoir engineering, from visualization to machine learning.   LUNDIsim meshes are available at https://doi.org/10.5281/zenodo.14641958",
        "authors": [
            "Laurent Duval",
            "Frédéric Payan",
            "Christophe Preux",
            "Lauriane Bouard"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-19"
    },
    "http://arxiv.org/abs/2508.13523v2": {
        "id": "http://arxiv.org/abs/2508.13523v2",
        "title": "LAMMPS-KOKKOS: Performance Portable Molecular Dynamics Across Exascale Architectures",
        "link": "http://arxiv.org/abs/2508.13523v2",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-20",
        "tldr": "",
        "abstract": "Since its inception in 1995, LAMMPS has grown to be a world-class molecular dynamics code, with thousands of users, over one million lines of code, and multi-scale simulation capabilities. We discuss how LAMMPS has adapted to the modern heterogeneous computing landscape by integrating the Kokkos performance portability library into the existing C++ code. We investigate performance portability of simple pairwise, many-body reactive, and machine-learned force-field interatomic potentials. We present results on GPUs across different vendors and generations, and analyze performance trends, probing FLOPS throughput, memory bandwidths, cache capabilities, and thread-atomic operation performance. Finally, we demonstrate strong scaling on three exascale machines -- OLCF Frontier, ALCF Aurora, and NNSA El Capitan -- as well as on the CSCS Alps supercomputer, for the three potentials.",
        "authors": [
            "Anders Johansson",
            "Evan Weinberg",
            "Christian R. Trott",
            "Megan J. McCarthy",
            "Stan G. Moore"
        ],
        "categories": [
            "cs.DC",
            "cs.PF",
            "physics.comp-ph"
        ],
        "submit_date": "2025-08-19"
    },
    "http://arxiv.org/abs/2508.13522v1": {
        "id": "http://arxiv.org/abs/2508.13522v1",
        "title": "DDoS Attacks in Cloud Computing: Detection and Prevention",
        "link": "http://arxiv.org/abs/2508.13522v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-20",
        "tldr": "",
        "abstract": "DDoS attacks are one of the most prevalent and harmful cybersecurity threats faced by organizations and individuals today. In recent years, the complexity and frequency of DDoS attacks have increased significantly, making it challenging to detect and mitigate them effectively. The study analyzes various types of DDoS attacks, including volumetric, protocol, and application layer attacks, and discusses the characteristics, impact, and potential targets of each type. It also examines the existing techniques used for DDoS attack detection, such as packet filtering, intrusion detection systems, and machine learning-based approaches, and their strengths and limitations. Moreover, the study explores the prevention techniques employed to mitigate DDoS attacks, such as firewalls, rate limiting , CPP and ELD mechanism. It evaluates the effectiveness of each approach and its suitability for different types of attacks and environments. In conclusion, this study provides a comprehensive overview of the different types of DDoS attacks, their detection, and prevention techniques. It aims to provide insights and guidelines for organizations and individuals to enhance their cybersecurity posture and protect against DDoS attacks.",
        "authors": [
            "Zain Ahmad",
            "Musab Ahmad",
            "Bilal Ahmad"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.CR"
        ],
        "submit_date": "2025-08-19"
    },
    "http://arxiv.org/abs/2508.13397v1": {
        "id": "http://arxiv.org/abs/2508.13397v1",
        "title": "Optimizing Allreduce Operations for Heterogeneous Architectures with Multiple Processes per GPU",
        "link": "http://arxiv.org/abs/2508.13397v1",
        "tags": [
            "training",
            "networking",
            "hardware"
        ],
        "relevant": true,
        "indexed_date": "2025-08-20",
        "tldr": "Addresses communication bottlenecks in large inter-GPU all-reduce operations for deep learning. Proposes multi-CPU-accelerated GPU-aware lane all-reduces leveraging idle CPU cores. Achieves speedup of up to 2.45× on NVIDIA A100 GPUs and up to 1.77×/1.71× on collective libraries.",
        "abstract": "Large inter-GPU all-reduce operations, prevalent throughout deep learning, are bottlenecked by communication costs. Emerging heterogeneous architectures are comprised of complex nodes, often containing $4$ GPUs and dozens to hundreds of CPU cores per node. Parallel applications are typically accelerated on the available GPUs, using only a single CPU core per GPU while the remaining cores sit idle. This paper presents novel optimizations to large GPU-aware all-reduce operations, extending lane-aware reductions to the GPUs, and notably using multiple CPU cores per GPU to accelerate these operations. These multi-CPU-accelerated GPU-aware lane all-reduces yield speedup of up to $2.45$x for large MPI all-reduces across the NVIDIA A100 GPUs of NCSA's Delta supercomputer. Finally, the approach is extended to NVIDIA's and AMD's collective communication libraries, achieving speedup of up to $1.77$x and $1.71$x, respectively, across $2$ state-of-the-art supercomputers.",
        "authors": [
            "Michael Adams",
            "Amanda Bienz"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-18"
    },
    "http://arxiv.org/abs/2508.13374v2": {
        "id": "http://arxiv.org/abs/2508.13374v2",
        "title": "OrbitChain: Orchestrating In-orbit Real-time Analytics of Earth Observation Data",
        "link": "http://arxiv.org/abs/2508.13374v2",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-20",
        "tldr": "",
        "abstract": "Earth observation analytics have the potential to serve many time-sensitive applications. However, due to limited bandwidth and duration of ground-satellite connections, it takes hours or even days to download and analyze data from existing Earth observation satellites, making real-time demands like timely disaster response impossible. Toward real-time analytics, we introduce OrbitChain, a collaborative analytics framework that orchestrates computational resources across multiple satellites in an Earth observation constellation. OrbitChain decomposes analytics applications into microservices and allocates computational resources for time-constrained analysis. A traffic routing algorithm is devised to minimize the inter-satellite communication overhead. OrbitChain adopts a pipeline workflow that completes Earth observation tasks in real-time, facilitates time-sensitive applications and inter-constellation collaborations such as tip-and-cue. To evaluate OrbitChain, we implement a hardware-in-the-loop orbital computing testbed. Experiments show that our system can complete up to 60% analytics workload than existing Earth observation analytics framework while reducing the communication overhead by up to 72%.",
        "authors": [
            "Zhouyu Li",
            "Zhijin Yang",
            "Huayue Gu",
            "Xiaojian Wang",
            "Yuchen Liu",
            "Ruozhou Yu"
        ],
        "categories": [
            "cs.DC",
            "cs.ET",
            "cs.LG",
            "cs.NI"
        ],
        "submit_date": "2025-08-18"
    },
    "http://arxiv.org/abs/2508.13370v1": {
        "id": "http://arxiv.org/abs/2508.13370v1",
        "title": "Persistent and Partitioned MPI for Stencil Communication",
        "link": "http://arxiv.org/abs/2508.13370v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-20",
        "tldr": "",
        "abstract": "Many parallel applications rely on iterative stencil operations, whose performance are dominated by communication costs at large scales. Several MPI optimizations, such as persistent and partitioned communication, reduce overheads and improve communication efficiency through amortized setup costs and reduced synchronization of threaded sends. This paper presents the performance of stencil communication in the Comb benchmarking suite when using non blocking, persistent, and partitioned communication routines. The impact of each optimization is analyzed at various scales. Further, the paper presents an analysis of the impact of process count, thread count, and message size on partitioned communication routines. Measured timings show that persistent MPI communication can provide a speedup of up to 37% over the baseline MPI communication, and partitioned MPI communication can provide a speedup of up to 68%.",
        "authors": [
            "Gerald Collom",
            "Jason Burmark",
            "Olga Pearce",
            "Amanda Bienz"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-18"
    },
    "http://arxiv.org/abs/2508.13298v2": {
        "id": "http://arxiv.org/abs/2508.13298v2",
        "title": "Harnessing the Full Potential of RRAMs through Scalable and Distributed In-Memory Computing with Integrated Error Correction",
        "link": "http://arxiv.org/abs/2508.13298v2",
        "tags": [
            "hardware",
            "storage",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-08-20",
        "tldr": "Introduces MELISO+, a distributed in-memory computing framework using RRAM with two-tier error correction for large-scale matrix computations. Reduces arithmetic errors by over 90%, improves energy efficiency by 3-5 orders of magnitude, and lowers latency 100×, enabling efficient LLM support.",
        "abstract": "Exponential growth in global computing demand is exacerbated due to the higher-energy requirements of conventional architectures, primarily due to energy-intensive data movement. In-memory computing with Resistive Random Access Memory (RRAM) addresses this by co-integrating memory and processing, but faces significant hurdles related to device-level non-idealities and poor scalability for large computing tasks. Here, we introduce MELISO+ (In-Memory Linear Solver), a full-stack, distributed framework for energy-efficient in-memory computing. MELISO+ proposes a novel two-tier error correction mechanism to mitigate device non-idealities and develops a distributed RRAM computing framework to enable matrix computations exceeding dimensions of $65,000\\times65,000$. This approach reduces first- and second-order arithmetic errors due to device non-idealities by over $90\\%$, enhances energy efficiency by three to five orders of magnitude, and decreases latency 100-fold. Hence, MELISO+ allows lower-precision RRAM devices to outperform high-precision device alternatives in accuracy, energy and latency metrics. By unifying algorithm-hardware co-design with scalable architecture, MELISO+ significantly advances sustainable, high-dimensional computing suitable for applications like large language models and generative AI.",
        "authors": [
            "Huynh Q. N. Vo",
            "Md Tawsif Rahman Chowdhury",
            "Paritosh Ramanan",
            "Murat Yildirim",
            "Gozde Tutuncuoglu"
        ],
        "categories": [
            "cs.DC",
            "cs.AR",
            "cs.ET",
            "cs.PF",
            "eess.SY"
        ],
        "submit_date": "2025-08-18"
    },
    "http://arxiv.org/abs/2508.14017v1": {
        "id": "http://arxiv.org/abs/2508.14017v1",
        "title": "Analog computation with transcriptional networks",
        "link": "http://arxiv.org/abs/2508.14017v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-20",
        "tldr": "",
        "abstract": "Transcriptional networks represent one of the most extensively studied types of systems in synthetic biology. Although the completeness of transcriptional networks for digital logic is well-established, *analog* computation plays a crucial role in biological systems and offers significant potential for synthetic biology applications. While transcriptional circuits typically rely on cooperativity and highly non-linear behavior of transcription factors to regulate *production* of proteins, they are often modeled with simple linear *degradation* terms. In contrast, general analog dynamics require both non-linear positive as well as negative terms, seemingly necessitating control over not just transcriptional (i.e., production) regulation but also the degradation rates of transcription factors.   Surprisingly, we prove that controlling transcription factor production (i.e., transcription rate) without explicitly controlling degradation is mathematically complete for analog computation, achieving equivalent capabilities to systems where both production and degradation are programmable. We demonstrate our approach on several examples including oscillatory and chaotic dynamics, analog sorting, memory, PID controller, and analog extremum seeking. Our result provides a systematic methodology for engineering novel analog dynamics using synthetic transcriptional networks without the added complexity of degradation control and informs our understanding of the capabilities of natural transcriptional circuits.   We provide a compiler, in the form of a Python package that can take any system of polynomial ODEs and convert it to an equivalent transcriptional network implementing the system *exactly*, under appropriate conditions.",
        "authors": [
            "David Doty",
            "Mina Latifi",
            "David Soloveichick"
        ],
        "categories": [
            "cs.CC",
            "cs.DC",
            "cs.ET"
        ],
        "submit_date": "2025-08-19"
    },
    "http://arxiv.org/abs/2508.13615v1": {
        "id": "http://arxiv.org/abs/2508.13615v1",
        "title": "PennyLane-Lightning MPI: A massively scalable quantum circuit simulator based on distributed computing in CPU clusters",
        "link": "http://arxiv.org/abs/2508.13615v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-20",
        "tldr": "",
        "abstract": "Quantum circuit simulations play a critical role in bridging the gap between theoretical quantum algorithms and their practical realization on physical quantum hardware, yet they face computational challenges due to the exponential growth of quantum state spaces with increasing qubit size. This work presents PennyLane-Lightning MPI, an MPI-based extension of the PennyLane-Lightning suite, developed to enable scalable quantum circuit simulations through parallelization of quantum state vectors and gate operations across distributed-memory systems. The core of this implementation is an index-dependent, gate-specific parallelization strategy, which fully exploits the characteristic of individual gates as well as the locality of computation associated with qubit indices in partitioned state vectors. Benchmarking tests with single gates and well-designed quantum circuits show that the present method offers advantages in performance over general methods based on unitary matrix operations and exhibits excellent scalability, supporting simulations of up to 41-qubit with hundreds of thousands of parallel processes. Being equipped with a Python plug-in for seamless integration to the PennyLane framework, this work contributes to extending the PennyLane ecosystem by enabling high-performance quantum simulations in standard multi-core CPU clusters with no library-specific requirements, providing a back-end resource for the cloud-based service framework of quantum computing that is under development in the Republic of Korea.",
        "authors": [
            "Ji-Hoon Kang",
            "Hoon Ryu"
        ],
        "categories": [
            "quant-ph",
            "cs.DC",
            "cs.MS"
        ],
        "submit_date": "2025-08-19"
    },
    "http://arxiv.org/abs/2508.13337v1": {
        "id": "http://arxiv.org/abs/2508.13337v1",
        "title": "X-MoE: Enabling Scalable Training for Emerging Mixture-of-Experts Architectures on HPC Platforms",
        "link": "http://arxiv.org/abs/2508.13337v1",
        "tags": [
            "training",
            "MoE",
            "sparse"
        ],
        "relevant": true,
        "indexed_date": "2025-08-20",
        "tldr": "Proposes X-MoE, a scalable training system for fine-grained MoE architectures. Utilizes padding-free kernels, redundancy-bypassing dispatch, and hybrid parallelism to overcome activation memory and communication bottlenecks. Achieves training of 545B-parameter models on 1024 AMD GPUs, 10x larger than prior methods.",
        "abstract": "Emerging expert-specialized Mixture-of-Experts (MoE) architectures, such as DeepSeek-MoE, deliver strong model quality through fine-grained expert segmentation and large top-k routing. However, their scalability is limited by substantial activation memory overhead and costly all-to-all communication. Furthermore, current MoE training systems - primarily optimized for NVIDIA GPUs - perform suboptimally on non-NVIDIA platforms, leaving significant computational potential untapped. In this work, we present X-MoE, a novel MoE training system designed to deliver scalable training performance for next-generation MoE architectures. X-MoE achieves this via several novel techniques, including efficient padding-free MoE training with cross-platform kernels, redundancy-bypassing dispatch, and hybrid parallelism with sequence-sharded MoE blocks. Our evaluation on the Frontier supercomputer, powered by AMD MI250X GPUs, shows that X-MoE scales DeepSeek-style MoEs up to 545 billion parameters across 1024 GPUs - 10x larger than the largest trainable model with existing methods under the same hardware budget, while maintaining high training throughput. The source code of X-MoE is available at https://github.com/Supercomputing-System-AI-Lab/X-MoE.",
        "authors": [
            "Yueming Yuan",
            "Ahan Gupta",
            "Jianping Li",
            "Sajal Dash",
            "Feiyi Wang",
            "Minjia Zhang"
        ],
        "categories": [
            "cs.LG",
            "cs.CL",
            "cs.DC"
        ],
        "submit_date": "2025-08-18"
    },
    "http://arxiv.org/abs/2508.13163v1": {
        "id": "http://arxiv.org/abs/2508.13163v1",
        "title": "Sustainable AI Training via Hardware-Software Co-Design on NVIDIA, AMD, and Emerging GPU Architectures",
        "link": "http://arxiv.org/abs/2508.13163v1",
        "tags": [
            "training",
            "hardware",
            "quantization"
        ],
        "relevant": true,
        "indexed_date": "2025-08-20",
        "tldr": "Proposes hardware-software co-design to optimize energy efficiency for large-scale AI training. Focuses on GPU architecture enhancements, memory-level/kernel-level optimizations, and techniques like mixed-precision arithmetic. Achieves significant improvements in performance-per-watt metrics.",
        "abstract": "In particular, large-scale deep learning and artificial intelligence model training uses a lot of computational power and energy, so it poses serious sustainability issues. The fast rise in model complexity has resulted in exponential increases in energy consumption, increasing the demand for techniques maximizing computational efficiency and lowering environmental impact. This work explores environmentally driven performance optimization methods especially intended for advanced GPU architectures from NVIDIA, AMD, and other emerging GPU architectures. Our main focus is on investigating hardware-software co-design techniques meant to significantly increase memory-level and kernel-level operations, so improving performance-per-watt measures. Our thorough research encompasses evaluations of specialized tensor and matrix cores, advanced memory optimization methods, and creative integration approaches that taken together result in notable energy efficiency increases. We also discuss important software-level optimizations that augment hardware capability including mixed-precision arithmetic, advanced energy-aware scheduling algorithms, and compiler-driven kernel enhancements. Moreover, we methodically point out important research gaps and suggest future directions necessary to create really sustainable artificial intelligence systems. This paper emphasizes how major increases in training efficiency can be obtained by co-design of hardware and software, so lowering the environmental impact of artificial intelligence without compromising performance. To back up our analysis, we use real-world case studies from top companies like Meta, Google, Amazon, and others that show how these sustainable AI training methods are used in the real world.",
        "authors": [
            "Yashasvi Makin",
            "Rahul Maliakkal"
        ],
        "categories": [
            "cs.AR",
            "cs.AI",
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-07-28"
    },
    "http://arxiv.org/abs/2508.13652v2": {
        "id": "http://arxiv.org/abs/2508.13652v2",
        "title": "Towards Timing Isolation for Mixed-Criticality Communication in Software-Defined Vehicles",
        "link": "http://arxiv.org/abs/2508.13652v2",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-20",
        "tldr": "",
        "abstract": "As the automotive industry transitions toward centralized Linux-based architectures, ensuring the predictable execution of mixed-criticality applications becomes essential. However, concurrent use of the Linux network stack introduces interference, resulting in unpredictable latency and jitter. To address this challenge, we present a layered software architecture that enforces timing isolation for Ethernet-based data exchange between mixed-criticality applications on Linux-based automotive control units. Our approach integrates traffic prioritization strategies at the middleware layer, the network stack layer, and the hardware layer to achieve isolation across the full software stack. At the middleware layer, we implement a fixed-priority, non-preemptive scheduler to manage publishers of varying criticality. At the network layer, we leverage the express data path (XDP) to route high-priority data directly from the network interface driver into critical application memory, bypassing the standard Linux network stack. At the hardware layer, we dedicate a network interface card (NIC) queue exclusively to real-time traffic. We demonstrate how our architecture performs in a Data Distribution Service (DDS)-based system. Our evaluation shows that the approach leads to consistent and predictable latencies for real-time traffic, even under heavy interference from best-effort applications.",
        "authors": [
            "Lóránt Meszlényi",
            "Julius Kahle",
            "Dominik Püllen",
            "Stefan Kowalewski",
            "Stefan Katzenbeisser",
            "Alexandru Kampmann"
        ],
        "categories": [
            "cs.NI",
            "cs.OS"
        ],
        "submit_date": "2025-08-19"
    },
    "http://arxiv.org/abs/2508.13084v1": {
        "id": "http://arxiv.org/abs/2508.13084v1",
        "title": "Team Formation and Applications",
        "link": "http://arxiv.org/abs/2508.13084v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-19",
        "tldr": "",
        "abstract": "A novel long-lived distributed problem, called Team Formation (TF), is introduced together with a message- and time-efficient randomized algorithm. The problem is defined over the asynchronous model with a complete communication graph, using bounded size messages, where a certain fraction of the nodes may experience a generalized, strictly stronger, version of initial failures. The goal of a TF algorithm is to assemble tokens injected by the environment, in a distributed manner, into teams of size $σ$, where $σ$ is a parameter of the problem.   The usefulness of TF is demonstrated by using it to derive efficient algorithms for many distributed problems. Specifically, we show that various (one-shot as well as long-lived) distributed problems reduce to TF. This includes well-known (and extensively studied) distributed problems such as several versions of leader election and threshold detection. For example, we are the first to break the linear message complexity bound for asynchronous implicit leader election. We also improve the time complexity of message-optimal algorithms for asynchronous explicit leader election. Other distributed problems that reduce to TF are new ones, including matching players in online gaming platforms, a generalization of gathering, constructing a perfect matching in an induced subgraph of the complete graph, quorum sensing in message-passing networks, and more. To complement our positive contribution, we establish a tight lower bound on the message complexity of TF algorithms.",
        "authors": [
            "Yuval Emek",
            "Shay Kutten",
            "Ido Rafael",
            "Gadi Taubenfeld"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-18"
    },
    "http://arxiv.org/abs/2508.13083v1": {
        "id": "http://arxiv.org/abs/2508.13083v1",
        "title": "Congested Clique Counting for Local Gibbs Distributions",
        "link": "http://arxiv.org/abs/2508.13083v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-19",
        "tldr": "",
        "abstract": "There are well established reductions between combinatorial sampling and counting problems (Jerrum, Valiant, Vazirani TCS 1986). Building off of a very recent parallel algorithm utilizing this connection (Liu, Yin, Zhang arxiv 2024), we demonstrate the first approximate counting algorithm in the CongestedClique for a wide range of problems. Most interestingly, we present an algorithm for approximating the number of $q$-colorings of a graph within $ε$-multiplicative error, when $q>αΔ$ for any constant $α>2$, in $\\Tilde{O}\\big(\\frac{n^{1/3}}{ε^2}\\big)$ rounds. More generally, we achieve a runtime of $\\Tilde{O}\\big(\\frac{n^{1/3}}{ε^2}\\big)$ rounds for approximating the partition function of Gibbs distributions defined over graphs when simple locality and fast mixing conditions hold. Gibbs distributions are widely used in fields such as machine learning and statistical physics. We obtain our result by providing an algorithm to draw $n$ random samples from a distributed Markov chain in parallel, using similar ideas to triangle counting (Dolev, Lenzen, Peled DISC 2012) and semiring matrix multiplication (Censor-Hillel, Kaski, Korhonen, Lenzen, Paz, Suomela PODC 2015). Aside from counting problems, this result may be interesting for other applications requiring a large number of samples. In the special case of estimating the partition function of the hardcore model, also known as counting weighted independent sets, we can do even better and achieve an $\\Tilde{O}\\big(\\frac{1}{ε^2}\\big)$ round algorithm, when the fugacity $λ\\leq \\fracα{Δ-1}$, where $α$ is an arbitrary constant less than $1$.",
        "authors": [
            "Joshua Z. Sobel"
        ],
        "categories": [
            "cs.DC",
            "cs.DS"
        ],
        "submit_date": "2025-08-18"
    },
    "http://arxiv.org/abs/2508.12961v1": {
        "id": "http://arxiv.org/abs/2508.12961v1",
        "title": "WANify: Gauging and Balancing Runtime WAN Bandwidth for Geo-distributed Data Analytics",
        "link": "http://arxiv.org/abs/2508.12961v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-19",
        "tldr": "",
        "abstract": "Accurate wide area network (WAN) bandwidth (BW) is essential for geo-distributed data analytics (GDA) systems to make optimal decisions such as data and task placement to improve performance. Existing GDA systems, however, measure WAN BW statically and independently between data centers (DCs), while data transfer occurs dynamically and simultaneously among DCs during workload execution. Also, they use a single connection WAN BW that cannot capture actual WAN capacities between distant DCs. Such inaccurate WAN BWs yield sub-optimal decisions, inflating overall query latency and cost. In this paper, we present WANify, a new framework that precisely and dynamically gauges achievable runtime WAN BW using a machine learning prediction scheme, decision tree-based Random Forest. This helps GDA systems make better decisions yielding reduced latency and costs including WAN BW monitoring costs. Based on predicted runtime WAN BW, WANify determines the optimal number of heterogeneous parallel connections for data transfer among DCs. This approach improves performance without additional, or even at reduced cost, by fully exploiting available WAN capacities. In addition, WANify considers dynamics like network and workloads, and heterogeneity like skewed data, heterogeneous compute resources, and a varying number of DCs while making decisions. The WANify prototype running on state-of-the-art GDA systems is evaluated on AWS with 8 geo-distributed DCs. Results show that WANify enhances WAN throughput by balancing between the strongest and weakest WAN links, enabling GDA systems to reduce latency and cost by up to 26% and 16% respectively with minimal effort, all while handling dynamics and heterogeneity efficiently.",
        "authors": [
            "Anshuman Das Mohapatra",
            "Kwangsung Oh"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-18"
    },
    "http://arxiv.org/abs/2508.12851v1": {
        "id": "http://arxiv.org/abs/2508.12851v1",
        "title": "Accelerating Edge Inference for Distributed MoE Models with Latency-Optimized Expert Placement",
        "link": "http://arxiv.org/abs/2508.12851v1",
        "tags": [
            "MoE",
            "edge",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-08-19",
        "tldr": "Addresses high latency and communication in edge-based distributed MoE inference. Proposes DanceMoE, an activation-aware expert placement algorithm with migration for dynamic workloads. Achieves up to 30.6% latency reduction and communication reduction over baselines.",
        "abstract": "Mixture-of-Experts (MoE) have become a cornerstone for training and scaling large language models (LLMs), offering substantial gains in model capacity and efficiency through sparse expert activation. However, serving these models remains challenging in practice, particularly in resource-constrained edge environments, due to their large memory footprint and complex communication demands. While centralized cloud inference is common, it incurs high infrastructure costs, along with latency and privacy concerns. A few recent edge MoE works propose memory-efficient strategies but typically focus on single-device or homogeneous setups. This paper presents DanceMoE, an efficient MoE inference framework that enables activation-aware expert placement across collaborative, heterogeneous, GPU-equipped edge servers. DanceMoE leverages the inherent sparsity of MoE models and workload locality to minimize cross-server communication and enable efficient expert placement under heterogeneous resource constraints. It introduces a data-driven, activation-aware placement algorithm that balances local coverage and memory usage across servers, alongside a lightweight migration mechanism that adapts expert assignments under evolving workloads. We evaluate DanceMoE on modern MoE models and widely used datasets, demonstrating up to 30.6\\% lower inference latency, and substantial communication reduction compared to state-of-the-art baselines, showcasing the effectiveness of collaborative edge-based MoE inference.",
        "authors": [
            "Tian Wu",
            "Liming Wang",
            "Zijian Wen",
            "Xiaoxi Zhang",
            "Jingpu Duan",
            "Xianwei Zhang",
            "Jinhang Zuo"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-18"
    },
    "http://arxiv.org/abs/2508.12743v1": {
        "id": "http://arxiv.org/abs/2508.12743v1",
        "title": "Dissecting CPU-GPU Unified Physical Memory on AMD MI300A APUs",
        "link": "http://arxiv.org/abs/2508.12743v1",
        "tags": [
            "hardware",
            "offloading",
            "training"
        ],
        "relevant": true,
        "indexed_date": "2025-08-19",
        "tldr": "Characterizes AMD MI300A's CPU-GPU unified physical memory (UPM) for HPC workloads. Analyzes UPM latency, bandwidth, coherence, and proposes porting strategies. Achieves up to 44% memory cost reduction while matching or outperforming explicit memory management.",
        "abstract": "Discrete GPUs are a cornerstone of HPC and data center systems, requiring management of separate CPU and GPU memory spaces. Unified Virtual Memory (UVM) has been proposed to ease the burden of memory management; however, at a high cost in performance. The recent introduction of AMD's MI300A Accelerated Processing Units (APUs)--as deployed in the El Capitan supercomputer--enables HPC systems featuring integrated CPU and GPU with Unified Physical Memory (UPM) for the first time. This work presents the first comprehensive characterization of the UPM architecture on MI300A. We first analyze the UPM system properties, including memory latency, bandwidth, and coherence overhead. We then assess the efficiency of the system software in memory allocation, page fault handling, TLB management, and Infinity Cache utilization. We propose a set of porting strategies for transforming applications for the UPM architecture and evaluate six applications on the MI300A APU. Our results show that applications on UPM using the unified memory model can match or outperform those in the explicitly managed model--while reducing memory costs by up to 44%.",
        "authors": [
            "Jacob Wahlgren",
            "Gabin Schieffer",
            "Ruimin Shi",
            "Edgar A. León",
            "Roger Pearce",
            "Maya Gokhale",
            "Ivy Peng"
        ],
        "categories": [
            "cs.DC",
            "cs.PF"
        ],
        "submit_date": "2025-08-18"
    },
    "http://arxiv.org/abs/2508.12671v1": {
        "id": "http://arxiv.org/abs/2508.12671v1",
        "title": "DIT: Dimension Reduction View on Optimal NFT Rarity Meters",
        "link": "http://arxiv.org/abs/2508.12671v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-19",
        "tldr": "",
        "abstract": "Non-fungible tokens (NFTs) have become a significant digital asset class, each uniquely representing virtual entities such as artworks. These tokens are stored in collections within smart contracts and are actively traded across platforms on Ethereum, Bitcoin, and Solana blockchains. The value of NFTs is closely tied to their distinctive characteristics that define rarity, leading to a growing interest in quantifying rarity within both industry and academia. While there are existing rarity meters for assessing NFT rarity, comparing them can be challenging without direct access to the underlying collection data. The Rating over all Rarities (ROAR) benchmark addresses this challenge by providing a standardized framework for evaluating NFT rarity. This paper explores a dimension reduction approach to rarity design, introducing new performance measures and meters, and evaluates them using the ROAR benchmark. Our contributions to the rarity meter design issue include developing an optimal rarity meter design using non-metric weighted multidimensional scaling, introducing Dissimilarity in Trades (DIT) as a performance measure inspired by dimension reduction techniques, and unveiling the non-interpretable rarity meter DIT, which demonstrates superior performance compared to existing methods.",
        "authors": [
            "Dmitry Belousov",
            "Yury Yanovich"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-08-18"
    },
    "http://arxiv.org/abs/2508.12308v1": {
        "id": "http://arxiv.org/abs/2508.12308v1",
        "title": "Proceedings 18th Interaction and Concurrency Experience",
        "link": "http://arxiv.org/abs/2508.12308v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-19",
        "tldr": "",
        "abstract": "This volume contains the proceedings of ICE'25, the 18th Interaction and Concurrency Experience, which was held on Friday 20th June 2025 at the École National Supérieure des Arts et Métiers in Lille, France, as a satellite workshop of DisCoTec 2025. The ICE workshop series features a distinguishing review and selection procedure: PC members are encouraged to interact, anonymously, with authors. The 2025 edition of ICE received 7 submissions, each reviewed by three PC members, and about 75 comments were exchanged during the review process, witnessing very lively discussions. Four papers were accepted for publication plus 1 oral communication, which was accepted for presentation at the workshop. We were proud to host one invited talk, by Kirstin Peters. The abstract of her talk is included in this volume, together with the final versions of the research papers, which take into account the discussion at the workshop and during the review process.",
        "authors": [
            "Clément Aubert",
            "Cinzia Di Giusto",
            "Simon Fowler",
            "Violet Ka I Pun"
        ],
        "categories": [
            "cs.DC",
            "cs.LO"
        ],
        "submit_date": "2025-08-17"
    },
    "http://arxiv.org/abs/2508.12560v1": {
        "id": "http://arxiv.org/abs/2508.12560v1",
        "title": "Data-driven Trust Bootstrapping for Mobile Edge Computing-based Industrial IoT Services",
        "link": "http://arxiv.org/abs/2508.12560v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-19",
        "tldr": "",
        "abstract": "We propose a data-driven and context-aware approach to bootstrap trustworthiness of homogeneous Internet of Things (IoT) services in Mobile Edge Computing (MEC) based industrial IoT (IIoT) systems. The proposed approach addresses key limitations in adapting existing trust bootstrapping approaches into MEC-based IIoT systems. These key limitations include, the lack of opportunity for a service consumer to interact with a lesser-known service over a prolonged period of time to get a robust measure of its trustworthiness, inability of service consumers to consistently interact with their peers to receive reliable recommendations of the trustworthiness of a lesser-known service as well as the impact of uneven context parameters in different MEC environments causing uneven trust environments for trust evaluation. In addition, the proposed approach also tackles the problem of data sparsity via enabling knowledge sharing among different MEC environments within a given MEC topology. To verify the effectiveness of the proposed approach, we carried out a comprehensive evaluation on two real-world datasets suitably adjusted to exhibit the context-dependent trust information accumulated in MEC environments within a given MEC topology. The experimental results affirmed the effectiveness of our approach and its suitability to bootstrap trustworthiness of services in MEC-based IIoT systems.",
        "authors": [
            "Prabath Abeysekara",
            "Hai Dong"
        ],
        "categories": [
            "cs.CR",
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-08-18"
    },
    "http://arxiv.org/abs/2508.12161v1": {
        "id": "http://arxiv.org/abs/2508.12161v1",
        "title": "Attack Graph Generation on HPC Clusters",
        "link": "http://arxiv.org/abs/2508.12161v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-19",
        "tldr": "",
        "abstract": "Attack graphs (AGs) are graphical tools to analyze the security of computer networks. By connecting the exploitation of individual vulnerabilities, AGs expose possible multi-step attacks against target networks, allowing system administrators to take preventive measures to enhance their network's security. As powerful analytical tools, however, AGs are both time- and memory-consuming to be generated. As the numbers of network assets, interconnections between devices, as well as vulnerabilities increase, the size and volume of the resulting AGs grow at a much higher rate, leading to the well-known state-space explosion. In this paper, we propose the use of high performance computing (HPC) clusters to implement AG generators. We evaluate the performance through experiments and provide insights into how cluster environments can help resolve the issues of slow speed and high memory demands in AG generation in a balanced way.",
        "authors": [
            "Ming Li",
            "John Hale"
        ],
        "categories": [
            "cs.CR",
            "cs.DC"
        ],
        "submit_date": "2025-08-16"
    },
    "http://arxiv.org/abs/2508.12551v1": {
        "id": "http://arxiv.org/abs/2508.12551v1",
        "title": "OS-R1: Agentic Operating System Kernel Tuning with Reinforcement Learning",
        "link": "http://arxiv.org/abs/2508.12551v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-19",
        "tldr": "",
        "abstract": "Linux kernel tuning is essential for optimizing operating system (OS) performance. However, existing methods often face challenges in terms of efficiency, scalability, and generalization. This paper introduces OS-R1, an agentic Linux kernel tuning framework powered by rule-based reinforcement learning (RL). By abstracting the kernel configuration space as an RL environment, OS-R1 facilitates efficient exploration by large language models (LLMs) and ensures accurate configuration modifications. Additionally, custom reward functions are designed to enhance reasoning standardization, configuration modification accuracy, and system performance awareness of the LLMs. Furthermore, we propose a two-phase training process that accelerates convergence and minimizes retraining across diverse tuning scenarios. Experimental results show that OS-R1 significantly outperforms existing baseline methods, achieving up to 5.6% performance improvement over heuristic tuning and maintaining high data efficiency. Notably, OS-R1 is adaptable across various real-world applications, demonstrating its potential for practical deployment in diverse environments. Our dataset and code are publicly available at https://github.com/LHY-24/OS-R1.",
        "authors": [
            "Hongyu Lin",
            "Yuchen Li",
            "Haoran Luo",
            "Kaichun Yao",
            "Libo Zhang",
            "Mingjie Xing",
            "Yanjun Wu"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.OS",
            "cs.SE"
        ],
        "submit_date": "2025-08-18"
    },
    "http://arxiv.org/abs/2508.11467v1": {
        "id": "http://arxiv.org/abs/2508.11467v1",
        "title": "Efficient GPU-Centered Singular Value Decomposition Using the Divide-and-Conquer Method",
        "link": "http://arxiv.org/abs/2508.11467v1",
        "tags": [
            "kernel",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-08-18",
        "tldr": "Enhances SVD computation efficiency on GPUs by redesigning the divide-and-conquer algorithm to eliminate CPU-GPU data transfers and optimize BLAS utilization. Achieves speedups up to 1293.64x vs rocSOLVER and 14.10x vs MAGMA on AMD/NVIDIA GPUs.",
        "abstract": "Singular Value Decomposition (SVD) is a fundamental matrix factorization technique in linear algebra, widely applied in numerous matrix-related problems. However, traditional SVD approaches are hindered by slow panel factorization and frequent CPU-GPU data transfers in heterogeneous systems, despite advancements in GPU computational capabilities. In this paper, we introduce a GPU-centered SVD algorithm, incorporating a novel GPU-based bidiagonal divide-and-conquer (BDC) method. We reformulate the algorithm and data layout of different steps for SVD computation, performing all panel-level computations and trailing matrix updates entirely on GPU to eliminate CPU-GPU data transfers. Furthermore, we integrate related computations to optimize BLAS utilization, thereby increasing arithmetic intensity and fully leveraging the computational capabilities of GPUs. Additionally, we introduce a newly developed GPU-based BDC algorithm that restructures the workflow to eliminate matrix-level CPU-GPU data transfers and enable asynchronous execution between the CPU and GPU. Experimental results on AMD MI210 and NVIDIA V100 GPUs demonstrate that our proposed method achieves speedups of up to 1293.64x/7.47x and 14.10x/12.38x compared to rocSOLVER/cuSOLVER and MAGMA, respectively.",
        "authors": [
            "Shifang Liu",
            "Huiyuan Li",
            "Hongjiao Sheng",
            "Haoyuan Gui",
            "Xiaoyu Zhang"
        ],
        "categories": [
            "cs.DC",
            "cs.PF"
        ],
        "submit_date": "2025-08-15"
    },
    "http://arxiv.org/abs/2508.11415v1": {
        "id": "http://arxiv.org/abs/2508.11415v1",
        "title": "Time, Fences and the Ordering of Events in TSO",
        "link": "http://arxiv.org/abs/2508.11415v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-18",
        "tldr": "",
        "abstract": "The Total Store Order (TSO) is arguably the most widely used relaxed memory model in multiprocessor architectures, widely implemented, for example in Intel's x86 and x64 platforms. It allows processes to delay the visibility of writes through store buffering. While this supports hardware-level optimizations and makes a significant contribution to multiprocessor efficiency, it complicates reasoning about correctness, as executions may violate sequential consistency. Ensuring correct behavior often requires inserting synchronization primitives such as memory fences ($F$) or atomic read-modify-write ($RMW$) operations, but this approach can incur significant performance costs. In this work, we develop a semantic framework that precisely characterizes when such synchronization is necessary under TSO. We introduce a novel TSO-specific occurs-before relation, which adapts Lamport's celebrated happens-before relation from asynchronous message-passing systems to the TSO setting. Our main result is a theorem that proves that the only way to ensure that two events that take place at different sites are temporally ordered is by having the execution create an occurs-before chain between the events. By studying the role of fences and $RMW$s in creating occurs-before chains, we are then able to capture cases in which these costly synchronization operations are unavoidable. Since proper real-time ordering of events is a fundamental aspect of consistency conditions such as Linearizability, our analysis provides a sound theoretical understanding of essential aspects of the TSO model. In particular, we are able to generalize prior lower bounds for linearizable implementations of shared memory objects. Our results capture the structure of information flow and causality in the TSO model by extending the standard communication-based reasoning from asynchronous systems to the TSO memory model.",
        "authors": [
            "Raïssa Nataf",
            "Yoram Moses"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-15"
    },
    "http://arxiv.org/abs/2508.11384v2": {
        "id": "http://arxiv.org/abs/2508.11384v2",
        "title": "Space-efficient population protocols for exact majority on general graphs",
        "link": "http://arxiv.org/abs/2508.11384v2",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-18",
        "tldr": "",
        "abstract": "We study exact majority consensus in the population protocol model. In this model, the system is described by a graph $G = (V,E)$ with $n$ nodes, and in each time step, a scheduler samples uniformly at random a pair of adjacent nodes to interact. In the exact majority consensus task, each node is given a binary input, and the goal is to design a protocol that almost surely reaches a stable configuration, where all nodes output the majority input value.   We give improved upper and lower bounds for exact majority in general graphs. First, we give asymptotically tight time lower bounds for general (unbounded space) protocols. Second, we obtain new upper bounds parameterized by the relaxation time $τ_{\\mathsf{rel}}$ of the random walk on $G$ induced by the scheduler and the degree imbalance $Δ/δ$ of $G$. Specifically, we give a protocol that stabilizes in $O\\left( \\tfracΔδ τ_{\\mathsf{rel}} \\log^2 n \\right)$ steps in expectation and with high probability and uses $O\\left( \\log n \\cdot \\left( \\log\\left(\\tfracΔδ\\right) + \\log \\left(\\tfrac{τ_{\\mathsf{rel}}}{n}\\right) \\right) \\right)$ states in any graph with minimum degree at least $δ$ and maximum degree at most $Δ$.   For regular expander graphs, this matches the optimal space complexity of $Θ(\\log n)$ for fast protocols in complete graphs [Alistarh et al., SODA 2016 and Doty et al., FOCS 2022] with a nearly optimal stabilization time of $O(n \\log^2 n)$ steps. Finally, we give a new upper bound of $O(τ_{\\mathsf{rel}} \\cdot n \\log n)$ for the stabilization time of a constant-state protocol.",
        "authors": [
            "Joel Rybicki",
            "Jakob Solnerzik",
            "Olivier Stietel",
            "Robin Vacus"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-15"
    },
    "http://arxiv.org/abs/2508.11298v2": {
        "id": "http://arxiv.org/abs/2508.11298v2",
        "title": "Inter-APU Communication on AMD MI300A Systems via Infinity Fabric: a Deep Dive",
        "link": "http://arxiv.org/abs/2508.11298v2",
        "tags": [
            "networking",
            "training",
            "hardware"
        ],
        "relevant": true,
        "indexed_date": "2025-08-18",
        "tldr": "Evaluates inter-APU communication efficiency on AMD MI300A systems via Infinity Fabric for multi-APU data movement and collectives. Proposes benchmarks for HIP, MPI, and RCCL optimizations, plus application tuning. Achieves optimized performance in HPC applications like Quicksilver and CloverLeaf.",
        "abstract": "The ever-increasing compute performance of GPU accelerators drives up the need for efficient data movements within HPC applications to sustain performance. Proposed as a solution to alleviate CPU-GPU data movement, AMD MI300A Accelerated Processing Unit (APU) combines CPU, GPU, and high-bandwidth memory (HBM) within a single physical package. Leadership supercomputers, such as El Capitan, group four APUs within a single compute node, using Infinity Fabric Interconnect. In this work, we design specific benchmarks to evaluate direct memory access from the GPU, explicit inter-APU data movement, and collective multi-APU communication. We also compare the efficiency of HIP APIs, MPI routines, and the GPU-specialized RCCL library. Our results highlight key design choices for optimizing inter-APU communication on multi-APU AMD MI300A systems with Infinity Fabric, including programming interfaces, allocators, and data movement. Finally, we optimize two real HPC applications, Quicksilver and CloverLeaf, and evaluate them on a four MI100A APU system.",
        "authors": [
            "Gabin Schieffer",
            "Jacob Wahlgren",
            "Ruimin Shi",
            "Edgar A. León",
            "Roger Pearce",
            "Maya Gokhale",
            "Ivy Peng"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-15"
    },
    "http://arxiv.org/abs/2508.11266v1": {
        "id": "http://arxiv.org/abs/2508.11266v1",
        "title": "Element and Everything Tokens: Two-Tier Architecture for Mobilizing Alternative Assets",
        "link": "http://arxiv.org/abs/2508.11266v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-18",
        "tldr": "",
        "abstract": "Alternative assets such as mines, power plants, or infrastructure projects are often large, heterogeneous bundles of resources, rights, and outputs whose value is difficult to trade or fractionalize under traditional frameworks. This paper proposes a novel two-tier tokenization architecture to enhance the liquidity and transparency of such complex assets. We introduce the concepts of Element Tokens and Everything Tokens: elemental tokens represent standardized, fully collateralized components of an asset (e.g., outputs, rights, or credits), while an everything token represents the entire asset as a fixed combination of those elements. The architecture enables both fine-grained partial ownership and integrated whole-asset ownership through a system of two-way convertibility. We detail the design and mechanics of this system, including an arbitrage mechanism that keeps the price of the composite token aligned with the net asset value of its constituents. Through illustrative examples in the energy and industrial sectors, we demonstrate that our approach allows previously illiquid, high-value projects to be fractionalized and traded akin to stocks or exchange-traded funds (ETFs). We discuss the benefits for investors and asset owners, such as lower entry barriers, improved price discovery, and flexible financing, as well as the considerations for implementation and regulation.",
        "authors": [
            "Ailiya Borjigin",
            "Cong He",
            "Charles CC Lee",
            "Wei Zhou"
        ],
        "categories": [
            "cs.DC",
            "cs.CY"
        ],
        "submit_date": "2025-08-15"
    },
    "http://arxiv.org/abs/2508.11035v1": {
        "id": "http://arxiv.org/abs/2508.11035v1",
        "title": "EMLIO: Minimizing I/O Latency and Energy Consumption for Large-Scale AI Training",
        "link": "http://arxiv.org/abs/2508.11035v1",
        "tags": [
            "training",
            "storage",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-08-18",
        "tldr": "Addresses data-loading bottlenecks and energy consumption in large-scale DNN training. Proposes EMLIO, a distributed I/O service with batching, prefetching, and GPU-integrated preprocessing. Achieves 8.6x faster I/O and 10.9x lower energy use across network environments.",
        "abstract": "Large-scale deep learning workloads increasingly suffer from I/O bottlenecks as datasets grow beyond local storage capacities and GPU compute outpaces network and disk latencies. While recent systems optimize data-loading time, they overlook the energy cost of I/O - a critical factor at large scale. We introduce EMLIO, an Efficient Machine Learning I/O service that jointly minimizes end-to-end data-loading latency T and I/O energy consumption E across variable-latency networked storage. EMLIO deploys a lightweight data-serving daemon on storage nodes that serializes and batches raw samples, streams them over TCP with out-of-order prefetching, and integrates seamlessly with GPU-accelerated (NVIDIA DALI) preprocessing on the client side. In exhaustive evaluations over local disk, LAN (0.05 ms & 10 ms RTT), and WAN (30 ms RTT) environments, EMLIO delivers up to 8.6X faster I/O and 10.9X lower energy use compared to state-of-the-art loaders, while maintaining constant performance and energy profiles irrespective of network distance. EMLIO's service-based architecture offers a scalable blueprint for energy-aware I/O in next-generation AI clouds.",
        "authors": [
            "Hasibul Jamil",
            "MD S Q Zulkar Nine",
            "Tevfik Kosar"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-14"
    },
    "http://arxiv.org/abs/2508.11477v1": {
        "id": "http://arxiv.org/abs/2508.11477v1",
        "title": "OpenCXD: An Open Real-Device-Guided Hybrid Evaluation Framework for CXL-SSDs",
        "link": "http://arxiv.org/abs/2508.11477v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-18",
        "tldr": "",
        "abstract": "The advent of Compute Express Link (CXL) enables SSDs to participate in the memory hierarchy as large-capacity, byte-addressable memory devices. These CXL-enabled SSDs (CXL-SSDs) offer a promising new tier between DRAM and traditional storage, combining NAND flash density with memory-like access semantics. However, evaluating the performance of CXL-SSDs remains difficult due to the lack of hardware that natively supports the CXL.mem protocol on SSDs. As a result, most prior work relies on hybrid simulators combining CPU models augmented with CXL.mem semantics and SSD simulators that approximate internal flash behaviors. While effective for early-stage exploration, this approach cannot faithfully model firmware-level interactions and low-level storage dynamics critical to CXL-SSD performance. In this paper, we present OpenCXD, a real-device-guided hybrid evaluation framework that bridges the gap between simulation and hardware. OpenCXD integrates a cycle-accurate CXL.mem simulator on the host side with a physical OpenSSD platform running real firmware. This enables in-situ firmware execution triggered by simulated memory requests. Through these contributions, OpenCXD reflects device-level phenomena unobservable in simulation-only setups, providing critical insights for future firmware design tailored to CXL-SSDs.",
        "authors": [
            "Hyunsun Chung",
            "Junhyeok Park",
            "Taewan Noh",
            "Seonghoon Ahn",
            "Kihwan Kim",
            "Ming Zhao",
            "Youngjae Kim"
        ],
        "categories": [
            "cs.AR",
            "cs.ET",
            "cs.OS"
        ],
        "submit_date": "2025-08-15"
    },
    "http://arxiv.org/abs/2508.10862v5": {
        "id": "http://arxiv.org/abs/2508.10862v5",
        "title": "Minimmit: Fast Finality with Even Faster Blocks",
        "link": "http://arxiv.org/abs/2508.10862v5",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-15",
        "tldr": "",
        "abstract": "Achieving low-latency consensus in geographically distributed systems remains a key challenge for blockchain and distributed database applications. To this end, there has been significant recent interest in State-Machine-Replication (SMR) protocols that achieve 2-round finality under the assumption that $5f+1\\leq n$, where $n$ is the number of processors and $f$ bounds the number of processors that may exhibit Byzantine faults. In these protocols, instructions are organised into views, each led by a different designated leader, and 2-round finality means that a leader's proposal can be finalised after just a single round of voting, meaning two rounds overall (one round for the proposal and one for voting).   We introduce Minimmit, a Byzantine-fault-tolerant SMR protocol with lower latency than previous 2-round finality approaches. Our key insight is that view progression and transaction finality can operate on different quorum thresholds without compromising safety or liveness. Experiments simulating a globally distributed network of 50 processors, uniformly assigned across ten virtual regions, show that the approach leads to a 23.1% reduction in view latency and a 10.7% reduction in transaction latency compared to the state-of-the-art.",
        "authors": [
            "Brendan Kobayashi Chou",
            "Andrew Lewis-Pye",
            "Patrick O'Grady"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-14"
    },
    "http://arxiv.org/abs/2508.10854v1": {
        "id": "http://arxiv.org/abs/2508.10854v1",
        "title": "Introducing CQ: A C-like API for Quantum Accelerated HPC",
        "link": "http://arxiv.org/abs/2508.10854v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-15",
        "tldr": "",
        "abstract": "In this paper we present CQ, a specification for a C-like API for quantum accelerated HPC, as well as CQ-SimBE, a reference implementation of CQ written in C99, and built on top of the statevector simulator QuEST. CQ focuses on enabling the incremental integration of quantum computing into classical HPC codes by supporting runtime offloading from languages such as C and Fortran. It provides a way of describing and offloading quantum computations which is compatible with strictly and strongly typed compiled languages, and gives the programmer fine-grained control over classical data movement. The CQ Simulated Backend (CQ-SimBE) provides both a way to demonstrate the usage and utility of CQ, and a space to experiment with new features such as support for analogue quantum computing. Both the CQ specification and CQ-SimBE are open-source, and available in public repositories.",
        "authors": [
            "Oliver Thomson Brown",
            "Mateusz Meller",
            "James Richings"
        ],
        "categories": [
            "cs.DC",
            "quant-ph"
        ],
        "submit_date": "2025-08-14"
    },
    "http://arxiv.org/abs/2508.10481v1": {
        "id": "http://arxiv.org/abs/2508.10481v1",
        "title": "Dalek: An Unconventional and Energy-Aware Heterogeneous Cluster",
        "link": "http://arxiv.org/abs/2508.10481v1",
        "tags": [
            "hardware",
            "training"
        ],
        "relevant": true,
        "indexed_date": "2025-08-15",
        "tldr": "Explores cost-effective heterogeneous clusters using consumer-grade hardware for prototyping and development. Introduces Dalek with custom energy monitoring at milliwatt-level resolution. Achieves 1000 averaged energy samples per second to enable energy-aware experiments.",
        "abstract": "Dalek is an experimental compute cluster designed to evaluate the performance of heterogeneous, consumer-grade hardware for software design, prototyping, and algorithm development. In contrast to traditional computing centers that rely on costly, server-class components, Dalek integrates CPUs and GPUs typically found in mini-PCs, laptops, and gaming desktops, providing a cost-effective yet versatile platform. This document details the cluster's architecture and software stack, and presents results from synthetic benchmarks. Furthermore, it introduces a custom energy monitoring platform capable of delivering 1000 averaged samples per second with milliwatt-level resolution. This high-precision monitoring capability enables a wide range of energy-aware research experiments in applied Computer Science.",
        "authors": [
            "Adrien Cassagne",
            "Noé Amiot",
            "Manuel Bouyer"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-14"
    },
    "http://arxiv.org/abs/2508.10305v1": {
        "id": "http://arxiv.org/abs/2508.10305v1",
        "title": "GPZ: GPU-Accelerated Lossy Compressor for Particle Data",
        "link": "http://arxiv.org/abs/2508.10305v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-15",
        "tldr": "",
        "abstract": "Particle-based simulations and point-cloud applications generate massive, irregular datasets that challenge storage, I/O, and real-time analytics. Traditional compression techniques struggle with irregular particle distributions and GPU architectural constraints, often resulting in limited throughput and suboptimal compression ratios. In this paper, we present GPZ, a high-performance, error-bounded lossy compressor designed specifically for large-scale particle data on modern GPUs. GPZ employs a novel four-stage parallel pipeline that synergistically balances high compression efficiency with the architectural demands of massively parallel hardware. We introduce a suite of targeted optimizations for computation, memory access, and GPU occupancy that enables GPZ to achieve near-hardware-limit throughput. We conduct an extensive evaluation on three distinct GPU architectures (workstation, data center, and edge) using six large-scale, real-world scientific datasets from five distinct domains. The results demonstrate that GPZ consistently and significantly outperforms five state-of-the-art GPU compressors, delivering up to 8x higher end-to-end throughput while simultaneously achieving superior compression ratios and data quality.",
        "authors": [
            "Ruoyu Li",
            "Yafan Huang",
            "Longtao Zhang",
            "Zhuoxun Yang",
            "Sheng Di",
            "Jiajun Huang",
            "Jinyang Liu",
            "Jiannan Tian",
            "Xin Liang",
            "Guanpeng Li",
            "Hanqi Guo",
            "Franck Cappello",
            "Kai Zhao"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-14"
    },
    "http://arxiv.org/abs/2508.10202v2": {
        "id": "http://arxiv.org/abs/2508.10202v2",
        "title": "Mixed-Precision Performance Portability of FFT-Based GPU-Accelerated Algorithms for Block-Triangular Toeplitz Matrices",
        "link": "http://arxiv.org/abs/2508.10202v2",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-15",
        "tldr": "",
        "abstract": "The hardware diversity in leadership-class computing facilities, alongside the immense performance boosts from today's GPUs when computing in lower precision, incentivizes scientific HPC workflows to adopt mixed-precision algorithms and performance portability models. We present an on-the-fly framework using hipify for performance portability and apply it to FFTMatvec - an HPC application that computes matrix-vector products with block-triangular Toeplitz matrices. Our approach enables FFTMatvec, initially a CUDA-only application, to run seamlessly on AMD GPUs with excellent performance. Performance optimizations for AMD GPUs are integrated into the open-source rocBLAS library, keeping the application code unchanged. We then present a dynamic mixed-precision framework for FFTMatvec; a Pareto front analysis determines the optimal mixed-precision configuration for a desired error tolerance. Results are shown for AMD Instinct MI250X, MI300X, and the newly launched MI355X GPUs. The performance-portable, mixed-precision FFTMatvec is scaled to 4,096 GPUs on the OLCF Frontier supercomputer.",
        "authors": [
            "Sreeram Venkat",
            "Kasia Swirydowicz",
            "Noah Wolfe",
            "Omar Ghattas"
        ],
        "categories": [
            "cs.DC",
            "cs.PF",
            "math.NA"
        ],
        "submit_date": "2025-08-13"
    },
    "http://arxiv.org/abs/2508.10141v1": {
        "id": "http://arxiv.org/abs/2508.10141v1",
        "title": "Hard Shell, Reliable Core: Improving Resilience in Replicated Systems with Selective Hybridization",
        "link": "http://arxiv.org/abs/2508.10141v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-15",
        "tldr": "",
        "abstract": "Hybrid fault models are known to be an effective means for enhancing the robustness of consensus-based replicated systems. However, existing hybridization approaches suffer from limited flexibility with regard to the composition of crash-tolerant and Byzantine fault-tolerant system parts and/or are associated with a significant diversification overhead. In this paper we address these issues with ShellFT, a framework that leverages the concept of micro replication to allow system designers to freely choose the parts of the replication logic that need to be resilient against Byzantine faults. As a key benefit, such a selective hybridization makes it possible to develop hybrid solutions that are tailored to the specific characteristics and requirements of individual use cases. To illustrate this flexibility, we present three custom ShellFT protocols and analyze the complexity of their implementations. Our evaluation shows that compared with traditional hybridization approaches, ShellFT is able to decrease diversification costs by more than 70%.",
        "authors": [
            "Laura Lawniczak",
            "Tobias Distler"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-13"
    },
    "http://arxiv.org/abs/2508.10259v1": {
        "id": "http://arxiv.org/abs/2508.10259v1",
        "title": "Leveraging OS-Level Primitives for Robotic Action Management",
        "link": "http://arxiv.org/abs/2508.10259v1",
        "tags": [
            "serving",
            "offloading",
            "edge"
        ],
        "relevant": true,
        "indexed_date": "2025-08-15",
        "tldr": "Proposes AMS, a robot action management system using OS primitives (exception, context switch, replay) to optimize robotic tasks. Enhances VLA model efficiency by reducing redundant computations and enabling action interruption. Achieves 29-74% execution time savings and 7-24× higher success rates.",
        "abstract": "End-to-end imitation learning frameworks (e.g., VLA) are increasingly prominent in robotics, as they enable rapid task transfer by learning directly from perception to control, eliminating the need for complex hand-crafted features. However, even when employing SOTA VLA-based models, they still exhibit limited generalization capabilities and suboptimal action efficiency, due to the constraints imposed by insufficient robotic training datasets. In addition to addressing this problem using model-based approaches, we observe that robotic action slices, which consist of contiguous action steps, exhibit strong analogies to the time slices of threads in traditional operating systems. This insight presents a novel opportunity to tackle the problem at the system level.   In this paper, we propose AMS, a robot action management system enhanced with OS-level primitives like exception, context switch and record-and-replay, that improves both execution efficiency and success rates of robotic tasks. AMS first introduces action exception, which facilitates the immediate interruption of robotic actions to prevent error propagation. Secondly, AMS proposes action context, which eliminates redundant computations for VLA-based models, thereby accelerating execution efficiency in robotic actions. Finally, AMS leverages action replay to facilitate repetitive or similar robotic tasks without the need for re-training efforts. We implement AMS in both an emulated environment and on a real robot platform. The evaluation results demonstrate that AMS significantly enhances the model's generalization ability and action efficiency, achieving task success rate improvements ranging from 7x to 24x and saving end-to-end execution time ranging from 29% to 74% compared to existing robotic system without AMS support.",
        "authors": [
            "Wenxin Zheng",
            "Boyang Li",
            "Bin Xu",
            "Erhu Feng",
            "Jinyu Gu",
            "Haibo Chen"
        ],
        "categories": [
            "cs.OS"
        ],
        "submit_date": "2025-08-14"
    },
    "http://arxiv.org/abs/2508.09663v1": {
        "id": "http://arxiv.org/abs/2508.09663v1",
        "title": "Closing the HPC-Cloud Convergence Gap: Multi-Tenant Slingshot RDMA for Kubernetes",
        "link": "http://arxiv.org/abs/2508.09663v1",
        "tags": [
            "networking",
            "hardware",
            "training"
        ],
        "relevant": true,
        "indexed_date": "2025-08-14",
        "tldr": "Addresses secure multi-tenant RDMA networking for HPC-Cloud convergence. Extends Slingshot stack with container-granular access in Kubernetes. Achieves minimal overhead while isolating high-throughput, low-latency communication for HPC workloads.",
        "abstract": "Converged HPC-Cloud computing is an emerging computing paradigm that aims to support increasingly complex and multi-tenant scientific workflows. These systems require reconciliation of the isolation requirements of native cloud workloads and the performance demands of HPC applications. In this context, networking hardware is a critical boundary component: it is the conduit for high-throughput, low-latency communication and enables isolation across tenants. HPE Slingshot is a high-speed network interconnect that provides up to 200 Gbps of throughput per port and targets high-performance computing (HPC) systems. The Slingshot host software, including hardware drivers and network middleware libraries, is designed to meet HPC deployments, which predominantly use single-tenant access modes. Hence, the Slingshot stack is not suited for secure use in multi-tenant deployments, such as converged HPC-Cloud deployments. In this paper, we design and implement an extension to the Slingshot stack targeting converged deployments on the basis of Kubernetes. Our integration provides secure, container-granular, and multi-tenant access to Slingshot RDMA networking capabilities at minimal overhead.",
        "authors": [
            "Philipp A. Friese",
            "Ahmed Eleliemy",
            "Utz-Uwe Haus",
            "Martin Schulz"
        ],
        "categories": [
            "cs.DC",
            "cs.NI"
        ],
        "submit_date": "2025-08-13"
    },
    "http://arxiv.org/abs/2508.09591v1": {
        "id": "http://arxiv.org/abs/2508.09591v1",
        "title": "HierMoE: Accelerating MoE Training with Hierarchical Token Deduplication and Expert Swap",
        "link": "http://arxiv.org/abs/2508.09591v1",
        "tags": [
            "MoE",
            "training",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-08-14",
        "tldr": "Accelerates MoE training by reducing communication and balancing loads with token deduplication and expert swap. HierMoE uses topology-aware strategies and theoretical models for optimization. Achieves up to 3.32× faster communication and 1.27× faster training.",
        "abstract": "The sparsely activated mixture-of-experts (MoE) transformer has become a common architecture for large language models (LLMs) due to its sparsity, which requires fewer computational demands while easily scaling the model size. In MoE models, each MoE layer requires to dynamically choose tokens to activate particular experts for computation while the activated experts may not be located in the same device or GPU as the token. However, this leads to substantial communication and load imbalances across all GPUs, which obstructs the scalability of distributed systems within a GPU cluster. To this end, we introduce HierMoE to accelerate the training of MoE models by two topology-aware techniques: 1) token deduplication to reduce the communication traffic, and 2) expert swap to balance the workloads among all GPUs. To enable the above two proposed approaches to be more general, we build theoretical models aimed at achieving the best token duplication and expert swap strategy under different model configurations and hardware environments. We implement our prototype HierMoE system atop Megatron-LM and conduct experiments on a 32-GPU cluster with DeepSeek-V3 and Qwen3-30B-A3B models. Experimental results show that our HierMoE achieves $1.55\\times$ to $3.32\\times$ faster communication and delivers $1.18\\times$ to $1.27\\times$ faster end-to-end training compared to state-of-the-art MoE training systems, Tutel-2DH, SmartMoE, and Megatron-LM.",
        "authors": [
            "Wenxiang Lin",
            "Xinglin Pan",
            "Lin Zhang",
            "Shaohuai Shi",
            "Xuan Wang",
            "Xiaowen Chu"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-08-13"
    },
    "http://arxiv.org/abs/2508.09505v1": {
        "id": "http://arxiv.org/abs/2508.09505v1",
        "title": "Verify Distributed Deep Learning Model Implementation Refinement with Iterative Relation Inference",
        "link": "http://arxiv.org/abs/2508.09505v1",
        "tags": [
            "training",
            "sparse",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-08-14",
        "tldr": "Proposes GraphGuard, a static approach to verify refinement between sequential and distributed deep learning model implementations. Uses iterative rewriting to check if sequential outputs can be reconstructed from distributed outputs. Evaluated on GPT and Llama-3, scales to large models and aids bug localization.",
        "abstract": "Distributed machine learning training and inference is common today because today's large models require more memory and compute than can be provided by a single GPU. Distributed models are generally produced by programmers who take a sequential model specification and apply several distribution strategies to distribute state and computation across GPUs. Unfortunately, bugs can be introduced in the process, and a distributed model implementation's outputs might differ from the sequential model's outputs. In this paper, we describe an approach to statically identify such bugs by checking model refinement, that is, can the sequential model's outputs be reconstructed from the distributed model's outputs? Our approach, implemented in GraphGuard, uses iterative rewriting to prove model refinement. Our approach can scale to today's large models and deployments: we evaluate it using GPT and Llama-3. Further, it provides actionable output that aids in bug localization.",
        "authors": [
            "Zhanghan Wang",
            "Ding Ding",
            "Hang Zhu",
            "Haibin Lin",
            "Aurojit Panda"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-08-13"
    },
    "http://arxiv.org/abs/2508.09638v2": {
        "id": "http://arxiv.org/abs/2508.09638v2",
        "title": "Distributed Rhombus Formation of Sliding Squares",
        "link": "http://arxiv.org/abs/2508.09638v2",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-14",
        "tldr": "",
        "abstract": "The sliding square model is a widely used abstraction for studying self-reconfigurable robotic systems, where modules are square-shaped robots that move by sliding or rotating over one another. In this paper, we propose a novel distributed algorithm that allows a group of modules to reconfigure into a rhombus shape, starting from an arbitrary side-connected configuration. It is connectivity-preserving and operates under minimal assumptions: one leader module, common chirality, constant memory per module, and visibility and communication restricted to immediate neighbors. Unlike prior work, which relaxes the original sliding square move-set, our approach uses the unmodified move-set, addressing the additional challenge of handling locked configurations. Our algorithm is sequential in nature and operates with a worst-case time complexity of $\\mathcal{O}(n^2)$ rounds, which is optimal for sequential algorithms. To improve runtime, we introduce two parallel variants of the algorithm. Both rely on a spanning tree data structure, allowing modules to make decisions based on local connectivity. Our experimental results show a significant speedup for the first variant, and linear average runtime for the second variant, which is worst-case optimal for parallel algorithms.",
        "authors": [
            "Irina Kostitsyna",
            "David Liedtke",
            "Christian Scheideler"
        ],
        "categories": [
            "cs.CG",
            "cs.DC"
        ],
        "submit_date": "2025-08-13"
    },
    "http://arxiv.org/abs/2508.09229v1": {
        "id": "http://arxiv.org/abs/2508.09229v1",
        "title": "Cluster Topology-Driven Placement of Experts Reduces Network Traffic in MoE Inference",
        "link": "http://arxiv.org/abs/2508.09229v1",
        "tags": [
            "serving",
            "MoE",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-08-14",
        "tldr": "Investigates how to reduce network traffic for MoE model inference by optimizing expert placement. Proposes an ILP-based placement strategy leveraging cluster topology. Achieves lower network traffic than competitors for models up to 671B parameters.",
        "abstract": "Efficient deployment of a pre-trained LLM to a cluster with multiple servers is a critical step for providing fast responses to users' queries. The recent success of Mixture-of-Experts (MoE) LLMs raises the question of how to deploy them efficiently, considering their underlying structure. During the inference in MoE LLMs, only a small part of the experts is selected to process a given token. Moreover, in practice, the experts' load is highly imbalanced. For efficient deployment, one has to distribute the model across a large number of servers using a model placement algorithm. Thus, to improve cluster utilization, the model placement algorithm has to take into account the network topology. This work focuses on the efficient topology-aware placement of the pre-trained MoE LLMs in the inference stage. We propose an integer linear program (ILP) that determines the optimal placement of experts, minimizing the expected number of transmissions. Due to the internal structure, this optimization problem can be solved with a standard ILP solver. We demonstrate that ILP-based placement strategy yields lower network traffic than competitors for small-scale (DeepSeekMoE~16B) and large-scale (DeepSeek-R1~671B) models.",
        "authors": [
            "Danil Sivtsov",
            "Aleksandr Katrutsa",
            "Ivan Oseledets"
        ],
        "categories": [
            "cs.NI",
            "cs.AI",
            "cs.DC"
        ],
        "submit_date": "2025-08-12"
    },
    "http://arxiv.org/abs/2508.09149v1": {
        "id": "http://arxiv.org/abs/2508.09149v1",
        "title": "Semantic-Aware LLM Orchestration for Proactive Resource Management in Predictive Digital Twin Vehicular Networks",
        "link": "http://arxiv.org/abs/2508.09149v1",
        "tags": [
            "edge",
            "RL",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-08-14",
        "tldr": "Proposes SP-LLM, a semantic-aware LLM orchestrator for proactive resource management in vehicular edge networks. Uses Predictive Digital Twin to forecast network states and an LLM to optimize offloading/resource allocation dynamically via natural language commands. Achieves superior adaptability and robustness, improving scalability over reactive baselines.",
        "abstract": "Next-generation automotive applications require vehicular edge computing (VEC), but current management systems are essentially fixed and reactive. They are suboptimal in extremely dynamic vehicular environments because they are constrained to static optimization objectives and base their decisions on the current network states. This paper presents a novel Semantic-Aware Proactive LLM Orchestration (SP-LLM) framework to address these issues. Our method transforms the traditional Digital Twin (DT) into a Predictive Digital Twin (pDT) that predicts important network parameters such as task arrivals, vehicle mobility, and channel quality. A Large Language Model (LLM) that serves as a cognitive orchestrator is at the heart of our framework. It makes proactive, forward-looking decisions about task offloading and resource allocation by utilizing the pDT's forecasts. The LLM's ability to decipher high-level semantic commands given in natural language is crucial because it enables it to dynamically modify its optimization policy to match evolving strategic objectives, like giving emergency services priority or optimizing energy efficiency. We show through extensive simulations that SP-LLM performs significantly better in terms of scalability, robustness in volatile conditions, and adaptability than state-of-the-art reactive and MARL-based approaches. More intelligent, autonomous, and goal-driven vehicular networks will be possible due to our framework's outstanding capacity to convert human intent into optimal network behavior.",
        "authors": [
            "Seyed Hossein Ahmadpanah"
        ],
        "categories": [
            "cs.NI",
            "cs.DC"
        ],
        "submit_date": "2025-08-02"
    },
    "http://arxiv.org/abs/2508.09503v1": {
        "id": "http://arxiv.org/abs/2508.09503v1",
        "title": "Holistic Heterogeneous Scheduling for Autonomous Applications using Fine-grained, Multi-XPU Abstraction",
        "link": "http://arxiv.org/abs/2508.09503v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-14",
        "tldr": "",
        "abstract": "Modern autonomous applications are increasingly utilizing multiple heterogeneous processors (XPUs) to accelerate different stages of algorithm modules. However, existing runtime systems for these applications, such as ROS, can only perform module-level task management, lacking awareness of the fine-grained usage of multiple XPUs. This paper presents XAUTO, a runtime system designed to cooperatively manage XPUs for latency-sensitive autonomous applications. The key idea is a fine-grained, multi-XPU programming abstraction -- XNODE, which aligns with the stage-level task granularity and can accommodate multiple XPU implementations. XAUTO holistically assigns XPUs to XNODEs and schedules their execution to minimize end-to-end latency. Experimental results show that XAUTO can reduce the end-to-end latency of a perception pipeline for autonomous driving by 1.61x compared to a state-of-the-art module-level scheduling system (ROS2).",
        "authors": [
            "Mingcong Han",
            "Weihang Shen",
            "Rong Chen",
            "Binyu Zang",
            "Haibo Chen"
        ],
        "categories": [
            "cs.OS"
        ],
        "submit_date": "2025-08-13"
    },
    "http://arxiv.org/abs/2508.09351v1": {
        "id": "http://arxiv.org/abs/2508.09351v1",
        "title": "A Limits Study of Memory-side Tiering Telemetry",
        "link": "http://arxiv.org/abs/2508.09351v1",
        "tags": [
            "offloading",
            "hardware",
            "storage"
        ],
        "relevant": true,
        "indexed_date": "2025-08-14",
        "tldr": "Studies memory access patterns for efficient memory tiering in large models. Proposes a Hotness Monitoring Unit with reactive placement, proactive movement, and compiler hints for offloading data. Achieves 1.94x speedup over Linux NUMA tiering while offloading 90% of pages to CXL memory.",
        "abstract": "Increasing workload demands and emerging technologies necessitate the use of various memory and storage tiers in computing systems. This paper presents results from a CXL-based Experimental Memory Request Logger that reveals precise memory access patterns at runtime without interfering with the running workloads. We use it for software emulation of future memory telemetry hardware. By combining reactive placement based on data address monitoring, proactive data movement, and compiler hints, a Hotness Monitoring Unit (HMU) within memory modules can greatly improve memory tiering solutions. Analysis of page placement using profiled access counts on a Deep Learning Recommendation Model (DLRM) indicates a potential 1.94x speedup over Linux NUMA balancing tiering, and only a 3% slowdown compared to Host-DRAM allocation while offloading over 90% of pages to CXL memory. The study underscores the limitations of existing tiering strategies in terms of coverage and accuracy, and makes a strong case for programmable, device-level telemetry as a scalable and efficient solution for future memory systems.",
        "authors": [
            "Vinicius Petrucci",
            "Felippe Zacarias",
            "David Roberts"
        ],
        "categories": [
            "cs.OS",
            "cs.AR",
            "cs.PF"
        ],
        "submit_date": "2025-08-12"
    },
    "http://arxiv.org/abs/2508.09035v1": {
        "id": "http://arxiv.org/abs/2508.09035v1",
        "title": "P/D-Device: Disaggregated Large Language Model between Cloud and Devices",
        "link": "http://arxiv.org/abs/2508.09035v1",
        "tags": [
            "disaggregation",
            "serving",
            "RAG"
        ],
        "relevant": true,
        "indexed_date": "2025-08-13",
        "tldr": "Proposes P/D-Device, a disaggregated LLM serving system between cloud and devices to reduce TTFT and cloud resource usage. Cloud assists device in prefill phase, followed by token scheduling. Achieves 60% lower TTFT, TPOT under 10ms, and 15x cloud throughput improvement.",
        "abstract": "Serving disaggregated large language models has been widely adopted in industrial practice for enhanced performance. However, too many tokens generated in decoding phase, i.e., occupying the resources for a long time, essentially hamper the cloud from achieving a higher throughput. Meanwhile, due to limited on-device resources, the time to first token (TTFT), i.e., the latency of prefill phase, increases dramatically with the growth on prompt length. In order to concur with such a bottleneck on resources, i.e., long occupation in cloud and limited on-device computing capacity, we propose to separate large language model between cloud and devices. That is, the cloud helps a portion of the content for each device, only in its prefill phase. Specifically, after receiving the first token from the cloud, decoupling with its own prefill, the device responds to the user immediately for a lower TTFT. Then, the following tokens from cloud are presented via a speed controller for smoothed TPOT (the time per output token), until the device catches up with the progress. On-device prefill is then amortized using received tokens while the resource usage in cloud is controlled. Moreover, during cloud prefill, the prompt can be refined, using those intermediate data already generated, to further speed up on-device inference. We implement such a scheme P/D-Device, and confirm its superiority over other alternatives. We further propose an algorithm to decide the best settings. Real-trace experiments show that TTFT decreases at least 60%, maximum TPOT is about tens of milliseconds, and cloud throughput increases by up to 15x.",
        "authors": [
            "Yibo Jin",
            "Yixu Xu",
            "Yue Chen",
            "Chengbin Wang",
            "Tao Wang",
            "Jiaqi Huang",
            "Rongfei Zhang",
            "Yiming Dong",
            "Yuting Yan",
            "Ke Cheng",
            "Yingjie Zhu",
            "Shulan Wang",
            "Qianqian Tang",
            "Shuaishuai Meng",
            "Guanxin Cheng",
            "Ze Wang",
            "Shuyan Miao",
            "Ketao Wang",
            "Wen Liu",
            "Yifan Yang",
            "Tong Zhang",
            "Anran Wang",
            "Chengzhou Lu",
            "Tiantian Dong",
            "Yongsheng Zhang",
            "Zhe Wang",
            "Hefei Guo",
            "Hongjie Liu",
            "Wei Lu",
            "Zhengyong Zhang"
        ],
        "categories": [
            "cs.DC",
            "cs.CL",
            "cs.LG"
        ],
        "submit_date": "2025-08-12"
    },
    "http://arxiv.org/abs/2508.08525v1": {
        "id": "http://arxiv.org/abs/2508.08525v1",
        "title": "A Reinforcement Learning-Driven Task Scheduling Algorithm for Multi-Tenant Distributed Systems",
        "link": "http://arxiv.org/abs/2508.08525v1",
        "tags": [
            "autoscaling",
            "RL",
            "training"
        ],
        "relevant": true,
        "indexed_date": "2025-08-13",
        "tldr": "Proposes a reinforcement learning-based task scheduler for multi-tenant distributed systems. Designs a PPO-driven framework to dynamically allocate resources while optimizing latency, utilization, and fairness. Achieves improved scheduling efficiency and outperforms baselines across multiple metrics.",
        "abstract": "This paper addresses key challenges in task scheduling for multi-tenant distributed systems, including dynamic resource variation, heterogeneous tenant demands, and fairness assurance. An adaptive scheduling method based on reinforcement learning is proposed. By modeling the scheduling process as a Markov decision process, the study defines the state space, action space, and reward function. A scheduling policy learning framework is designed using Proximal Policy Optimization (PPO) as the core algorithm. This enables dynamic perception of complex system states and real-time decision-making. Under a multi-objective reward mechanism, the scheduler jointly optimizes task latency, resource utilization, and tenant fairness. The coordination between the policy network and the value network continuously refines the scheduling strategy. This enhances overall system performance. To validate the effectiveness of the proposed method, a series of experiments were conducted in multi-scenario environments built using a real-world public dataset. The experiments evaluated task latency control, resource efficiency, policy stability, and fairness. The results show that the proposed method outperforms existing scheduling approaches across multiple evaluation metrics. It demonstrates strong stability and generalization ability. The proposed scheduling framework provides practical and engineering value in policy design, dynamic resource modeling, and multi-tenant service assurance. It effectively improves scheduling efficiency and resource management in distributed systems under complex conditions.",
        "authors": [
            "Xiaopei Zhang",
            "Xingang Wang",
            "Xin Wang"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-11"
    },
    "http://arxiv.org/abs/2508.08430v1": {
        "id": "http://arxiv.org/abs/2508.08430v1",
        "title": "Profiling Concurrent Vision Inference Workloads on NVIDIA Jetson -- Extended",
        "link": "http://arxiv.org/abs/2508.08430v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-13",
        "tldr": "",
        "abstract": "The proliferation of IoT devices and advancements in network technologies have intensified the demand for real-time data processing at the network edge. To address these demands, low-power AI accelerators, particularly GPUs, are increasingly deployed for inference tasks, enabling efficient computation while mitigating cloud-based systems' latency and bandwidth limitations. Despite their growing deployment, GPUs remain underutilised even in computationally intensive workloads. This underutilisation stems from the limited understanding of GPU resource sharing, particularly in edge computing scenarios. In this work, we conduct a detailed analysis of both high- and low-level metrics, including GPU utilisation, memory usage, streaming multiprocessor (SM) utilisation, and tensor core usage, to identify bottlenecks and guide hardware-aware optimisations. By integrating traces from multiple profiling tools, we provide a comprehensive view of resource behaviour on NVIDIA Jetson edge devices under concurrent vision inference workloads. Our findings indicate that while GPU utilisation can reach $100\\%$ under specific optimisations, critical low-level resources, such as SMs and tensor cores, often operate only at $15\\%$ to $30\\%$ utilisation. Moreover, we observe that certain CPU-side events, such as thread scheduling, context switching, etc., frequently emerge as bottlenecks, further constraining overall GPU performance. We provide several key observations for users of vision inference workloads on NVIDIA edge devices.",
        "authors": [
            "Abhinaba Chakraborty",
            "Wouter Tavernier",
            "Akis Kourtis",
            "Mario Pickavet",
            "Andreas Oikonomakis",
            "Didier Colle"
        ],
        "categories": [
            "cs.DC",
            "cs.AR",
            "cs.PF"
        ],
        "submit_date": "2025-08-11"
    },
    "http://arxiv.org/abs/2508.08906v1": {
        "id": "http://arxiv.org/abs/2508.08906v1",
        "title": "Ultra Ethernet's Design Principles and Architectural Innovations",
        "link": "http://arxiv.org/abs/2508.08906v1",
        "tags": [
            "networking",
            "training"
        ],
        "relevant": true,
        "indexed_date": "2025-08-13",
        "tldr": "Proposes Ultra Ethernet Transport (UET), a hardware-accelerated protocol for high-performance networking in AI/HPC systems. Designed for reliable and efficient communication at extreme scale by leveraging computational efficiency gains and Ethernet ecosystem. Achieves transformative performance for large-scale computing.",
        "abstract": "The recently released Ultra Ethernet (UE) 1.0 specification defines a transformative High-Performance Ethernet standard for future Artificial Intelligence (AI) and High-Performance Computing (HPC) systems. This paper, written by the specification's authors, provides a high-level overview of UE's design, offering crucial motivations and scientific context to understand its innovations. While UE introduces advancements across the entire Ethernet stack, its standout contribution is the novel Ultra Ethernet Transport (UET), a potentially fully hardware-accelerated protocol engineered for reliable, fast, and efficient communication in extreme-scale systems. Unlike InfiniBand, the last major standardization effort in high-performance networking over two decades ago, UE leverages the expansive Ethernet ecosystem and the 1,000x gains in computational efficiency per moved bit to deliver a new era of high-performance networking.",
        "authors": [
            "Torsten Hoefler",
            "Karen Schramm",
            "Eric Spada",
            "Keith Underwood",
            "Cedell Alexander",
            "Bob Alverson",
            "Paul Bottorff",
            "Adrian Caulfield",
            "Mark Handley",
            "Cathy Huang",
            "Costin Raiciu",
            "Abdul Kabbani",
            "Eugene Opsasnick",
            "Rong Pan",
            "Adee Ran",
            "Rip Sohan"
        ],
        "categories": [
            "cs.NI",
            "cs.AR",
            "cs.DC",
            "cs.OS",
            "cs.PF"
        ],
        "submit_date": "2025-08-12"
    },
    "http://arxiv.org/abs/2508.08744v2": {
        "id": "http://arxiv.org/abs/2508.08744v2",
        "title": "Scalable Graph Indexing using GPUs for Approximate Nearest Neighbor Search",
        "link": "http://arxiv.org/abs/2508.08744v2",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-13",
        "tldr": "",
        "abstract": "Approximate nearest neighbor search (ANNS) in high-dimensional vector spaces has a wide range of real-world applications. Numerous methods have been proposed to handle ANNS efficiently, while graph-based indexes have gained prominence due to their high accuracy and efficiency. However, the indexing overhead of graph-based indexes remains substantial. With exponential growth in data volume and increasing demands for dynamic index adjustments, this overhead continues to escalate, posing a critical challenge. In this paper, we introduce Tagore, a fast library accelerated by GPUs for graph indexing, which has powerful capabilities of constructing refinement-based graph indexes such as NSG and Vamana. We first introduce GNN-Descent, a GPU-specific algorithm for efficient k-Nearest Neighbor (k-NN) graph initialization. GNN-Descent speeds up the similarity comparison by a two-phase descent procedure and enables highly parallelized neighbor updates. Next, aiming to support various k-NN graph pruning strategies, we formulate a universal computing procedure termed CFS and devise two generalized GPU kernels for parallel processing complex dependencies in neighbor relationships. For large-scale datasets exceeding GPU memory capacity, we propose an asynchronous GPU-CPU-disk indexing framework with a cluster-aware caching mechanism to minimize the I/O pressure on the disk. Extensive experiments on 7 real-world datasets exhibit that Tagore achieves 1.32x-112.79x speedup while maintaining the index quality.",
        "authors": [
            "Zhonggen Li",
            "Xiangyu Ke",
            "Yifan Zhu",
            "Bocheng Yu",
            "Baihua Zheng",
            "Yunjun Gao"
        ],
        "categories": [
            "cs.DB",
            "cs.DC"
        ],
        "submit_date": "2025-08-12"
    },
    "http://arxiv.org/abs/2508.08740v1": {
        "id": "http://arxiv.org/abs/2508.08740v1",
        "title": "Two for One, One for All: Deterministic LDC-based Robust Computation in Congested Clique",
        "link": "http://arxiv.org/abs/2508.08740v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-13",
        "tldr": "",
        "abstract": "We design a deterministic compiler that makes any computation in the Congested Clique model robust to a constant fraction $α<1$ of adversarial crash faults. In particular, we show how a network of $n$ nodes can compute any circuit of depth $d$, width $ω$, and gate total fan $Δ$, in $d\\cdot\\lceil\\fracω{n^2}+\\fracΔ{n}\\rceil\\cdot 2^{O(\\sqrt{\\log{n}}\\log\\log{n})}$ rounds in such a faulty model. As a corollary, any $T$-round Congested Clique algorithm can be compiled into an algorithm that completes in $T^2 n^{o(1)}$ rounds in this model.   Our compiler obtains resilience to node crashes by coding information across the network, where we leverage locally-decodable codes (LDCs) to maintain a low complexity overhead, as these allow recovering the information needed at each computational step by querying only small parts of the codeword.   The main technical contribution is that because erasures occur in known locations, which correspond to crashed nodes, we can derandomize classical LDC constructions by deterministically selecting query sets that avoid sufficiently many erasures. Moreover, when decoding multiple codewords in parallel, our derandomization load-balances the queries per-node, thereby preventing congestion and maintaining a low round complexity.   Deterministic decoding of LDCs presents a new challenge: the adversary can target precisely the (few) nodes that are queried for decoding a certain codeword. We overcome this issue via an adaptive doubling strategy: if a decoding attempt for a codeword fails, the node doubles the number of its decoding attempts. Similarly, when the adversary crashes the decoding node itself, we replace it dynamically with two other non-crashed nodes. By carefully combining these two doubling processes, we overcome the challenges posed by the combination of a deterministic LDC with a worst case pattern of crashes.",
        "authors": [
            "Keren Censor-Hillel",
            "Orr Fischer",
            "Ran Gelles",
            "Pedro Soto"
        ],
        "categories": [
            "cs.DS",
            "cs.DC"
        ],
        "submit_date": "2025-08-12"
    },
    "http://arxiv.org/abs/2508.08712v3": {
        "id": "http://arxiv.org/abs/2508.08712v3",
        "title": "A Survey on Parallel Text Generation: From Parallel Decoding to Diffusion Language Models",
        "link": "http://arxiv.org/abs/2508.08712v3",
        "tags": [
            "inference",
            "optimization",
            "parallelization"
        ],
        "relevant": true,
        "indexed_date": "2025-08-13",
        "tldr": "Surveys parallel text generation methods to break autoregressive bottlenecks. Focuses on categorizing and analyzing AR-based and Non-AR techniques. Highlights techniques achieving up to several times speedup over sequential decoding.",
        "abstract": "As text generation has become a core capability of modern Large Language Models (LLMs), it underpins a wide range of downstream applications. However, most existing LLMs rely on autoregressive (AR) generation, producing one token at a time based on previously generated context-resulting in limited generation speed due to the inherently sequential nature of the process. To address this challenge, an increasing number of researchers have begun exploring parallel text generation-a broad class of techniques aimed at breaking the token-by-token generation bottleneck and improving inference efficiency. Despite growing interest, there remains a lack of comprehensive analysis on what specific techniques constitute parallel text generation and how they improve inference performance. To bridge this gap, we present a systematic survey of parallel text generation methods. We categorize existing approaches into AR-based and Non-AR-based paradigms, and provide a detailed examination of the core techniques within each category. Following this taxonomy, we assess their theoretical trade-offs in terms of speed, quality, and efficiency, and examine their potential for combination and comparison with alternative acceleration strategies. Finally, based on our findings, we highlight recent advancements, identify open challenges, and outline promising directions for future research in parallel text generation. We have also created a GitHub repository for indexing relevant papers and open resources available at https://github.com/zhanglingzhe0820/Awesome-Parallel-Text-Generation.",
        "authors": [
            "Lingzhe Zhang",
            "Liancheng Fang",
            "Chiming Duan",
            "Minghua He",
            "Leyi Pan",
            "Pei Xiao",
            "Shiyu Huang",
            "Yunpeng Zhai",
            "Xuming Hu",
            "Philip S. Yu",
            "Aiwei Liu"
        ],
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.DC"
        ],
        "submit_date": "2025-08-12"
    },
    "http://arxiv.org/abs/2508.08469v1": {
        "id": "http://arxiv.org/abs/2508.08469v1",
        "title": "Vector-Centric Machine Learning Systems: A Cross-Stack Approach",
        "link": "http://arxiv.org/abs/2508.08469v1",
        "tags": [
            "RAG",
            "serving",
            "quantization"
        ],
        "relevant": true,
        "indexed_date": "2025-08-13",
        "tldr": "Proposes cross-stack optimizations for vector-centric ML systems like RAG and recommender systems. Introduces PipeRAG, RAGO, Chameleon for RAG serving, FANNS/Falcon for vector search co-design, and MicroRec/FleetRec for recommender efficiency. Achieves significant improvements in serving efficiency for heterogeneous components.",
        "abstract": "Today, two major trends are shaping the evolution of ML systems. First, modern AI systems are becoming increasingly complex, often integrating components beyond the model itself. A notable example is Retrieval-Augmented Generation (RAG), which incorporates not only multiple models but also vector databases, leading to heterogeneity in both system components and underlying hardware. Second, with the end of Moore's Law, achieving high system efficiency is no longer feasible without accounting for the rapid evolution of the hardware landscape.   Building on the observations above, this thesis adopts a cross-stack approach to improving ML system efficiency, presenting solutions that span algorithms, systems, and hardware. First, it introduces several pioneering works about RAG serving efficiency across the computing stack. PipeRAG focuses on algorithm-level improvements, RAGO introduces system-level optimizations, and Chameleon explores heterogeneous accelerator systems for RAG. Second, this thesis investigates algorithm-hardware co-design for vector search. Specifically, FANNS and Falcon optimize quantization-based and graph-based vector search, the two most popular paradigms of retrieval algorithms. Third, this thesis addresses the serving efficiency of recommender systems, another example of vector-centric ML systems, where the memory-intensive lookup operations on embedding vector tables often represent a major performance bottleneck. MicroRec and FleetRec propose solutions at the hardware and system levels, respectively, optimizing both data movement and computation to enhance the efficiency of large-scale recommender models.",
        "authors": [
            "Wenqi Jiang"
        ],
        "categories": [
            "cs.DB",
            "cs.AR",
            "cs.DC",
            "cs.PF"
        ],
        "submit_date": "2025-08-11"
    },
    "http://arxiv.org/abs/2508.08448v1": {
        "id": "http://arxiv.org/abs/2508.08448v1",
        "title": "Towards Efficient and Practical GPU Multitasking in the Era of LLM",
        "link": "http://arxiv.org/abs/2508.08448v1",
        "tags": [
            "training",
            "serving",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-08-13",
        "tldr": "Proposes a GPU resource management layer to enable multitasking for diverse AI workloads, addressing inefficiency in modern hardware. Envisions a solution analogous to a CPU OS for GPU resource sharing and management, aiming to improve utilization and efficiency in large-scale model serving.",
        "abstract": "GPU singletasking is becoming increasingly inefficient and unsustainable as hardware capabilities grow and workloads diversify. We are now at an inflection point where GPUs must embrace multitasking, much like CPUs did decades ago, to meet the demands of modern AI workloads. In this work, we highlight the key requirements for GPU multitasking, examine prior efforts, and discuss why they fall short. To advance toward efficient and practical GPU multitasking, we envision a resource management layer, analogous to a CPU operating system, to handle various aspects of GPU resource management and sharing. We outline the challenges and potential solutions, and hope this paper inspires broader community efforts to build the next-generation GPU compute paradigm grounded in multitasking.",
        "authors": [
            "Jiarong Xing",
            "Yifan Qiao",
            "Simon Mo",
            "Xingqi Cui",
            "Gur-Eyal Sela",
            "Yang Zhou",
            "Joseph Gonzalez",
            "Ion Stoica"
        ],
        "categories": [
            "cs.OS",
            "cs.DC"
        ],
        "submit_date": "2025-08-11"
    },
    "http://arxiv.org/abs/2508.08433v1": {
        "id": "http://arxiv.org/abs/2508.08433v1",
        "title": "Extremely Scalable Distributed Computation of Contour Trees via Pre-Simplification",
        "link": "http://arxiv.org/abs/2508.08433v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-13",
        "tldr": "",
        "abstract": "Contour trees offer an abstract representation of the level set topology in scalar fields and are widely used in topological data analysis and visualization. However, applying contour trees to large-scale scientific datasets remains challenging due to scalability limitations. Recent developments in distributed hierarchical contour trees have addressed these challenges by enabling scalable computation across distributed systems. Building on these structures, advanced analytical tasks -- such as volumetric branch decomposition and contour extraction -- have been introduced to facilitate large-scale scientific analysis. Despite these advancements, such analytical tasks substantially increase memory usage, which hampers scalability. In this paper, we propose a pre-simplification strategy to significantly reduce the memory overhead associated with analytical tasks on distributed hierarchical contour trees. We demonstrate enhanced scalability through strong scaling experiments, constructing the largest known contour tree -- comprising over half a trillion nodes with complex topology -- in under 15 minutes on a dataset containing 550 billion elements.",
        "authors": [
            "Mingzhe Li",
            "Hamish Carr",
            "Oliver Rübel",
            "Bei Wang",
            "Gunther H. Weber"
        ],
        "categories": [
            "cs.CG",
            "cs.DC",
            "cs.DS"
        ],
        "submit_date": "2025-08-11"
    },
    "http://arxiv.org/abs/2508.08396v1": {
        "id": "http://arxiv.org/abs/2508.08396v1",
        "title": "XDMA: A Distributed, Extensible DMA Architecture for Layout-Flexible Data Movements in Heterogeneous Multi-Accelerator SoCs",
        "link": "http://arxiv.org/abs/2508.08396v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-13",
        "tldr": "",
        "abstract": "As modern AI workloads increasingly rely on heterogeneous accelerators, ensuring high-bandwidth and layout-flexible data movements between accelerator memories has become a pressing challenge. Direct Memory Access (DMA) engines promise high bandwidth utilization for data movements but are typically optimal only for contiguous memory access, thus requiring additional software loops for data layout transformations. This, in turn, leads to excessive control overhead and underutilized on-chip interconnects. To overcome this inefficiency, we present XDMA, a distributed and extensible DMA architecture that enables layout-flexible data movements with high link utilization. We introduce three key innovations: (1) a data streaming engine as XDMA Frontend, replacing software address generators with hardware ones; (2) a distributed DMA architecture that maximizes link utilization and separates configuration from data transfer; (3) flexible plugins for XDMA enabling on-the-fly data manipulation during data transfers. XDMA demonstrates up to 151.2x/8.2x higher link utilization than software-based implementations in synthetic workloads and achieves 2.3x average speedup over accelerators with SoTA DMA in real-world applications. Our design incurs <2% area overhead over SoTA DMA solutions while consuming 17% of system power. XDMA proves that co-optimizing memory access, layout transformation, and interconnect protocols is key to unlocking heterogeneous multi-accelerator SoC performance.",
        "authors": [
            "Fanchen Kong",
            "Yunhao Deng",
            "Xiaoling Yi",
            "Ryan Antonio",
            "Marian Verhelst"
        ],
        "categories": [
            "cs.AR",
            "cs.DC"
        ],
        "submit_date": "2025-08-11"
    },
    "http://arxiv.org/abs/2508.08438v1": {
        "id": "http://arxiv.org/abs/2508.08438v1",
        "title": "Selective KV-Cache Sharing to Mitigate Timing Side-Channels in LLM Inference",
        "link": "http://arxiv.org/abs/2508.08438v1",
        "tags": [
            "serving",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-08-13",
        "tldr": "Addresses timing side-channel vulnerabilities in globally shared KV-cache for LLM inference. Proposes SafeKV, a privacy-aware cache management system using hybrid detection, multi-tier memory indexing, and entropy monitoring. Achieves 40.58% better time-to-first-token and 2.66× throughput vs. isolation methods while mitigating 94–97% of attacks.",
        "abstract": "Global KV-cache sharing has emerged as a key optimization for accelerating large language model (LLM) inference. However, it exposes a new class of timing side-channel attacks, enabling adversaries to infer sensitive user inputs via shared cache entries. Existing defenses, such as per-user isolation, eliminate leakage but degrade performance by up to 38.9% in time-to-first-token (TTFT), making them impractical for high-throughput deployment. To address this gap, we introduce SafeKV (Secure and Flexible KV Cache Sharing), a privacy-aware KV-cache management framework that selectively shares non-sensitive entries while confining sensitive content to private caches. SafeKV comprises three components: (i) a hybrid, multi-tier detection pipeline that integrates rule-based pattern matching, a general-purpose privacy detector, and context-aware validation; (ii) a unified radix-tree index that manages public and private entries across heterogeneous memory tiers (HBM, DRAM, SSD); and (iii) entropy-based access monitoring to detect and mitigate residual information leakage. Our evaluation shows that SafeKV mitigates 94% - 97% of timing-based side-channel attacks. Compared to per-user isolation method, SafeKV improves TTFT by up to 40.58% and throughput by up to 2.66X across diverse LLMs and workloads. SafeKV reduces cache-induced TTFT overhead from 50.41% to 11.74% on Qwen3-235B. By combining fine-grained privacy control with high cache reuse efficiency, SafeKV reclaims the performance advantages of global sharing while providing robust runtime privacy guarantees for LLM inference.",
        "authors": [
            "Kexin Chu",
            "Zecheng Lin",
            "Dawei Xiang",
            "Zixu Shen",
            "Jianchang Su",
            "Cheng Chu",
            "Yiwei Yang",
            "Wenhui Zhang",
            "Wenfei Wu",
            "Wei Zhang"
        ],
        "categories": [
            "cs.CR",
            "cs.LG",
            "cs.OS"
        ],
        "submit_date": "2025-08-11"
    },
    "http://arxiv.org/abs/2508.08064v2": {
        "id": "http://arxiv.org/abs/2508.08064v2",
        "title": "On the Operational Resilience of CBDC: Threats and Prospects of Formal Validation for Offline Payments",
        "link": "http://arxiv.org/abs/2508.08064v2",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-12",
        "tldr": "",
        "abstract": "Information and communication technologies are by now employed in most activities, including economics and finance. Despite the extraordinary power of modern computers and the vast amount of available memory, some results of theoretical computer science imply the impossibility of certifying software quality in general. With the exception of safety-critical systems, this has primarily concerned the information processed by confined systems, with limited socio-economic consequences. In the emerging era of technologies for exchanging digital money and tokenized assets over the Internet, such as in particular central bank digital currencies (CBDCs), even a minor bug could trigger a financial collapse. Although the aforementioned impossibility results cannot be overcome in an absolute sense, there exist formal methods that can provide correctness assertions for computing systems. We advocate their use to validate the operational resilience of software infrastructures enabling CBDCs, with special emphasis on offline payments as they constitute a very critical issue.",
        "authors": [
            "Marco Bernardo",
            "Federico Calandra",
            "Andrea Esposito",
            "Francesco Fabris"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-11"
    },
    "http://arxiv.org/abs/2508.07934v1": {
        "id": "http://arxiv.org/abs/2508.07934v1",
        "title": "Performance Evaluation of Brokerless Messaging Libraries",
        "link": "http://arxiv.org/abs/2508.07934v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-12",
        "tldr": "",
        "abstract": "Messaging systems are essential for efficiently transferring large volumes of data, ensuring rapid response times and high-throughput communication. The state-of-the-art on messaging systems mainly focuses on the performance evaluation of brokered messaging systems, which use an intermediate broker to guarantee reliability and quality of service. However, over the past decade, brokerless messaging systems have emerged, eliminating the single point of failure and trading off reliability guarantees for higher performance. Still, the state-of-the-art on evaluating the performance of brokerless systems is scarce. In this work, we solely focus on brokerless messaging systems. First, we perform a qualitative analysis of several possible candidates, to find the most promising ones. We then design and implement an extensive open-source benchmarking suite to systematically and fairly evaluate the performance of the chosen libraries, namely, ZeroMQ, NanoMsg, and NanoMsg-Next-Generation (NNG). We evaluate these libraries considering different metrics and workload conditions, and provide useful insights into their limitations. Our analysis enables practitioners to select the most suitable library for their requirements.",
        "authors": [
            "Lorenzo La Corte",
            "Syed Aftab Rashid",
            "Andrei-Marian Dan"
        ],
        "categories": [
            "cs.DC",
            "cs.NI"
        ],
        "submit_date": "2025-08-11"
    },
    "http://arxiv.org/abs/2508.07756v1": {
        "id": "http://arxiv.org/abs/2508.07756v1",
        "title": "Towards Lock Modularization for Heterogeneous Environments",
        "link": "http://arxiv.org/abs/2508.07756v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-12",
        "tldr": "",
        "abstract": "Modern hardware environments are becoming increasingly heterogeneous, leading to the emergence of applications specifically designed to exploit this heterogeneity. Efficiently adopting locks in these applications poses distinct challenges. The uneven distribution of resources in such environments can create bottlenecks for lock operations, severely hindering application performance. Existing solutions are often tailored to specific types of hardware, which underutilizes resources on other components within heterogeneous environments.   This paper introduces a new design principle: decomposing locks across hardware components to fully utilize unevenly distributed resources in heterogeneous environments. Following this principle, we propose lock modularization, a systematic approach that decomposes a lock into independent modules and assigns them to appropriate hardware components. This approach aligns the resource requirements of lock modules with the attributes of specific hardware components, maximizing strengths while minimizing weaknesses.",
        "authors": [
            "Hanze Zhang",
            "Rong Chen",
            "Haibo Chen"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-11"
    },
    "http://arxiv.org/abs/2508.07744v1": {
        "id": "http://arxiv.org/abs/2508.07744v1",
        "title": "Over-the-Top Resource Broker System for Split Computing: An Approach to Distribute Cloud Computing Infrastructure",
        "link": "http://arxiv.org/abs/2508.07744v1",
        "tags": [
            "networking",
            "offloading",
            "edge"
        ],
        "relevant": true,
        "indexed_date": "2025-08-12",
        "tldr": "Proposes an over-the-top resource broker for split computing to dynamically allocate resources across cloud-edge infrastructure. Abstracts provider complexities for uniform service deployment. Proof-of-concept achieves seamless integration and tailored performance in 6G scenarios.",
        "abstract": "6G network architectures will usher in a wave of innovative services and capabilities, introducing concepts like split computing and dynamic processing nodes. This implicates a paradigm where accessing resources seamlessly aligns with diverse processing node characteristics, ensuring a uniform interface. In this landscape, the identity of the operator becomes inconsequential, paving the way for a collaborative ecosystem where multiple providers contribute to a shared pool of resources. At the core of this vision is the guarantee of specific performance parameters, precisely tailored to the location and service requirements. A consistent layer, as the abstraction of the complexities of different infrastructure providers, is needed to simplify service deployment. One promising approach is the introduction of an over-the-top broker for resource allocation, which streamlines the integration of these services into the network and cloud infrastructure of the future. This paper explores the role of the broker in two split computing scenarios. By abstracting the complexities of various infrastructures, the broker proves to be a versatile solution applicable not only to cloud environments but also to networks and beyond. Additionally, a detailed discussion of a proof-of-concept implementation provides insights into the broker's actual architectural framework.",
        "authors": [
            "Ingo Friese",
            "Jochen Klaffer",
            "Mandy Galkow-Schneider",
            "Sergiy Melnyk",
            "Qiuheng Zhou",
            "Hans Dieter Schotten"
        ],
        "categories": [
            "cs.DC",
            "cs.NI",
            "eess.SP"
        ],
        "submit_date": "2025-08-11"
    },
    "http://arxiv.org/abs/2508.07703v1": {
        "id": "http://arxiv.org/abs/2508.07703v1",
        "title": "Perpetual exploration in anonymous synchronous networks with a Byzantine black hole",
        "link": "http://arxiv.org/abs/2508.07703v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-12",
        "tldr": "",
        "abstract": "In this paper, we investigate: ``How can a group of initially co-located mobile agents perpetually explore an unknown graph, when one stationary node occasionally behaves maliciously, under an adversary's control?'' We call this node a ``Byzantine black hole (BBH)'' and at any given round it may choose to destroy all visiting agents, or none. This subtle power can drastically undermine classical exploration strategies designed for an always active black hole. We study this perpetual exploration problem in the presence of at most one BBH, without initial knowledge of the network size. Since the underlying graph may be 1-connected, perpetual exploration of the entire graph may be infeasible. We thus define two variants: \\pbmPerpExpl\\ and \\pbmPerpExplHome. In the former, the agents are tasked to perform perpetual exploration of at least one component, obtained after the exclusion of the BBH. In the latter, the agents are tasked to perform perpetual exploration of the component which contains the \\emph{home} node, where agents are initially co-located. Naturally, \\pbmPerpExplHome\\ is a special case of \\pbmPerpExpl. Agents operate under a synchronous scheduler and communicate in a face-to-face model. Our goal is to determine the minimum number of agents necessary and sufficient to solve these problems. In acyclic networks, we obtain optimal algorithms that solve \\pbmPerpExpl\\ with $4$ agents, and \\pbmPerpExplHome\\ with $6$ agents in trees. The lower bounds hold even in path graphs. In general graphs, we give a non-trivial lower bound of $2Δ-1$ agents for \\pbmPerpExpl, and an upper bound of $3Δ+3$ agents for \\pbmPerpExplHome. To our knowledge, this is the first study of a black-hole variant in arbitrary networks without initial topological knowledge.",
        "authors": [
            "Adri Bhattacharya",
            "Pritam Goswami",
            "Evangelos Bampas",
            "Partha Sarathi Mandal"
        ],
        "categories": [
            "cs.DC",
            "cs.MA"
        ],
        "submit_date": "2025-08-11"
    },
    "http://arxiv.org/abs/2508.07640v1": {
        "id": "http://arxiv.org/abs/2508.07640v1",
        "title": "Taming Cold Starts: Proactive Serverless Scheduling with Model Predictive Control",
        "link": "http://arxiv.org/abs/2508.07640v1",
        "tags": [
            "autoscaling"
        ],
        "relevant": true,
        "indexed_date": "2025-08-12",
        "tldr": "Addresses serverless cold start delays for latency-sensitive workloads. Proposes an MPC-based scheduler that jointly optimizes container prewarming and request dispatching using invocation forecasts. Achieves 85% lower tail latency and 34% reduced resource usage versus baselines.",
        "abstract": "Serverless computing has transformed cloud application deployment by introducing a fine-grained, event-driven execution model that abstracts away infrastructure management. Its on-demand nature makes it especially appealing for latency-sensitive and bursty workloads. However, the cold start problem, i.e., where the platform incurs significant delay when provisioning new containers, remains the Achilles' heel of such platforms.   This paper presents a predictive serverless scheduling framework based on Model Predictive Control to proactively mitigate cold starts, thereby improving end-to-end response time. By forecasting future invocations, the controller jointly optimizes container prewarming and request dispatching, improving latency while minimizing resource overhead.   We implement our approach on Apache OpenWhisk, deployed on a Kubernetes-based testbed. Experimental results using real-world function traces and synthetic workloads demonstrate that our method significantly outperforms state-of-the-art baselines, achieving up to 85% lower tail latency and a 34% reduction in resource usage.",
        "authors": [
            "Chanh Nguyen",
            "Monowar Bhuyan",
            "Erik Elmroth"
        ],
        "categories": [
            "cs.DC",
            "cs.PF"
        ],
        "submit_date": "2025-08-11"
    },
    "http://arxiv.org/abs/2508.07605v2": {
        "id": "http://arxiv.org/abs/2508.07605v2",
        "title": "Coordinated Power Management on Heterogeneous Systems",
        "link": "http://arxiv.org/abs/2508.07605v2",
        "tags": [
            "training",
            "offline",
            "hardware"
        ],
        "relevant": true,
        "indexed_date": "2025-08-12",
        "tldr": "Addresses the challenge of performance prediction for energy-efficient training on CPU-GPU systems. OPEN uses collaborative filtering to combine offline model building with lightweight online profiling. Achieves 98.29% prediction accuracy, reducing profiling costs for power-aware computing.",
        "abstract": "Performance prediction is essential for energy-efficient computing in heterogeneous computing systems that integrate CPUs and GPUs. However, traditional performance modeling methods often rely on exhaustive offline profiling, which becomes impractical due to the large setting space and the high cost of profiling large-scale applications. In this paper, we present OPEN, a framework consists of offline and online phases. The offline phase involves building a performance predictor and constructing an initial dense matrix. In the online phase, OPEN performs lightweight online profiling, and leverages the performance predictor with collaborative filtering to make performance prediction. We evaluate OPEN on multiple heterogeneous systems, including those equipped with A100 and A30 GPUs. Results show that OPEN achieves prediction accuracy up to 98.29\\%. This demonstrates that OPEN effectively reduces profiling cost while maintaining high accuracy, making it practical for power-aware performance modeling in modern HPC environments. Overall, OPEN provides a lightweight solution for performance prediction under power constraints, enabling better runtime decisions in power-aware computing environments.",
        "authors": [
            "Zhong Zheng",
            "Zhiling Lan",
            "Xingfu Wu",
            "Valerie E. Taylor",
            "Michael E. Papka"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-11"
    },
    "http://arxiv.org/abs/2508.07317v1": {
        "id": "http://arxiv.org/abs/2508.07317v1",
        "title": "An Experimental Exploration of In-Memory Computing for Multi-Layer Perceptrons",
        "link": "http://arxiv.org/abs/2508.07317v1",
        "tags": [
            "inference",
            "hardware",
            "quantization"
        ],
        "relevant": true,
        "indexed_date": "2025-08-12",
        "tldr": "Investigates processing-in-memory (PiM) for accelerating neural network inference. Uses UPMEM PiM system to implement MLPs, comparing against CPU and GPU. Achieves up to 259× speedup over CPU for large batch inference and kernel times under 3ms.",
        "abstract": "In modern computer architectures, the performance of many memory-bound workloads (e.g., machine learning, graph processing, databases) is limited by the data movement bottleneck that emerges when transferring large amounts of data between the main memory and the central processing unit (CPU). Processing-in-memory is an emerging computing paradigm that aims to alleviate this data movement bottleneck by performing computation close to or within the memory units, where data resides. One example of a prevalent workload whose performance is bound by the data movement bottleneck is the training and inference process of artificial neural networks. In this work, we analyze the potential of modern general-purpose PiM architectures to accelerate neural networks. To this end, we selected the UPMEM PiM system, the first commercially available real-world general-purpose PiM architecture. We compared the implementation of multilayer perceptrons (MLPs) in PiM with a sequential baseline running on an Intel Xeon CPU. The UPMEM implementation achieves up to $259\\times$ better performance for inference of large batch sizes when compared against the CPU that exploits the size of the available PiM memory. Additionally, two smaller MLPs were implemented using UPMEM's working SRAM (WRAM), a scratchpad memory, to evaluate their performance against a low-power Nvidia Jetson graphics processing unit (GPU), providing further insights into the efficiency of UPMEM's PiM for neural network inference. Results show that using WRAM achieves kernel execution times for MLP inference of under $3$ ms, which is within the same order of magnitude as low-power GPUs.",
        "authors": [
            "Pedro Carrinho",
            "Hamid Moghadaspour",
            "Oscar Ferraz",
            "João Dinis Ferreira",
            "Yann Falevoz",
            "Vitor Silva",
            "Gabriel Falcao"
        ],
        "categories": [
            "cs.DC",
            "eess.SP"
        ],
        "submit_date": "2025-08-10"
    },
    "http://arxiv.org/abs/2508.07193v2": {
        "id": "http://arxiv.org/abs/2508.07193v2",
        "title": "FlashMP: Fast Discrete Transform-Based Solver for Preconditioning Maxwell's Equations on GPUs",
        "link": "http://arxiv.org/abs/2508.07193v2",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-12",
        "tldr": "",
        "abstract": "Efficiently solving large-scale linear systems is a critical challenge in electromagnetic simulations, particularly when using the Crank-Nicolson Finite-Difference Time-Domain (CN-FDTD) method. Existing iterative solvers are commonly employed to handle the resulting sparse systems but suffer from slow convergence due to the ill-conditioned nature of the double-curl operator. Approximate preconditioners, like Successive Over-Relaxation (SOR) and Incomplete LU decomposition (ILU), provide insufficient convergence, while direct solvers are impractical due to excessive memory requirements. To address this, we propose FlashMP, a novel preconditioning system that designs a subdomain exact solver based on discrete transforms. FlashMP provides an efficient GPU implementation that achieves multi-GPU scalability through domain decomposition. Evaluations on AMD MI60 GPU clusters (up to 1000 GPUs) show that FlashMP reduces iteration counts by up to 16x and achieves speedups of 2.5x to 4.9x compared to baseline implementations in state-of-the-art libraries such as Hypre. Weak scalability tests show parallel efficiencies up to 84.1%.",
        "authors": [
            "Haoyuan Zhang",
            "Yaqian Gao",
            "Xinxin Zhang",
            "Jialin Li",
            "Runfeng Jin",
            "Yidong Chen",
            "Feng Zhang",
            "Wu Yuan",
            "Wenpeng Ma",
            "Shan Liang",
            "Jian Zhang",
            "Zhonghua Lu"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-10"
    },
    "http://arxiv.org/abs/2508.07071v2": {
        "id": "http://arxiv.org/abs/2508.07071v2",
        "title": "The Fused Kernel Library: A C++ API to Develop Highly-Efficient GPU Libraries",
        "link": "http://arxiv.org/abs/2508.07071v2",
        "tags": [
            "kernel",
            "training",
            "inference"
        ],
        "relevant": true,
        "indexed_date": "2025-08-12",
        "tldr": "This paper presents a GPU library methodology for automatic kernel fusion of arbitrary operator combinations using C++17 metaprogramming, optimizing resource utilization by reducing off-chip memory access. Achieves 2x to 1000x speedups in benchmarks.",
        "abstract": "Existing GPU libraries often struggle to fully exploit the parallel resources and on-chip memory (SRAM) of GPUs when chaining multiple GPU functions as individual kernels. While Kernel Fusion (KF) techniques like Horizontal Fusion (HF) and Vertical Fusion (VF) can mitigate this, current library implementations often require library developers to manually create fused kernels. Hence, library users rely on limited sets of pre-compiled or template-based fused kernels. This limits the use cases that can benefit from HF and VF and increases development costs. In order to solve these issues, we present a novel methodology for building GPU libraries that enables automatic on-demand HF and VF for arbitrary combinations of GPU library functions. Our methodology defines reusable, fusionable components that users combine via high-level programming interfaces. Leveraging C++17 metaprogramming features available in compilers like nvcc, our methodology generates a single and optimized fused kernel tailored to the user's specific sequence of operations at compile time, without needing a custom compiler or manual development and pre-compilation of kernel combinations. This approach abstracts low-level GPU complexities while maximizing GPU resource utilization and keeping intermediate data in SRAM. We provide an open-source implementation demonstrating significant speedups compared to traditional libraries in various benchmarks, validating the effectiveness of this methodology for improving GPU performance in the range of 2x to more than 1000x, while preserving high-level programmability.",
        "authors": [
            "Oscar Amoros",
            "Albert Andaluz",
            "Johnny Nunez",
            "Antonio J. Pena"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-09"
    },
    "http://arxiv.org/abs/2508.06949v1": {
        "id": "http://arxiv.org/abs/2508.06949v1",
        "title": "Convergence Sans Synchronization",
        "link": "http://arxiv.org/abs/2508.06949v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-12",
        "tldr": "",
        "abstract": "We currently see a steady rise in the usage and size of multiprocessor systems, and so the community is evermore interested in developing fast parallel processing algorithms. However, most algorithms require a synchronization mechanism, which is costly in terms of computational resources and time. If an algorithm can be executed in asynchrony, then it can use all the available computation power, and the nodes can execute without being scheduled or locked. However, to show that an algorithm guarantees convergence in asynchrony, we need to generate the entire global state transition graph and check for the absence of cycles. This takes time exponential in the size of the global state space. In this dissertation, we present a theory that explains the necessary and sufficient properties of a multiprocessor algorithm that guarantees convergence even without synchronization. We develop algorithms for various problems that do not require synchronization. Additionally, we show for several existing algorithms that they can be executed without any synchronization mechanism. A significant theoretical benefit of our work is in proving that an algorithm can converge even in asynchrony. Our theory implies that we can make such conclusions about an algorithm, by only showing that the local state transition graph of a computing node forms a partial order, rather than generating the entire global state space and determining the absence of cycles in it. Thus, the complexity of rendering such proofs, formal or social, is phenomenally reduced. Experiments show a significant reduction in time taken to converge, when we compare the execution time of algorithms in the literature versus the algorithms that we design. We get similar results when we run an algorithm, that guarantees convergence in asynchrony, under a scheduler versus in asynchrony.",
        "authors": [
            "Arya Tanmay Gupta"
        ],
        "categories": [
            "cs.DC",
            "cs.DM"
        ],
        "submit_date": "2025-08-09"
    },
    "http://arxiv.org/abs/2508.06948v1": {
        "id": "http://arxiv.org/abs/2508.06948v1",
        "title": "Kairos: Low-latency Multi-Agent Serving with Shared LLMs and Excessive Loads in the Public Cloud",
        "link": "http://arxiv.org/abs/2508.06948v1",
        "tags": [
            "serving",
            "autoscaling",
            "RL"
        ],
        "relevant": true,
        "indexed_date": "2025-08-12",
        "tldr": "Addresses high latency in multi-agent LLM serving caused by shared model loads. Proposes Kairos, featuring a workflow orchestrator, priority scheduler, and memory-aware dispatcher. Reduces end-to-end latency by up to 28.4% compared to baselines.",
        "abstract": "Multi-agent applications utilize the advanced capabilities of large language models (LLMs) for intricate task completion through agent collaboration in a workflow. Under this situation, requests from different agents usually access the same shared LLM to perform different kinds of tasks, forcing the shared LLM to suffer excessive loads. However, existing works have low serving performance for these multi-agent applications, mainly due to the ignorance of inter-agent latency and resource differences for request scheduling. We therefore propose Kairos, a multi-agent orchestration system that optimizes end-to-end latency for multi-agent applications. Kairos consists of a workflow orchestrator, a workflow-aware priority scheduler, and a memory-aware dispatcher. The orchestrator collects agent-specific information for online workflow analysis. The scheduler decides the serving priority of the requests based on their latency characteristics to reduce the overall queuing. The dispatcher dispatches the requests to different LLM instances based on their memory demands to avoid GPU overloading. Experimental results show that Kairos reduces end-to-end latency by 17.8% to 28.4% compared to state-of-the-art works.",
        "authors": [
            "Jinyuan Chen",
            "Jiuchen Shi",
            "Quan Chen",
            "Minyi Guo"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-09"
    },
    "http://arxiv.org/abs/2508.06526v1": {
        "id": "http://arxiv.org/abs/2508.06526v1",
        "title": "PiKV: KV Cache Management System for Mixture of Experts",
        "link": "http://arxiv.org/abs/2508.06526v1",
        "tags": [
            "serving",
            "MoE",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-08-12",
        "tldr": "Addresses KV cache overhead in MoE inference by introducing PiKV—a distributed cache framework using expert-sharded storage, token-to-KV routing, and adaptive compression. Reduces KV cache memory usage by 48% and accelerates decoding throughput by 2.9×.",
        "abstract": "As large language models continue to scale up in both size and context length, the memory and communication cost of key-value (KV) cache storage has become a major bottleneck in multi-GPU and multi-node inference. While MoE-based architectures sparsify computation across experts, the corresponding KV caches remain dense and globally synchronized, resulting in significant overhead.   We introduce \\textbf{PiKV}, a parallel and distributed KV cache serving framework tailored for MoE architecture. PiKV leverages \\textit{expert-sharded KV storage} to partition caches across GPUs, \\textit{PiKV routing} to reduce token-to-KV access, and a \\textit{PiKV Scheduling} to adaptively retain query-relevant entries. To further reduce memory usage, PiKV integrates \\textit{PiKV Compression} modules the caching pipeline for acceleration.   PiKV is recently publicly available as an open-source software library: \\href{https://github.com/NoakLiu/PiKV}{https://github.com/NoakLiu/PiKV}. Experiments details is recorded at: \\href{https://github.com/NoakLiu/PiKV/blob/main/downstream_tasks/README.md}{https://github.com/NoakLiu/PiKV/Experimental\\_Results}. We also have PiKV integrated with Nvidia kvpress for acceleration, details see \\href{https://github.com/NoakLiu/PiKVpress}{https://github.com/NoakLiu/PiKVpress}. PiKV is still a living project, aiming to become a comprehesive KV Cache management system for MoE Architectures.",
        "authors": [
            "Dong Liu",
            "Yanxuan Yu",
            "Ben Lengerich",
            "Ying Nian Wu",
            "Xuhong Wang"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.AR"
        ],
        "submit_date": "2025-08-02"
    },
    "http://arxiv.org/abs/2508.08068v1": {
        "id": "http://arxiv.org/abs/2508.08068v1",
        "title": "Fully-Fluctuating Participation in Sleepy Consensus",
        "link": "http://arxiv.org/abs/2508.08068v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-12",
        "tldr": "",
        "abstract": "Proof-of-work allows Bitcoin to boast security amidst arbitrary fluctuations in participation of miners throughout time, so long as, at any point in time, a majority of hash power is honest. In recent years, however, the pendulum has shifted in favor of proof-of-stake-based consensus protocols. There, the sleepy model is the most prominent model for handling fluctuating participation of nodes. However, to date, no protocol in the sleepy model rivals Bitcoin in its robustness to drastic fluctuations in participation levels, with state-of-the-art protocols making various restrictive assumptions. In this work, we present a new adversary model, called external adversary. Intuitively, in our model, corrupt nodes do not divulge information about their secret keys. In this model, we show that protocols in the sleepy model can meaningfully claim to remain secure against fully fluctuating participation, without compromising efficiency or corruption resilience. Our adversary model is quite natural, and arguably naturally captures the process via which malicious behavior arises in protocols, as opposed to traditional worst-case modeling. On top of which, the model is also theoretically appealing, circumventing a barrier established in a recent work of Malkhi, Momose, and Ren.",
        "authors": [
            "Yuval Efron",
            "Joachim Neu",
            "Toniann Pitassi"
        ],
        "categories": [
            "cs.CR",
            "cs.DC"
        ],
        "submit_date": "2025-08-11"
    },
    "http://arxiv.org/abs/2508.07879v1": {
        "id": "http://arxiv.org/abs/2508.07879v1",
        "title": "GPU-Accelerated Syndrome Decoding for Quantum LDPC Codes below the 63 $μ$s Latency Threshold",
        "link": "http://arxiv.org/abs/2508.07879v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-12",
        "tldr": "",
        "abstract": "This paper presents a GPU-accelerated decoder for quantum low-density parity-check (QLDPC) codes that achieves sub-$63$ $μ$s latency, below the surface code decoder's real-time threshold demonstrated on Google's Willow quantum processor. While surface codes have demonstrated below-threshold performance, the encoding rates approach zero as code distances increase, posing challenges for scalability. Recently proposed QLDPC codes, such as those by Panteleev and Kalachev, offer constant-rate encoding and asymptotic goodness but introduce higher decoding complexity. To address such limitation, this work presents a parallelized belief propagation decoder leveraging syndrome information on commodity GPU hardware. Parallelism was exploited to maximize performance within the limits of target latency, allowing decoding latencies under $50$ $μ$s for [[$784$, $24$, $24$]] codes and as low as $23.3$ $μ$s for smaller codes, meeting the tight timing constraints of superconducting qubit cycles. These results show that real-time, scalable decoding of asymptotically good quantum codes is achievable using widely available commodity hardware, advancing the feasibility of fault-tolerant quantum computation beyond surface codes.",
        "authors": [
            "Oscar Ferraz",
            "Bruno Coutinho",
            "Gabriel Falcao",
            "Marco Gomes",
            "Francisco A. Monteiro",
            "Vitor Silva"
        ],
        "categories": [
            "quant-ph",
            "cs.DC"
        ],
        "submit_date": "2025-08-11"
    },
    "http://arxiv.org/abs/2508.07505v1": {
        "id": "http://arxiv.org/abs/2508.07505v1",
        "title": "Enhancing Privacy in Decentralized Min-Max Optimization: A Differentially Private Approach",
        "link": "http://arxiv.org/abs/2508.07505v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-12",
        "tldr": "",
        "abstract": "Decentralized min-max optimization allows multi-agent systems to collaboratively solve global min-max optimization problems by facilitating the exchange of model updates among neighboring agents, eliminating the need for a central server. However, sharing model updates in such systems carry a risk of exposing sensitive data to inference attacks, raising significant privacy concerns. To mitigate these privacy risks, differential privacy (DP) has become a widely adopted technique for safeguarding individual data. Despite its advantages, implementing DP in decentralized min-max optimization poses challenges, as the added noise can hinder convergence, particularly in non-convex scenarios with complex agent interactions in min-max optimization problems. In this work, we propose an algorithm called DPMixSGD (Differential Private Minmax Hybrid Stochastic Gradient Descent), a novel privacy-preserving algorithm specifically designed for non-convex decentralized min-max optimization. Our method builds on the state-of-the-art STORM-based algorithm, one of the fastest decentralized min-max solutions. We rigorously prove that the noise added to local gradients does not significantly compromise convergence performance, and we provide theoretical bounds to ensure privacy guarantees. To validate our theoretical findings, we conduct extensive experiments across various tasks and models, demonstrating the effectiveness of our approach.",
        "authors": [
            "Yueyang Quan",
            "Chang Wang",
            "Shengjie Zhai",
            "Minghong Fang",
            "Zhuqing Liu"
        ],
        "categories": [
            "cs.LG",
            "cs.CR",
            "cs.DC"
        ],
        "submit_date": "2025-08-10"
    },
    "http://arxiv.org/abs/2508.07423v3": {
        "id": "http://arxiv.org/abs/2508.07423v3",
        "title": "Real-Time Analysis of Unstructured Data with Machine Learning on Heterogeneous Architectures",
        "link": "http://arxiv.org/abs/2508.07423v3",
        "tags": [
            "edge",
            "offloading",
            "quantization"
        ],
        "relevant": true,
        "indexed_date": "2025-08-12",
        "tldr": "Addresses real-time ML inference for particle physics data filtering on heterogeneous hardware. Proposes a graph neural network pipeline optimized for GPUs and FPGAs, focusing on throughput and energy efficiency. Achieves high processing speeds at 40 MHz with reduced power consumption compared to classical methods.",
        "abstract": "As the particle physics community needs higher and higher precisions in order to test our current model of the subatomic world, larger and larger datasets are necessary. With upgrades scheduled for the detectors of colliding-beam experiments around the world, and specifically at the Large Hadron Collider at CERN, more collisions and more complex interactions are expected. This directly implies an increase in data produced and consequently in the computational resources needed to process them. At CERN, the amount of data produced is gargantuan. This is why the data have to be heavily filtered and selected in real time before being permanently stored. This data can then be used to perform physics analyses, in order to expand our current understanding of the universe and improve the Standard Model of physics. This real-time filtering, known as triggering, involves complex processing happening often at frequencies as high as 40 MHz. This thesis contributes to understanding how machine learning models can be efficiently deployed in such environments, in order to maximize throughput and minimize energy consumption. Inevitably, modern hardware designed for such tasks and contemporary algorithms are needed in order to meet the challenges posed by the stringent, high-frequency data rates. In this work, I present our graph neural network-based pipeline, developed for charged particle track reconstruction at the LHCb experiment at CERN. The pipeline was implemented end-to-end inside LHCb's first-level trigger, entirely on GPUs. Its performance was compared against the classical tracking algorithms currently in production at LHCb. The pipeline was also accelerated on the FPGA architecture, and its performance in terms of power consumption and processing speed was compared against the GPU implementation.",
        "authors": [
            "Fotis I. Giasemis"
        ],
        "categories": [
            "physics.data-an",
            "cs.AI",
            "cs.DC",
            "cs.LG",
            "hep-ex"
        ],
        "submit_date": "2025-08-10"
    },
    "http://arxiv.org/abs/2508.06972v3": {
        "id": "http://arxiv.org/abs/2508.06972v3",
        "title": "DSperse: A Framework for Targeted Verification in Zero-Knowledge Machine Learning",
        "link": "http://arxiv.org/abs/2508.06972v3",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-12",
        "tldr": "",
        "abstract": "DSperse is a modular framework for distributed machine learning inference with strategic cryptographic verification. Operating within the emerging paradigm of distributed zero-knowledge machine learning, DSperse avoids the high cost and rigidity of full-model circuitization by enabling targeted verification of strategically chosen subcomputations. These verifiable segments, or \"slices\", may cover part or all of the inference pipeline, with global consistency enforced through audit, replication, or economic incentives. This architecture supports a pragmatic form of trust minimization, localizing zero-knowledge proofs to the components where they provide the greatest value. We evaluate DSperse using multiple proving systems and report empirical results on memory usage, runtime, and circuit behavior under sliced and unsliced configurations. By allowing proof boundaries to align flexibly with the model's logical structure, DSperse supports scalable, targeted verification strategies suited to diverse deployment needs.",
        "authors": [
            "Dan Ivanov",
            "Tristan Freiberg",
            "Shirin Shahabi",
            "Jonathan Gold",
            "Haruna Isah"
        ],
        "categories": [
            "cs.AI",
            "cs.CR",
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-08-09"
    },
    "http://arxiv.org/abs/2508.06771v1": {
        "id": "http://arxiv.org/abs/2508.06771v1",
        "title": "A Portable Multi-GPU Solver for Collisional Plasmas with Coulombic Interactions",
        "link": "http://arxiv.org/abs/2508.06771v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-12",
        "tldr": "",
        "abstract": "We study parallel particle-in-cell (PIC) methods for low-temperature plasmas (LTPs), which discretize kinetic formulations that capture the time evolution of the probability density function of particles as a function of position and velocity. We use a kinetic description for electrons and a fluid approximation for heavy species. In this paper, we focus on GPU acceleration of algorithms for velocity-space interactions and in particular, collisions of electrons with neutrals, ions, and electrons. Our work has two thrusts. The first is algorithmic exploration and analysis. The second is examining the viability of rapid-prototyping implementations using Python-based HPC tools, in particular PyKokkos. We discuss several common PIC kernels and present performance results on NVIDIA Volta V100 and AMD MI250X GPUs. Overall, the MI250X is slightly faster for most kernels but shows more sensitivity to register pressure. We also report scaling results for a distributed memory implementation on up to 16 MPI ranks.",
        "authors": [
            "James Almgren-Bell",
            "Nader Al Awar",
            "Dilip S Geethakrishnan",
            "Milos Gligoric",
            "George Biros"
        ],
        "categories": [
            "cs.CE",
            "cs.DC"
        ],
        "submit_date": "2025-08-09"
    },
    "http://arxiv.org/abs/2508.06767v1": {
        "id": "http://arxiv.org/abs/2508.06767v1",
        "title": "PANAMA: A Network-Aware MARL Framework for Multi-Agent Path Finding in Digital Twin Ecosystems",
        "link": "http://arxiv.org/abs/2508.06767v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-12",
        "tldr": "",
        "abstract": "Digital Twins (DTs) are transforming industries through advanced data processing and analysis, positioning the world of DTs, Digital World, as a cornerstone of nextgeneration technologies including embodied AI. As robotics and automated systems scale, efficient data-sharing frameworks and robust algorithms become critical. We explore the pivotal role of data handling in next-gen networks, focusing on dynamics between application and network providers (AP/NP) in DT ecosystems. We introduce PANAMA, a novel algorithm with Priority Asymmetry for Network Aware Multi-agent Reinforcement Learning (MARL) based multi-agent path finding (MAPF). By adopting a Centralized Training with Decentralized Execution (CTDE) framework and asynchronous actor-learner architectures, PANAMA accelerates training while enabling autonomous task execution by embodied AI. Our approach demonstrates superior pathfinding performance in accuracy, speed, and scalability compared to existing benchmarks. Through simulations, we highlight optimized data-sharing strategies for scalable, automated systems, ensuring resilience in complex, real-world environments. PANAMA bridges the gap between network-aware decision-making and robust multi-agent coordination, advancing the synergy between DTs, wireless networks, and AI-driven automation.",
        "authors": [
            "Arman Dogru",
            "R. Irem Bor-Yaliniz",
            "Nimal Gamini Senarath"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC",
            "cs.MA",
            "cs.RO"
        ],
        "submit_date": "2025-08-09"
    },
    "http://arxiv.org/abs/2508.06339v1": {
        "id": "http://arxiv.org/abs/2508.06339v1",
        "title": "Performant Unified GPU Kernels for Portable Singular Value Computation Across Hardware and Precision",
        "link": "http://arxiv.org/abs/2508.06339v1",
        "tags": [
            "kernel",
            "training"
        ],
        "relevant": true,
        "indexed_date": "2025-08-11",
        "tldr": "Develops portable GPU kernels for SVD computation to support low-rank adaptation (LoRA) in large language models. Uses Julia's metaprogramming and multiple dispatch for unified, high-performance implementation across GPUs. Outperforms libraries like MAGMA/SLATE and achieves 80-90% of cuSOLVER performance for large matrices.",
        "abstract": "This paper presents a portable, GPU-accelerated implementation of a QR-based singular value computation algorithm in Julia. The singular value ecomposition (SVD) is a fundamental numerical tool in scientific computing and machine learning, providing optimal low-rank matrix approximations. Its importance has increased even more in large-scale machine learning pipelines, including large language models (LLMs), where it enables low-rank adaptation (LoRA). The implemented algorithm is based on the classic two-stage QR reduction, consisting of successive matrix reduction to band form and bidiagonal form. Our implementation leverages Julia's multiple dispatch and metaprogramming capabilities, integrating with the GPUArrays and KernelAbstractions frameworks to provide a unified type and hardware-agnostic function. It supports diverse GPU architectures and data types, and is, to our knowledge, the first GPU-accelerated singular value implementation to support Apple Metal GPUs and half precision. Performance results on multiple GPU backends and data types demonstrate that portability does not require sacrificing performance: the unified function outperforms most linear algebra libraries (MAGMA, SLATE, rocSOLVER, oneMKL) for matrix sizes larger than 1024x1024, and achieves 80%-90% of the performance of cuSOLVER for large matrices.",
        "authors": [
            "Evelyne Ringoot",
            "Rabab Alomairy",
            "Valentin Churavy",
            "Alan Edelman"
        ],
        "categories": [
            "cs.DC",
            "cs.MS"
        ],
        "submit_date": "2025-08-08"
    },
    "http://arxiv.org/abs/2508.06297v1": {
        "id": "http://arxiv.org/abs/2508.06297v1",
        "title": "KV Cache Compression for Inference Efficiency in LLMs: A Review",
        "link": "http://arxiv.org/abs/2508.06297v1",
        "tags": [
            "serving",
            "sparse",
            "quantization"
        ],
        "relevant": true,
        "indexed_date": "2025-08-11",
        "tldr": "Analyzes KV cache optimization techniques for LLM inference efficiency. Reviews compression strategies like selective tokens, quantization, and attention compression. Reduces memory usage by 2-5× while maintaining accuracy in various models.",
        "abstract": "Withtherapid advancement of large language models (LLMs), the context length for inference has been continuously increasing, leading to an exponential growth in the demand for Key-Value (KV) caching. This has resulted in a significant memory bottleneck, limiting the inference efficiency and scalability of the models. Therefore, optimizing the KV cache during inference is crucial for enhancing performance and efficiency. This review systematically examines current KV cache optimization techniques, including compression strategies such as selective token strategies, quantization, and attention compression. We evaluate the effectiveness, trade-offs, and application scenarios of these methods, providing a comprehensive analysis of their impact on memory usage and inference speed. We focus on identifying the limitations and challenges of existing methods, such as compatibility issues with different models and tasks. Additionally, this review highlights future research directions, including hybrid optimization techniques, adaptive dynamic strategies, and software-hardware co-design. These approaches aim to improve inference efficiency and promote the practical application of large language models.",
        "authors": [
            "Yanyu Liu",
            "Jingying Fu",
            "Sixiang Liu",
            "Yitian Zou",
            "You Fu",
            "Jiehan Zhou",
            "Shouhua Zhang"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-08"
    },
    "http://arxiv.org/abs/2508.06024v1": {
        "id": "http://arxiv.org/abs/2508.06024v1",
        "title": "EC2MoE: Adaptive End-Cloud Pipeline Collaboration Enabling Scalable Mixture-of-Experts Inference",
        "link": "http://arxiv.org/abs/2508.06024v1",
        "tags": [
            "MoE",
            "serving",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-08-11",
        "tldr": "Proposes EC2MoE for scalable MoE inference in end-cloud environments via adaptive expert scheduling and pipeline optimization. Uses hardware-aware group gating and route-aware pipeline scheduling. Achieves 2.2-5.1x higher throughput and 53-67% lower latency.",
        "abstract": "The Mixture-of-Experts (MoE) paradigm has emerged as a promising solution to scale up model capacity while maintaining inference efficiency. However, deploying MoE models across heterogeneous end-cloud environments poses new challenges in expert scheduling, communication overhead, and resource heterogeneity. In this paper, we propose EC2MoE, an adaptive framework for scalable MoE inference via end-cloud pipeline collaboration. First, we design a hardware-aware lightweight group gate network that enhances expert selection and computational efficiency. By incorporating a hardware-aware local expert selection mechanism, the system adaptively filters candidate experts based on real-time device profiles. A lightweight group gate module then integrates local and global gating outputs to achieve high-quality expert routing with minimal overhead. Second, we develop a pipeline optimization mechanism based on endcloud collaboration to accelerate MoE inference. This includes an encoder-decoder structure based on low-rank compression, which reduces transmission and computation costs. And a route-aware heuristic pipeline scheduling algorithm that dynamically allocates inference stages across devices according to workload and network topology. Extensive experiments show that EC2MoE can increase throughput by 2.2x to 5.1x and reduce end-to-end latency by 53% to 67% while maintaining high accuracy compared to state-of-the-art methods. It also maintains good scalability under dynamic load and network environments.",
        "authors": [
            "Zheming Yang",
            "Yunqing Hu",
            "Sheng Sun",
            "Wen Ji"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-08"
    },
    "http://arxiv.org/abs/2508.06001v1": {
        "id": "http://arxiv.org/abs/2508.06001v1",
        "title": "KnapFormer: An Online Load Balancer for Efficient Diffusion Transformers Training",
        "link": "http://arxiv.org/abs/2508.06001v1",
        "tags": [
            "training",
            "diffusion",
            "sparse"
        ],
        "relevant": true,
        "indexed_date": "2025-08-11",
        "tldr": "Proposes KnapFormer, a load balancer using sequence parallelism and knapsack solving for imbalance in Diffusion Transformers training. Balances token distribution across GPUs with semi-empirical workload model. Achieves 2×–3× speedup in training FLUX models with <1% workload discrepancy.",
        "abstract": "We present KnapFormer, an efficient and versatile framework to combine workload balancing and sequence parallelism in distributed training of Diffusion Transformers (DiT). KnapFormer builds on the insight that strong synergy exists between sequence parallelism and the need to address the significant token imbalance across ranks. This imbalance arises from variable-length text inputs and varying visual token counts in mixed-resolution and image-video joint training. KnapFormer redistributes tokens by first gathering sequence length metadata across all ranks in a balancing group and solving a global knapsack problem. The solver aims to minimize the variances of total workload per-GPU, while accounting for the effect of sequence parallelism. By integrating DeepSpeed-Ulysees-based sequence parallelism in the load-balancing decision process and utilizing a simple semi-empirical workload model, KnapFormers achieves minimal communication overhead and less than 1% workload discrepancy in real-world training workloads with sequence length varying from a few hundred to tens of thousands. It eliminates straggler effects and achieves 2x to 3x speedup when training state-of-the-art diffusion models like FLUX on mixed-resolution and image-video joint data corpora. We open-source the KnapFormer implementation at https://github.com/Kai-46/KnapFormer/",
        "authors": [
            "Kai Zhang",
            "Peng Wang",
            "Sai Bi",
            "Jianming Zhang",
            "Yuanjun Xiong"
        ],
        "categories": [
            "cs.DC",
            "cs.CV"
        ],
        "submit_date": "2025-08-08"
    },
    "http://arxiv.org/abs/2508.05904v1": {
        "id": "http://arxiv.org/abs/2508.05904v1",
        "title": "Snowpark: Performant, Secure, User-Friendly Data Engineering and AI/ML Next To Your Data",
        "link": "http://arxiv.org/abs/2508.05904v1",
        "tags": [
            "training",
            "RL",
            "storage"
        ],
        "relevant": true,
        "indexed_date": "2025-08-11",
        "tldr": "Introduces Snowpark, a managed solution for performant and secure data engineering and AI/ML workloads. Proposes sandbox isolation, Python package caching, and optimized scheduling to reduce initialization latency and manage data skew. Achieves 5x faster data engineering tasks in case studies.",
        "abstract": "Snowflake revolutionized data analytics with an elastic architecture that decouples compute and storage, enabling scalable solutions supporting data architectures like data lake, data warehouse, data lakehouse, and data mesh. Building on this foundation, Snowflake has advanced its AI Data Cloud vision by introducing Snowpark, a managed turnkey solution that supports data engineering and AI and ML workloads using Python and other programming languages.   This paper outlines Snowpark's design objectives towards high performance, strong security and governance, and ease of use. We detail the architecture of Snowpark, highlighting its elastic scalability and seamless integration with Snowflake core compute infrastructure. This includes leveraging Snowflake control plane for distributed computing and employing a secure sandbox for isolating Snowflake SQL workloads from Snowpark executions. Additionally, we present core innovations in Snowpark that drive further performance enhancements, such as query initialization latency reduction through Python package caching, improved workload scheduling for customized workloads, and data skew management via efficient row redistribution. Finally, we showcase real-world case studies that illustrate Snowpark's efficiency and effectiveness for large-scale data engineering and AI and ML tasks.",
        "authors": [
            "Brandon Baker",
            "Elliott Brossard",
            "Chenwei Xie",
            "Zihao Ye",
            "Deen Liu",
            "Yijun Xie",
            "Arthur Zwiegincew",
            "Nitya Kumar Sharma",
            "Gaurav Jain",
            "Eugene Retunsky",
            "Mike Halcrow",
            "Derek Denny-Brown",
            "Istvan Cseri",
            "Tyler Akidau",
            "Yuxiong He"
        ],
        "categories": [
            "cs.DC",
            "cs.DB"
        ],
        "submit_date": "2025-08-07"
    },
    "http://arxiv.org/abs/2508.05821v1": {
        "id": "http://arxiv.org/abs/2508.05821v1",
        "title": "A Dynamic Approach to Load Balancing in Cloud Infrastructure: Enhancing Energy Efficiency and Resource Utilization",
        "link": "http://arxiv.org/abs/2508.05821v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-11",
        "tldr": "",
        "abstract": "Cloud computing has grown rapidly in recent years, mainly due to the sharp increase in data transferred over the internet. This growth makes load balancing a key part of cloud systems, as it helps distribute user requests across servers to maintain performance, prevent overload, and ensure a smooth user experience. Despite its importance, managing server resources and keeping workloads balanced over time remains a major challenge in cloud environments. This paper introduces a novel Score-Based Dynamic Load Balancer (SBDLB) that allocates workloads to virtual machines based on real-time performance metrics. The objective is to enhance resource utilization and overall system efficiency. The method was thoroughly tested using the CloudSim 7G platform, comparing its performance against the throttled load balancing strategy. Evaluations were conducted across a variety of workloads and scenarios, demonstrating the SBDLB's ability to adapt dynamically to workload fluctuations while optimizing resource usage. The proposed method outperformed the throttled strategy, improving average response times by 34% and 37% in different scenarios. It also reduced data center processing times by an average of 13%. Over a 24-hour simulation, the method decreased operational costs by 15%, promoting a more energy-efficient and sustainable cloud infrastructure through reduced energy consumption.",
        "authors": [
            "Shadman Sakib",
            "Ajay Katangur",
            "Rahul Dubey"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-07"
    },
    "http://arxiv.org/abs/2508.05797v1": {
        "id": "http://arxiv.org/abs/2508.05797v1",
        "title": "Accelerating Data Chunking in Deduplication Systems using Vector Instructions",
        "link": "http://arxiv.org/abs/2508.05797v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-11",
        "tldr": "",
        "abstract": "Content-defined Chunking (CDC) algorithms dictate the overall space savings that deduplication systems achieve. However, due to their need to scan each file in its entirety, they are slow and often the main performance bottleneck within data deduplication. We present VectorCDC, a method to accelerate hashless CDC algorithms using vector CPU instructions, such as SSE / AVX. Our evaluation shows that VectorCDC is effective on Intel, AMD, ARM, and IBM CPUs, achieving 8.35x - 26.2x higher throughput than existing vector-accelerated techniques without affecting the deduplication space savings.",
        "authors": [
            "Sreeharsha Udayashankar",
            "Abdelrahman Baba",
            "Samer Al-Kiswany"
        ],
        "categories": [
            "cs.DC",
            "cs.AR"
        ],
        "submit_date": "2025-08-07"
    },
    "http://arxiv.org/abs/2508.06489v3": {
        "id": "http://arxiv.org/abs/2508.06489v3",
        "title": "Voting-Based Semi-Parallel Proof-of-Work Protocol",
        "link": "http://arxiv.org/abs/2508.06489v3",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-11",
        "tldr": "",
        "abstract": "Parallel Proof-of-Work (PoW) protocols are suggested to improve the safety guarantees, transaction throughput and confirmation latencies of Nakamoto consensus. In this work, we first consider the existing parallel PoW protocols and develop hard-coded incentive attack structures. Our theoretical results and simulations show that the existing parallel PoW protocols are more vulnerable to incentive attacks than the Nakamoto consensus, e.g., attacks have smaller profitability threshold and they result in higher relative rewards. Next, we introduce a voting-based semi-parallel PoW protocol that outperforms both Nakamoto consensus and the existing parallel PoW protocols from most practical perspectives such as communication overheads, throughput, transaction conflicts, incentive compatibility of the protocol as well as a fair distribution of transaction fees among the voters and the leaders. We use state-of-the-art analysis to evaluate the consistency of the protocol and consider Markov decision process (MDP) models to substantiate our claims about the resilience of our protocol against incentive attacks.",
        "authors": [
            "Mustafa Doger",
            "Sennur Ulukus"
        ],
        "categories": [
            "cs.CR",
            "cs.DC",
            "cs.DM",
            "cs.IT",
            "math.PR"
        ],
        "submit_date": "2025-08-08"
    },
    "http://arxiv.org/abs/2508.05370v1": {
        "id": "http://arxiv.org/abs/2508.05370v1",
        "title": "Simulating LLM training workloads for heterogeneous compute and network infrastructure",
        "link": "http://arxiv.org/abs/2508.05370v1",
        "tags": [
            "training",
            "simulation"
        ],
        "relevant": true,
        "indexed_date": "2025-08-08",
        "tldr": null,
        "abstract": "The growing demand for large-scale GPU clusters in distributed model training presents a significant barrier to innovation, particularly in model optimization, performance tuning, and system-level enhancements. To address this challenge, LLM training simulators are employed to estimate training time and guide design decisions. However, the state-of-the-art LLM training simulators assume homogeneous compute and network infrastructure. In practice, device heterogeneity is inevitable due to resource sharing in cloud environments, frequent shifts in device generations, and inherent intra-chip interconnect heterogeneity. To address the gap between state-of-the-art and practical requirements, we propose the design of a heterogeneity-aware distributed LLM simulator capable of predicting training time while enabling abstractions to specify custom configurations for device groups and device-to-parallelism mapping. We present the design requirements and challenges in building a heterogeneity-aware distributed ML training simulator, and design components such as non-uniform workload partitioning. Our initial simulation results demonstrate the impact of heterogeneity on the model computation and communication time.",
        "authors": [
            "Sumit Kumar",
            "Arjun Temura",
            "Naman Sharma",
            "Ramanjeet Singh",
            "Meet Dadhania",
            "Praveen Tammana",
            "Satananda Burla",
            "Abed Mohammad Kamaluddin",
            "Rinku Shah"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-07"
    },
    "http://arxiv.org/abs/2508.04271v1": {
        "id": "http://arxiv.org/abs/2508.04271v1",
        "title": "S2M3: Split-and-Share Multi-Modal Models for Distributed Multi-Task Inference on the Edge",
        "link": "http://arxiv.org/abs/2508.04271v1",
        "tags": [
            "edge",
            "multi-modal",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-08-07",
        "tldr": "Proposes S2M3, a split-and-share architecture for multi-modal models on edge devices, using module splitting and sharing to reduce resource usage. Implements greedy module placement with parallel routing. Reduces memory by up to 62% and latency by 56.9% on edge devices.",
        "abstract": "With the advancement of Artificial Intelligence (AI) towards multiple modalities (language, vision, speech, etc.), multi-modal models have increasingly been used across various applications (e.g., visual question answering or image generation/captioning). Despite the success of AI as a service for multi-modal applications, it relies heavily on clouds, which are constrained by bandwidth, latency, privacy concerns, and unavailability under network or server failures. While on-device AI becomes popular, supporting multiple tasks on edge devices imposes significant resource challenges. To address this, we introduce S2M3, a split-and-share multi-modal architecture for multi-task inference on edge devices. Inspired by the general-purpose nature of multi-modal models, which are composed of multiple modules (encoder, decoder, classifier, etc.), we propose to split multi-modal models at functional-level modules; and then share common modules to reuse them across tasks, thereby reducing resource usage. To address cross-model dependency arising from module sharing, we propose a greedy module-level placement with per-request parallel routing by prioritizing compute-intensive modules. Through experiments on a testbed consisting of 14 multi-modal models across 5 tasks and 10 benchmarks, we demonstrate that S2M3 can reduce memory usage by up to 50% and 62% in single-task and multi-task settings, respectively, without sacrificing accuracy. Furthermore, S2M3 achieves optimal placement in 89 out of 95 instances (93.7%) while reducing inference latency by up to 56.9% on resource-constrained devices, compared to cloud AI.",
        "authors": [
            "JinYi Yoon",
            "JiHo Lee",
            "Ting He",
            "Nakjung Choi",
            "Bo Ji"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-06"
    },
    "http://arxiv.org/abs/2508.03611v2": {
        "id": "http://arxiv.org/abs/2508.03611v2",
        "title": "Block: Balancing Load in LLM Serving with Context, Knowledge and Predictive Scheduling",
        "link": "http://arxiv.org/abs/2508.03611v2",
        "tags": [
            "serving",
            "autoscaling"
        ],
        "relevant": true,
        "indexed_date": "2025-08-06",
        "tldr": "Addresses inefficient load balancing and auto-provisioning in LLM serving. Proposes Block, a distributed predictive scheduler using input context, knowledge of host configurations, and response length predictions for task assignment. Boosts serving capacity by 16.7% and reduces P99 tail latency by 49.5%.",
        "abstract": "This paper presents Block, a distributed scheduling framework designed to optimize load balancing and auto-provisioning across instances in large language model serving frameworks by leveraging contextual information from incoming requests. Unlike popular model serving systems that rely on monolithic and heuristic task schedulers, Block operates as a fully distributed, stateless, and predictive scheduling system to achieve low overhead, reliability, and scalability. It leverages the deterministic and predictable characteristics of LLM inferences, such as host configurations, response lengths, and hardware performance, to make scheduling decisions based on accurately predicted metrics. Evaluation on a 12 GPUs cluster shows that Block significantly outperforms heuristic schedulers, boosting serving capacity by up to 16.7\\% and reducing P99 tail latency by up to 49.5\\%. These performance gains remain consistent across diverse models, workloads and configurations. Code and data are open-sourced.",
        "authors": [
            "Wei Da",
            "Evangelia Kalyvianaki"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-08-05"
    },
    "http://arxiv.org/abs/2508.03148v1": {
        "id": "http://arxiv.org/abs/2508.03148v1",
        "title": "Frontier: Simulating the Next Generation of LLM Inference Systems",
        "link": "http://arxiv.org/abs/2508.03148v1",
        "tags": [
            "serving",
            "disaggregation",
            "MoE"
        ],
        "relevant": true,
        "indexed_date": "2025-08-06",
        "tldr": "Addresses the simulation gap for next-gen LLM inference systems with MoE and disaggregated architectures. Proposes Frontier, a high-fidelity simulator with unified modeling of co-located/disaggregated systems, expert parallelism, and advanced pipelining. Enables exploration of optimization strategies for scale.",
        "abstract": "Large Language Model (LLM) inference is growing increasingly complex with the rise of Mixture-of-Experts (MoE) models and disaggregated architectures that decouple components like prefill/decode (PD) or attention/FFN (AF) for heterogeneous scaling. Existing simulators, architected for co-located, dense models, are unable to capture the intricate system dynamics of these emerging paradigms. We present Frontier, a high-fidelity simulator designed from the ground up for this new landscape. Frontier introduces a unified framework to model both co-located and disaggregated systems, providing native support for MoE inference with expert parallelism (EP). It enables the simulation of complex workflows like cross-cluster expert routing and advanced pipelining strategies for latency hiding. To ensure fidelity and usability, Frontier incorporates refined operator models for improved accuracy. Frontier empowers the community to design and optimize the future of LLM inference at scale.",
        "authors": [
            "Yicheng Feng",
            "Xin Tan",
            "Kin Hang Sew",
            "Yimin Jiang",
            "Yibo Zhu",
            "Hong Xu"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC"
        ],
        "submit_date": "2025-08-05"
    },
    "http://arxiv.org/abs/2508.02309v1": {
        "id": "http://arxiv.org/abs/2508.02309v1",
        "title": "PUSHtap: PIM-based In-Memory HTAP with Unified Data Storage Format",
        "link": "http://arxiv.org/abs/2508.02309v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-05",
        "tldr": "",
        "abstract": "Hybrid transaction/analytical processing (HTAP) is an emerging database paradigm that supports both online transaction processing (OLTP) and online analytical processing (OLAP) workloads. Computing-intensive OLTP operations, involving row-wise data manipulation, are suitable for row-store format. In contrast, memory-intensive OLAP operations, which are column-centric, benefit from column-store format. This \\emph{data-format dilemma} prevents HTAP systems from concurrently achieving three design goals: performance isolation, data freshness, and workload-specific optimization. Another background technology is Processing-in-Memory (PIM), which integrates computing units (PIM units) inside DRAM memory devices to accelerate memory-intensive workloads, including OLAP.   Our key insight is to combine the interleaved CPU access and localized PIM unit access to provide two-dimensional access to address the data format contradictions inherent in HTAP. First, we propose a unified data storage format with novel data alignment and placement techniques to optimize the effective bandwidth of CPUs and PIM units and exploit the PIM's parallelism. Second, we implement the multi-version concurrency control (MVCC) essential for single-instance HTAP. Third, we extend the commercial PIM architecture to support the OLAP operations and concurrent access from PIM and CPU. Experiments show that PUSHtap can achieve 3.4\\texttimes{}/4.4\\texttimes{} OLAP/OLTP throughput improvement compared to multi-instance PIM-based design.",
        "authors": [
            "Yilong Zhao",
            "Mingyu Gao",
            "Huanchen Zhang",
            "Fangxin Liu",
            "Gongye Chen",
            "He Xian",
            "Haibing Guan",
            "Li Jiang"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-04"
    },
    "http://arxiv.org/abs/2508.01989v1": {
        "id": "http://arxiv.org/abs/2508.01989v1",
        "title": "Prefill-Decode Aggregation or Disaggregation? Unifying Both for Goodput-Optimized LLM Serving",
        "link": "http://arxiv.org/abs/2508.01989v1",
        "tags": [
            "serving",
            "disaggregation",
            "autoscaling"
        ],
        "relevant": true,
        "indexed_date": "2025-08-05",
        "tldr": "Proposes TaiChi, a system unifying prefill-decode aggregation and disaggregation for LLM serving. Uses differentiated GPU instances and configurable sliders to adapt to varied SLOs. Achieves up to 77% goodput improvement under balanced TTFT and TPOT constraints via latency shifting.",
        "abstract": "An ongoing debate considers whether prefill-decode (PD) aggregation or disaggregation is superior for serving large language models (LLMs). This has driven optimizations for both approaches, each showing distinct advantages. This paper compares PD aggregation and disaggregation, showing that each excels under different service-level objectives (SLOs): aggregation is optimal for tight time-to-first-token (TTFT) and relaxed time-per-output-token (TPOT), while disaggregation excels for strict TPOT and relaxed TTFT. However, under balanced TTFT and TPOT SLOs, neither approach delivers optimal goodput.   This paper proposes TaiChi, an LLM serving system that unifies PD disaggregation and aggregation for optimal goodput under any combination of TTFT and TPOT SLOs. TaiChi uses a unified disaggregation-aggregation architecture with differentiated-capability GPU instances: prefill-heavy (fast prefill, high-interference decode) and decode-heavy (low-interference decode, slow prefill). Three configurable sliders control the ratio between these instances and their chunk sizes. TaiChi adapts to various SLO regimes by adjusting sliders. When TTFT constraints are tight, TaiChi resembles a PD aggregation configuration; when TPOT dominates, it adapts toward PD disaggregation. Crucially, under balanced SLOs, TaiChi enables a hybrid mode for superior goodput. The key innovation behind this hybrid mode is latency shifting: selectively reallocating GPU resources from requests that meet SLOs to those at risk of violation, maximizing the number of SLO-satisfied requests. This fine-grained latency shifting is orchestrated by two scheduling mechanisms: flowing decode scheduling to control TPOTs and length-aware prefill scheduling to manage TTFTs, which jointly optimize request assignment. Our experiments show TaiChi improves goodput by up to 77% over state-of-the-art systems under balanced TTFT and TPOT SLOs.",
        "authors": [
            "Chao Wang",
            "Pengfei Zuo",
            "Zhangyu Chen",
            "Yunkai Liang",
            "Zhou Yu",
            "Ming-Chang Yang"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-04"
    },
    "http://arxiv.org/abs/2508.00622v2": {
        "id": "http://arxiv.org/abs/2508.00622v2",
        "title": "SwarmRaft: Leveraging Consensus for Robust Drone Swarm Coordination in GNSS-Degraded Environments",
        "link": "http://arxiv.org/abs/2508.00622v2",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-08-04",
        "tldr": "",
        "abstract": "Unmanned aerial vehicle (UAV) swarms are increasingly used in critical applications such as aerial mapping, environmental monitoring, and autonomous delivery. However, the reliability of these systems is highly dependent on uninterrupted access to the Global Navigation Satellite Systems (GNSS) signals, which can be disrupted in real-world scenarios due to interference, environmental conditions, or adversarial attacks, causing disorientation, collision risks, and mission failure. This paper proposes SwarmRaft, a blockchain-inspired positioning and consensus framework for maintaining coordination and data integrity in UAV swarms operating under GNSS-denied conditions. SwarmRaft leverages the Raft consensus algorithm to enable distributed drones (nodes) to agree on state updates such as location and heading, even in the absence of GNSS signals for one or more nodes. In our prototype, each node uses GNSS and local sensing, and communicates over WiFi in a simulated swarm. Upon signal loss, consensus is used to reconstruct or verify the position of the failed node based on its last known state and trajectory. Our system demonstrates robustness in maintaining swarm coherence and fault tolerance through a lightweight, scalable communication model. This work offers a practical and secure foundation for decentralized drone operation in unpredictable environments.",
        "authors": [
            "Kapel Dev",
            "Yash Madhwal",
            "Sofia Shevelo",
            "Pavel Osinenko",
            "Yury Yanovich"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-08-01"
    },
    "http://arxiv.org/abs/2508.00806v2": {
        "id": "http://arxiv.org/abs/2508.00806v2",
        "title": "Adacc: An Adaptive Framework Unifying Compression and Activation Recomputation for LLM Training",
        "link": "http://arxiv.org/abs/2508.00806v2",
        "tags": [
            "training",
            "sparse",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-08-04",
        "tldr": "Proposes an adaptive framework, Adacc, unifying compression and activation recomputation for efficient LLM training. It uses tensor-level decisions, layer-specific compression for outliers, and MILP-based scheduling. Achieves 1.01x-1.37x throughput gain with baseline accuracy.",
        "abstract": "Training large language models (LLMs) is often constrained by GPU memory limitations. To alleviate memory pressure, activation recomputation and data compression have been proposed as two major strategies. However, both approaches have limitations: recomputation introduces significant training overhead, while compression can lead to accuracy degradation and computational inefficiency when applied naively. In this paper, we propose Adacc, the first adaptive memory optimization framework that unifies activation recomputation and data compression to improve training efficiency for LLMs while preserving model accuracy. Unlike existing methods that apply static, rule-based strategies or rely solely on one technique, Adacc makes fine-grained, tensor-level decisions, dynamically selecting between recomputation, retention, and compression based on tensor characteristics and runtime hardware constraints.   Adacc tackles three key challenges: (1) it introduces layer-specific compression algorithms that mitigate accuracy loss by accounting for outliers in LLM activations; (2) it employs a MILP-based scheduling policy to globally optimize memory strategies across layers; and (3) it integrates an adaptive policy evolution mechanism to update strategies during training in response to changing data distributions. Experimental results show that Adacc improves training throughput by 1.01x to 1.37x compared to state-of-the-art frameworks, while maintaining accuracy comparable to the baseline.",
        "authors": [
            "Ping Chen",
            "Zhuohong Deng",
            "Ping Li",
            "Shuibing He",
            "Hongzi Zhu",
            "Yi Zheng",
            "Zhefeng Wang",
            "Baoxing Huai",
            "Minyi Guo"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "submit_date": "2025-08-01"
    },
    "http://arxiv.org/abs/2508.00234v1": {
        "id": "http://arxiv.org/abs/2508.00234v1",
        "title": "Quality-of-Service Aware LLM Routing for Edge Computing with Multiple Experts",
        "link": "http://arxiv.org/abs/2508.00234v1",
        "tags": [
            "edge",
            "serving",
            "RL"
        ],
        "relevant": true,
        "indexed_date": "2025-08-04",
        "tldr": "Addresses QoS-aware routing of user requests to edge-deployed LLM experts. Proposes a DRL-based framework with dynamic state abstraction via graph attention networks and tailored reward design. Achieves higher average QoS and resource efficiency over baselines on real and synthetic workloads.",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities, leading to a significant increase in user demand for LLM services. However, cloud-based LLM services often suffer from high latency, unstable responsiveness, and privacy concerns. Therefore, multiple LLMs are usually deployed at the network edge to boost real-time responsiveness and protect data privacy, particularly for many emerging smart mobile and IoT applications. Given the varying response quality and latency of LLM services, a critical issue is how to route user requests from mobile and IoT devices to an appropriate LLM service (i.e., edge LLM expert) to ensure acceptable quality-of-service (QoS). Existing routing algorithms fail to simultaneously address the heterogeneity of LLM services, the interference among requests, and the dynamic workloads necessary for maintaining long-term stable QoS. To meet these challenges, in this paper we propose a novel deep reinforcement learning (DRL)-based QoS-aware LLM routing framework for sustained high-quality LLM services. Due to the dynamic nature of the global state, we propose a dynamic state abstraction technique to compactly represent global state features with a heterogeneous graph attention network (HAN). Additionally, we introduce an action impact estimator and a tailored reward function to guide the DRL agent in maximizing QoS and preventing latency violations. Extensive experiments on both Poisson and real-world workloads demonstrate that our proposed algorithm significantly improves average QoS and computing resource efficiency compared to existing baselines.",
        "authors": [
            "Jin Yang",
            "Qiong Wu",
            "Zhiying Feng",
            "Zhi Zhou",
            "Deke Guo",
            "Xu Chen"
        ],
        "categories": [
            "cs.NI",
            "cs.AI",
            "cs.DC",
            "cs.MA"
        ],
        "submit_date": "2025-08-01"
    },
    "http://arxiv.org/abs/2507.22801v1": {
        "id": "http://arxiv.org/abs/2507.22801v1",
        "title": "DSPE: Profit Maximization in Edge-Cloud Storage System using Dynamic Space Partitioning with Erasure Code",
        "link": "http://arxiv.org/abs/2507.22801v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-07-31",
        "tldr": "",
        "abstract": "Edge Storage Systems have emerged as a critical enabler of low latency data access in modern cloud networks by bringing storage and computation closer to end users. However, the limited storage capacity of edge servers poses significant challenges in handling high volume and latency sensitive data access requests, particularly under dynamic workloads. In this work, we propose a profit driven framework that integrates three key mechanisms which are collaborative caching, erasure coding, and elastic storage partitioning. Unlike traditional replication, erasure coding enables space efficient redundancy, allowing data to be reconstructed from any subset of K out of K plus M coded blocks. We dynamically partition each edge server s storage into private and public regions. The private region is further subdivided among access points based on their incoming request rates, enabling adaptive control over data locality and ownership. We design a data placement and replacement policy that determines how and where to store or evict coded data blocks to maximize data access within deadlines. While the private region serves requests from local APs, the public region handles cooperative storage requests from neighboring servers. Our proposed Dynamic Space Partitioning and Elastic caching strategy is evaluated on both synthetic and real world traces from Netflix and Spotify. Experimental results show that our method improves overall system profitability by approximately 5 to 8% compared to state of the art approaches under varied workload conditions.",
        "authors": [
            "Shubhradeep Roy",
            "Suvarthi Sarkar",
            "Vivek Verma",
            "Aryabartta Sahu"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-07-30"
    },
    "http://arxiv.org/abs/2507.22372v1": {
        "id": "http://arxiv.org/abs/2507.22372v1",
        "title": "Leveraging Caliper and Benchpark to Analyze MPI Communication Patterns: Insights from AMG2023, Kripke, and Laghos",
        "link": "http://arxiv.org/abs/2507.22372v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-07-31",
        "tldr": "",
        "abstract": "We introduce ``communication regions'' into the widely used Caliper HPC profiling tool. A communication region is an annotation enabling capture of metrics about the data being communicated (including statistics of these metrics), and metrics about the MPI processes involved in the communications, something not previously possible in Caliper. We explore the utility of communication regions with three representative modeling and simulation applications, AMG2023, Kripke, and Laghos, all part of the comprehensive Benchpark suite that includes Caliper annotations. Enhanced Caliper reveals detailed communication behaviors. Using Caliper and Thicket in tandem, we create new visualizations of MPI communication patterns, including halo exchanges. Our findings reveal communication bottlenecks and detailed behaviors, indicating significant utility of the special-regions addition to Caliper. The comparative scaling behavior of both CPU and GPU oriented systems are shown; we are able to look at different regions within a given application, and see how scalability and message-traffic metrics differ.",
        "authors": [
            "Grace Nansamba",
            "Evelyn Namugwanya",
            "David Boehme",
            "Dewi Yokelson",
            "Riley Shipley",
            "Derek Schafer",
            "Michael McKinsey",
            "Olga Pearce",
            "Anthony Skjellum"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-07-30"
    },
    "http://arxiv.org/abs/2507.21276v1": {
        "id": "http://arxiv.org/abs/2507.21276v1",
        "title": "LeMix: Unified Scheduling for LLM Training and Inference on Multi-GPU Systems",
        "link": "http://arxiv.org/abs/2507.21276v1",
        "tags": [
            "training",
            "serving",
            "autoscaling"
        ],
        "relevant": true,
        "indexed_date": "2025-07-30",
        "tldr": "Proposes LeMix, a system for co-locating LLM training and inference workloads using adaptive scheduling. Integrates profiling, prediction, and runtime resource allocation to handle dynamic requests. Achieves 3.53x higher throughput and 0.61x reduced inference loss.",
        "abstract": "Modern deployment of large language models (LLMs) frequently involves both inference serving and continuous retraining to stay aligned with evolving data and user feedback. Common practices separate these workloads onto distinct servers in isolated phases, causing substantial inefficiencies (e.g., GPU idleness) and delayed adaptation to new data in distributed settings. Our empirical analysis reveals that these inefficiencies stem from dynamic request arrivals during serving and workload heterogeneity in pipeline-parallel training. To address these challenges, we propose LeMix, a system for co-locating and managing concurrent LLM serving and training workloads. LeMix integrates offline profiling, execution prediction mechanisms, and runtime scheduling to dynamically adapt resource allocation based on workload characteristics and system conditions. By understanding task-specific behaviors and co-execution interference across shared nodes, LeMix improves utilization and serving quality without compromising serving responsiveness. Our evaluation shows that LeMix improves throughput by up to 3.53x, reduces inference loss by up to 0.61x, and delivers up to 2.12x higher response time SLO attainment over traditional separate setups. To our knowledge, this is the first work to uncover and exploit the opportunities of joint LLM inference and training, paving the way for more resource-efficient deployment of LLMs in production environments.",
        "authors": [
            "Yufei Li",
            "Zexin Li",
            "Yinglun Zhu",
            "Cong Liu"
        ],
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.DC"
        ],
        "submit_date": "2025-07-28"
    },
    "http://arxiv.org/abs/2507.21199v1": {
        "id": "http://arxiv.org/abs/2507.21199v1",
        "title": "Advancing Compositional LLM Reasoning with Structured Task Relations in Interactive Multimodal Communications",
        "link": "http://arxiv.org/abs/2507.21199v1",
        "tags": [
            "LoRA",
            "RL",
            "edge"
        ],
        "relevant": true,
        "indexed_date": "2025-07-30",
        "tldr": "Proposes ContextLoRA for training a single compositional LLM to handle diverse interactive multimodal applications via task dependency graphs and step-wise fine-tuning, plus ContextGear scheduler for cost optimization in mobile contexts. Achieves 5.8× latency reduction on wireless testbed.",
        "abstract": "Interactive multimodal applications (IMAs), such as route planning in the Internet of Vehicles, enrich users' personalized experiences by integrating various forms of data over wireless networks. Recent advances in large language models (LLMs) utilize mixture-of-experts (MoE) mechanisms to empower multiple IMAs, with each LLM trained individually for a specific task that presents different business workflows. In contrast to existing approaches that rely on multiple LLMs for IMAs, this paper presents a novel paradigm that accomplishes various IMAs using a single compositional LLM over wireless networks. The two primary challenges include 1) guiding a single LLM to adapt to diverse IMA objectives and 2) ensuring the flexibility and efficiency of the LLM in resource-constrained mobile environments. To tackle the first challenge, we propose ContextLoRA, a novel method that guides an LLM to learn the rich structured context among IMAs by constructing a task dependency graph. We partition the learnable parameter matrix of neural layers for each IMA to facilitate LLM composition. Then, we develop a step-by-step fine-tuning procedure guided by task relations, including training, freezing, and masking phases. This allows the LLM to learn to reason among tasks for better adaptation, capturing the latent dependencies between tasks. For the second challenge, we introduce ContextGear, a scheduling strategy to optimize the training procedure of ContextLoRA, aiming to minimize computational and communication costs through a strategic grouping mechanism. Experiments on three benchmarks show the superiority of the proposed ContextLoRA and ContextGear. Furthermore, we prototype our proposed paradigm on a real-world wireless testbed, demonstrating its practical applicability for various IMAs. We will release our code to the community.",
        "authors": [
            "Xinye Cao",
            "Hongcan Guo",
            "Guoshun Nan",
            "Jiaoyang Cui",
            "Haoting Qian",
            "Yihan Lin",
            "Yilin Peng",
            "Diyang Zhang",
            "Yanzhao Hou",
            "Huici Wu",
            "Xiaofeng Tao",
            "Tony Q. S. Quek"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC",
            "cs.HC"
        ],
        "submit_date": "2025-07-28"
    },
    "http://arxiv.org/abs/2507.19845v1": {
        "id": "http://arxiv.org/abs/2507.19845v1",
        "title": "MegatronApp: Efficient and Comprehensive Management on Distributed LLM Training",
        "link": "http://arxiv.org/abs/2507.19845v1",
        "tags": [
            "training",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-07-29",
        "tldr": "Addresses systems-level challenges in distributed LLM training. Proposes MegatronApp, a toolchain with four modules for performance optimization, diagnosis, and interpretability. Achieves elevated reliability and efficiency in trillion-parameter training.",
        "abstract": "The rapid escalation in the parameter count of large language models (LLMs) has transformed model training from a single-node endeavor into a highly intricate, cross-node activity. While frameworks such as Megatron-LM successfully integrate tensor (TP), pipeline (PP), and data (DP) parallelism to enable trillion-parameter training, they simultaneously expose practitioners to unprecedented systems-level challenges in performance optimization, diagnosis, and interpretability. MegatronApp is an open-source toolchain expressly designed to meet these challenges. It introduces four orthogonal, yet seamlessly composable modules--MegaScan, MegaFBD, MegaDPP, and MegaScope--that collectively elevate the reliability, efficiency, and transparency of production-scale training. This paper presents the motivation, architecture, and distinctive contributions of each module, and elucidates how their synergistic integration augments the Megatron-LM ecosystem.",
        "authors": [
            "Bohan Zhao",
            "Guang Yang",
            "Shuo Chen",
            "Ruitao Liu",
            "Tingrui Zhang",
            "Yongchao He",
            "Wei Xu"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-07-26"
    },
    "http://arxiv.org/abs/2507.18889v1": {
        "id": "http://arxiv.org/abs/2507.18889v1",
        "title": "RailX: A Flexible, Scalable, and Low-Cost Network Architecture for Hyper-Scale LLM Training Systems",
        "link": "http://arxiv.org/abs/2507.18889v1",
        "tags": [
            "training",
            "networking",
            "autoscaling"
        ],
        "relevant": true,
        "indexed_date": "2025-07-28",
        "tldr": "Proposes RailX, a reconfigurable network architecture for hyperscale LLM training, using intra-node connectivity and inter-node circuit switching with Hamiltonian Decomposition. Reduces cost per bandwidth to less than 10% of Fat-Tree and interconnects 100K+ chips at $1.3B for 200K chips with 1.8TB bandwidth.",
        "abstract": "Increasingly large AI workloads are calling for hyper-scale infrastructure; however, traditional interconnection network architecture is neither scalable nor cost-effective enough. Tree-based topologies such as the \\textit{Rail-optimized} network are extremely expensive, while direct topologies such as \\textit{Torus} have insufficient bisection bandwidth and flexibility. In this paper, we propose \\textit{RailX}, a reconfigurable network architecture based on intra-node direct connectivity and inter-node circuit switching. Nodes and optical switches are physically 2D-organized, achieving better scalability than existing centralized circuit switching networks. We propose a novel interconnection method based on \\textit{Hamiltonian Decomposition} theory to organize separate rail-based rings into \\textit{all-to-all} topology, simultaneously optimizing ring-collective and all-to-all communication. More than $100$K chips with hyper bandwidth can be interconnected with a flat switching layer, and the diameter is only $2\\sim4$ inter-node hops. The network cost per injection/All-Reduce bandwidth of \\textit{RailX} is less than $10\\%$ of the Fat-Tree, and the cost per bisection/All-to-All bandwidth is less than $50\\%$ of the Fat-Tree. Specifically, only $\\sim$\\$$1.3$B is required to interconnect 200K chips with 1.8TB bandwidth. \\textit{RailX} can also be used in the ML-as-a-service (MLaaS) scenario, where single or multiple training workloads with various shapes, scales, and parallelism strategies can be flexibly mapped, and failures can be worked around.",
        "authors": [
            "Yinxiao Feng",
            "Tiancheng Chen",
            "Yuchen Wei",
            "Siyuan Shen",
            "Shiju Wang",
            "Wei Li",
            "Kaisheng Ma",
            "Torsten Hoefler"
        ],
        "categories": [
            "cs.AR",
            "cs.DC",
            "cs.NI"
        ],
        "submit_date": "2025-07-25"
    },
    "http://arxiv.org/abs/2507.18007v1": {
        "id": "http://arxiv.org/abs/2507.18007v1",
        "title": "Cloud Native System for LLM Inference Serving",
        "link": "http://arxiv.org/abs/2507.18007v1",
        "tags": [
            "serving",
            "autoscaling"
        ],
        "relevant": true,
        "indexed_date": "2025-07-25",
        "tldr": "Explores improving LLM inference resource efficiency using cloud native technologies. Leverages Kubernetes for dynamic autoscaling, containerization, and microservices to optimize resource allocation and handling workload fluctuations. Achieves reduced latency and enhanced throughput in high-demand scenarios.",
        "abstract": "Large Language Models (LLMs) are revolutionizing numerous industries, but their substantial computational demands create challenges for efficient deployment, particularly in cloud environments. Traditional approaches to inference serving often struggle with resource inefficiencies, leading to high operational costs, latency issues, and limited scalability. This article explores how Cloud Native technologies, such as containerization, microservices, and dynamic scheduling, can fundamentally improve LLM inference serving. By leveraging these technologies, we demonstrate how a Cloud Native system enables more efficient resource allocation, reduces latency, and enhances throughput in high-demand scenarios. Through real-world evaluations using Kubernetes-based autoscaling, we show that Cloud Native architectures can dynamically adapt to workload fluctuations, mitigating performance bottlenecks while optimizing LLM inference serving performance. This discussion provides a broader perspective on how Cloud Native frameworks could reshape the future of scalable LLM inference serving, offering key insights for researchers, practitioners, and industry leaders in cloud computing and artificial intelligence.",
        "authors": [
            "Minxian Xu",
            "Junhan Liao",
            "Jingfeng Wu",
            "Yiyuan He",
            "Kejiang Ye",
            "Chengzhong Xu"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-07-24"
    },
    "http://arxiv.org/abs/2507.18006v1": {
        "id": "http://arxiv.org/abs/2507.18006v1",
        "title": "Unlock the Potential of Fine-grained LLM Serving via Dynamic Module Scaling",
        "link": "http://arxiv.org/abs/2507.18006v1",
        "tags": [
            "serving",
            "autoscaling"
        ],
        "relevant": true,
        "indexed_date": "2025-07-25",
        "tldr": "Proposes CoCoServe, an elastic LLM serving system with dynamic module-level scaling (replication/migration of decoder layers) to optimize resource use under varying workloads. Achieves 14%-75% lower latency and 1.16x-4x higher throughput versus vLLM/Transformers while reducing costs by 46%.",
        "abstract": "The rise of large language models (LLMs) has created new opportunities across various fields but has also introduced significant challenges in resource management. Current LLM serving systems face a fundamental tension: balancing serving demands with limited resources while adapting to unpredictable traffic patterns. Static deployments lead to suboptimal resource utilization and performance degradation under dynamic workloads. Furthermore, the high cost of adjusting instances hinders dynamic scaling, limiting the true potential of efficient LLM serving.   To address this, we propose CoCoServe, an elastic system that facilitates dynamic and fine-grained scaling. Its key innovation lies in the module-level operations for the replication and migration of LLM modules, such as decoder layers and projections. Through a comprehensive analysis of the trade-offs associated with these operations, we develop an auto-scaling mechanism that dynamically regulates module-level resource allocation and performance optimization, enabling a more cost-effective deployment of LLMs. Our evaluation demonstrates that the scaling operations employed by CoCoServe exhibit excellent scalability and can reduce costs by 46% while maintaining availability. Compared to state-of-the-art LLM serving systems (e.g., Hugging Face Transformers and vLLM), our approach reduces latency by 14%-75% and achieves 1.16x-4x throughput on average across different model sizes and workloads.",
        "authors": [
            "Jingfeng Wu",
            "Yiyuan He",
            "Minxian Xu",
            "Xitong Gao",
            "Kejiang Ye",
            "Chengzhong Xu"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-07-24"
    },
    "http://arxiv.org/abs/2507.18454v1": {
        "id": "http://arxiv.org/abs/2507.18454v1",
        "title": "Sandwich: Separating Prefill-Decode Compilation for Efficient CPU LLM Serving",
        "link": "http://arxiv.org/abs/2507.18454v1",
        "tags": [
            "serving",
            "disaggregation",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-07-25",
        "tldr": "Proposes Sandwich, a CPU-based LLM serving engine that optimizes prefill and decode phases separately. Implements hardware-centric execution plans and kernel generation, achieving 2.01x average throughput improvement and lower latency requirements.",
        "abstract": "Utilizing CPUs to serve large language models (LLMs) is a resource-friendly alternative to GPU serving. Existing CPU-based solutions ignore workload differences between the prefill and the decode phases of LLM inference, applying a static per-NUMA (Non-Uniform Memory Access) node model partition and utilizing vendor libraries for operator-level execution, which is suboptimal. We propose Sandwich, a hardware-centric CPU-based LLM serving engine that uses different execution plans for the prefill and decode phases and optimizes them separately.   We evaluate Sandwich across diverse baselines and datasets on five CPU platforms, including x86 with AVX-2 and AVX-512, as well as ARM with NEON. Sandwich achieves an average 2.01x throughput improvement and 90% satisfactory time-to-first-token (TTFT) and time-per-output-token (TPOT) latencies with up to 3.40x lower requirements in single sequence serving, and significant improvement in Goodput in continuous-batching serving. The GEMM kernels generated by Sandwich outperform representative vendor kernels and other dynamic shape solutions, achieving performance comparable to static compilers with three orders of magnitude less kernel tuning costs.",
        "authors": [
            "Juntao Zhao",
            "Jiuru Li",
            "Chuan Wu"
        ],
        "categories": [
            "cs.AR",
            "cs.AI",
            "cs.DC",
            "cs.PL"
        ],
        "submit_date": "2025-05-19"
    },
    "http://arxiv.org/abs/2507.17133v1": {
        "id": "http://arxiv.org/abs/2507.17133v1",
        "title": "BrownoutServe: SLO-Aware Inference Serving under Bursty Workloads for MoE-based LLMs",
        "link": "http://arxiv.org/abs/2507.17133v1",
        "tags": [
            "serving",
            "MoE",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-07-24",
        "tldr": "Addresses inefficiency in serving MoE-based LLMs under bursty workloads due to static placement. Proposes BrownoutServe with dynamic token processing (brownout) and united experts to reduce expert accesses and latency. Achieves 2.07x throughput gain over vLLM and 90.28% fewer SLO violations.",
        "abstract": "In recent years, the Mixture-of-Experts (MoE) architecture has been widely applied to large language models (LLMs), providing a promising solution that activates only a subset of the model's parameters during computation, thereby reducing overall memory requirements and allowing for faster inference compared to dense models. Despite these advantages, existing systems still face issues of low efficiency due to static model placement and lack of dynamic workloads adaptation. This leads to suboptimal resource utilization and increased latency, especially during bursty requests periods.   To address these challenges, this paper introduces BrownoutServe, a novel serving framework designed to optimize inference efficiency and maintain service reliability for MoE-based LLMs under dynamic computational demands and traffic conditions. BrownoutServe introduces \"united experts\" that integrate knowledge from multiple experts, reducing the times of expert access and inference latency. Additionally, it proposes a dynamic brownout mechanism to adaptively adjust the processing of certain tokens, optimizing inference performance while guaranteeing service level objectives (SLOs) are met. Our evaluations show the effectiveness of BrownoutServe under various workloads: it achieves up to 2.07x throughput improvement compared to vLLM and reduces SLO violations by 90.28%, showcasing its robustness under bursty traffic while maintaining acceptable inference accuracy.",
        "authors": [
            "Jianmin Hu",
            "Minxian Xu",
            "Kejiang Ye",
            "Chengzhong Xu"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-07-23"
    },
    "http://arxiv.org/abs/2507.17120v1": {
        "id": "http://arxiv.org/abs/2507.17120v1",
        "title": "BucketServe: Bucket-Based Dynamic Batching for Smart and Efficient LLM Inference Serving",
        "link": "http://arxiv.org/abs/2507.17120v1",
        "tags": [
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-07-24",
        "tldr": "Proposes BucketServe, a bucket-based dynamic batching framework for LLM inference. Groups requests by sequence length to minimize padding overhead and adaptively adjusts batch sizes to optimize GPU memory. Achieves up to 3.58x higher throughput than UELLM and handles 1.93x more requests at 80% SLO attainment.",
        "abstract": "Large language models (LLMs) have become increasingly popular in various areas, traditional business gradually shifting from rule-based systems to LLM-based solutions. However, the inference of LLMs is resource-intensive or latency-sensitive, posing significant challenges for serving systems. Existing LLM serving systems often use static or continuous batching strategies, which can lead to inefficient GPU memory utilization and increased latency, especially under heterogeneous workloads. These methods may also struggle to adapt to dynamic workload fluctuations, resulting in suboptimal throughput and potential service level objective (SLO) violations. In this paper, we introduce BucketServe, a bucket-based dynamic batching framework designed to optimize LLM inference performance. By grouping requests into size-homogeneous buckets based on sequence length, BucketServe minimizes padding overhead and optimizes GPU memory usage through real-time batch size adjustments preventing out-of-memory (OOM) errors. It introduces adaptive bucket splitting/merging and priority-aware scheduling to mitigate resource fragmentation and ensure SLO compliance. Experiment shows that BucketServe significantly outperforms UELLM in throughput, achieving up to 3.58x improvement. It can also handle 1.93x more request load under the SLO attainment of 80% compared with DistServe and demonstrates 1.975x higher system load capacity compared to the UELLM.",
        "authors": [
            "Wanyi Zheng",
            "Minxian Xu",
            "Shengye Song",
            "Kejiang Ye"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-07-23"
    },
    "http://arxiv.org/abs/2507.16781v1": {
        "id": "http://arxiv.org/abs/2507.16781v1",
        "title": "Cooling Matters: Benchmarking Large Language Models and Vision-Language Models on Liquid-Cooled Versus Air-Cooled H100 GPU Systems",
        "link": "http://arxiv.org/abs/2507.16781v1",
        "tags": [
            "hardware",
            "training",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-07-23",
        "tldr": "Compares thermal and performance impacts of liquid vs. air cooling for LLM/VLM training on H100 GPU clusters. Measures GPU temperatures, FLOPs, and energy efficiency via controlled benchmarking. Liquid cooling achieves 17% higher TFLOPs/GPU and better power efficiency than air cooling.",
        "abstract": "The unprecedented growth in artificial intelligence (AI) workloads, recently dominated by large language models (LLMs) and vision-language models (VLMs), has intensified power and cooling demands in data centers. This study benchmarks LLMs and VLMs on two HGX nodes, each with 8x NVIDIA H100 graphics processing units (GPUs), using liquid and air cooling. Leveraging GPU Burn, Weights and Biases, and IPMItool, we collect detailed thermal, power, and computation data. Results show that the liquid-cooled systems maintain GPU temperatures between 41-50 degrees Celsius, while the air-cooled counterparts fluctuate between 54-72 degrees Celsius under load. This thermal stability of liquid-cooled systems yields 17 percent higher performance (54 TFLOPs per GPU vs. 46 TFLOPs per GPU), improved performance per watt, reduced energy overhead, and greater system efficiency than the air-cooled counterparts. These findings underscore the energy and sustainability benefits of liquid cooling, offering a compelling path forward for hyperscale data centers s",
        "authors": [
            "Imran Latif",
            "Muhammad Ali Shafique",
            "Hayat Ullah",
            "Alex C. Newkirk",
            "Xi Yu",
            "Arslan Munir"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-07-22"
    },
    "http://arxiv.org/abs/2507.16731v1": {
        "id": "http://arxiv.org/abs/2507.16731v1",
        "title": "Collaborative Inference and Learning between Edge SLMs and Cloud LLMs: A Survey of Algorithms, Execution, and Open Challenges",
        "link": "http://arxiv.org/abs/2507.16731v1",
        "tags": [
            "edge",
            "serving",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-07-23",
        "tldr": "Surveys collaborative inference and learning between cloud LLMs and edge SLMs. Explores strategies like task assignment, task division, speculative decoding, and resource-aware offloading. Addresses latency, cost, and privacy to enable scalable edge-cloud intelligence.",
        "abstract": "As large language models (LLMs) evolve, deploying them solely in the cloud or compressing them for edge devices has become inadequate due to concerns about latency, privacy, cost, and personalization. This survey explores a collaborative paradigm in which cloud-based LLMs and edge-deployed small language models (SLMs) cooperate across both inference and training. We present a unified taxonomy of edge-cloud collaboration strategies. For inference, we categorize approaches into task assignment, task division, and mixture-based collaboration at both task and token granularity, encompassing adaptive scheduling, resource-aware offloading, speculative decoding, and modular routing. For training, we review distributed adaptation techniques, including parameter alignment, pruning, bidirectional distillation, and small-model-guided optimization. We further summarize datasets, benchmarks, and deployment cases, and highlight privacy-preserving methods and vertical applications. This survey provides the first systematic foundation for LLM-SLM collaboration, bridging system and algorithm co-design to enable efficient, scalable, and trustworthy edge-cloud intelligence.",
        "authors": [
            "Senyao Li",
            "Haozhao Wang",
            "Wenchao Xu",
            "Rui Zhang",
            "Song Guo",
            "Jingling Yuan",
            "Xian Zhong",
            "Tianwei Zhang",
            "Ruixuan Li"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-07-22"
    },
    "http://arxiv.org/abs/2507.16274v2": {
        "id": "http://arxiv.org/abs/2507.16274v2",
        "title": "STAlloc: Enhancing Memory Efficiency in Large-Scale Model Training with Spatio-Temporal Planning",
        "link": "http://arxiv.org/abs/2507.16274v2",
        "tags": [
            "training",
            "MoE",
            "sparse"
        ],
        "relevant": true,
        "indexed_date": "2025-07-23",
        "tldr": "Addresses GPU memory fragmentation in large model training via STAlloc, a spatio-temporal planned memory allocator. Combines offline planning and online allocation to handle dense and MoE models. Reduces fragmentation by 85.1% on average, improving training throughput by up to 32.5%.",
        "abstract": "The rapid scaling of large language models (LLMs) has significantly increased GPU memory pressure, which is further aggravated by training optimization techniques such as virtual pipeline and recomputation that disrupt tensor lifespans and introduce considerable memory fragmentation. Such fragmentation stems from the use of online GPU memory allocators in popular deep learning frameworks like PyTorch, which disregard tensor lifespans. As a result, this inefficiency can waste as much as 43% of memory and trigger out-of-memory errors, undermining the effectiveness of optimization methods. To address this, we introduce STAlloc, a GPU memory allocator for deep learning frameworks that reduces fragmentation by exploiting the spatial and temporal regularity in memory allocation behaviors of training workloads. STAlloc introduces a novel paradigm that combines offline planning with online allocation. The offline planning leverages spatio-temporal regularities to generate a near-optimal allocation plan, while the online allocation handles complex and dynamic models such as Mixture-of-Experts (MoE). Built as a pluggable PyTorch memory allocator, STAlloc reduces fragmentation ratio on average by 85.1% (up to 100%) across both dense and MoE models, with negligible overhead. This enables more efficient, high-throughput training configurations and improves throughput performance by up to 32.5%.",
        "authors": [
            "Zixiao Huang",
            "Junhao Hu",
            "Hao Lin",
            "Chunyang Zhu",
            "Yueran Tang",
            "Quanlu Zhang",
            "Zhen Guo",
            "Zhenhua Li",
            "Shengen Yan",
            "Zhenhua Zhu",
            "Guohao Dai",
            "Yu Wang"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC",
            "cs.PF"
        ],
        "submit_date": "2025-07-22"
    },
    "http://arxiv.org/abs/2507.15553v1": {
        "id": "http://arxiv.org/abs/2507.15553v1",
        "title": "Efficient Routing of Inference Requests across LLM Instances in Cloud-Edge Computing",
        "link": "http://arxiv.org/abs/2507.15553v1",
        "tags": [
            "serving",
            "edge",
            "autoscaling"
        ],
        "relevant": true,
        "indexed_date": "2025-07-22",
        "tldr": "Proposes an NSGA-II-based routing algorithm for cloud-edge LLM inference that balances response quality, time, and cost. Evaluated on diverse datasets, it improves response time by up to 95.2% and reduces cost by 34.9% compared to baselines.",
        "abstract": "The rising demand for Large Language Model (LLM) inference services has intensified pressure on computational resources, resulting in latency and cost challenges. This paper introduces a novel routing algorithm based on the Non-dominated Sorting Genetic Algorithm II (NSGA-II) to distribute inference requests across heterogeneous LLM instances in a cloud-edge computing environment. Formulated as a multi-objective optimization problem, the algorithm balances response quality, response time, and inference cost, adapting to request heterogeneity (e.g., varying complexity and prompt lengths) and node diversity (e.g., edge vs. cloud resources). This adaptive routing algorithm optimizes performance under dynamic workloads. We benchmark the approach using a testbed with datasets including Stanford Question Answering Dataset (SQuAD), Mostly Basic Python Problems (MBPP), Hella Situations With Adversarial Generations (HellaSwag), and Grade School Math 8K (GSM8K). Experimental results show our solution, compared to the baselines, achieves up to 95.2% and 34.9% improvements in terms of response time and cost, respectively. These findings validate the algorithm's effectiveness for scalable LLM deployments.",
        "authors": [
            "Shibo Yu",
            "Mohammad Goudarzi",
            "Adel Nadjaran Toosi"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-07-21"
    },
    "http://arxiv.org/abs/2507.15230v3": {
        "id": "http://arxiv.org/abs/2507.15230v3",
        "title": "GALE: Leveraging Heterogeneous Systems for Efficient Unstructured Mesh Data Analysis",
        "link": "http://arxiv.org/abs/2507.15230v3",
        "tags": [
            "offloading",
            "kernel",
            "storage"
        ],
        "relevant": true,
        "indexed_date": "2025-07-22",
        "tldr": "Accelerates unstructured mesh data analysis by offloading connectivity computation to GPUs. Proposes GALE, a CUDA-based data structure enabling CPU-GPU task parallelism for visualization algorithms. Achieves 2.7x speedup over state-of-the-art while maintaining memory efficiency.",
        "abstract": "Unstructured meshes present challenges in scientific data analysis due to irregular distribution and complex connectivity. Computing and storing connectivity information is a major bottleneck for visualization algorithms, affecting both time and memory performance. Recent task-parallel data structures address this by precomputing connectivity information at runtime while the analysis algorithm executes, effectively hiding computation costs and improving performance. However, existing approaches are CPU-bound, forcing the data structure and analysis algorithm to compete for the same computational resources, limiting potential speedups. To overcome this limitation, we introduce a novel task-parallel approach optimized for heterogeneous CPU-GPU systems. Specifically, we offload the computation of mesh connectivity information to GPU threads, enabling CPU threads to focus on executing the visualization algorithm. Following this paradigm, we propose GALE (GPU-Aided Localized data structurE), the first open-source CUDA-based data structure designed for heterogeneous task parallelism. Experiments on two 20-core CPUs and an NVIDIA V100 GPU show that GALE achieves up to 2.7x speedup over state-of-the-art localized data structures while maintaining memory efficiency.",
        "authors": [
            "Guoxi Liu",
            "Thomas Randall",
            "Rong Ge",
            "Federico Iuricich"
        ],
        "categories": [
            "cs.DC",
            "cs.GR"
        ],
        "submit_date": "2025-07-21"
    },
    "http://arxiv.org/abs/2507.14928v1": {
        "id": "http://arxiv.org/abs/2507.14928v1",
        "title": "Byzantine-Robust Decentralized Coordination of LLM Agents",
        "link": "http://arxiv.org/abs/2507.14928v1",
        "tags": [
            "agentic",
            "networking",
            "RL"
        ],
        "relevant": true,
        "indexed_date": "2025-07-22",
        "tldr": "Proposes DecentLLMs for Byzantine-robust decentralized coordination of LLM agents. Introduces worker-evaluator architecture with concurrent answer generation and independent scoring to avoid leader vulnerabilities. Achieves faster consensus and higher-quality answer selection under Byzantine attacks.",
        "abstract": "Collaboration among multiple large language model (LLM) agents is a promising approach to overcome inherent limitations of single-agent systems, such as hallucinations and single points of failure. As LLM agents are increasingly deployed on open blockchain platforms, multi-agent systems capable of tolerating malicious (Byzantine) agents have become essential.   Recent Byzantine-robust multi-agent systems typically rely on leader-driven coordination, which suffers from two major drawbacks. First, they are inherently vulnerable to targeted attacks against the leader. If consecutive leaders behave maliciously, the system repeatedly fails to achieve consensus, forcing new consensus rounds, which is particularly costly given the high latency of LLM invocations. Second, an underperforming proposal from the leader can be accepted as the final answer even when higher-quality alternatives are available, as existing methods finalize the leader's proposal once it receives a quorum of votes.   To address these issues, we propose DecentLLMs, a novel decentralized consensus approach for multi-agent LLM systems, where worker agents generate answers concurrently and evaluator agents independently score and rank these answers to select the best available one. This decentralized architecture enables faster consensus despite the presence of Byzantine agents and consistently selects higher-quality answers through Byzantine-robust aggregation techniques.   Experimental results demonstrate that DecentLLMs effectively tolerates Byzantine agents and significantly improves the quality of selected answers.",
        "authors": [
            "Yongrae Jo",
            "Chanik Park"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-07-20"
    },
    "http://arxiv.org/abs/2507.14392v1": {
        "id": "http://arxiv.org/abs/2507.14392v1",
        "title": "Characterizing Communication Patterns in Distributed Large Language Model Inference",
        "link": "http://arxiv.org/abs/2507.14392v1",
        "tags": [
            "serving",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-07-22",
        "tldr": "Analyzes communication patterns in distributed LLM inference. Combines profiling and analytical models to compare parallelism strategies: tensor (low latency), pipeline (low data transfer), and hybrid. Shows tensor parallelism improves response times for short sequences. Identifies optimization opportunities for inference frameworks.",
        "abstract": "Large Language Models (LLMs) built on transformer architectures have transformed natural language processing, achieving remarkable performance across diverse applications. While distributed inference frameworks enable practical deployment of these models, inter-GPU communication creates significant performance constraints that limit service quality in real-world systems. This paper investigates communication dynamics in distributed LLM serving-analyzing how various parallelization approaches coordinate data exchange between GPU workers during inference. We study dense transformer-based models as representative examples of contemporary architectures widely used in operational deployments. Our work combines detailed profiling measurements with predictive analytical models to characterize communication behavior across different parallelization configurations. Results show that tensor parallelism incurs substantial network overhead but delivers superior response times for brief sequences, pipeline parallelism minimizes data transfer requirements while increasing total latency, and combined approaches demand careful tuning to achieve balanced performance. These insights offer practical recommendations for selecting appropriate parallelization schemes in production LLM services and identify key opportunities for optimizing inference frameworks and communication infrastructure.",
        "authors": [
            "Lang Xu",
            "Kaushik Kandadi Suresh",
            "Quentin Anthony",
            "Nawras Alnaasan",
            "Dhabaleswar K. Panda"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-07-18"
    },
    "http://arxiv.org/abs/2507.14682v1": {
        "id": "http://arxiv.org/abs/2507.14682v1",
        "title": "IDSS, a Novel P2P Relational Data Storage Service",
        "link": "http://arxiv.org/abs/2507.14682v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-07-22",
        "tldr": "",
        "abstract": "The rate at which data is generated has been increasing rapidly, raising challenges related to its management. Traditional database management systems suffer from scalability and are usually inefficient when dealing with large-scale and heterogeneous data. This paper introduces IDSS (InnoCyPES Data Storage Service), a novel large-scale data storage tool that leverages peer-to-peer networks and embedded relational databases. We present the IDSS architecture and its design, and provide details related to the implementation. The peer-to-peer framework is used to provide support for distributed queries leveraging a relational database architecture based on a common schema. Furthermore, methods to support complex distributed query processing, enabling robust and efficient management of vast amounts of data are presented.",
        "authors": [
            "Massimo Cafaro",
            "Italo Epicoco",
            "Marco Pulimeno",
            "Lunodzo J. Mwinuka",
            "Lucas Pereira",
            "Hugo Morais"
        ],
        "categories": [
            "cs.DB",
            "cs.DC"
        ],
        "submit_date": "2025-07-19"
    },
    "http://arxiv.org/abs/2507.14179v1": {
        "id": "http://arxiv.org/abs/2507.14179v1",
        "title": "A Sparsity Predicting Approach for Large Language Models via Activation Pattern Clustering",
        "link": "http://arxiv.org/abs/2507.14179v1",
        "tags": [
            "sparse",
            "kernel",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-07-22",
        "tldr": "Proposes a clustering-based activation pattern compression to predict sparsity in LLM activations for efficient inference. Groups neuron activations into clusters to reduce prediction overhead. Achieves 79.34% clustering precision with low perplexity degradation (PPL=12.49).",
        "abstract": "Large Language Models (LLMs) exhibit significant activation sparsity, where only a subset of neurons are active for a given input. Although this sparsity presents opportunities to reduce computational cost, efficiently utilizing it requires predicting activation patterns in a scalable manner. However, direct prediction at the neuron level is computationally expensive due to the vast number of neurons in modern LLMs. To enable efficient prediction and utilization of activation sparsity, we propose a clustering-based activation pattern compression framework. Instead of treating each neuron independently, we group similar activation patterns into a small set of representative clusters. Our method achieves up to 79.34% clustering precision, outperforming standard binary clustering approaches while maintaining minimal degradation in perplexity (PPL) scores. With a sufficiently large number of clusters, our approach attains a PPL score as low as 12.49, demonstrating its effectiveness in preserving model quality while reducing computational overhead. By predicting cluster assignments rather than individual neuron states, future models can efficiently infer activation patterns from pre-computed centroids. We detail the clustering algorithm, analyze its effectiveness in capturing meaningful activation structures, and demonstrate its potential to improve sparse computation efficiency. This clustering-based formulation serves as a foundation for future work on activation pattern prediction, paving the way for efficient inference in large-scale language models.",
        "authors": [
            "Nobel Dhar",
            "Bobin Deng",
            "Md Romyull Islam",
            "Xinyue Zhang",
            "Kazi Fahim Ahmad Nasif",
            "Kun Suo"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "cs.DC"
        ],
        "submit_date": "2025-07-11"
    },
    "http://arxiv.org/abs/2507.13833v3": {
        "id": "http://arxiv.org/abs/2507.13833v3",
        "title": "DistFlow: A Fully Distributed RL Framework for Scalable and Efficient LLM Post-Training",
        "link": "http://arxiv.org/abs/2507.13833v3",
        "tags": [
            "RL",
            "training",
            "scaling"
        ],
        "relevant": true,
        "indexed_date": "2025-07-21",
        "tldr": "Addresses scalability bottlenecks in reinforcement learning post-training for LLMs. Proposes DistFlow, a fully distributed RL framework with multi-controller architecture eliminating centralized nodes. Achieves near-linear scalability up to 1024 GPUs and 7x end-to-end throughput improvement over SOTA.",
        "abstract": "Reinforcement learning (RL) has become the pivotal post-training technique for large language model (LLM). Effectively scaling reinforcement learning is now the key to unlocking advanced reasoning capabilities and ensuring safe, goal-aligned behavior in the most powerful LLMs. Mainstream frameworks usually employ a hybrid-controller architecture where a single-controller dispatches the overall execution logic and manages overall data transfer and the multi-controller executes distributed computation. For large-scale reinforcement learning, minor load imbalances can introduce significant bottlenecks, ultimately constraining the scalability of the system. To address this limitation, we introduce DistFlow, a novel, fully distributed RL framework designed to break scaling barrier. We adopt a multi-controller paradigm that dispatches data transfer and execution tasks to all workers, which eliminates the centralized node. This allows each worker to operate independently, leading to near-linear scalability up to 1024 GPUs and dramatic efficiency gains. Furthermore, our architecture decouples resource configuration from execution logic, allowing each worker to have a unique execution flow, offering significant flexibility for rapid and cost-effective algorithmic experimentation. Extensive experiments show that DistFlow achieves excellent linear scalability and up to a 7x end-to-end throughput improvement in specific scenarios over state-of-the-art (SOTA) frameworks.",
        "authors": [
            "Zhixin Wang",
            "Tianyi Zhou",
            "Liming Liu",
            "Ao Li",
            "Jiarui Hu",
            "Dian Yang",
            "Yinhui Lu",
            "Jinlong Hou",
            "Siyuan Feng",
            "Yuan Cheng",
            "Yuan Qi"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-07-18"
    },
    "http://arxiv.org/abs/2507.13601v1": {
        "id": "http://arxiv.org/abs/2507.13601v1",
        "title": "Leveraging Multi-Instance GPUs through moldable task scheduling",
        "link": "http://arxiv.org/abs/2507.13601v1",
        "tags": [
            "training",
            "scheduling",
            "autoscaling"
        ],
        "relevant": true,
        "indexed_date": "2025-07-21",
        "tldr": "Proposes moldable task scheduling with dynamic MIG GPU reconfigurations to minimize makespan in multi-task execution. FAR algorithm combines classic moldability, novel repartitioning heuristics, and local search. Achieves up to 1.10-1.22x makespan ratio to optimum in experiments with synthetic and benchmark tasks.",
        "abstract": "NVIDIA MIG (Multi-Instance GPU) allows partitioning a physical GPU into multiple logical instances with fully-isolated resources, which can be dynamically reconfigured. This work highlights the untapped potential of MIG through moldable task scheduling with dynamic reconfigurations. Specifically, we propose a makespan minimization problem for multi-task execution under MIG constraints. Our profiling shows that assuming monotonicity in task work with respect to resources is not viable, as is usual in multicore scheduling. Relying on a state-of-the-art proposal that does not require such an assumption, we present FAR, a 3-phase algorithm to solve the problem. Phase 1 of FAR builds on a classical task moldability method, phase 2 combines Longest Processing Time First and List Scheduling with a novel repartitioning tree heuristic tailored to MIG constraints, and phase 3 employs local search via task moves and swaps. FAR schedules tasks in batches offline, concatenating their schedules on the fly in an improved way that favors resource reuse. Excluding reconfiguration costs, the List Scheduling proof shows an approximation factor of 7/4 on the NVIDIA A30 model. We adapt the technique to the particular constraints of an NVIDIA A100/H100 to obtain an approximation factor of 2. Including the reconfiguration cost, our real-world experiments reveal a makespan with respect to the optimum no worse than 1.22x for a well-known suite of benchmarks, and 1.10x for synthetic inputs inspired by real kernels. We obtain good experimental results for each batch of tasks, but also in the concatenation of batches, with large improvements over the state-of-the-art and proposals without GPU reconfiguration. Beyond the algorithm, the paper demonstrates the research potential of the MIG technology and suggests useful metrics, workload characterizations and evaluation techniques for future work in this field.",
        "authors": [
            "Jorge Villarrubia",
            "Luis Costero",
            "Francisco D. Igual",
            "Katzalin Olcoz"
        ],
        "categories": [
            "cs.DC",
            "cs.ET",
            "cs.PF"
        ],
        "submit_date": "2025-07-18"
    },
    "http://arxiv.org/abs/2507.13736v1": {
        "id": "http://arxiv.org/abs/2507.13736v1",
        "title": "An End-to-End DNN Inference Framework for the SpiNNaker2 Neuromorphic MPSoC",
        "link": "http://arxiv.org/abs/2507.13736v1",
        "tags": [
            "edge",
            "kernel",
            "quantization"
        ],
        "relevant": true,
        "indexed_date": "2025-07-21",
        "tldr": "Proposes an end-to-end DNN inference framework for transformers on SpiNNaker2 neuromorphic MPSoC. Uses quantization and multi-layer scheduling to enable edge execution. Achieves execution of large transformer models on a single chip.",
        "abstract": "This work presents a multi-layer DNN scheduling framework as an extension of OctopuScheduler, providing an end-to-end flow from PyTorch models to inference on a single SpiNNaker2 chip. Together with a front-end comprised of quantization and lowering steps, the proposed framework enables the edge-based execution of large and complex DNNs up to transformer scale using the neuromorphic platform SpiNNaker2.",
        "authors": [
            "Matthias Jobst",
            "Tim Langer",
            "Chen Liu",
            "Mehmet Alici",
            "Hector A. Gonzalez",
            "Christian Mayr"
        ],
        "categories": [
            "cs.LG",
            "cs.AR",
            "cs.DC"
        ],
        "submit_date": "2025-07-18"
    },
    "http://arxiv.org/abs/2507.12619v1": {
        "id": "http://arxiv.org/abs/2507.12619v1",
        "title": "BootSeer: Analyzing and Mitigating Initialization Bottlenecks in Large-Scale LLM Training",
        "link": "http://arxiv.org/abs/2507.12619v1",
        "tags": [
            "training",
            "storage",
            "sparse"
        ],
        "relevant": true,
        "indexed_date": "2025-07-18",
        "tldr": "Addresses startup overhead in large-scale LLM training jobs. Proposes Bootseer, a framework with hot block prefetch, dependency snapshotting, and striped HDFS-FUSE to reduce initialization delays. Achieves 50% reduction in startup overhead in production.",
        "abstract": "Large Language Models (LLMs) have become a cornerstone of modern AI, driving breakthroughs in natural language processing and expanding into multimodal jobs involving images, audio, and video. As with most computational software, it is important to distinguish between ordinary runtime performance and startup overhead. Prior research has focused on runtime performance: improving training efficiency and stability. This work focuses instead on the increasingly critical issue of startup overhead in training: the delay before training jobs begin execution. Startup overhead is particularly important in large, industrial-scale LLMs, where failures occur more frequently and multiple teams operate in iterative update-debug cycles. In one of our training clusters, more than 3.5% of GPU time is wasted due to startup overhead alone.   In this work, we present the first in-depth characterization of LLM training startup overhead based on real production data. We analyze the components of startup cost, quantify its direct impact, and examine how it scales with job size. These insights motivate the design of Bootseer, a system-level optimization framework that addresses three primary startup bottlenecks: (a) container image loading, (b) runtime dependency installation, and (c) model checkpoint resumption. To mitigate these bottlenecks, Bootseer introduces three techniques: (a) hot block record-and-prefetch, (b) dependency snapshotting, and (c) striped HDFS-FUSE. Bootseer has been deployed in a production environment and evaluated on real LLM training workloads, demonstrating a 50% reduction in startup overhead.",
        "authors": [
            "Rui Li",
            "Xiaoyun Zhi",
            "Jinxin Chi",
            "Menghan Yu",
            "Lixin Huang",
            "Jia Zhu",
            "Weilun Zhang",
            "Xing Ma",
            "Wenjia Liu",
            "Zhicheng Zhu",
            "Daowen Luo",
            "Zuquan Song",
            "Xin Yin",
            "Chao Xiang",
            "Shuguang Wang",
            "Wencong Xiao",
            "Gene Cooperman"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC"
        ],
        "submit_date": "2025-07-16"
    },
    "http://arxiv.org/abs/2507.12205v1": {
        "id": "http://arxiv.org/abs/2507.12205v1",
        "title": "Toward Efficient SpMV in Sparse LLMs via Block Extraction and Compressed Storage",
        "link": "http://arxiv.org/abs/2507.12205v1",
        "tags": [
            "kernel",
            "sparse",
            "inference"
        ],
        "relevant": true,
        "indexed_date": "2025-07-17",
        "tldr": "Addresses inefficient SpMV for sparse LLM inference with batch size one. Proposes EC-SpMV with hierarchical block extraction and EC-CSR format using delta indexing for compression. Achieves 6.44x speedup over prior SpMV libraries and 55.4% storage reduction.",
        "abstract": "Sparse Matrix-Vector Multiplication (SpMV) has become a critical performance bottleneck in the local deployment of sparse Large Language Models (LLMs), where inference predominantly operates on workloads during the decoder phase with a batch size of one. Existing SpMV kernels and sparse matrix formats, originally designed for scientific computing, fail to exploit the unique structure patterns inherent in sparse LLMs, resulting in suboptimal performance and excessive storage overhead. This paper presents EC-SpMV, a GPU-optimized SpMV approach for accelerating sparse LLM inference. EC-SpMV introduces (1) a hierarchical block extraction algorithm that captures multiple granularities of block structures within sparse LLMs, and (2) a novel compressed sparse format (EC-CSR) that employs delta indexing to reduce storage overhead and enhance memory access efficiency. Evaluated on real sparse weight matrices from LLaMA and OPT models, EC-SpMV achieves up to 6.44x speedup over state-of-the-art SpMV libraries and reduces storage overhead by up to 55.4% compared to CSR.",
        "authors": [
            "Junqing Lin",
            "Jingwei Sun",
            "Mingge Lu",
            "Guangzhong Sun"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-07-16"
    },
    "http://arxiv.org/abs/2507.11830v1": {
        "id": "http://arxiv.org/abs/2507.11830v1",
        "title": "Arctic Inference with Shift Parallelism: Fast and Efficient Open Source Inference System for Enterprise AI",
        "link": "http://arxiv.org/abs/2507.11830v1",
        "tags": [
            "serving",
            "quantization",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-07-17",
        "tldr": "Addresses trade-offs in LLM inference between latency, throughput, and cost. Introduces Shift Parallelism and integrates speculative decoding, SwiftKV compute reduction, and optimized embeddings. Achieves 3.4× faster request completion and 1.6M tokens/sec per GPU for embeddings.",
        "abstract": "Inference is now the dominant AI workload, yet existing systems force trade-offs between latency, throughput, and cost. Arctic Inference, an open-source vLLM plugin from Snowflake AI Research, introduces Shift Parallelism, a dynamic parallelism strategy that adapts to real-world traffic while integrating speculative decoding, SwiftKV compute reduction, and optimized embedding inference. It achieves up to 3.4 times faster request completion, 1.75 times faster generation, and 1.6M tokens/sec per GPU for embeddings, outperforming both latency- and throughput-optimized deployments. Already powering Snowflake Cortex AI, Arctic Inference delivers state-of-the-art, cost-effective inference for enterprise AI and is now available to the community.",
        "authors": [
            "Samyam Rajbhandari",
            "Mert Hidayetoglu",
            "Aurick Qiao",
            "Ye Wang",
            "Juncheng Yang",
            "Jeff Rasley",
            "Michael Wyatt",
            "Yuxiong He"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-07-16"
    },
    "http://arxiv.org/abs/2507.11417v1": {
        "id": "http://arxiv.org/abs/2507.11417v1",
        "title": "Quantifying the Energy Consumption and Carbon Emissions of LLM Inference via Simulations",
        "link": "http://arxiv.org/abs/2507.11417v1",
        "tags": [
            "serving",
            "quantization",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-07-16",
        "tldr": "Proposes a high-fidelity LLM inference simulation framework integrating GPU power modeling to quantify energy consumption and carbon emissions. Explores batch size, sequence lengths, and carbon-aware scheduling. Demonstrates renewable offset potential up to 69.2% in deployment scenarios.",
        "abstract": "The environmental impact of Large Language Models (LLMs) is rising significantly, with inference now accounting for more than half of their total lifecycle carbon emissions. However, existing simulation frameworks, which are increasingly used to determine efficient LLM deployments, lack any concept of power and, therefore, cannot accurately estimate inference-related emissions. We present a simulation framework to assess the energy and carbon implications of LLM inference under varying deployment setups. First, we extend a high-fidelity LLM inference simulator with a GPU power model that estimates power consumption based on utilization metrics, enabling analysis across configurations like batch size, sequence length, and model parallelism. Second, we integrate simulation outputs into an energy system co-simulation environment to quantify carbon emissions under specific grid conditions and explore the potential of carbon-aware scheduling. Through scenario-based analysis, our framework reveals how inference parameters affect energy demand and carbon footprint, demonstrates a renewable offset potential of up to 69.2% in an illustrative deployment case, and provides a foundation for future carbon-aware inference infrastructure design.",
        "authors": [
            "Miray Özcan",
            "Philipp Wiesner",
            "Philipp Weiß",
            "Odej Kao"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-07-15"
    },
    "http://arxiv.org/abs/2507.11507v2": {
        "id": "http://arxiv.org/abs/2507.11507v2",
        "title": "Oneiros: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving",
        "link": "http://arxiv.org/abs/2507.11507v2",
        "tags": [
            "serving",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-07-16",
        "tldr": "Proposes Oneiros, a KV cache optimization method for multi-tenant LLM serving that remaps and repurposes parameter memory for KV cache storage. This approach avoids CPU-offloading overheads, achieving up to 82.5% reduction in tail latency and 86.7% higher throughput than vLLM.",
        "abstract": "KV cache accelerates LLM inference by avoiding redundant computation, at the expense of memory. To support larger KV caches, prior work extends GPU memory with CPU memory via CPU-offloading. This involves swapping KV cache between GPU and CPU memory. However, because the cache updates dynamically, such swapping incurs high CPU memory traffic. We make a key observation that model parameters remain constant during runtime, unlike the dynamically updated KV cache. Building on this, we introduce Oneiros, which avoids KV cache swapping by remapping, and thereby repurposing, the memory allocated to model parameters for KV cache. This parameter remapping is especially beneficial in multi-tenant environments, where the memory used for the parameters of the inactive models can be more aggressively reclaimed. Exploiting the high CPU-GPU bandwidth offered by the modern hardware, such as the NVIDIA Grace Hopper Superchip, we show that Oneiros significantly outperforms state-of-the-art solutions, achieving a reduction of 44.8%-82.5% in tail time-between-token latency, 20.7%-99.3% in tail time-to-first-token latency, and 6.6%-86.7% higher throughput compared to vLLM. Source code of Oneiros is available at https://github.com/UT-SysML/Oneiros/.",
        "authors": [
            "Ruihao Li",
            "Shagnik Pal",
            "Vineeth Narayan Pullu",
            "Prasoon Sinha",
            "Jeeho Ryoo",
            "Lizy K. John",
            "Neeraja J. Yadwadkar"
        ],
        "categories": [
            "cs.OS"
        ],
        "submit_date": "2025-07-15"
    },
    "http://arxiv.org/abs/2507.10392v1": {
        "id": "http://arxiv.org/abs/2507.10392v1",
        "title": "Zorse: Optimizing LLM Training Efficiency on Heterogeneous GPU Clusters",
        "link": "http://arxiv.org/abs/2507.10392v1",
        "tags": [
            "training",
            "storage",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-07-15",
        "tldr": "Addresses efficient LLM training on heterogeneous GPU clusters. Proposes Zorse, integrating asymmetric pipeline and data parallelism with adaptive partitioning for varied hardware. Achieves superior performance over SOTA in heterogeneous setups.",
        "abstract": "Large language models (LLMs) require vast amounts of GPU compute to train, but limited availability and high costs of GPUs make homogeneous clusters impractical for many organizations. Instead, assembling heterogeneous clusters by pooling together GPUs of different generations allows them to achieve higher aggregate compute and make use of all available GPUs. However, training on heterogeneous clusters presents several challenges, including load balancing across GPUs, optimizing memory usage to accommodate varying memory capacities, and ensuring communication-efficient training over diverse network interconnects potentially spanning multiple datacenters. In this paper, we make the case that efficient training on heterogeneous clusters requires (1) the integration of pipeline parallelism and data parallelism in a manner that is both communication- and memory-efficient, and (2) a more adaptable configuration of pipeline and data parallelism, which includes the capability to flexibly partition GPUs into asymmetric pipeline parallel stages and to incorporate heterogeneous GPUs within the same data parallelism group. We propose Zorse, the first system to unify all these capabilities while incorporating a planner that automatically configures training strategies for a given workload. Our evaluation shows that Zorse significantly outperforms state-of-the-art systems in heterogeneous training scenarios.",
        "authors": [
            "Runsheng Benson Guo",
            "Utkarsh Anand",
            "Khuzaima Daudjee",
            "Rathijit Sen"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-07-14"
    },
    "http://arxiv.org/abs/2507.10259v2": {
        "id": "http://arxiv.org/abs/2507.10259v2",
        "title": "Temporal-Aware GPU Resource Allocation for Distributed LLM Inference via Reinforcement Learning",
        "link": "http://arxiv.org/abs/2507.10259v2",
        "tags": [
            "serving",
            "autoscaling",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-07-15",
        "tldr": "Proposes TORTA, a spatiotemporal framework with two-layer scheduling for distributed LLM inference. Uses reinforcement learning for inter-region task coordination and fine-grained server assignment to reduce latency. Cuts response time by 15%, improves load balance by 5%, and lowers operational costs by 20%.",
        "abstract": "The rapid growth of large language model (LLM) services imposes increasing demands on distributed GPU inference infrastructure. Most existing scheduling systems follow a reactive paradigm, relying solely on the current system state to make decisions, without considering how task demand and resource availability evolve over time. This lack of temporal awareness in reactive approaches leads to inefficient GPU utilization, high task migration overhead, and poor system responsiveness under dynamic workloads. In this work, we identify the fundamental limitations of these instantaneous-state-only scheduling approaches and propose Temporal Optimal Resource scheduling via Two-layer Architecture (TORTA). TORTA introduces a spatiotemporal scheduling framework that captures both long-term workload patterns and short-term execution constraints. It adopts a two-layer design: a macro-level scheduler leverages reinforcement learning and optimal transport to coordinate inter-region task distribution, while a micro-level allocator refines task-to-server assignments within each region to reduce latency and switching costs. Experimental results across multiple network topologies show that TORTA reduces average inference response time by up to 15\\%, improves load balance by approximately 4-5\\%, and cuts total operational cost by 10-20\\% compared to state-of-the-art baseline methods.",
        "authors": [
            "Chengze Du",
            "Zhiwei Yu",
            "Heng Xu",
            "Haojie Wang",
            "Bo liu",
            "Jialong Li"
        ],
        "categories": [
            "cs.DC",
            "cs.NI"
        ],
        "submit_date": "2025-07-14"
    },
    "http://arxiv.org/abs/2507.10150v1": {
        "id": "http://arxiv.org/abs/2507.10150v1",
        "title": "Past-Future Scheduler for LLM Serving under SLA Guarantees",
        "link": "http://arxiv.org/abs/2507.10150v1",
        "tags": [
            "serving",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-07-15",
        "tldr": "Addresses memory misestimation challenges in LLM serving to meet SLAs. Proposes Past-Future scheduler using historical output length distributions to predict peak memory usage, reducing queuing and evictions. LightLLM framework achieves 2-3x higher goodput under heavy loads.",
        "abstract": "The exploration and application of Large Language Models (LLMs) is thriving. To reduce deployment costs, continuous batching has become an essential feature in current service frameworks. The effectiveness of continuous batching relies on an accurate estimate of the memory requirements of requests. However, due to the diversity in request output lengths, existing frameworks tend to adopt aggressive or conservative schedulers, which often result in significant overestimation or underestimation of memory consumption. Consequently, they suffer from harmful request evictions or prolonged queuing times, failing to achieve satisfactory throughput under strict Service Level Agreement (SLA) guarantees (a.k.a. goodput), across various LLM application scenarios with differing input-output length distributions. To address this issue, we propose a novel Past-Future scheduler that precisely estimates the peak memory resources required by the running batch via considering the historical distribution of request output lengths and calculating memory occupancy at each future time point. It adapts to applications with all types of input-output length distributions, balancing the trade-off between request queuing and harmful evictions, thereby consistently achieving better goodput. Furthermore, to validate the effectiveness of the proposed scheduler, we developed a high-performance LLM serving framework, LightLLM, that implements the Past-Future scheduler. Compared to existing aggressive or conservative schedulers, LightLLM demonstrates superior goodput, achieving up to 2-3$\\times$ higher goodput than other schedulers under heavy loads. LightLLM is open source to boost the research in such direction (https://github.com/ModelTC/lightllm).",
        "authors": [
            "Ruihao Gong",
            "Shihao Bai",
            "Siyu Wu",
            "Yunqian Fan",
            "Zaijun Wang",
            "Xiuhong Li",
            "Hailong Yang",
            "Xianglong Liu"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-07-14"
    },
    "http://arxiv.org/abs/2507.10069v3": {
        "id": "http://arxiv.org/abs/2507.10069v3",
        "title": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal Parallelism",
        "link": "http://arxiv.org/abs/2507.10069v3",
        "tags": [
            "serving",
            "multi-modal",
            "autoscaling"
        ],
        "relevant": true,
        "indexed_date": "2025-07-15",
        "tldr": "Addresses the challenge of efficiently serving multimodal LLMs under heterogeneous workloads. Proposes Elastic Multimodal Parallelism (EMP) with dynamic resource allocation, stage decoupling, and multimodal caching. Reduces TTFT latency by up to 4.2x and increases throughput by 3.2-4.5x.",
        "abstract": "Multimodal large language models (MLLMs) extend LLMs to handle images, videos, and audio by incorporating feature extractors and projection modules. However, these additional components -- combined with complex inference pipelines and heterogeneous workloads -- introduce significant inference overhead. Therefore, efficiently serving MLLMs remains a major challenge. Current tightly coupled serving architectures struggle to distinguish between mixed request types or adapt parallelism strategies to different inference stages, leading to increased time-to-first-token (TTFT) latency and poor resource utilization. To address this, we introduce Elastic Multimodal Parallelism (EMP), a new serving paradigm that elastically adapts to resource heterogeneity across request types and inference stages. Building upon EMP, we develop ElasticMM, an MLLM serving system that (1) separates requests into independent modality groups with dynamic resource allocation via a modality-aware load balancer; (2) decouples inference stages and enables parallelism adjustment and adaptive scaling via elastic partition scheduling; and (3) improves inference efficiency through unified multimodal prefix caching and non-blocking encoding. Experiments on diverse real-world datasets show that ElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by up to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level objectives (SLOs).",
        "authors": [
            "Zedong Liu",
            "Shenggan Cheng",
            "Guangming Tan",
            "Yang You",
            "Dingwen Tao"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-07-14"
    },
    "http://arxiv.org/abs/2507.10026v1": {
        "id": "http://arxiv.org/abs/2507.10026v1",
        "title": "EAT: QoS-Aware Edge-Collaborative AIGC Task Scheduling via Attention-Guided Diffusion Reinforcement Learning",
        "link": "http://arxiv.org/abs/2507.10026v1",
        "tags": [
            "edge",
            "serving",
            "RL"
        ],
        "relevant": true,
        "indexed_date": "2025-07-15",
        "tldr": "Proposes EAT, a QoS-aware task scheduler for edge-based AIGC services with RL and attention-guided diffusion policy. Schedules subdivided tasks across heterogeneous edge servers to optimize latency and quality. Reduces inference latency by up to 56% versus baselines.",
        "abstract": "The growth of Artificial Intelligence (AI) and large language models has enabled the use of Generative AI (GenAI) in cloud data centers for diverse AI-Generated Content (AIGC) tasks. Models like Stable Diffusion introduce unavoidable delays and substantial resource overhead, which are unsuitable for users at the network edge with high QoS demands. Deploying AIGC services on edge servers reduces transmission times but often leads to underutilized resources and fails to optimally balance inference latency and quality. To address these issues, this paper introduces a QoS-aware \\underline{E}dge-collaborative \\underline{A}IGC \\underline{T}ask scheduling (EAT) algorithm. Specifically: 1) We segment AIGC tasks and schedule patches to various edge servers, formulating it as a gang scheduling problem that balances inference latency and quality while considering server heterogeneity, such as differing model distributions and cold start issues. 2) We propose a reinforcement learning-based EAT algorithm that uses an attention layer to extract load and task queue information from edge servers and employs a diffusion-based policy network for scheduling, efficiently enabling model reuse. 3) We develop an AIGC task scheduling system that uses our EAT algorithm to divide tasks and distribute them across multiple edge servers for processing. Experimental results based on our system and large-scale simulations show that our EAT algorithm can reduce inference latency by up to 56\\% compared to baselines. We release our open-source code at https://github.com/zzf1955/EAT.",
        "authors": [
            "Zhifei Xu",
            "Zhiqing Tang",
            "Jiong Lou",
            "Zhi Yao",
            "Xuan Xie",
            "Tian Wang",
            "Yinglong Wang",
            "Weijia Jia"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-07-14"
    },
    "http://arxiv.org/abs/2507.09942v1": {
        "id": "http://arxiv.org/abs/2507.09942v1",
        "title": "Green-LLM: Optimal Workload Allocation for Environmentally-Aware Distributed Inference",
        "link": "http://arxiv.org/abs/2507.09942v1",
        "tags": [
            "serving",
            "edge",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-07-15",
        "tldr": "Proposes optimizing LLM inference workload allocation across edge data centers with renewable energy to minimize environmental impact and costs. Formulates a spatiotemporal scheduling model considering electricity prices and renewable availability. Reduces carbon emissions by 37% and energy costs by 28% compared to baselines.",
        "abstract": "This letter investigates the optimal allocation of large language model (LLM) inference workloads across heterogeneous edge data centers (DCs) over time. Each DC features on-site renewable generation and faces dynamic electricity prices and spatiotemporal variability in renewable availability. The central question is: how can inference workloads be optimally distributed to the DCs to minimize energy consumption, carbon emissions, and water usage while enhancing user experience? This letter proposes a novel optimization model for LLM service providers to reduce operational costs and environmental impacts. Numerical results validate the efficacy of the proposed approach.",
        "authors": [
            "Jiaming Cheng",
            "Duong Tung Nguyen"
        ],
        "categories": [
            "cs.NI",
            "cs.DC",
            "eess.SY",
            "math.OC"
        ],
        "submit_date": "2025-07-14"
    },
    "http://arxiv.org/abs/2507.09201v1": {
        "id": "http://arxiv.org/abs/2507.09201v1",
        "title": "SLIM: A Heterogeneous Accelerator for Edge Inference of Sparse Large Language Model via Adaptive Thresholding",
        "link": "http://arxiv.org/abs/2507.09201v1",
        "tags": [
            "edge",
            "sparse",
            "hardware"
        ],
        "relevant": true,
        "indexed_date": "2025-07-15",
        "tldr": "Proposes SLIM, an algorithm-hardware co-design for efficient sparse LLM inference on edge devices. Uses adaptive thresholding for configurable sparsity and heterogeneous architecture combining near-storage processing (FFN) and processing-in-memory (MHA). Achieves 13-18x throughput improvement over SSD-GPU systems and 9-10x energy efficiency gain.",
        "abstract": "Large language models (LLMs) have demonstrated exceptional proficiency in understanding and generating human language, but efficient inference on resource-constrained embedded devices remains challenging due to large model sizes and memory-intensive operations in feedforward network (FFN) and multi-head attention (MHA) layers. While existing accelerators offload LLM inference to expensive heterogeneous computing systems, they fail to exploit the significant sparsity inherent in LLM operations, leaving hardware resources underutilized. We propose SLIM, an algorithm-hardware co-design optimized for sparse LLM serving on edge devices. SLIM exploits LLM sparsity through an adaptive thresholding algorithm that enables runtime-configurable sparsity with negligible accuracy loss, fetching only activated neurons to dramatically reduce data movement. Our heterogeneous hardware architecture strategically combines near-storage processing (NSP) and processing-in-memory (PIM): FFN weights are stored in high-density 3D NAND and computed using NSP units, while memory-intensive MHA operations are processed in PIM modules. This design significantly reduces memory footprint, data movement, and energy consumption. Our comprehensive evaluation demonstrates SLIM's effectiveness, achieving 13-18x throughput improvements over SSD-GPU systems and 9-10x better energy efficiency over DRAM-GPU systems while maintaining low latency, making cost-effective LLM deployment viable for edge computing environments.",
        "authors": [
            "Weihong Xu",
            "Haein Choi",
            "Po-kai Hsu",
            "Shimeng Yu",
            "Tajana Rosing"
        ],
        "categories": [
            "cs.AR",
            "cs.DC"
        ],
        "submit_date": "2025-07-12"
    },
    "http://arxiv.org/abs/2507.09019v1": {
        "id": "http://arxiv.org/abs/2507.09019v1",
        "title": "On Evaluating Performance of LLM Inference Serving Systems",
        "link": "http://arxiv.org/abs/2507.09019v1",
        "tags": [
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-07-15",
        "tldr": "Identifies flawed evaluation practices in LLM inference serving systems and proposes a checklist to avoid anti-patterns. Demonstrates through a speculative decoding case study that improper evaluation can misrepresent bursty generation performance, enabling more accurate system comparisons.",
        "abstract": "The rapid evolution of Large Language Model (LLM) inference systems has yielded significant efficiency improvements. However, our systematic analysis reveals that current evaluation methodologies frequently exhibit fundamental flaws, often manifesting as common evaluation anti-patterns that obscure true performance characteristics and impede scientific progress. Through a comprehensive examination of recent systems, we identify recurring anti-patterns across three key dimensions: Baseline Fairness, Evaluation Setup, and Metric Design. These anti-patterns are uniquely problematic for LLM inference due to its dual-phase nature combining distinct prefill and decode operations, its handling of highly heterogeneous workloads, and its strict temporal requirements for interactive use. We demonstrate how common anti-patterns -- such as inadequate baseline comparisons that conflate engineering effort with algorithmic novelty, workload selections that fail to represent production scenarios, and metric normalizations that hide substantial performance variability like generation stalls-lead to misleading conclusions. To address these challenges, we provide a comprehensive checklist derived from our analysis, establishing a framework for recognizing and avoiding these anti-patterns in favor of robust LLM inference evaluation. To demonstrate the practical application of our framework, we present a case study analyzing speculative decoding, a technique whose bursty, non-uniform token generation is easily misinterpreted when evaluated using approaches characteristic of these anti-patterns. Our work establishes a rigorous foundation for evaluation methodology, enabling meaningful comparisons, ensuring reproducible results, and ultimately accelerating genuine progress in LLM inference systems by moving beyond common anti-patterns to align evaluation with real-world requirements.",
        "authors": [
            "Amey Agrawal",
            "Nitin Kedia",
            "Anmol Agarwal",
            "Jayashree Mohan",
            "Nipun Kwatra",
            "Souvik Kundu",
            "Ramachandran Ramjee",
            "Alexey Tumanov"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC"
        ],
        "submit_date": "2025-07-11"
    },
    "http://arxiv.org/abs/2507.07932v1": {
        "id": "http://arxiv.org/abs/2507.07932v1",
        "title": "KIS-S: A GPU-Aware Kubernetes Inference Simulator with RL-Based Auto-Scaling",
        "link": "http://arxiv.org/abs/2507.07932v1",
        "tags": [
            "autoscaling",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-07-11",
        "tldr": "Addresses inefficient GPU inference autoscaling in Kubernetes under dynamic traffic. Proposes a GPU-aware simulator (KISim) and PPO-based autoscaler (KIScaler) that trains in simulation and deploys directly. Achieves 75.2% average reward improvement and up to 6.7× lower P95 latency.",
        "abstract": "Autoscaling GPU inference workloads in Kubernetes remains challenging due to the reactive and threshold-based nature of default mechanisms such as the Horizontal Pod Autoscaler (HPA), which struggle under dynamic and bursty traffic patterns and lack integration with GPU-level metrics. We present KIS-S, a unified framework that combines KISim, a GPU-aware Kubernetes Inference Simulator, with KIScaler, a Proximal Policy Optimization (PPO)-based autoscaler. KIScaler learns latency-aware and resource-efficient scaling policies entirely in simulation, and is directly deployed without retraining. Experiments across four traffic patterns show that KIScaler improves average reward by 75.2%, reduces P95 latency up to 6.7x over CPU baselines, and generalizes without retraining. Our work bridges the gap between reactive autoscaling and intelligent orchestration for scalable GPU-accelerated environments.",
        "authors": [
            "Guilin Zhang",
            "Wulan Guo",
            "Ziqi Tan",
            "Qiang Guan",
            "Hailong Jiang"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-07-10"
    },
    "http://arxiv.org/abs/2507.07400v1": {
        "id": "http://arxiv.org/abs/2507.07400v1",
        "title": "KVFlow: Efficient Prefix Caching for Accelerating LLM-Based Multi-Agent Workflows",
        "link": "http://arxiv.org/abs/2507.07400v1",
        "tags": [
            "serving",
            "offloading",
            "agentic"
        ],
        "relevant": true,
        "indexed_date": "2025-07-11",
        "tldr": "Addresses inefficient KV cache management in LLM-based multi-agent workflows. Proposes KVFlow with a workflow-aware eviction policy and overlapped prefetching for shared prefixes. Achieves up to 2.19× speedup over SGLang for concurrent workflows.",
        "abstract": "Large language model (LLM) based agentic workflows have become a popular paradigm for coordinating multiple specialized agents to solve complex tasks. To improve serving efficiency, existing LLM systems employ prefix caching to reuse key-value (KV) tensors corresponding to agents' fixed prompts, thereby avoiding redundant computation across repeated invocations. However, current systems typically evict KV caches using a Least Recently Used (LRU) policy, which fails to anticipate future agent usage and often discards KV caches shortly before their reuse. This leads to frequent cache misses and substantial recomputation or swapping overhead. We present KVFlow, a workflow-aware KV cache management framework tailored for agentic workloads. KVFlow abstracts the agent execution schedule as an Agent Step Graph and assigns each agent a steps-to-execution value that estimates its temporal proximity to future activation. These values guide a fine-grained eviction policy at the KV node level, allowing KVFlow to preserve entries likely to be reused and efficiently manage shared prefixes in tree-structured caches. Moreover, KVFlow introduces a fully overlapped KV prefetching mechanism, which proactively loads required tensors from CPU to GPU in background threads for agents scheduled in the next step, thereby avoiding cache miss stalls during generation. Compared to SGLang with hierarchical radix cache, KVFlow achieves up to 1.83$\\times$ speedup for single workflows with large prompts, and up to 2.19$\\times$ speedup for scenarios with many concurrent workflows.",
        "authors": [
            "Zaifeng Pan",
            "Ajjkumar Patel",
            "Zhengding Hu",
            "Yipeng Shen",
            "Yue Guan",
            "Wan-Lu Li",
            "Lianhui Qin",
            "Yida Wang",
            "Yufei Ding"
        ],
        "categories": [
            "cs.DC",
            "cs.MA"
        ],
        "submit_date": "2025-07-10"
    },
    "http://arxiv.org/abs/2507.07120v1": {
        "id": "http://arxiv.org/abs/2507.07120v1",
        "title": "Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding",
        "link": "http://arxiv.org/abs/2507.07120v1",
        "tags": [
            "serving",
            "sparse",
            "MoE"
        ],
        "relevant": true,
        "indexed_date": "2025-07-11",
        "tldr": "Addresses bottlenecks in long-context LLM serving by introducing Helix Parallelism, a hybrid sharding method for attention and FFN layers that reduces KV duplication and enables batch overlap. Reduces Token-to-Token Latency by 1.5x and increases batch size by 32x for DeepSeek-R1 inference.",
        "abstract": "As LLMs scale to multi-million-token KV histories, real-time autoregressive decoding under tight Token-to-Token Latency (TTL) constraints faces growing pressure. Two core bottlenecks dominate: accessing Feed-Forward Network (FFN) weights and reading long KV caches. While Tensor Parallelism (TP) helps mitigate the cost of FFN weight reads, it does not scale well for attention. When TP width exceeds the number of KV heads, it leads to inefficient KV duplication, limits parallelism, and constrains batch size. Simultaneously, DRAM reads for long KV histories scale linearly with batch size, further capping efficiency.   We introduce Helix Parallelism, a hybrid execution strategy that applies KV parallelism during attention to shard KV caches across GPUs, then reuses the same GPUs for TP in dense LLMs or TPxExpert Parallel (EP) in MoEs during FFN computation. To preserve exact attention behavior, Helix includes a lightweight communication step. To minimize the exposed communication cost, we introduce Helix HOP-B. Helix HOP-B effectively minimizes communication overhead through batchwise overlap, preserving low TTL while improving GPU efficiency. Compared to conventional parallelism approaches, Helix reduces TTL by up to 1.5x at fixed batch sizes and supports up to 32x larger batches under the same latency budget for DeepSeek-R1, pushing forward the throughput-latency Pareto on Blackwell and making real-time inference with ultra-long-sequence practical.",
        "authors": [
            "Nidhi Bhatia",
            "Ankit More",
            "Ritika Borkar",
            "Tiyasa Mitra",
            "Ramon Matas",
            "Ritchie Zhao",
            "Maximilian Golub",
            "Dheevatsa Mudigere",
            "Brian Pharris",
            "Bita Darvish Rouhani"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-07-07"
    },
    "http://arxiv.org/abs/2507.07116v1": {
        "id": "http://arxiv.org/abs/2507.07116v1",
        "title": "Analysing semantic data storage in Distributed Ledger Technologies for Data Spaces",
        "link": "http://arxiv.org/abs/2507.07116v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-07-11",
        "tldr": "",
        "abstract": "Data spaces are emerging as decentralised infrastructures that enable sovereign, secure, and trustworthy data exchange among multiple participants. To achieve semantic interoperability within these environments, the use of semantic web technologies and knowledge graphs has been proposed. Although distributed ledger technologies (DLT) fit as the underlying infrastructure for data spaces, there remains a significant gap in terms of the efficient storage of semantic data on these platforms. This paper presents a systematic evaluation of semantic data storage across different types of DLT (public, private, and hybrid), using a real-world knowledge graph as an experimental basis. The study compares performance, storage efficiency, resource consumption, and the capabilities to update and query semantic data. The results show that private DLTs are the most efficient for storing and managing semantic content, while hybrid DLTs offer a balanced trade-off between public auditability and operational efficiency. This research leads to a discussion on the selection of the most appropriate DLT infrastructure based on the data sovereignty requirements of decentralised data ecosystems.",
        "authors": [
            "Juan Cano-Benito",
            "Andrea Cimmino",
            "Sven Hertling",
            "Heiko Paulheim",
            "Raúl García-Castro"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.ET"
        ],
        "submit_date": "2025-07-03"
    },
    "http://arxiv.org/abs/2507.06608v5": {
        "id": "http://arxiv.org/abs/2507.06608v5",
        "title": "Nexus:Proactive Intra-GPU Disaggregation of Prefill and Decode in LLM Serving",
        "link": "http://arxiv.org/abs/2507.06608v5",
        "tags": [
            "serving",
            "disaggregation"
        ],
        "relevant": true,
        "indexed_date": "2025-07-10",
        "tldr": "Proposes proactive intra-GPU disaggregation of prefill and decode phases in LLM serving to reduce phase interference. Dynamically partitions GPU resources based on workload-aware saturation points and memory bandwidth contention. Achieves up to 2.2x higher throughput and 20x lower TTFT than vLLM.",
        "abstract": "Monolithic serving with chunked prefill improves GPU utilization by batching prefill and decode together, but suffers from fine-grained phase interference. Engine-level prefill-decode (PD) disaggregation avoids interference but incurs higher hardware and coordination overhead. Prior intra-GPU disaggregation approaches multiplex prefill and decode within a single GPU, using SLO-based tuning guided by heuristics from offline profiling or reactive feedback loops. However, these methods respond reactively to performance issues rather than anticipating them, limiting adaptability under dynamic workloads.   We ask: can we achieve proactive intra-GPU disaggregation that adapts effectively to dynamic workloads? The key challenge lies in managing the conflicting resource demands of prefill and decode under varying conditions. We first show that GPU resources exhibit diminishing returns -- beyond a saturation point, more allocation yields minimal latency benefit. Second, we observe that memory bandwidth contention becomes a critical bottleneck. These insights motivate a design that dynamically partitions GPU resources across prefill and decode phases, while jointly considering compute capacity, memory footprint, and bandwidth contention.   Evaluated on diverse LLMs and workloads, our system Nexus achieves up to 2.2x higher throughput, 20x lower TTFT, and 2.5x lower TBT than vLLM; outperforms SGLang by up to 2x; and matches or exceeds disaggregated vLLM.",
        "authors": [
            "Xiaoxiang Shi",
            "Colin Cai",
            "Junjia Du",
            "Zhihao Jia"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-07-09"
    },
    "http://arxiv.org/abs/2507.06567v2": {
        "id": "http://arxiv.org/abs/2507.06567v2",
        "title": "SlimCaching: Edge Caching of Mixture-of-Experts for Distributed Inference",
        "link": "http://arxiv.org/abs/2507.06567v2",
        "tags": [
            "MoE",
            "serving",
            "edge"
        ],
        "relevant": true,
        "indexed_date": "2025-07-10",
        "tldr": "Optimizes expert caching in edge networks for distributed MoE LLM inference. Designs greedy and decomposition algorithms for latency minimization under storage constraints. Achieves significant latency reduction compared to baselines in simulations.",
        "abstract": "Mixture-of-Experts (MoE) models improve the scalability of large language models (LLMs) by activating only a small subset of relevant experts per input. However, the sheer number of expert networks in an MoE model introduces a significant storage burden for an edge device. To address this challenge, we consider a scenario where experts are dispersed across an edge network for distributed inference. Based on the popular Top-$K$ expert selection strategy, we formulate a latency minimization problem by optimizing expert caching on edge servers under storage constraints. When $K=1$, the problem reduces to a monotone submodular maximization problem with knapsack constraints, for which we design a greedy-based algorithm with a $(1 - 1/e)$-approximation guarantee. For the general case where $K \\geq 1$, expert co-activation within the same MoE layer introduces non-submodularity, which renders greedy methods ineffective. To tackle this issue, we propose a successive greedy decomposition method to decompose the original problem into a series of subproblems, with each being solved by a dynamic programming approach. Furthermore, we design an accelerated algorithm based on the max-convolution technique to obtain the approximate solution with a provable guarantee in polynomial time. Simulation results on various MoE models demonstrate that our method significantly reduces inference latency compared to existing baselines.",
        "authors": [
            "Qian Chen",
            "Xianhao Chen",
            "Kaibin Huang"
        ],
        "categories": [
            "cs.LG",
            "cs.DC",
            "cs.NI"
        ],
        "submit_date": "2025-07-09"
    },
    "http://arxiv.org/abs/2507.03849v1": {
        "id": "http://arxiv.org/abs/2507.03849v1",
        "title": "On Fault Tolerance of Data Storage Systems: A Holistic Perspective",
        "link": "http://arxiv.org/abs/2507.03849v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-07-08",
        "tldr": "",
        "abstract": "Data storage systems serve as the foundation of digital society. The enormous data generated by people on a daily basis make the fault tolerance of data storage systems increasingly important. Unfortunately, modern storage systems consist of complicated hardware and software layers interacting with each other, which may contain latent bugs that elude extensive testing and lead to data corruption, system downtime, or even unrecoverable data loss in practice. In this chapter, we take a holistic view to introduce the typical architecture and major components of modern data storage systems (e.g., solid state drives, persistent memories, local file systems, and distributed storage management at scale). Next, we discuss a few representative bug detection and fault tolerance techniques across layers with a focus on issues that affect system recovery and data integrity. Finally, we conclude with open challenges and future work.",
        "authors": [
            "Mai Zheng",
            "Duo Zhang",
            "Ahmed Dajani"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-07-05"
    },
    "http://arxiv.org/abs/2507.03305v2": {
        "id": "http://arxiv.org/abs/2507.03305v2",
        "title": "Analysis and Optimized CXL-Attached Memory Allocation for Long-Context LLM Fine-Tuning",
        "link": "http://arxiv.org/abs/2507.03305v2",
        "tags": [
            "training",
            "offloading",
            "storage"
        ],
        "relevant": true,
        "indexed_date": "2025-07-08",
        "tldr": "Addresses memory bottleneck in long-context LLM fine-tuning by augmenting GPU memory with CXL-attached CPU memory. Introduces a PyTorch extension with CXL-aware allocator for per-tensor NUMA placement: latency-critical tensors in local DRAM, others striped across CXL. Achieves 97-99% throughput of DRAM-only with single device, 21% improvement over naive.",
        "abstract": "The substantial memory requirements of Large Language Models (LLMs), particularly for long-context fine-tuning, have renewed interest in CPU offloading to augment limited GPU memory. However, as context lengths grow, relying on CPU memory for intermediate states introduces a significant bottleneck that can exhaust the capacity of mainstream client platforms. To address this limitation, this work investigates the effectiveness of Compute Express Link (CXL) add-in card (AIC) memory as an extension to CPU memory, enabling larger model sizes and longer context lengths during fine-tuning. Extensive benchmarking reveals two critical challenges. First, current deep learning frameworks such as PyTorch lack fine-grained, per-tensor control over NUMA memory allocation, exposing only coarse, process-level policies. Second, due to this lack of control, when the memory footprint of fine-tuning is offloaded across local DRAM and CXL-attached memory, naively placing optimizer data in higher-latency CXL leads to substantial slowdowns in the optimizer step (e.g., 4x once data exceeds 20M elements). To overcome these challenges, this work introduces a PyTorch extension that enables tensor-level system memory control and a CXL-aware memory allocator that pins latency-critical tensors in local DRAM while maximizing bandwidth by striping latency-tolerant tensors across one or more CXL devices. Evaluated on a real hardware setup with 7B and 12B models, 4K-32K contexts, and a single GPU, our approach recovers throughput to 97-99% of DRAM-only with a single AIC and approximately 100% with two AICs, delivering up to 21% improvement over naive interleaving while preserving DRAM-like DMA bandwidth for GPU transfers. These results show that carefully managed CXL-attached memory is a practical path to scaling long-context fine-tuning beyond DRAM limits.",
        "authors": [
            "Yong-Cheng Liaw",
            "Shuo-Han Chen"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-07-04"
    },
    "http://arxiv.org/abs/2507.03220v3": {
        "id": "http://arxiv.org/abs/2507.03220v3",
        "title": "Symbiosis: Multi-Adapter Inference and Fine-Tuning",
        "link": "http://arxiv.org/abs/2507.03220v3",
        "tags": [
            "offline",
            "LoRA",
            "training"
        ],
        "relevant": true,
        "indexed_date": "2025-07-08",
        "tldr": "Proposes Symbiosis, a framework for multi-adapter inference and fine-tuning that shares base model layers across jobs. Uses split-execution to decouple adapter processing from base model, allowing resource management and privacy. Simultaneously fine-tunes 20 adapters for Gemma2-27B on 8 GPUs.",
        "abstract": "Parameter-efficient fine-tuning (PEFT) allows model builders to capture the task-specific parameters into adapters, which are a fraction of the size of the original base model. Popularity of PEFT technique for fine-tuning has led to the creation of a large number of adapters for popular Large Language Models (LLMs). However, existing frameworks fall short in supporting inference or fine-tuning with multiple adapters in the following ways. 1) For fine-tuning, each job needs to deploy its dedicated base model instance, which results in excessive GPU memory consumption and poor GPU utilization. 2) While popular inference platforms can serve multiple PEFT adapters, they do not allow independent resource management or mixing of different PEFT methods. 3) They cannot make effective use of heterogeneous accelerators. 4) They do not provide privacy to users who may not wish to expose their fine-tuned parameters to service providers. In Symbiosis, we address the above problems by enabling the as-a-service deployment of the base model. The base model layers can be shared across multiple inference or fine-tuning processes. Our split-execution technique decouples the execution of client-specific adapters and layers from the frozen base model layers offering them flexibility to manage their resources, to select their fine-tuning method, to achieve their performance goals. Our approach is transparent to models and works out-of-the-box for most models in the transformers library. We demonstrate the use of Symbiosis to simultaneously fine-tune 20 Gemma2-27B adapters on 8 GPUs.",
        "authors": [
            "Saransh Gupta",
            "Umesh Deshpande",
            "Travis Janssen",
            "Swami Sundararaman"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.LG"
        ],
        "submit_date": "2025-07-03"
    },
    "http://arxiv.org/abs/2507.02871v1": {
        "id": "http://arxiv.org/abs/2507.02871v1",
        "title": "ZettaLith: An Architectural Exploration of Extreme-Scale AI Inference Acceleration",
        "link": "http://arxiv.org/abs/2507.02871v1",
        "tags": [
            "hardware",
            "serving",
            "inference"
        ],
        "relevant": true,
        "indexed_date": "2025-07-08",
        "tldr": "Introduces ZettaLith, a specialized architecture for extreme-scale AI inference acceleration. Utilizes co-designed innovations to achieve over 1000x cost/power reduction versus GPUs. Projected as 1047x faster, 1490x more power-efficient, and 2325x more cost-effective for FP4 transformer inference by 2027.",
        "abstract": "The high computational cost and power consumption of current and anticipated AI systems present a major challenge for widespread deployment and further scaling. Current hardware approaches face fundamental efficiency limits. This paper introduces ZettaLith, a scalable computing architecture designed to reduce the cost and power of AI inference by over 1,000x compared to current GPU-based systems. Based on architectural analysis and technology projections, a single ZettaLith rack could potentially achieve 1.507 zettaFLOPS in 2027 - representing a theoretical 1,047x improvement in inference performance, 1,490x better power efficiency, and could be 2,325x more cost-effective than current leading GPU racks for FP4 transformer inference. The ZettaLith architecture achieves these gains by abandoning general purpose GPU applications, and via the multiplicative effect of numerous co-designed architectural innovations using established digital electronic technologies, as detailed in this paper. ZettaLith's core architectural principles scale down efficiently to exaFLOPS desktop systems and petaFLOPS mobile chips, maintaining their roughly 1,000x advantage. ZettaLith presents a simpler system architecture compared to the complex hierarchy of current GPU clusters. ZettaLith is optimized exclusively for AI inference and is not applicable for AI training.",
        "authors": [
            "Kia Silverbrook"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.AR",
            "cs.LG"
        ],
        "submit_date": "2025-06-08"
    },
    "http://arxiv.org/abs/2507.04697v1": {
        "id": "http://arxiv.org/abs/2507.04697v1",
        "title": "Performance Evaluation of General Purpose Large Language Models for Basic Linear Algebra Subprograms Code Generation",
        "link": "http://arxiv.org/abs/2507.04697v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-07-08",
        "tldr": "",
        "abstract": "Generative AI technology based on Large Language Models (LLM) has been developed and applied to assist or automatically generate program codes. In this paper, we evaluate the capability of existing general LLMs for Basic Linear Algebra Subprograms (BLAS) code generation for CPUs. We use two LLMs provided by OpenAI: GPT-4.1, a Generative Pre-trained Transformer (GPT) model, and o4-mini, one of the o-series of Reasoning models. Both have been released in April 2025. For the routines from level-1 to 3 BLAS, we tried to generate (1) C code without optimization from routine name only, (2) C code with basic performance optimizations (thread parallelization, SIMD vectorization, and cache blocking) from routine name only, and (3) C code with basic performance optimizations based on Fortran reference code. As a result, we found that correct code can be generated in many cases even when only routine name are given. We also confirmed that thread parallelization with OpenMP, SIMD vectorization, and cache blocking can be implemented to some extent, and that the code is faster than the reference code.",
        "authors": [
            "Daichi Mukunoki",
            "Shun-ichiro Hayashi",
            "Tetsuya Hoshino",
            "Takahiro Katagiri"
        ],
        "categories": [
            "cs.LG",
            "cs.DC",
            "cs.MS"
        ],
        "submit_date": "2025-07-07"
    },
    "http://arxiv.org/abs/2507.02620v2": {
        "id": "http://arxiv.org/abs/2507.02620v2",
        "title": "FlowSpec: Continuous Pipelined Speculative Decoding for Efficient Distributed LLM Inference",
        "link": "http://arxiv.org/abs/2507.02620v2",
        "tags": [
            "serving",
            "offloading",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-07-04",
        "tldr": "Proposes FlowSpec, a pipeline-parallel tree-based speculative decoding framework for distributed LLM inference. Uses step-wise verification, draft management, and dynamic expansion to enhance pipeline utilization. Achieves 1.28×-1.79× speedup over baselines.",
        "abstract": "Distributed inference serves as a promising approach to enabling the inference of large language models (LLMs) at the network edge. It distributes the inference process to multiple devices to ensure that the LLMs can fit into the device memory. Recent pipeline-based approaches have the potential to parallelize communication and computation, which helps reduce inference latency. However, the benefit diminishes when the inference request at the network edge is sparse, where pipeline is typically at low utilization. To enable efficient distributed LLM inference at the edge, we propose \\textbf{FlowSpec}, a pipeline-parallel tree-based speculative decoding framework. FlowSpec incorporates three key mechanisms to improve decoding efficiency: 1) score-based step-wise verification prioritizes more important draft tokens to bring earlier accpeted tokens; 2) efficient draft management to prune invalid tokens while maintaining correct causal relationship during verification; 3) dynamic draft expansion strategies to supply high-quality speculative inputs. These techniques work in concert to enhance both pipeline utilization and speculative efficiency. We evaluate FlowSpec on a real-world testbed with other baselines. Experimental results demonstrate that our proposed framework significantly improves inference speed across diverse models and configurations, achieving speedup ratios 1.28$\\times$-1.79$\\times$ compared to baselines. Our code is publicly available at \\href{https://github.com/Leosang-lx/FlowSpec#}{https://github.com/Leosang-lx/FlowSpec\\#}",
        "authors": [
            "Xing Liu",
            "Lizhuo Luo",
            "Ming Tang",
            "Chao Huang"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-07-03"
    },
    "http://arxiv.org/abs/2507.02135v1": {
        "id": "http://arxiv.org/abs/2507.02135v1",
        "title": "Dissecting the Impact of Mobile DVFS Governors on LLM Inference Performance and Energy Efficiency",
        "link": "http://arxiv.org/abs/2507.02135v1",
        "tags": [
            "edge",
            "offloading",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-07-04",
        "tldr": "Investigates how suboptimal DVFS governors affect LLM inference on mobile devices. Proposes FUSE, a unified energy-aware governor coordinating CPU, GPU, and memory frequencies. Reduces latencies by up to 36.8% with same energy-per-token.",
        "abstract": "Large Language Models (LLMs) are increasingly being integrated into various applications and services running on billions of mobile devices. However, deploying LLMs on resource-limited mobile devices faces a significant challenge due to their high demand for computation, memory, and ultimately energy. While current LLM frameworks for mobile use three power-hungry components-CPU, GPU, and Memory-even when running primarily-GPU LLM models, optimized DVFS governors for CPU, GPU, and memory featured in modern mobile devices operate independently and are oblivious of each other. Motivated by the above observation, in this work, we first measure the energy-efficiency of a SOTA LLM framework consisting of various LLM models on mobile phones which showed the triplet mobile governors result in up to 40.4% longer prefilling and decoding latency compared to optimal combinations of CPU, GPU, and memory frequencies with the same energy consumption for sampled prefill and decode lengths. Second, we conduct an in-depth measurement study to uncover how the intricate interplay (or lack of) among the mobile governors cause the above inefficiency in LLM inference. Finally, based on these insights, we design FUSE - a unified energy-aware governor for optimizing the energy efficiency of LLM inference on mobile devices. Our evaluation using a ShareGPT dataset shows FUSE reduces the time-to-first-token and time-per-output-token latencies by 7.0%-16.9% and 25.4%-36.8% on average with the same energy-per-token for various mobile LLM models.",
        "authors": [
            "Zongpu Zhang",
            "Pranab Dash",
            "Y. Charlie Hu",
            "Qiang Xu",
            "Jian Li",
            "Haibing Guan"
        ],
        "categories": [
            "cs.OS",
            "cs.CL"
        ],
        "submit_date": "2025-07-02"
    },
    "http://arxiv.org/abs/2507.01676v1": {
        "id": "http://arxiv.org/abs/2507.01676v1",
        "title": "Deep Recommender Models Inference: Automatic Asymmetric Data Flow Optimization",
        "link": "http://arxiv.org/abs/2507.01676v1",
        "tags": [
            "recommendation",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-07-03",
        "tldr": "Proposes tailored data flow and asymmetric mapping strategies to optimize embedding lookups in Deep Recommender Models inference. Achieves 1.5x to 6.5x speedup on real workloads and over 20x for unbalanced distributions compared to baseline.",
        "abstract": "Deep Recommender Models (DLRMs) inference is a fundamental AI workload accounting for more than 79% of the total AI workload in Meta's data centers. DLRMs' performance bottleneck is found in the embedding layers, which perform many random memory accesses to retrieve small embedding vectors from tables of various sizes. We propose the design of tailored data flows to speedup embedding look-ups. Namely, we propose four strategies to look up an embedding table effectively on one core, and a framework to automatically map the tables asymmetrically to the multiple cores of a SoC. We assess the effectiveness of our method using the Huawei Ascend AI accelerators, comparing it with the default Ascend compiler, and we perform high-level comparisons with Nvidia A100. Results show a speed-up varying from 1.5x up to 6.5x for real workload distributions, and more than 20x for extremely unbalanced distributions. Furthermore, the method proves to be much more independent of the query distribution than the baseline.",
        "authors": [
            "Giuseppe Ruggeri",
            "Renzo Andri",
            "Daniele Jahier Pagliari",
            "Lukas Cavigelli"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.AR",
            "cs.IR"
        ],
        "submit_date": "2025-07-02"
    },
    "http://arxiv.org/abs/2507.01438v1": {
        "id": "http://arxiv.org/abs/2507.01438v1",
        "title": "EdgeLoRA: An Efficient Multi-Tenant LLM Serving System on Edge Devices",
        "link": "http://arxiv.org/abs/2507.01438v1",
        "tags": [
            "edge",
            "serving",
            "LoRA"
        ],
        "relevant": true,
        "indexed_date": "2025-07-03",
        "tldr": "Proposes EdgeLoRA for efficient multi-tenant LLM serving on edge devices via adaptive adapter selection, memory management, and batch processing. Achieves 4x higher throughput and supports orders of magnitude more adapters compared to baseline.",
        "abstract": "Large Language Models (LLMs) have gained significant attention due to their versatility across a wide array of applications. Fine-tuning LLMs with parameter-efficient adapters, such as Low-Rank Adaptation (LoRA), enables these models to efficiently adapt to downstream tasks without extensive retraining. Deploying fine-tuned LLMs on multi-tenant edge devices offers substantial benefits, such as reduced latency, enhanced privacy, and personalized responses. However, serving LLMs efficiently on resource-constrained edge devices presents critical challenges, including the complexity of adapter selection for different tasks and memory overhead from frequent adapter swapping. Moreover, given the multiple requests in multi-tenant settings, processing requests sequentially results in underutilization of computational resources and increased latency. This paper introduces EdgeLoRA, an efficient system for serving LLMs on edge devices in multi-tenant environments. EdgeLoRA incorporates three key innovations: (1) an adaptive adapter selection mechanism to streamline the adapter configuration process; (2) heterogeneous memory management, leveraging intelligent adapter caching and pooling to mitigate memory operation overhead; and (3) batch LoRA inference, enabling efficient batch processing to significantly reduce computational latency. Comprehensive evaluations using the Llama3.1-8B model demonstrate that EdgeLoRA significantly outperforms the status quo (i.e., llama.cpp) in terms of both latency and throughput. The results demonstrate that EdgeLoRA can achieve up to a 4 times boost in throughput. Even more impressively, it can serve several orders of magnitude more adapters simultaneously. These results highlight EdgeLoRA's potential to transform edge deployment of LLMs in multi-tenant scenarios, offering a scalable and efficient solution for resource-constrained environments.",
        "authors": [
            "Zheyu Shen",
            "Yexiao He",
            "Ziyao Wang",
            "Yuning Zhang",
            "Guoheng Sun",
            "Wanghao Ye",
            "Ang Li"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.LG"
        ],
        "submit_date": "2025-07-02"
    },
    "http://arxiv.org/abs/2507.00716v1": {
        "id": "http://arxiv.org/abs/2507.00716v1",
        "title": "Accelerating Loading WebGraphs in ParaGrapher",
        "link": "http://arxiv.org/abs/2507.00716v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-07-02",
        "tldr": "",
        "abstract": "ParaGrapher is a graph loading API and library that enables graph processing frameworks to load large-scale compressed graphs with minimal overhead. This capability accelerates the design and implementation of new high-performance graph algorithms and their evaluation on a wide range of graphs and across different frameworks. However, our previous study identified two major limitations in ParaGrapher: inefficient utilization of high-bandwidth storage and reduced decompression bandwidth due to increased compression ratios. To address these limitations, we present two optimizations for ParaGrapher in this paper. To improve storage utilization, particularly for high-bandwidth storage, we introduce ParaGrapher-FUSE (PG-Fuse) a filesystem based on the FUSE (Filesystem in User Space). PG-Fuse optimizes storage access by increasing the size of requested blocks, reducing the number of calls to the underlying filesystem, and caching the received blocks in memory for future calls. To improve the decompression bandwidth, we introduce CompBin, a compact binary representation of the CSR format. CompBin facilitates direct accesses to neighbors while preventing storage usage for unused bytes. Our evaluation on 12 real-world and synthetic graphs with up to 128 billion edges shows that PG-Fuse and CompBin achieve up to 7.6 and 21.8 times speedup, respectively.",
        "authors": [
            "Mohsen Koohi Esfahani"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-07-01"
    },
    "http://arxiv.org/abs/2507.00576v1": {
        "id": "http://arxiv.org/abs/2507.00576v1",
        "title": "DynoStore: A wide-area distribution system for the management of data over heterogeneous storage",
        "link": "http://arxiv.org/abs/2507.00576v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-07-02",
        "tldr": "",
        "abstract": "Data distribution across different facilities offers benefits such as enhanced resource utilization, increased resilience through replication, and improved performance by processing data near its source. However, managing such data is challenging due to heterogeneous access protocols, disparate authentication models, and the lack of a unified coordination framework. This paper presents DynoStore, a system that manages data across heterogeneous storage systems. At the core of DynoStore are data containers, an abstraction that provides standardized interfaces for seamless data management, irrespective of the underlying storage systems. Multiple data container connections create a cohesive wide-area storage network, ensuring resilience using erasure coding policies. Furthermore, a load-balancing algorithm ensures equitable and efficient utilization of storage resources. We evaluate DynoStore using benchmarks and real-world case studies, including the management of medical and satellite data across geographically distributed environments. Our results demonstrate a 10\\% performance improvement compared to centralized cloud-hosted systems while maintaining competitive performance with state-of-the-art solutions such as Redis and IPFS. DynoStore also exhibits superior fault tolerance, withstanding more failures than traditional systems.",
        "authors": [
            "Dante D. Sanchez-Gallegos",
            "J. L. Gonzalez-Compean",
            "Maxime Gonthier",
            "Valerie Hayot-Sasson",
            "J. Gregory Pauloski",
            "Haochen Pan",
            "Kyle Chard",
            "Jesus Carretero",
            "Ian Foster"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-07-01"
    },
    "http://arxiv.org/abs/2507.00507v2": {
        "id": "http://arxiv.org/abs/2507.00507v2",
        "title": "Towards Resource-Efficient Serverless LLM Inference with SLINFER",
        "link": "http://arxiv.org/abs/2507.00507v2",
        "tags": [
            "serving",
            "offloading",
            "quantization"
        ],
        "relevant": true,
        "indexed_date": "2025-07-02",
        "tldr": "Investigates resource-efficient serverless LLM inference for small-to-mid-sized models. Proposes SLINFER, which enables elastic sharing across CPUs/GPUs via token-level compute allocation, memory scaling, and instance consolidation. Achieves 47%-154% serving capacity improvement by leveraging sharing and CPU acceleration.",
        "abstract": "The rise of LLMs has driven demand for private serverless deployments, characterized by moderate-sized models and infrequent requests. While existing serverless solutions follow exclusive GPU allocation, we take a step back to explore modern platforms and find that: Emerging CPU architectures with built-in accelerators are capable of serving LLMs but remain underutilized, and both CPUs and GPUs can accommodate multiple LLMs simultaneously.   We propose SLINFER, a resource-efficient serverless inference scheme tailored for small- to mid-sized LLMs that enables elastic and on-demand sharing across heterogeneous hardware. SLINFER tackles three fundamental challenges: (1) precise, fine-grained compute resource allocation at token-level to handle fluctuating computational demands; (2) a coordinated and forward-looking memory scaling mechanism to detect out-of-memory hazards and reduce operational overhead; and (3) a dual approach that consolidates fragmented instances through proactive preemption and reactive bin-packing. Experimental results on 4 32-core CPUs and 4 A100 GPUs show that SLINFER improves serving capacity by 47% - 62% through sharing, while further leveraging CPUs boosts this to 86% - 154%.",
        "authors": [
            "Chuhao Xu",
            "Zijun Li",
            "Quan Chen",
            "Han Zhao",
            "Xueyan Tang",
            "Minyi Guo"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-07-01"
    },
    "http://arxiv.org/abs/2507.00418v3": {
        "id": "http://arxiv.org/abs/2507.00418v3",
        "title": "Serving LLMs in HPC Clusters: A Comparative Study of Qualcomm Cloud AI 100 Ultra and NVIDIA Data Center GPUs",
        "link": "http://arxiv.org/abs/2507.00418v3",
        "tags": [
            "serving",
            "hardware"
        ],
        "relevant": true,
        "indexed_date": "2025-07-02",
        "tldr": "Compares Qualcomm Cloud AI 100 Ultra and NVIDIA A100 GPUs for energy-efficient LLM serving in HPC. Benchmarks 12 LLMs using vLLM, measuring throughput per watt. QAic achieves 20x lower power for 70B models (148W vs 2.9kW) with fewer cards per model.",
        "abstract": "This study presents a benchmarking analysis of the Qualcomm Cloud AI 100 Ultra (QAic) accelerator for large language model (LLM) inference, evaluating its energy efficiency (throughput per watt), performance, and hardware scalability against NVIDIA A100 GPUs (in 4x and 8x configurations) within the National Research Platform (NRP) ecosystem. A total of 12 open-source LLMs, ranging from 124 million to 70 billion parameters, are served using the vLLM framework. Our analysis reveals that QAic achieves competitive energy efficiency with advantages on specific models while enabling more granular hardware allocation: some 70B models operate on as few as 1 QAic card versus 8 A100 GPUs required, with 20x lower power consumption (148W vs 2,983W). For smaller models, single QAic devices achieve up to 35x lower power consumption compared to our 4-GPU A100 configuration (36W vs 1,246W). The findings offer insights into the potential of the Qualcomm Cloud AI 100 Ultra for energy-constrained and resource-efficient HPC deployments within the National Research Platform (NRP).",
        "authors": [
            "Mohammad Firas Sada",
            "John J. Graham",
            "Elham E Khoda",
            "Mahidhar Tatineni",
            "Dmitry Mishin",
            "Rajesh K. Gupta",
            "Rick Wagner",
            "Larry Smarr",
            "Thomas A. DeFanti",
            "Frank Würthwein"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-07-01"
    },
    "http://arxiv.org/abs/2507.00672v1": {
        "id": "http://arxiv.org/abs/2507.00672v1",
        "title": "Toward Edge General Intelligence with Multiple-Large Language Model (Multi-LLM): Architecture, Trust, and Orchestration",
        "link": "http://arxiv.org/abs/2507.00672v1",
        "tags": [
            "edge",
            "multi-modal"
        ],
        "relevant": true,
        "indexed_date": "2025-07-02",
        "tldr": "Proposes multi-LLM architectures for edge computing, using specialized LLMs per modality with dynamic orchestration and resource scheduling for resource-constrained environments. Achieves improved task adaptability and real-time performance in multi-modal processing.",
        "abstract": "Edge computing enables real-time data processing closer to its source, thus improving the latency and performance of edge-enabled AI applications. However, traditional AI models often fall short when dealing with complex, dynamic tasks that require advanced reasoning and multimodal data processing. This survey explores the integration of multi-LLMs (Large Language Models) to address this in edge computing, where multiple specialized LLMs collaborate to enhance task performance and adaptability in resource-constrained environments. We review the transition from conventional edge AI models to single LLM deployment and, ultimately, to multi-LLM systems. The survey discusses enabling technologies such as dynamic orchestration, resource scheduling, and cross-domain knowledge transfer that are key for multi-LLM implementation. A central focus is on trusted multi-LLM systems, ensuring robust decision-making in environments where reliability and privacy are crucial. We also present multimodal multi-LLM architectures, where multiple LLMs specialize in handling different data modalities, such as text, images, and audio, by integrating their outputs for comprehensive analysis. Finally, we highlight future directions, including improving resource efficiency, trustworthy governance multi-LLM systems, while addressing privacy, trust, and robustness concerns. This survey provides a valuable reference for researchers and practitioners aiming to leverage multi-LLM systems in edge computing applications.",
        "authors": [
            "Haoxiang Luo",
            "Yinqiu Liu",
            "Ruichen Zhang",
            "Jiacheng Wang",
            "Gang Sun",
            "Dusit Niyato",
            "Hongfang Yu",
            "Zehui Xiong",
            "Xianbin Wang",
            "Xuemin Shen"
        ],
        "categories": [
            "cs.NI",
            "cs.DC"
        ],
        "submit_date": "2025-07-01"
    },
    "http://arxiv.org/abs/2507.00394v1": {
        "id": "http://arxiv.org/abs/2507.00394v1",
        "title": "HelixPipe: Efficient Distributed Training of Long Sequence Transformers with Attention Parallel Pipeline Parallelism",
        "link": "http://arxiv.org/abs/2507.00394v1",
        "tags": [
            "training",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-07-02",
        "tldr": "Proposes HelixPipe, a pipeline parallelism method for long-sequence transformers with attention parallel partitioning and optimized scheduling. Reduces pipeline bubbles and memory overhead via two-fold FIFO-LIFO scheduling and recomputation techniques. Achieves 26% speedup over baselines on 128k-sequence 7B model training with 64 H20 GPUs.",
        "abstract": "As transformer sequence lengths grow, existing pipeline parallelisms incur suboptimal performance due to the quadratic attention computation and the substantial memory overhead. To relieve these challenges, we propose HelixPipe, a novel pipeline parallelism for long sequence transformer training. First, HelixPipe introduces attention parallel partition, which schedules attention computations of different micro batches across different pipeline stages in parallel, reducing pipeline bubbles. Second, it employs a two-fold first-in-last-out micro batch schedule to balance memory usage and overlap communication with computation. Additionally, HelixPipe utilizes recomputation without attention and chunked MLP to mitigate fragmentation and enable longer sequences. Experiments demonstrate that HelixPipe gains increasing advantages with longer sequence lengths, and outperforms existing methods in throughput and scalability across varying pipeline sizes, model sizes, and cluster configurations. Notably, it achieves a 26\\% speedup over baseline methods when training a 7B model with 128k sequence length on 64 H20 GPUs. Code is available at https://github.com/code-tunnel/Megatron-LM/tree/dev.",
        "authors": [
            "Geng Zhang",
            "Shenggan Cheng",
            "Xuanlei Zhao",
            "Ziming Liu",
            "Yang You"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "submit_date": "2025-07-01"
    },
    "http://arxiv.org/abs/2506.24045v1": {
        "id": "http://arxiv.org/abs/2506.24045v1",
        "title": "Agent.xpu: Efficient Scheduling of Agentic LLM Workloads on Heterogeneous SoC",
        "link": "http://arxiv.org/abs/2506.24045v1",
        "tags": [
            "agentic",
            "edge",
            "scheduling"
        ],
        "relevant": true,
        "indexed_date": "2025-07-01",
        "tldr": "Proposes Agent.xpu for scheduling agentic LLM workloads on heterogeneous SoCs. Utilizes offline profiling to fuse kernels for elastic accelerator mapping and online kernel-level preemption for reactive tasks. Achieves 4.6× lower reactive task latency and up to 6.8× proactive task throughput.",
        "abstract": "The proliferation of agentic Large Language Models (LLMs) on personal devices introduces a new class of workloads characterized by a dichotomy of objectives. Reactive tasks, initiated by users, demand immediate, low-latency responses, while proactive tasks operate invisibly and prioritize throughput. Existing on-device LLM engines, designed for isolated inferences, fail to efficiently manage these concurrent and conflicting requests on consumer-grade heterogeneous SoCs with CPU, integrated GPU, and NPU. This paper introduces Agent.xpu, an efficient serving system for agentic LLM workloads on memory-unified heterogeneous SoCs. With dedicated offline profiling, Agent.xpu first constructs a heterogeneous execution graph, which fuses and chunks model kernels for affinity-guided, elastic accelerator mapping with predictive kernel annotation. At runtime, its online scheduler enables fine-grained, kernel-level preemption to guarantee the responsiveness of reactive tasks. To maximize SoC utilization, it adopts slack-aware kernel backfill to opportunistically append proactive tasks, and mitigates NPU-iGPU contention via bandwidth-aware dispatch. Evaluation on an Intel Core Ultra SoC shows that Agent.xpu achieves 4.6$\\times$ lower latency for reactive tasks and sustains 1.6$\\times$-6.8$\\times$ higher throughput for proactive tasks compared to state-of-the-art inference engines.",
        "authors": [
            "Xinming Wei",
            "Jiahao Zhang",
            "Haoran Li",
            "Jiayu Chen",
            "Rui Qu",
            "Maoliang Li",
            "Xiang Chen",
            "Guojie Luo"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-06-30"
    },
    "http://arxiv.org/abs/2506.23934v1": {
        "id": "http://arxiv.org/abs/2506.23934v1",
        "title": "QPART: Adaptive Model Quantization and Dynamic Workload Balancing for Accuracy-aware Edge Inference",
        "link": "http://arxiv.org/abs/2506.23934v1",
        "tags": [
            "quantization",
            "edge",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-07-01",
        "tldr": "Proposes QPART, an accuracy-aware edge inference system that adaptively quantizes models and partitions workload between server and edge devices. Jointly optimizes layer-wise quantization bit-width and partition points to minimize time and cost while meeting accuracy constraints. Reduces computation payload by over 80% with under 1% accuracy loss.",
        "abstract": "As machine learning inferences increasingly move to edge devices, adapting to diverse computational capabilities, hardware, and memory constraints becomes more critical. Instead of relying on a pre-trained model fixed for all future inference queries across diverse edge devices, we argue that planning an inference pattern with a request-specific model tailored to the device's computational capacity, accuracy requirements, and time constraints is more cost-efficient and robust to diverse scenarios. To this end, we propose an accuracy-aware and workload-balanced inference system that integrates joint model quantization and inference partitioning. In this approach, the server dynamically responds to inference queries by sending a quantized model and adaptively sharing the inference workload with the device. Meanwhile, the device's computational power, channel capacity, and accuracy requirements are considered when deciding.   Furthermore, we introduce a new optimization framework for the inference system, incorporating joint model quantization and partitioning. Our approach optimizes layer-wise quantization bit width and partition points to minimize time consumption and cost while accounting for varying accuracy requirements of tasks through an accuracy degradation metric in our optimization model. To our knowledge, this work represents the first exploration of optimizing quantization layer-wise bit-width in the inference serving system, by introducing theoretical measurement of accuracy degradation. Simulation results demonstrate a substantial reduction in overall time and power consumption, with computation payloads decreasing by over 80% and accuracy degradation kept below 1%.",
        "authors": [
            "Xiangchen Li",
            "Saeid Ghafouri",
            "Bo Ji",
            "Hans Vandierendonck",
            "Deepu John",
            "Dimitrios S. Nikolopoulos"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.LG",
            "cs.PF"
        ],
        "submit_date": "2025-06-30"
    },
    "http://arxiv.org/abs/2506.23635v1": {
        "id": "http://arxiv.org/abs/2506.23635v1",
        "title": "Towards Building Private LLMs: Exploring Multi-Node Expert Parallelism on Apple Silicon for Mixture-of-Experts Large Language Model",
        "link": "http://arxiv.org/abs/2506.23635v1",
        "tags": [
            "MoE",
            "serving",
            "edge"
        ],
        "relevant": true,
        "indexed_date": "2025-07-01",
        "tldr": "Addresses cost and scalability in private LLM serving for small groups. Utilizes a Mac Studio cluster with M2 Ultra chips and MoE parallelism, optimized to reduce Apple software overhead. Achieves 1.15× higher cost efficiency than H100-based supercomputers in inference.",
        "abstract": "Large Language Models (LLMs) have revolutionized Artificial Intelligence (AI) with significant advancements such as OpenAI's ChatGPT, Meta's Llama, and Databricks' DBRX. This paper addresses the cost and scalability challenges encountered when constructing private LLM systems for personal or small group services, as aimed by Apple Intelligence. A Mac Studio cluster with Apple's M2 Ultra chips is established as a cost-efficient solution to host and accelerate the pretrained DBRX model with the Mixture-of-Experts (MoE) architecture. Our performance analysis reveal that parallel execution of the model's experts across two to four machine nodes significantly reduces inference time. We find that computation time for the experts is comparable to the communication time for exchanging their outputs, emphasizing the importance of network latency over bandwidth. We also observe significant management overhead due to Apple software stack's memory management logic. Based on these findings, we develop optimization schemes to eliminate the memory management overhead. As a result, the Mac Studio cluster is 1.15 times more cost-efficient than the state-of-the-art AI supercomputer with NVIDIA H100 GPUs. In addition, we construct a performance model to estimate system performance under varying configurations, and the model provides valuable insights for designing private LLM systems.",
        "authors": [
            "Mu-Chi Chen",
            "Po-Hsuan Huang",
            "Xiangrui Ke",
            "Chia-Heng Tu",
            "Chun Jason Xue",
            "Shih-Hao Hung"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.PF"
        ],
        "submit_date": "2025-06-30"
    },
    "http://arxiv.org/abs/2506.22175v1": {
        "id": "http://arxiv.org/abs/2506.22175v1",
        "title": "MPipeMoE: Memory Efficient MoE for Pre-trained Models with Adaptive Pipeline Parallelism",
        "link": "http://arxiv.org/abs/2506.22175v1",
        "tags": [
            "training",
            "MoE",
            "sparse"
        ],
        "relevant": true,
        "indexed_date": "2025-06-30",
        "tldr": "Addresses communication and memory inefficiencies in MoE model training. Proposes MPipeMoE with adaptive pipeline parallelism and memory reusing strategies for activation and buffer redundancy elimination. Achieves 2.8x training speedup and 47% memory reduction compared to state-of-the-art.",
        "abstract": "Recently, Mixture-of-Experts (MoE) has become one of the most popular techniques to scale pre-trained models to extraordinarily large sizes. Dynamic activation of experts allows for conditional computation, increasing the number of parameters of neural networks, which is critical for absorbing the vast amounts of knowledge available in many deep learning areas. However, despite the existing system and algorithm optimizations, there are significant challenges to be tackled when it comes to the inefficiencies of communication and memory consumption.   In this paper, we present the design and implementation of MPipeMoE, a high-performance library that accelerates MoE training with adaptive and memory-efficient pipeline parallelism. Inspired by that the MoE training procedure can be divided into multiple independent sub-stages, we design adaptive pipeline parallelism with an online algorithm to configure the granularity of the pipelining. Further, we analyze the memory footprint breakdown of MoE training and identify that activations and temporary buffers are the primary contributors to the overall memory footprint. Toward memory efficiency, we propose memory reusing strategies to reduce memory requirements by eliminating memory redundancies, and develop an adaptive selection component to determine the optimal strategy that considers both hardware capacities and model characteristics at runtime. We implement MPipeMoE upon PyTorch and evaluate it with common MoE models in a physical cluster consisting of 8 NVIDIA DGX A100 servers. Compared with the state-of-art approach, MPipeMoE achieves up to 2.8x speedup and reduces memory footprint by up to 47% in training large models.",
        "authors": [
            "Zheng Zhang",
            "Donglin Yang",
            "Yaqi Xia",
            "Liang Ding",
            "Dacheng Tao",
            "Xiaobo Zhou",
            "Dazhao Cheng"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-06-27"
    },
    "http://arxiv.org/abs/2506.22033v1": {
        "id": "http://arxiv.org/abs/2506.22033v1",
        "title": "SiPipe: Bridging the CPU-GPU Utilization Gap for Efficient Pipeline-Parallel LLM Inference",
        "link": "http://arxiv.org/abs/2506.22033v1",
        "tags": [
            "serving",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-06-30",
        "tldr": "Proposes SiPipe, a heterogeneous pipeline design for efficient LLM inference that offloads auxiliary computations to CPUs to mitigate pipeline bubbles. Key techniques include CPU sampling, token-safe execution, and structure-aware transmission. Achieves up to 2.1× throughput gain and 43% lower latency versus vLLM.",
        "abstract": "As inference workloads for large language models (LLMs) scale to meet growing user demand, pipeline parallelism (PP) has become a widely adopted strategy for multi-GPU deployment, particularly in cross-node setups, to improve key-value (KV) cache capacity and inference throughput. However, PP suffers from inherent inefficiencies caused by three types of execution bubbles-load-imbalance, intra-stage, and inter-stage-which limit pipeline saturation. We present SiPipe, a heterogeneous pipeline design that improves throughput by leveraging underutilized CPU resources to offload auxiliary computation and communication. SiPipe incorporates three key techniques-CPU sampling, a token-safe execution model, and structure-aware transmission-to mitigate pipeline bubbles and improve execution efficiency. Across diverse LLMs, SiPipe achieves up to 2.1 times higher throughput, 43% lower per-token latency, and up to 23% higher average GPU utilization compared to the state-of-the-art vLLM under the same PP configuration, demonstrating its generality across LLMs and deployment scenarios.",
        "authors": [
            "Yongchao He",
            "Bohan Zhao",
            "Zheng Cao"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-06-27"
    },
    "http://arxiv.org/abs/2506.20938v2": {
        "id": "http://arxiv.org/abs/2506.20938v2",
        "title": "ParEval-Repo: A Benchmark Suite for Evaluating LLMs with Repository-level HPC Translation Tasks",
        "link": "http://arxiv.org/abs/2506.20938v2",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-06-27",
        "tldr": "",
        "abstract": "GPGPU architectures have become significantly more diverse in recent years, which has led to an emergence of a variety of specialized programming models and software stacks to support them. Portable programming models exist, but they require significant developer effort to port to and optimize for different hardware architectures. Large language models (LLMs) may help to reduce this programmer burden. In this paper, we present a novel benchmark and testing framework, ParEval-Repo, which can be used to evaluate the efficacy of LLM-based approaches in automatically translating entire codebases across GPGPU execution models. ParEval-Repo includes several scientific computing and AI mini-applications in a range of programming models and levels of repository complexity. We use ParEval-Repo to evaluate a range of state-of-the-art open-source and commercial LLMs, with both a non-agentic and a top-down agentic approach. We assess code generated by the LLMs and approaches in terms of compilability, functional correctness, categories of build errors, and the cost of translation in terms of the number of inference tokens. Our results demonstrate that LLM translation of scientific applications is feasible for small programs but difficulty with generating functional build systems and cross-file dependencies pose challenges in scaling to larger codebases.",
        "authors": [
            "Joshua H. Davis",
            "Daniel Nichols",
            "Ishan Khillan",
            "Abhinav Bhatele"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-06-26"
    },
    "http://arxiv.org/abs/2506.20187v2": {
        "id": "http://arxiv.org/abs/2506.20187v2",
        "title": "Breaking the Boundaries of Long-Context LLM Inference: Adaptive KV Management on a Single Commodity GPU",
        "link": "http://arxiv.org/abs/2506.20187v2",
        "tags": [
            "offloading",
            "serving",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-06-26",
        "tldr": "Addresses long-context LLM inference memory bottlenecks on commodity GPUs. Proposes LeoAM with adaptive hierarchical KV management, lightweight KV abstracts, and compression/pipelining. Achieves 3.46x average latency speedup and up to 5.47x speedup at larger batch sizes.",
        "abstract": "Advanced Large Language Models (LLMs) have achieved impressive performance across a wide range of complex and long-context natural language tasks. However, performing long-context LLM inference locally on a commodity GPU (a PC) with privacy concerns remains challenging due to the increasing memory demands of the key-value (KV) cache. Existing systems typically identify important tokens and selectively offload their KV data to GPU and CPU memory. The KV data needs to be offloaded to disk due to the limited memory on a commodity GPU, but the process is bottlenecked by token importance evaluation overhead and the disk's low bandwidth. In this paper, we present LeoAM, the first efficient importance-aware long-context LLM inference system for a single commodity GPU with adaptive hierarchical GPU-CPU-Disk KV management. Our system employs an adaptive KV management strategy that partitions KV data into variable-sized chunks based on the skewed distribution of attention weights across different layers to reduce computational and additional transmission overheads. Moreover, we propose a lightweight KV abstract method, which minimizes transmission latency by storing and extracting the KV abstract of each chunk on disk instead of the full KV data. LeoAM also leverages the dynamic compression and pipeline techniques to further accelerate inference. Experimental results demonstrate that LongInfer achieves an average inference latency speedup of 3.46x, while maintaining comparable LLM response quality. In scenarios with larger batch sizes, it achieves up to a 5.47x speedup.",
        "authors": [
            "He Sun",
            "Li Li",
            "Mingjun Xiao",
            "Chengzhong Xu"
        ],
        "categories": [
            "cs.OS",
            "cs.CR"
        ],
        "submit_date": "2025-06-25"
    },
    "http://arxiv.org/abs/2506.19884v1": {
        "id": "http://arxiv.org/abs/2506.19884v1",
        "title": "MNN-AECS: Energy Optimization for LLM Decoding on Mobile Devices via Adaptive Core Selection",
        "link": "http://arxiv.org/abs/2506.19884v1",
        "tags": [
            "edge"
        ],
        "relevant": true,
        "indexed_date": "2025-06-26",
        "tldr": "Proposes MNN-AECS, an energy optimization system for mobile LLM decoding via adaptive low-power core selection. Reduces energy usage by 23% without slowdown on average across devices, compared to baseline MNN.",
        "abstract": "As the demand for on-device Large Language Model (LLM) inference grows, energy efficiency has become a major concern, especially for battery-limited mobile devices. Our analysis shows that the memory-bound LLM decode phase dominates energy use, and yet most existing works focus on accelerating the prefill phase, neglecting energy concerns. We introduce Adaptive Energy-Centric Core Selection (AECS) and integrate it into MNN to create the energy-efficient version, MNN-AECS, the first engine-level system solution without requiring root access or OS modifications for energy-efficient LLM decoding. MNN-AECS is designed to reduce LLM decoding energy while keeping decode speed within an acceptable slowdown threshold by dynamically selecting low-power CPU cores. MNN-AECS is evaluated across 5 Android and 2 iOS devices on 5 popular LLMs of various sizes. Compared to original MNN, MNN-AECS cuts down energy use by 23% without slowdown averaged over all 7 devices and 4 datasets. Against other engines, including llama.cpp, executorch, mllm, and MediaPipe, MNN-AECS delivers 39% to 78% energy saving and 12% to 363% speedup on average.",
        "authors": [
            "Zhengxiang Huang",
            "Chaoyue Niu",
            "Zhaode Wang",
            "Jiarui Xue",
            "Hanming Zhang",
            "Yugang Wang",
            "Zewei Xin",
            "Xiaotang Jiang",
            "Chengfei Lv",
            "Fan Wu",
            "Guihai Chen"
        ],
        "categories": [
            "cs.OS",
            "cs.AI",
            "cs.PF",
            "cs.SE"
        ],
        "submit_date": "2025-06-24"
    },
    "http://arxiv.org/abs/2506.19233v1": {
        "id": "http://arxiv.org/abs/2506.19233v1",
        "title": "Shelby: Decentralized Storage Designed to Serve",
        "link": "http://arxiv.org/abs/2506.19233v1",
        "tags": [
            "storage",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-06-25",
        "tldr": "Addresses poor performance of decentralized storage for demanding workloads like AI training. Designs Shelby protocol with control/data plane separation, low-overhead erasure coding, and dedicated backbone network. Achieves Web2-grade throughput and latency for read-intensive applications.",
        "abstract": "Existing decentralized storage protocols fall short of the service required by real-world applications. Their throughput, latency, cost-effectiveness, and availability are insufficient for demanding workloads such as video streaming, large-scale data analytics, or AI training. As a result, Web3 data-intensive applications are predominantly dependent on centralized infrastructure.   Shelby is a high-performance decentralized storage protocol designed to meet demanding needs. It achieves fast, reliable access to large volumes of data while preserving decentralization guarantees. The architecture reflects lessons from Web2 systems: it separates control and data planes, uses erasure coding with low replication overhead and minimal repair bandwidth, and operates over a dedicated backbone connecting RPC and storage nodes. Reads are paid, which incentivizes good performance. Shelby also introduces a novel auditing protocol that provides strong cryptoeconomic guarantees without compromising performance, a common limitation of other decentralized solutions. The result is a decentralized system that brings Web2-grade performance to production-scale, read-intensive Web3 applications.",
        "authors": [
            "Guy Goren",
            "Andrew Hariri",
            "Timothy D. R. Hartley",
            "Ravi Kappiyoor",
            "Alexander Spiegelman",
            "David Zmick"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-06-24"
    },
    "http://arxiv.org/abs/2506.19197v3": {
        "id": "http://arxiv.org/abs/2506.19197v3",
        "title": "Vertex addition to a ball graph with application to reliability and area coverage in autonomous swarms",
        "link": "http://arxiv.org/abs/2506.19197v3",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-06-25",
        "tldr": "",
        "abstract": "A unit ball graph consists of a set of vertices, labeled by points in Euclidean space, and edges joining all pairs of points within distance 1. These geometric graphs are used to model a variety of spatial networks, including communication networks between agents in an autonomous swarm. In such an application, vertices and/or edges of the graph may not be perfectly reliable; an agent may experience failure or a communication link rendered inoperable. With the goal of designing robust swarm formations, or unit ball graphs with high reliability (probability of connectedness), in a preliminary conference paper we provided an algorithm with cubic time complexity to determine all possible changes to a unit ball graph by repositioning a single vertex. Using this algorithm and Monte Carlo simulations, one obtains an efficient method to modify a unit ball graph by moving a single vertex to a location which maximizes the reliability. Another important consideration in many swarm missions is area coverage, yet highly reliable ball graphs often contain clusters of vertices. Here, we generalize our previous algorithm to improve area coverage as well as reliability. Our algorithm determines a location to add or move a vertex within a unit ball graph which maximizes the reliability, under the constraint that no other vertices of the graph be within some fixed distance. We compare this method of obtaining graphs with high reliability and evenly distributed area coverage to another method which uses a modified Fruchterman-Reingold algorithm for ball graphs.",
        "authors": [
            "Calum Buchanan",
            "Puck Rombach",
            "James Bagrow",
            "Hamid R. Ossareh"
        ],
        "categories": [
            "cs.DC",
            "math.CO",
            "math.OC"
        ],
        "submit_date": "2025-06-23"
    },
    "http://arxiv.org/abs/2506.19175v1": {
        "id": "http://arxiv.org/abs/2506.19175v1",
        "title": "Binsparse: A Specification for Cross-Platform Storage of Sparse Matrices and Tensors",
        "link": "http://arxiv.org/abs/2506.19175v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-06-25",
        "tldr": "",
        "abstract": "Sparse matrices and tensors are ubiquitous throughout multiple subfields of computing. The widespread usage of sparse data has inspired many in-memory and on-disk storage formats, but the only widely adopted storage specifications are the Matrix Market and FROSTT file formats, which both use ASCII text. Due to the inefficiency of text storage, these files typically have larger file sizes and longer parsing times than binary storage formats, which directly store an in-memory representation to disk. This can be a major bottleneck; since sparse computation is often bandwidth-bound, the cost of loading or storing a matrix to disk often exceeds the cost of performing a sparse computation. While it is common practice for practitioners to develop their own, custom, non-portable binary formats for high-performance sparse matrix storage, there is currently no cross-platform binary sparse matrix storage format. We present Binsparse, a cross-platform binary sparse matrix and tensor format specification. Binsparse is a modular, embeddable format, consisting of a JSON descriptor, which describes the matrix or tensor dimensions, type, and format, and a series of binary arrays, which can be stored in all modern binary containers, such as HDF5, Zarr, or NPZ. We provide several reference implementations of Binsparse spanning 5 languages, 5 frameworks, and 4 binary containers. We evaluate our Binsparse format on every matrix in the SuiteSparse Matrix Collection and a selection of tensors from the FROSTT collection. The Binsparse HDF5 CSR format shows file size reductions of 2.4x on average without compression and 7.5x with compression. We evaluate our parser's read/write performance against a state-of-the-art Matrix Market parser, demonstrating warm cache mean read speedups of 26.5x without compression and 2.6x with compression, and write speedups of 31x without compression and 1.4x with compression.",
        "authors": [
            "Benjamin Brock",
            "Willow Ahrens",
            "Hameer Abbasi",
            "Timothy A. Davis",
            "Juni Kim",
            "James Kitchen",
            "Spencer Patty",
            "Isaac Virshup",
            "Erik Welch"
        ],
        "categories": [
            "cs.MS",
            "cs.DC",
            "cs.DS"
        ],
        "submit_date": "2025-06-23"
    },
    "http://arxiv.org/abs/2506.18024v1": {
        "id": "http://arxiv.org/abs/2506.18024v1",
        "title": "Leveraging Cloud-Fog Automation for Autonomous Collision Detection and Classification in Intelligent Unmanned Surface Vehicles",
        "link": "http://arxiv.org/abs/2506.18024v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-06-24",
        "tldr": "",
        "abstract": "Industrial Cyber-Physical Systems (ICPS) technologies are foundational in driving maritime autonomy, particularly for Unmanned Surface Vehicles (USVs). However, onboard computational constraints and communication latency significantly restrict real-time data processing, analysis, and predictive modeling, hence limiting the scalability and responsiveness of maritime ICPS. To overcome these challenges, we propose a distributed Cloud-Edge-IoT architecture tailored for maritime ICPS by leveraging design principles from the recently proposed Cloud-Fog Automation paradigm. Our proposed architecture comprises three hierarchical layers: a Cloud Layer for centralized and decentralized data aggregation, advanced analytics, and future model refinement; an Edge Layer that executes localized AI-driven processing and decision-making; and an IoT Layer responsible for low-latency sensor data acquisition. Our experimental results demonstrated improvements in computational efficiency, responsiveness, and scalability. When compared with our conventional approaches, we achieved a classification accuracy of 86\\%, with an improved latency performance. By adopting Cloud-Fog Automation, we address the low-latency processing constraints and scalability challenges in maritime ICPS applications. Our work offers a practical, modular, and scalable framework to advance robust autonomy and AI-driven decision-making and autonomy for intelligent USVs in future maritime ICPS.",
        "authors": [
            "Thien Tran",
            "Quang Nguyen",
            "Jonathan Kua",
            "Minh Tran",
            "Toan Luu",
            "Thuong Hoang",
            "Jiong Jin"
        ],
        "categories": [
            "cs.DC",
            "cs.RO"
        ],
        "submit_date": "2025-06-22"
    },
    "http://arxiv.org/abs/2506.17551v2": {
        "id": "http://arxiv.org/abs/2506.17551v2",
        "title": "Research on Model Parallelism and Data Parallelism Optimization Methods in Large Language Model-Based Recommendation Systems",
        "link": "http://arxiv.org/abs/2506.17551v2",
        "tags": [
            "training",
            "recommendation",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-06-24",
        "tldr": "Optimizes distributed training for LLM-based recommendations using hybrid model and data parallelism with adaptive load balancing and gradient compression. Achieves 30% higher throughput and 20% better resource utilization over single-mode parallelism.",
        "abstract": "With the rapid adoption of large language models (LLMs) in recommendation systems, the computational and communication bottlenecks caused by their massive parameter sizes and large data volumes have become increasingly prominent. This paper systematically investigates two classes of optimization methods-model parallelism and data parallelism-for distributed training of LLMs in recommendation scenarios. For model parallelism, we implement both tensor parallelism and pipeline parallelism, and introduce an adaptive load-balancing mechanism to reduce cross-device communication overhead. For data parallelism, we compare synchronous and asynchronous modes, combining gradient compression and sparsification techniques with an efficient aggregation communication framework to significantly improve bandwidth utilization. Experiments conducted on a real-world recommendation dataset in a simulated service environment demonstrate that our proposed hybrid parallelism scheme increases training throughput by over 30% and improves resource utilization by approximately 20% compared to traditional single-mode parallelism, while maintaining strong scalability and robustness. Finally, we discuss trade-offs among different parallel strategies in online deployment and outline future directions involving heterogeneous hardware integration and automated scheduling technologies.",
        "authors": [
            "Haowei Yang",
            "Yu Tian",
            "Zhongheng Yang",
            "Zhao Wang",
            "Chengrui Zhou",
            "Dannier Li"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-06-21"
    },
    "http://arxiv.org/abs/2506.17900v1": {
        "id": "http://arxiv.org/abs/2506.17900v1",
        "title": "Leveraging Large Language Model for Intelligent Log Processing and Autonomous Debugging in Cloud AI Platforms",
        "link": "http://arxiv.org/abs/2506.17900v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-06-24",
        "tldr": "",
        "abstract": "With the increasing complexity and rapid expansion of the scale of AI systems in cloud platforms, the log data generated during system operation is massive, unstructured, and semantically ambiguous, which brings great challenges to fault location and system self-repair. In order to solve this problem, this paper proposes an intelligent log processing and automatic debugging framework based on Large Language Model (LLM), named Intelligent Debugger (LLM-ID). This method is extended on the basis of the existing pre-trained Transformer model, and integrates a multi-stage semantic inference mechanism to realize the context understanding of system logs and the automatic reconstruction of fault chains. Firstly, the system log is dynamically structured, and the unsupervised clustering and embedding mechanism is used to extract the event template and semantic schema. Subsequently, the fine-tuned LLM combined with the multi-round attention mechanism to perform contextual reasoning on the log sequence to generate potential fault assumptions and root cause paths. Furthermore, this paper introduces a reinforcement learning-based policy-guided recovery planner, which is driven by the remediation strategy generated by LLM to support dynamic decision-making and adaptive debugging in the cloud environment. Compared with the existing rule engine or traditional log analysis system, the proposed model has stronger semantic understanding ability, continuous learning ability and heterogeneous environment adaptability. Experiments on the cloud platform log dataset show that LLM-ID improves the fault location accuracy by 16.2%, which is significantly better than the current mainstream methods",
        "authors": [
            "Cheng Ji",
            "Huaiying Luo"
        ],
        "categories": [
            "cs.AI",
            "cs.DC"
        ],
        "submit_date": "2025-06-22"
    },
    "http://arxiv.org/abs/2506.17506v1": {
        "id": "http://arxiv.org/abs/2506.17506v1",
        "title": "VeriLocc: End-to-End Cross-Architecture Register Allocation via LLM",
        "link": "http://arxiv.org/abs/2506.17506v1",
        "tags": [
            "kernel",
            "RL",
            "hardware"
        ],
        "relevant": true,
        "indexed_date": "2025-06-24",
        "tldr": "Proposes VeriLocc, an LLM-based framework for automating GPU register allocation. Combines fine-tuned LLMs with static analysis and verifier-guided regeneration to generate architecture-specific assignments. Achieves 85-99% accuracy and outperforms rocBLAS by over 10% in runtime for matrix multiplication and attention kernels.",
        "abstract": "Modern GPUs evolve rapidly, yet production compilers still rely on hand-crafted register allocation heuristics that require substantial re-tuning for each hardware generation. We introduce VeriLocc, a framework that combines large language models (LLMs) with formal compiler techniques to enable generalizable and verifiable register allocation across GPU architectures. VeriLocc fine-tunes an LLM to translate intermediate representations (MIRs) into target-specific register assignments, aided by static analysis for cross-architecture normalization and generalization and a verifier-guided regeneration loop to ensure correctness. Evaluated on matrix multiplication (GEMM) and multi-head attention (MHA), VeriLocc achieves 85-99% single-shot accuracy and near-100% pass@100. Case study shows that VeriLocc discovers more performant assignments than expert-tuned libraries, outperforming rocBLAS by over 10% in runtime.",
        "authors": [
            "Lesheng Jin",
            "Zhenyuan Ruan",
            "Haohui Mai",
            "Jingbo Shang"
        ],
        "categories": [
            "cs.CL",
            "cs.OS"
        ],
        "submit_date": "2025-06-20"
    },
    "http://arxiv.org/abs/2506.15961v2": {
        "id": "http://arxiv.org/abs/2506.15961v2",
        "title": "TrainVerify: Equivalence-Based Verification for Distributed LLM Training",
        "link": "http://arxiv.org/abs/2506.15961v2",
        "tags": [
            "training",
            "quantization",
            "disaggregation"
        ],
        "relevant": true,
        "indexed_date": "2025-06-23",
        "tldr": "Proposes TrainVerify, a system for formally verifying equivalence between distributed training execution plans and logical specifications to prevent silent errors. Uses shape-reduction and stage-wise parallel verification to handle large scale. Verified Llama3 (405B) and DeepSeek-V3 (671B) plans, saving potential GPU hours.",
        "abstract": "Training large language models (LLMs) at scale requires parallel execution across thousands of devices, incurring enormous computational costs. Yet, these costly distributed trainings are rarely verified, leaving them prone to silent errors and potentially wasting millions of GPU hours. We introduce TrainVerify, a system for verifiable distributed training of LLMs. Given a deep learning model's logical specification as the ground truth, TrainVerify formally verifies that a distributed parallel execution plan is mathematically equivalent to it. Direct verification is notoriously difficult due to the sheer scale of LLMs which often involves billions of variables and highly intricate computation graphs. Therefore, TrainVerify introduces shape-reduction techniques and a stage-wise parallel verification algorithm that significantly reduces complexity while preserving formal correctness. TrainVerify scales to frontier LLMs, including the successful verification of the Llama3 (405B) and DeepSeek-V3 (671B) training plans.",
        "authors": [
            "Yunchi Lu",
            "Youshan Miao",
            "Cheng Tan",
            "Peng Huang",
            "Yi Zhu",
            "Xian Zhang",
            "Fan Yang"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.LG"
        ],
        "submit_date": "2025-06-19"
    },
    "http://arxiv.org/abs/2506.15461v1": {
        "id": "http://arxiv.org/abs/2506.15461v1",
        "title": "All is Not Lost: LLM Recovery without Checkpoints",
        "link": "http://arxiv.org/abs/2506.15461v1",
        "tags": [
            "training",
            "offloading",
            "storage"
        ],
        "relevant": true,
        "indexed_date": "2025-06-19",
        "tldr": "Proposes CheckFree and CheckFree+ for efficient LLM training recovery from node failures without checkpoints. Uses weighted averaging of neighbouring stages and out-of-order execution to mimic lost layers. Achieves over 12% faster convergence than checkpointing at 5-10% failure rates.",
        "abstract": "Training LLMs on decentralized and wimpy computation nodes, e.g., multiple on-spot instances, lowers the training cost and enables model democratization. The inevitable challenge here is the churn of nodes due to failures and the operator's scheduling policies, leading to losing a stage - a part of the model. The conventional approaches to recover from failures are to either use checkpointing, where periodically a copy of the entire model is sent to an additional storage, or redundant computation. These approaches yield significant communication and/or computation overhead even in non-failure cases and scale poorly in settings with large models. In this paper, we propose, CheckFree, an efficient recovery method where a failing stage is substituted by a weighted average of the closest neighboring stages. In contrast to the state of the art, CheckFree requires no additional computation or storage. However, because of the nature of averaging neighbouring stages, it can only recover failures of intermediate stages. We further extend our method to CheckFree+ with out-of-order pipeline execution to tolerate crashes of the first and last stages. Thanks to out-of-order pipelining, behaviour of those stages is mimicked by their neighboring ones, which allows CheckFree+ to recover them by simply copying the weights from the immediate neighbour. To be able to recover the (de)embedding layers, CheckFree+ copies those layers to the neighboring stages, which requires relatively small storage overhead. We extensively evaluate our method on LLaMa models of model sizes from 124M to 1.5B with varying failure frequencies. In the case of low and medium failure rates (5-10%), CheckFree and CheckFree+ outperform both checkpointing and redundant computation in terms of convergence in wall-clock time by over 12%. Both of our proposals can be run via our code available at: https://github.com/gensyn-ai/CheckFree.",
        "authors": [
            "Nikolay Blagoev",
            "Oğuzhan Ersoy",
            "Lydia Yiyu Chen"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-06-18"
    },
    "http://arxiv.org/abs/2506.15155v1": {
        "id": "http://arxiv.org/abs/2506.15155v1",
        "title": "eLLM: Elastic Memory Management Framework for Efficient LLM Serving",
        "link": "http://arxiv.org/abs/2506.15155v1",
        "tags": [
            "serving",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-06-19",
        "tldr": "Addresses inefficient memory utilization in LLM serving due to isolated management of runtime memory and KV caches. Proposes eLLM with virtual tensor abstraction and elastic memory mechanism using CPU buffering. Achieves 2.32x higher decoding throughput and 3x larger batch sizes for 128K-token inputs.",
        "abstract": "Large Language Models are increasingly being deployed in datacenters. Serving these models requires careful memory management, as their memory usage includes static weights, dynamic activations, and key-value caches. While static weights are constant and predictable, dynamic components such as activations and KV caches change frequently during runtime, presenting significant challenges for efficient memory management. Modern LLM serving systems typically handle runtime memory and KV caches at distinct abstraction levels: runtime memory management relies on static tensor abstractions, whereas KV caches utilize a page table-based virtualization layer built on top of the tensor abstraction. This virtualization dynamically manages KV caches to mitigate memory fragmentation. However, this dual-level approach fundamentally isolates runtime memory and KV cache management, resulting in suboptimal memory utilization under dynamic workloads, which can lead to a nearly 20% drop in throughput.   To address these limitations, we propose eLLM, an elastic memory management framework inspired by the classical memory ballooning mechanism in operating systems. The core components of eLLM include: (1) Virtual Tensor Abstraction, which decouples the virtual address space of tensors from the physical GPU memory, creating a unified and flexible memory pool; (2) an Elastic Memory Mechanism that dynamically adjusts memory allocation through runtime memory inflation and deflation, leveraging CPU memory as an extensible buffer; and (3) a Lightweight Scheduling Strategy employing SLO-aware policies to optimize memory utilization and effectively balance performance trade-offs under stringent SLO constraints. Comprehensive evaluations demonstrate that eLLM significantly outperforms state-of-the-art systems, 2.32x higher decoding throughput, and supporting 3x larger batch sizes for 128K-token inputs.",
        "authors": [
            "Jiale Xu",
            "Rui Zhang",
            "Yi Xiong",
            "Cong Guo",
            "Zihan Liu",
            "Yangjie Zhou",
            "Weiming Hu",
            "Hao Wu",
            "Changxu Shao",
            "Ziqing Wang",
            "Yongjie Yuan",
            "Junping Zhao",
            "Minyi Guo",
            "Jingwen Leng"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-06-18"
    },
    "http://arxiv.org/abs/2506.14852v1": {
        "id": "http://arxiv.org/abs/2506.14852v1",
        "title": "Cost-Efficient Serving of LLM Agents via Test-Time Plan Caching",
        "link": "http://arxiv.org/abs/2506.14852v1",
        "tags": [
            "serving",
            "RL",
            "agentic"
        ],
        "relevant": true,
        "indexed_date": "2025-06-19",
        "tldr": "Addresses high serving costs of LLM agent workflows by caching reusable plan templates. Proposes extracting templates from prior executions, matching via keywords, and adapting with lightweight models. Reduces costs by 46.62% while maintaining performance.",
        "abstract": "LLM-based agentic applications have shown increasingly remarkable capabilities in complex workflows but incur substantial costs due to extensive planning and reasoning requirements. Existing LLM caching techniques (like context caching and semantic caching), primarily designed for serving chatbots, are insufficient for agentic applications where outputs depend on external data or environmental contexts. We propose agentic plan caching, a novel approach that extracts, stores, adapts, and reuses structured plan templates from planning stages of agentic applications across semantically similar tasks to reduce the cost of serving. Unlike traditional semantic caching, our system extracts plan templates from completed agent executions at test-time, employs keyword extraction to match new requests against cached plans, and utilizes lightweight models to adapt these templates to task-specific plans with contexts. Evaluation across multiple real-world agentic applications shows that our system can reduce costs by 46.62% on average while maintaining performance, offering a more efficient solution for serving LLM-based agents that complements existing LLM serving infrastructures.",
        "authors": [
            "Qizheng Zhang",
            "Michael Wornow",
            "Kunle Olukotun"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.CL",
            "cs.LG",
            "cs.PF"
        ],
        "submit_date": "2025-06-17"
    },
    "http://arxiv.org/abs/2506.14851v1": {
        "id": "http://arxiv.org/abs/2506.14851v1",
        "title": "Efficient Serving of LLM Applications with Probabilistic Demand Modeling",
        "link": "http://arxiv.org/abs/2506.14851v1",
        "tags": [
            "serving",
            "autoscaling",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-06-19",
        "tldr": "Proposes Hermes, an LLM serving system that models resource demands via Probabilistic Demand Graphs (PDGraphs). Applies Gittins scheduling policy to minimize completion time and pre-warms backends using predictive modeling. Reduces average and P95 completion times by 70% and 80% respectively.",
        "abstract": "Applications based on Large Language Models (LLMs) contains a series of tasks to address real-world problems with boosted capability, which have dynamic demand volumes on diverse backends. Existing serving systems treat the resource demands of LLM applications as a blackbox, compromising end-to-end efficiency due to improper queuing order and backend warm up latency. We find that the resource demands of LLM applications can be modeled in a general and accurate manner with Probabilistic Demand Graph (PDGraph). We then propose Hermes, which leverages PDGraph for efficient serving of LLM applications. Confronting probabilistic demand description, Hermes applies the Gittins policy to determine the scheduling order that can minimize the average application completion time. It also uses the PDGraph model to help prewarm cold backends at proper moments. Experiments with diverse LLM applications confirm that Hermes can effectively improve the application serving efficiency, reducing the average completion time by over 70% and the P95 completion time by over 80%.",
        "authors": [
            "Yifei Liu",
            "Zuo Gan",
            "Zhenghao Gan",
            "Weiye Wang",
            "Chen Chen",
            "Yizhou Shan",
            "Xusheng Chen",
            "Zhenhua Han",
            "Yifei Zhu",
            "Shixuan Sun",
            "Minyi Guo"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.LG"
        ],
        "submit_date": "2025-06-17"
    },
    "http://arxiv.org/abs/2506.14630v1": {
        "id": "http://arxiv.org/abs/2506.14630v1",
        "title": "Keigo: Co-designing Log-Structured Merge Key-Value Stores with a Non-Volatile, Concurrency-aware Storage Hierarchy (Extended Version)",
        "link": "http://arxiv.org/abs/2506.14630v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-06-18",
        "tldr": "",
        "abstract": "We present Keigo, a concurrency- and workload-aware storage middleware that enhances the performance of log-structured merge key-value stores (LSM KVS) when they are deployed on a hierarchy of storage devices. The key observation behind Keigo is that there is no one-size-fits-all placement of data across the storage hierarchy that optimizes for all workloads. Hence, to leverage the benefits of combining different storage devices, Keigo places files across different devices based on their parallelism, I/O bandwidth, and capacity. We introduce three techniques - concurrency-aware data placement, persistent read-only caching, and context-based I/O differentiation. Keigo is portable across different LSMs, is adaptable to dynamic workloads, and does not require extensive profiling. Our system enables established production KVS such as RocksDB, LevelDB, and Speedb to benefit from heterogeneous storage setups. We evaluate Keigo using synthetic and realistic workloads, showing that it improves the throughput of production-grade LSMs up to 4x for write- and 18x for read-heavy workloads when compared to general-purpose storage systems and specialized LSM KVS.",
        "authors": [
            "Rúben Adão",
            "Zhongjie Wu",
            "Changjun Zhou",
            "Oana Balmau",
            "João Paulo",
            "Ricardo Macedo"
        ],
        "categories": [
            "cs.DC",
            "cs.DB"
        ],
        "submit_date": "2025-06-17"
    },
    "http://arxiv.org/abs/2506.12708v3": {
        "id": "http://arxiv.org/abs/2506.12708v3",
        "title": "Serving Large Language Models on Huawei CloudMatrix384",
        "link": "http://arxiv.org/abs/2506.12708v3",
        "tags": [
            "serving",
            "MoE",
            "quantization"
        ],
        "relevant": true,
        "indexed_date": "2025-06-17",
        "tldr": "Introduces CloudMatrix-Infer, a serving system for large MoE LLMs on Huawei CloudMatrix384. Combines peer-to-peer serving architecture, expert parallelism via UB token dispatch, and hardware optimizations like INT8 quantization. Achieves prefill throughput of 6,688 tokens/s/NPU and decode at 1,943 tokens/s/NPU with <50ms TPOT.",
        "abstract": "The rapid evolution of large language models (LLMs), driven by growing parameter scales, adoption of mixture-of-experts (MoE) architectures, and expanding context lengths, imposes unprecedented demands on AI infrastructure. Traditional AI clusters face limitations in compute intensity, memory bandwidth, inter-chip communication, and latency, compounded by variable workloads and strict service-level objectives. Addressing these issues requires fundamentally redesigned hardware-software integration. This paper introduces Huawei CloudMatrix, a next-generation AI datacenter architecture, realized in the production-grade CloudMatrix384 supernode. It integrates 384 Ascend 910 NPUs and 192 Kunpeng CPUs interconnected via an ultra-high-bandwidth Unified Bus (UB) network, enabling direct all-to-all communication and dynamic pooling of resources. These features optimize performance for communication-intensive operations, such as large-scale MoE expert parallelism and distributed key-value cache access. To fully leverage CloudMatrix384, we propose CloudMatrix-Infer, an advanced LLM serving solution incorporating three core innovations: a peer-to-peer serving architecture that independently scales prefill, decode, and caching; a large-scale expert parallelism strategy supporting EP320 via efficient UB-based token dispatch; and hardware-aware optimizations including specialized operators, microbatch-based pipelining, and INT8 quantization. Evaluation with the DeepSeek-R1 model shows CloudMatrix-Infer achieves state-of-the-art efficiency: prefill throughput of 6,688 tokens/s per NPU and decode throughput of 1,943 tokens/s per NPU (<50 ms TPOT). It effectively balances throughput and latency, sustaining 538 tokens/s per NPU even under stringent 15 ms latency constraints, while INT8 quantization maintains model accuracy across benchmarks.",
        "authors": [
            "Pengfei Zuo",
            "Huimin Lin",
            "Junbo Deng",
            "Nan Zou",
            "Xingkun Yang",
            "Yingyu Diao",
            "Weifeng Gao",
            "Ke Xu",
            "Zhangyu Chen",
            "Shirui Lu",
            "Zhao Qiu",
            "Peiyang Li",
            "Xianyu Chang",
            "Zhengzhong Yu",
            "Fangzheng Miao",
            "Jia Zheng",
            "Ying Li",
            "Yuan Feng",
            "Bei Wang",
            "Zaijian Zong",
            "Mosong Zhou",
            "Wenli Zhou",
            "Houjiang Chen",
            "Xingyu Liao",
            "Yipeng Li",
            "Wenxiao Zhang",
            "Ping Zhu",
            "Yinggang Wang",
            "Chuanjie Xiao",
            "Depeng Liang",
            "Dong Cao",
            "Juncheng Liu",
            "Yongqiang Yang",
            "Xiaolong Bai",
            "Yi Li",
            "Huaguo Xie",
            "Huatao Wu",
            "Zhibin Yu",
            "Lv Chen",
            "Hu Liu",
            "Yujun Ding",
            "Haipei Zhu",
            "Jing Xia",
            "Yi Xiong",
            "Zhou Yu",
            "Heng Liao"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.AR",
            "cs.LG"
        ],
        "submit_date": "2025-06-15"
    },
    "http://arxiv.org/abs/2506.12417v2": {
        "id": "http://arxiv.org/abs/2506.12417v2",
        "title": "HarMoEny: Efficient Multi-GPU Inference of MoE Models",
        "link": "http://arxiv.org/abs/2506.12417v2",
        "tags": [
            "MoE",
            "serving",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-06-17",
        "tldr": "Addresses MoE inference load imbalance in multi-GPU systems. Proposes HarMoEny with dynamic token redistribution and asynchronous prefetching to balance expert utilization. Achieves 37%-70% higher throughput and 34%-41% lower time-to-first-token under imbalance.",
        "abstract": "Mixture-of-Experts (MoE) models offer computational efficiency during inference by activating only a subset of specialized experts for a given input. This enables efficient model scaling on multi-GPU systems that use expert parallelism without compromising performance. However, load imbalance among experts and GPUs introduces waiting times, which can significantly increase inference latency. To address this challenge, we propose HarMoEny, a novel solution to address MoE load imbalance through two simple techniques: (i) dynamic token redistribution to underutilized GPUs and (ii) asynchronous prefetching of experts from the system to GPU memory. These techniques achieve a near-perfect load balance among experts and GPUs and mitigate delays caused by overloaded GPUs. We implement HarMoEny and compare its latency and throughput with four MoE baselines using real-world and synthetic datasets. Under heavy load imbalance, HarMoEny increases throughput by 37%-70% and reduces time-to-first-token by 34%-41%, compared to the next-best baseline. Moreover, our ablation study demonstrates that HarMoEny's scheduling policy reduces the GPU idling time by up to 84% compared to the baseline policies.",
        "authors": [
            "Zachary Doucet",
            "Rishi Sharma",
            "Martijn de Vos",
            "Rafael Pires",
            "Anne-Marie Kermarrec",
            "Oana Balmau"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-06-14"
    },
    "http://arxiv.org/abs/2506.13028v1": {
        "id": "http://arxiv.org/abs/2506.13028v1",
        "title": "NaSh: Guardrails for an LLM-Powered Natural Language Shell",
        "link": "http://arxiv.org/abs/2506.13028v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-06-17",
        "tldr": "",
        "abstract": "We explore how a shell that uses an LLM to accept natural language input might be designed differently from the shells of today. As LLMs may produce unintended or unexplainable outputs, we argue that a natural language shell should provide guardrails that empower users to recover from such errors. We concretize some ideas for doing so by designing a new shell called NaSh, identify remaining open problems in this space, and discuss research directions to address them.",
        "authors": [
            "Bimal Raj Gyawali",
            "Saikrishna Achalla",
            "Konstantinos Kallas",
            "Sam Kumar"
        ],
        "categories": [
            "cs.OS",
            "cs.AI"
        ],
        "submit_date": "2025-06-16"
    },
    "http://arxiv.org/abs/2506.12204v1": {
        "id": "http://arxiv.org/abs/2506.12204v1",
        "title": "Semantic Scheduling for LLM Inference",
        "link": "http://arxiv.org/abs/2506.12204v1",
        "tags": [
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-06-17",
        "tldr": "Proposes semantic scheduling for LLM inference by prioritizing requests based on contextual importance. Designs an optimal-time algorithm to minimize waiting time for urgent tasks, demonstrated in medical emergencies. Achieves reduced latency for high-priority requests.",
        "abstract": "Conventional operating system scheduling algorithms are largely content-ignorant, making decisions based on factors such as latency or fairness without considering the actual intents or semantics of processes. Consequently, these algorithms often do not prioritize tasks that require urgent attention or carry higher importance, such as in emergency management scenarios. However, recent advances in language models enable semantic analysis of processes, allowing for more intelligent and context-aware scheduling decisions. In this paper, we introduce the concept of semantic scheduling in scheduling of requests from large language models (LLM), where the semantics of the process guide the scheduling priorities. We present a novel scheduling algorithm with optimal time complexity, designed to minimize the overall waiting time in LLM-based prompt scheduling. To illustrate its effectiveness, we present a medical emergency management application, underscoring the potential benefits of semantic scheduling for critical, time-sensitive tasks. The code and data are available at https://github.com/Wenyueh/latency_optimization_with_priority_constraints.",
        "authors": [
            "Wenyue Hua",
            "Dujian Ding",
            "Yile Gu",
            "Yujie Ren",
            "Kai Mei",
            "Minghua Ma",
            "William Yang Wang"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.OS"
        ],
        "submit_date": "2025-06-13"
    },
    "http://arxiv.org/abs/2506.11800v1": {
        "id": "http://arxiv.org/abs/2506.11800v1",
        "title": "A retrospective on DISPEED -- Leveraging heterogeneity in a drone swarm for IDS execution",
        "link": "http://arxiv.org/abs/2506.11800v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-06-16",
        "tldr": "",
        "abstract": "Swarms of drones are gaining more and more autonomy and efficiency during their missions. However, security threats can disrupt their missions' progression. To overcome this problem, Network Intrusion Detection Systems ((N)IDS) are promising solutions to detect malicious behavior on network traffic. However, modern NIDS rely on resource-hungry machine learning techniques, that can be difficult to deploy on a swarm of drones. The goal of the DISPEED project is to leverage the heterogeneity (execution platforms, memory) of the drones composing a swarm to deploy NIDS. It is decomposed in two phases: (1) a characterization phase that consists in characterizing various IDS implementations on diverse embedded platforms, and (2) an IDS implementation mapping phase that seeks to develop selection strategies to choose the most relevant NIDS depending on the context. On the one hand, the characterization phase allowed us to identify 36 relevant IDS implementations on three different embedded platforms: a Raspberry Pi 4B, a Jetson Xavier, and a Pynq-Z2. On the other hand, the IDS implementation mapping phase allowed us to design both standalone and distributed strategies to choose the best NIDSs to deploy depending on the context. The results of the project have led to three publications in international conferences, and one publication in a journal.",
        "authors": [
            "Vincent Lannurien",
            "Camélia Slimani",
            "Louis Morge-Rollet",
            "Laurent Lemarchand",
            "David Espes",
            "Frédéric Le Roy",
            "Jalil Boukhobza"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-06-13"
    },
    "http://arxiv.org/abs/2506.11309v1": {
        "id": "http://arxiv.org/abs/2506.11309v1",
        "title": "SwiftSpec: Ultra-Low Latency LLM Decoding by Scaling Asynchronous Speculative Decoding",
        "link": "http://arxiv.org/abs/2506.11309v1",
        "tags": [
            "serving",
            "disaggregation",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-06-16",
        "tldr": "Addresses ultra-low latency LLM decoding challenges. Proposes SwiftSpec, an asynchronous speculative decoding system with parallel tree generation, tree-aware KV cache management, and fused kernels. Achieves 1.75× average speedup and serves Llama3-70B at 348 tokens/s on 8 GPUs.",
        "abstract": "Low-latency decoding for large language models (LLMs) is crucial for applications like chatbots and code assistants, yet generating long outputs remains slow in single-query settings. Prior work on speculative decoding (which combines a small draft model with a larger target model) and tensor parallelism has each accelerated decoding. However, conventional approaches fail to apply both simultaneously due to imbalanced compute requirements (between draft and target models), KV-cache inconsistencies, and communication overheads under small-batch tensor-parallelism. This paper introduces SwiftSpec, a system that targets ultra-low latency for LLM decoding. SwiftSpec redesigns the speculative decoding pipeline in an asynchronous and disaggregated manner, so that each component can be scaled flexibly and remove draft overhead from the critical path. To realize this design, SwiftSpec proposes parallel tree generation, tree-aware KV cache management, and fused, latency-optimized kernels to overcome the challenges listed above. Across 5 model families and 6 datasets, SwiftSpec achieves an average of 1.75x speedup over state-of-the-art speculative decoding systems and, as a highlight, serves Llama3-70B at 348 tokens/s on 8 Nvidia Hopper GPUs, making it the fastest known system for low-latency LLM serving at this scale.",
        "authors": [
            "Ziyi Zhang",
            "Ziheng Jiang",
            "Chengquan Jiang",
            "Menghan Yu",
            "Size Zheng",
            "Haibin Lin",
            "Henry Hoffmann",
            "Xin Liu"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-06-12"
    },
    "http://arxiv.org/abs/2506.10470v1": {
        "id": "http://arxiv.org/abs/2506.10470v1",
        "title": "TD-Pipe: Temporally-Disaggregated Pipeline Parallelism Architecture for High-Throughput LLM Inference",
        "link": "http://arxiv.org/abs/2506.10470v1",
        "tags": [
            "serving",
            "disaggregation",
            "autoscaling"
        ],
        "relevant": true,
        "indexed_date": "2025-06-13",
        "tldr": "Proposes TD-Pipe, a temporally-disaggregated pipeline parallelism architecture for LLM inference, decoupling prefill/decode phases to eliminate bubbles. Uses hierarchy-controller, AI-based greedy prefill, work stealing, and intensity comparison. Achieves up to 1.91x higher throughput over tensor parallelism.",
        "abstract": "As the model size continuously increases, pipeline parallelism shows great promise in throughput-oriented LLM inference due to its low demand on communications. However, imbalanced pipeline workloads and complex data dependencies in the prefill and decode phases result in massive pipeline bubbles and further severe performance reduction. To better exploit the pipeline parallelism for high-throughput LLM inference, we propose TD-Pipe, with the key idea lies in the temporally-disaggregated pipeline parallelism architecture. Specifically, this architecture disaggregates the prefill and decode phases in the temporal dimension, so as to eliminate pipeline bubbles caused by the phase switching. TD-Pipe identifies potential issues of exploiting the novel architecture and provides solutions. First, a hierarchy-controller structure is used to better coordinate devices in pipeline parallelism by decoupling the scheduling from execution. Second, the AI-based greedy prefill approach aggressively performs more prefills by predicting the output length and simulating the memory usage. Third, the inter-batch work stealing approach dynamically balances decode phase workloads between different batches to reduce bubbles. Forth, the spatial-temporal intensity comparison approach determines the optimal switch from decode to prefill by comparing the performance drop from reduced computational intensity with that from phase switching bubbles. Extensive experiments show that TD-Pipe effectively increases the throughput of LLM inference by up to 1.91x over the existing tensor parallel approach and 2.73x over the existing pipeline parallel approach on GPU nodes with only PCIe interconnection.",
        "authors": [
            "Hongbin Zhang",
            "Taosheng Wei",
            "Zhenyi Zheng",
            "Jiangsu Du",
            "Zhiguang Chen",
            "Yutong Lu"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-06-12"
    },
    "http://arxiv.org/abs/2506.10401v2": {
        "id": "http://arxiv.org/abs/2506.10401v2",
        "title": "HPCTransCompile: An AI Compiler Generated Dataset for High-Performance CUDA Transpilation and LLM Preliminary Exploration",
        "link": "http://arxiv.org/abs/2506.10401v2",
        "tags": [
            "kernel",
            "serving",
            "training"
        ],
        "relevant": true,
        "indexed_date": "2025-06-13",
        "tldr": "Proposes an AI compiler framework for transpiling high-performance CUDA code to other platforms using LLMs, with a graph-based augmentation method. Introduces HPCTransEval benchmark. Achieves 43.8% average speedup in CPU operators compared to existing methods.",
        "abstract": "The rapid growth of deep learning has driven exponential increases in model parameters and computational demands. NVIDIA GPUs and their CUDA-based software ecosystem provide robust support for parallel computing, significantly alleviating computational bottlenecks. Meanwhile, due to the cultivation of user programming habits and the high performance of GPUs, the CUDA ecosystem has established a dominant position in the field of parallel software. This dominance requires other hardware platforms to support CUDA-based software with performance portability. However, translating CUDA code to other platforms poses significant challenges due to differences in parallel programming paradigms and hardware architectures. Existing approaches rely on language extensions, domain-specific languages (DSLs), or compilers but face limitations in workload coverage and generalizability. Moreover, these methods often incur substantial development costs. Recently, LLMs have demonstrated extraordinary potential in various vertical domains, especially in code-related tasks. However, the performance of existing LLMs in CUDA transpilation, particularly for high-performance code, remains suboptimal. To address these challenges, we propose a novel framework for generating high-performance CUDA and corresponding platform code pairs, leveraging AI compiler and automatic optimization technology. We further enhance the framework with a graph-based data augmentation method and introduce HPCTransEval, a benchmark for evaluating LLM performance on CUDA transpilation. We conduct experiments using CUDA-to-CPU transpilation as a case study on leading LLMs. The speedup ratio of the CPU operators has an average improvemnet of 43.8\\%, highlighting the potential of LLMs to address compatibility challenges within the CUDA ecosystem. Our code is available at https://github.com/PJLAB-CHIP/HPCTransCompile.",
        "authors": [
            "Jiaqi Lv",
            "Xufeng He",
            "Yanchen Liu",
            "Xu Dai",
            "Aocheng Shen",
            "Yinghao Li",
            "Jiachen Hao",
            "Jianrong Ding",
            "Yang Hu",
            "Shouyi Yin"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-06-12"
    },
    "http://arxiv.org/abs/2506.09554v2": {
        "id": "http://arxiv.org/abs/2506.09554v2",
        "title": "Understanding the Performance and Power of LLM Inferencing on Edge Accelerators",
        "link": "http://arxiv.org/abs/2506.09554v2",
        "tags": [
            "edge",
            "serving",
            "quantization"
        ],
        "relevant": true,
        "indexed_date": "2025-06-12",
        "tldr": "Evaluates LLM inference feasibility on edge accelerators (Nvidia Jetson Orin AGX) under varying batch sizes, sequence lengths, and quantization. Explores latency, throughput, power trade-offs and provides optimization insights. Quantization shows mixed speed impacts, e.g., makes smaller models slower.",
        "abstract": "Large Language Models (LLMs) have demonstrated exceptional benefits to a wide range of domains, for tasks as diverse as code generation and robot navigation. While LLMs are usually served from cloud data centers, mission-critical and privacy-sensitive applications may require local hosting of open LLM models. Given the large GPU memory footprint needed for LLMs, edge accelerators such as Nvidia Jetson Orin AGX with 64GB of shared GPU-CPU RAM are a compelling choice. However, the feasibility and performance of LLM inference on edge accelerators is under-explored. This study presents a detailed evaluation of LLM inference on the NVIDIA Jetson Orin AGX, on four SOTA models ranging from 2.7B to 32.8B parameters, such as Meta Llama3.1, Microsoft-Phi2, Deepseek-R1-Qwen. We investigate the impact of varying batch sizes, sequence lengths, and quantization levels on latency, throughput, and perplexity, and also explore various custom power modes on the Orin AGX to perform power and energy consumption analysis. Our findings offer interesting insights on the trade-offs between efficiency, inference speed and resource use, e.g., increasing the sequence length causes a decrease in token throughput and quantization causes smaller LLMs to be slower. These results can help optimize LLM serving on edge accelerators for practical applications.",
        "authors": [
            "Mayank Arya",
            "Yogesh Simmhan"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-06-11"
    },
    "http://arxiv.org/abs/2506.09397v5": {
        "id": "http://arxiv.org/abs/2506.09397v5",
        "title": "SLED: A Speculative LLM Decoding Framework for Efficient Edge Serving",
        "link": "http://arxiv.org/abs/2506.09397v5",
        "tags": [
            "edge",
            "serving",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-06-12",
        "tldr": "Proposes SLED, a speculative decoding framework for efficient LLM inference on edge devices. Uses lightweight local drafters and a shared edge server for batched verification. Achieves 2.2× higher throughput and 2.8× system capacity without accuracy loss.",
        "abstract": "The growing gap between the increasing complexity of large language models (LLMs) and the limited computational budgets of edge devices poses a key challenge for efficient on-device inference, despite gradual improvements in hardware capabilities. Existing strategies, such as aggressive quantization, pruning, or remote inference, trade accuracy for efficiency or lead to substantial cost burdens. This position paper introduces a new framework that leverages speculative decoding, previously viewed primarily as a decoding acceleration technique for autoregressive generation of LLMs, as a promising approach specifically adapted for edge computing by orchestrating computation across heterogeneous devices. We propose \\acronym, a framework that allows lightweight edge devices to draft multiple candidate tokens locally using diverse draft models, while a single, shared edge server verifies the tokens utilizing a more precise target model. To further increase the efficiency of verification, the edge server batch the diverse verification requests from devices. This approach supports device heterogeneity and reduces server-side memory footprint by sharing the same upstream target model across multiple devices. Our initial experiments with Jetson Orin Nano, Raspberry Pi 4B/5, and an edge server equipped with 4 Nvidia A100 GPUs indicate substantial benefits: 2.2 more system throughput, 2.8 more system capacity, and better cost efficiency, all without sacrificing model accuracy.",
        "authors": [
            "Xiangchen Li",
            "Dimitrios Spatharakis",
            "Saeid Ghafouri",
            "Jiakun Fan",
            "Hans Vandierendonck",
            "Deepu John",
            "Bo Ji",
            "Dimitrios Nikolopoulos"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.LG",
            "cs.NI"
        ],
        "submit_date": "2025-06-11"
    },
    "http://arxiv.org/abs/2506.09282v1": {
        "id": "http://arxiv.org/abs/2506.09282v1",
        "title": "ScalableHD: Scalable and High-Throughput Hyperdimensional Computing Inference on Multi-Core CPUs",
        "link": "http://arxiv.org/abs/2506.09282v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-06-12",
        "tldr": "",
        "abstract": "Hyperdimensional Computing (HDC) is a brain-inspired computing paradigm that represents and manipulates information using high-dimensional vectors, called hypervectors (HV). Traditional HDC methods, while robust to noise and inherently parallel, rely on single-pass, non-parametric training and often suffer from low accuracy. To address this, recent approaches adopt iterative training of base and class HVs, typically accelerated on GPUs. Inference, however, remains lightweight and well-suited for real-time execution. Yet, efficient HDC inference has been studied almost exclusively on specialized hardware such as FPGAs and GPUs, with limited attention to general-purpose multi-core CPUs. To address this gap, we propose ScalableHD for scalable and high-throughput HDC inference on multi-core CPUs. ScalableHD employs a two-stage pipelined execution model, where each stage is parallelized across cores and processes chunks of base and class HVs. Intermediate results are streamed between stages using a producer-consumer mechanism, enabling on-the-fly consumption and improving cache locality. To maximize performance, ScalableHD integrates memory tiling and NUMA-aware worker-to-core binding. Further, it features two execution variants tailored for small and large batch sizes, each designed to exploit compute parallelism based on workload characteristics while mitigating the memory-bound compute pattern that limits HDC inference performance on modern multi-core CPUs. ScalableHD achieves up to 10x speedup in throughput (samples per second) over state-of-the-art baselines such as TorchHD, across a diverse set of tasks ranging from human activity recognition to image classification, while preserving task accuracy. Furthermore, ScalableHD exhibits robust scalability: increasing the number of cores yields near-proportional throughput improvements.",
        "authors": [
            "Dhruv Parikh",
            "Viktor Prasanna"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-06-10"
    },
    "http://arxiv.org/abs/2506.09061v3": {
        "id": "http://arxiv.org/abs/2506.09061v3",
        "title": "EdgeProfiler: A Fast Profiling Framework for Lightweight LLMs on Edge Using Analytical Model",
        "link": "http://arxiv.org/abs/2506.09061v3",
        "tags": [
            "edge",
            "quantization"
        ],
        "relevant": true,
        "indexed_date": "2025-06-12",
        "tldr": "Proposes EdgeProfiler, an analytical profiling framework for lightweight LLM inference on edge devices. Uses aggressive quantization (e.g., 4-bit) and analytical modeling for performance estimation. Achieves 2-3x inference speedup and 35-50% energy reduction compared to FP16 baselines.",
        "abstract": "This paper introduces EdgeProfiler, a fast profiling framework designed for evaluating lightweight Large Language Models (LLMs) on edge systems. While LLMs offer remarkable capabilities in natural language understanding and generation, their high computational, memory, and power requirements often confine them to cloud environments. EdgeProfiler addresses these challenges by providing a systematic methodology for assessing LLM performance in resource-constrained edge settings. The framework profiles compact LLMs, including TinyLLaMA, Gemma3.1B, Llama3.2-1B, and DeepSeek-r1-1.5B, using aggressive quantization techniques and strict memory constraints. Analytical modeling is used to estimate latency, FLOPs, and energy consumption. The profiling reveals that 4-bit quantization reduces model memory usage by approximately 60-70%, while maintaining accuracy within 2-5% of full-precision baselines. Inference speeds are observed to improve by 2-3x compared to FP16 baselines across various edge devices. Power modeling estimates a 35-50% reduction in energy consumption for INT4 configurations, enabling practical deployment on hardware such as Raspberry Pi 4/5 and Jetson Orin Nano Super. Our findings emphasize the importance of efficient profiling tailored to lightweight LLMs in edge environments, balancing accuracy, energy efficiency, and computational feasibility.",
        "authors": [
            "Alyssa Pinnock",
            "Shakya Jayakody",
            "Kawsher A Roxy",
            "Md Rubel Ahmed"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.PF"
        ],
        "submit_date": "2025-06-06"
    },
    "http://arxiv.org/abs/2506.08027v2": {
        "id": "http://arxiv.org/abs/2506.08027v2",
        "title": "Recipes for Pre-training LLMs with MXFP8",
        "link": "http://arxiv.org/abs/2506.08027v2",
        "tags": [
            "training",
            "quantization"
        ],
        "relevant": true,
        "indexed_date": "2025-06-11",
        "tldr": "Studies parameter choices for MXFP8 quantization to enable efficient LLM pre-training. Proposes MXFP8-E4M3 datatype and conversion algorithm to maintain BF16 accuracy. Achieves comparable training results on 8B models using 15T tokens with 8-bit precision.",
        "abstract": "Using fewer bits to represent model parameters and related tensors during pre-training has become a required technique for improving GPU efficiency without sacrificing accuracy. Microscaling (MX) formats introduced in NVIDIA Blackwell generation of GPUs represent a major advancement of this technique, making it practical to combine narrow floating-point data types with finer granularity per-block scaling factors. In turn, this enables both quantization of more tensors than previous approaches and more efficient execution of operations on those tensors.   Effective use of MX-formats requires careful choices of various parameters. In this paper we review these choices and show how MXFP8-E4M3 datatype and a specific number conversion algorithm result in training sessions that match those carried out in BF16. We present results using models with up to 8B parameters, trained on high-quality datasets of up to 15T tokens.",
        "authors": [
            "Asit Mishra",
            "Dusan Stosic",
            "Simon Layton",
            "Paulius Micikevicius"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC"
        ],
        "submit_date": "2025-05-30"
    },
    "http://arxiv.org/abs/2506.07379v1": {
        "id": "http://arxiv.org/abs/2506.07379v1",
        "title": "Addressing tokens dynamic generation, propagation, storage and renewal to secure the GlideinWMS pilot based jobs and system",
        "link": "http://arxiv.org/abs/2506.07379v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-06-10",
        "tldr": "",
        "abstract": "GlideinWMS has been one of the first middleware in the WLCG community to transition from X.509 to support also tokens. The first step was to get from the prototype in 2019 to using tokens in production in 2022. This paper will present the challenges introduced by the wider adoption of tokens and the evolution plans for securing the pilot infrastructure of GlideinWMS and supporting the new requirements. In the last couple of years, the GlideinWMS team supported the migration of experiments and resources to tokens. Inadequate support in the current infrastructure, more stringent requirements, and the higher spatial and temporal granularity forced GlideinWMS to revisit once more how credentials are generated, used, and propagated. The new credential modules have been designed to be used in multiple systems (GlideinWMS, HEPCloud) and use a model where credentials have type, purpose, and different flows. Credentials are dynamically generated in order to customize the duration and limit the scope to the targeted resource. This allows to enforce the least privilege principle. Finally, we also considered adding credential storage, renewal, and invalidation mechanisms within the GlideinWMS infrastructure to better serve the experiments' needs.",
        "authors": [
            "Bruno Moreira Coimbra",
            "Marco Mambelli"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-06-09"
    },
    "http://arxiv.org/abs/2506.06472v1": {
        "id": "http://arxiv.org/abs/2506.06472v1",
        "title": "Cost-Efficient LLM Training with Lifetime-Aware Tensor Offloading via GPUDirect Storage",
        "link": "http://arxiv.org/abs/2506.06472v1",
        "tags": [
            "training",
            "offloading",
            "storage"
        ],
        "relevant": true,
        "indexed_date": "2025-06-10",
        "tldr": "Proposes TERAIO, a lifetime-aware tensor offloading framework for GPU memory expansion during LLM training. By profiling tensor lifetimes and optimizing offloading/prefetching via GPUDirect Storage, it minimizes training stalls. Achieves 1.47x average speedup over ZeRO-Offload/Infinity, reaching 80.7% of unlimited GPU memory performance.",
        "abstract": "We present the design and implementation of a new lifetime-aware tensor offloading framework for GPU memory expansion using low-cost PCIe-based solid-state drives (SSDs). Our framework, TERAIO, is developed explicitly for large language model (LLM) training with multiple GPUs and multiple SSDs. Its design is driven by our observation that the active tensors take only a small fraction (1.7% on average) of allocated GPU memory in each LLM training iteration, the inactive tensors are usually large and will not be used for a long period of time, creating ample opportunities for offloading/prefetching tensors to/from slow SSDs without stalling the GPU training process. TERAIO accurately estimates the lifetime (active period of time in GPU memory) of each tensor with the profiling of the first few iterations in the training process. With the tensor lifetime analysis, TERAIO will generate an optimized tensor offloading/prefetching plan and integrate it into the compiled LLM program via PyTorch. TERAIO has a runtime tensor migration engine to execute the offloading/prefetching plan via GPUDirect storage, which allows direct tensor migration between GPUs and SSDs for alleviating the CPU bottleneck and maximizing the SSD bandwidth utilization. In comparison with state-of-the-art studies such as ZeRO-Offload and ZeRO-Infinity, we show that TERAIO improves the training performance of various LLMs by 1.47x on average, and achieves 80.7% of the ideal performance assuming unlimited GPU memory.",
        "authors": [
            "Ziqi Yuan",
            "Haoyang Zhang",
            "Yirui Eric Zhou",
            "Apoorve Mohan",
            "I-Hsin Chung",
            "Seetharami Seelam",
            "Jian Huang"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.LG",
            "cs.PF"
        ],
        "submit_date": "2025-06-06"
    },
    "http://arxiv.org/abs/2506.06579v1": {
        "id": "http://arxiv.org/abs/2506.06579v1",
        "title": "Towards Efficient Multi-LLM Inference: Characterization and Analysis of LLM Routing and Hierarchical Techniques",
        "link": "http://arxiv.org/abs/2506.06579v1",
        "tags": [
            "serving",
            "edge",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-06-10",
        "tldr": "Surveys model routing and hierarchical techniques for efficient multi-LLM inference. Analyzes strategies that dynamically allocate queries to lightweight or larger models based on complexity to reduce computation. Highlights approaches achieving up to 5x speed-up while maintaining accuracy in edge environments.",
        "abstract": "Recent progress in Language Models (LMs) has dramatically advanced the field of natural language processing (NLP), excelling at tasks like text generation, summarization, and question answering. However, their inference remains computationally expensive and energy intensive, especially in settings with limited hardware, power, or bandwidth. This makes it difficult to deploy LMs in mobile, edge, or cost sensitive environments. To address these challenges, recent approaches have introduced multi LLM intelligent model selection strategies that dynamically allocate computational resources based on query complexity -- using lightweight models for simpler queries and escalating to larger models only when necessary. This survey explores two complementary strategies for efficient LLM inference: (i) routing, which selects the most suitable model based on the query, and (ii) cascading or hierarchical inference (HI), which escalates queries through a sequence of models until a confident response is found. Both approaches aim to reduce computation by using lightweight models for simpler tasks while offloading only when needed. We provide a comparative analysis of these techniques across key performance metrics, discuss benchmarking efforts, and outline open challenges. Finally, we outline future research directions to enable faster response times, adaptive model selection based on task complexity, and scalable deployment across heterogeneous environments, making LLM based systems more efficient and accessible for real world applications.",
        "authors": [
            "Adarsh Prasad Behera",
            "Jaya Prakash Champati",
            "Roberto Morabito",
            "Sasu Tarkoma",
            "James Gross"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "cs.DC"
        ],
        "submit_date": "2025-06-06"
    },
    "http://arxiv.org/abs/2506.05508v1": {
        "id": "http://arxiv.org/abs/2506.05508v1",
        "title": "Beyond the Buzz: A Pragmatic Take on Inference Disaggregation",
        "link": "http://arxiv.org/abs/2506.05508v1",
        "tags": [
            "serving",
            "disaggregation"
        ],
        "relevant": true,
        "indexed_date": "2025-06-09",
        "tldr": "Investigates practical optimization of disaggregated LLM serving across multi-node deployments. Analyzes design points for phases like prefill/decode, emphasizing dynamic rate matching and elastic scaling. Quantifies effectiveness for prefill-heavy workloads with larger models, achieving Pareto-optimal throughput-interactivity trade-offs.",
        "abstract": "As inference scales to multi-node deployments, disaggregation - splitting inference into distinct phases - offers a promising path to improving the throughput-interactivity Pareto frontier. Despite growing enthusiasm and a surge of open-source efforts, practical deployment of disaggregated serving remains limited due to the complexity of the optimization search space and system-level coordination. In this paper, we present the first systematic study of disaggregated inference at scale, evaluating hundreds of thousands of design points across diverse workloads and hardware configurations. We find that disaggregation is most effective for prefill-heavy traffic patterns and larger models. Our results highlight the critical role of dynamic rate matching and elastic scaling in achieving Pareto-optimal performance. Our findings offer actionable insights for efficient disaggregated deployments to navigate the trade-off between system throughput and interactivity.",
        "authors": [
            "Tiyasa Mitra",
            "Ritika Borkar",
            "Nidhi Bhatia",
            "Ramon Matas",
            "Shivam Raj",
            "Dheevatsa Mudigere",
            "Ritchie Zhao",
            "Maximilian Golub",
            "Arpan Dutta",
            "Sailaja Madduri",
            "Dharmesh Jani",
            "Brian Pharris",
            "Bita Darvish Rouhani"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-06-05"
    },
    "http://arxiv.org/abs/2506.04667v3": {
        "id": "http://arxiv.org/abs/2506.04667v3",
        "title": "FlashMoE: Fast Distributed MoE in a Single Kernel",
        "link": "http://arxiv.org/abs/2506.04667v3",
        "tags": [
            "MoE",
            "kernel",
            "training"
        ],
        "relevant": true,
        "indexed_date": "2025-06-06",
        "tldr": "Addresses low GPU utilization and latency overhead in distributed MoE training. Proposes FlashMoE, a single fused GPU kernel with fine-grained pipelining and one-sided RDMA transfers. Achieves up to 9x higher GPU utilization and 6x lower latency compared to baselines.",
        "abstract": "The computational sparsity of Mixture-of-Experts (MoE) models enables sub-linear growth in compute cost as model size increases, thus offering a scalable path to training massive neural networks. However, existing implementations suffer from low GPU utilization, significant latency overhead, and a fundamental inability to leverage task locality, primarily due to CPU-managed scheduling, host-initiated communication, and frequent kernel launches. To overcome these limitations, we develop FlashMoE, a fully GPU-resident MoE operator that fuses expert computation and inter-GPU communication into a single persistent GPU kernel. FlashMoE enables fine-grained pipelining of dispatch, compute, and combine phases, eliminating launch overheads and reducing idle gaps. Unlike existing work, FlashMoE eliminates bulk-synchronous collectives for one-sided, device-initiated, inter-GPU (R)DMA transfers, thereby unlocking payload efficiency by eliminating bloated or redundant network payloads in sparsely activated layers. When evaluated on an 8-H100 GPU node with MoE models comprising up to 128 experts and 16K token sequences, FlashMoE achieves up to 9x higher GPU utilization, 6x lower latency, 5.7x higher throughput, and 4x better overlap efficiency compared to state-of-the-art baselines, despite using FP32, whereas the baselines use FP16. FlashMoE shows that principled GPU kernel-hardware co-design is key to unlocking the performance ceiling of large-scale distributed ML. We provide code at https://github.com/osayamenja/FlashMoE.",
        "authors": [
            "Osayamen Jonathan Aimuyo",
            "Byungsoo Oh",
            "Rachee Singh"
        ],
        "categories": [
            "cs.DC",
            "cs.AR",
            "cs.LG"
        ],
        "submit_date": "2025-06-05"
    },
    "http://arxiv.org/abs/2506.04507v1": {
        "id": "http://arxiv.org/abs/2506.04507v1",
        "title": "SkimROOT: Accelerating LHC Data Filtering with Near-Storage Processing",
        "link": "http://arxiv.org/abs/2506.04507v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-06-06",
        "tldr": "",
        "abstract": "Data analysis in high-energy physics (HEP) begins with data reduction, where vast datasets are filtered to extract relevant events. At the Large Hadron Collider (LHC), this process is bottlenecked by slow data transfers between storage and compute nodes. To address this, we introduce SkimROOT, a near-data filtering system leveraging Data Processing Units (DPUs) to accelerate LHC data analysis. By performing filtering directly on storage servers and returning only the relevant data, SkimROOT minimizes data movement and reduces processing delays. Our prototype demonstrates significant efficiency gains, achieving a 44.3$\\times$ performance improvement, paving the way for faster physics discoveries.",
        "authors": [
            "Narangerelt Batsoyol",
            "Jonathan Guiang",
            "Diego Davila",
            "Aashay Arora",
            "Philip Chang",
            "Frank Würthwein",
            "Steven Swanson"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-06-04"
    },
    "http://arxiv.org/abs/2506.04456v1": {
        "id": "http://arxiv.org/abs/2506.04456v1",
        "title": "Knowledge-Guided Attention-Inspired Learning for Task Offloading in Vehicle Edge Computing",
        "link": "http://arxiv.org/abs/2506.04456v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-06-06",
        "tldr": "",
        "abstract": "Vehicle edge computing (VEC) brings abundant computing resources close to vehicles by deploying them at roadside units (RSUs) or base stations, thereby enabling diverse computation-intensive and delay sensitive applications. Existing task offloading strategies are often computationally expensive to execute or generate suboptimal solutions. In this paper, we propose a novel learning-based approach, Knowledge-guided Attention-inspired Task Offloading (KATO), designed to efficiently offload tasks from moving vehicles to nearby RSUs. KATO integrates an attention-inspired encoder-decoder model for selecting a subset of RSUs that can reduce overall task processing time, along with an efficient iterative algorithm for computing optimal task allocation among the selected RSUs. Simulation results demonstrate that KATO achieves optimal or near-optimal performance with significantly lower computational overhead and generalizes well across networks of varying sizes and configurations.",
        "authors": [
            "Ke Ma",
            "Junfei Xie"
        ],
        "categories": [
            "cs.DC",
            "cs.AR"
        ],
        "submit_date": "2025-06-04"
    },
    "http://arxiv.org/abs/2506.04645v1": {
        "id": "http://arxiv.org/abs/2506.04645v1",
        "title": "Inference economics of language models",
        "link": "http://arxiv.org/abs/2506.04645v1",
        "tags": [
            "serving",
            "offline",
            "quantization"
        ],
        "relevant": true,
        "indexed_date": "2025-06-06",
        "tldr": "Develops an economic model for LLM inference, optimizing parallelism and batch size to trade off cost per token versus serial generation speed. Computes Pareto frontiers for speed versus cost, considering arithmetic, memory, and network constraints.",
        "abstract": "We develop a theoretical model that addresses the economic trade-off between cost per token versus serial token generation speed when deploying LLMs for inference at scale. Our model takes into account arithmetic, memory bandwidth, network bandwidth and latency constraints; and optimizes over different parallelism setups and batch sizes to find the ones that optimize serial inference speed at a given cost per token. We use the model to compute Pareto frontiers of serial speed versus cost per token for popular language models.",
        "authors": [
            "Ege Erdil"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "submit_date": "2025-06-05"
    },
    "http://arxiv.org/abs/2506.04203v2": {
        "id": "http://arxiv.org/abs/2506.04203v2",
        "title": "Cascadia: An Efficient Cascade Serving System for Large Language Models",
        "link": "http://arxiv.org/abs/2506.04203v2",
        "tags": [
            "serving",
            "autoscaling",
            "recommendation"
        ],
        "relevant": true,
        "indexed_date": "2025-06-05",
        "tldr": "Improves LLM serving efficiency with cascaded models to balance latency and quality. Introduces Cascadia, a bi-level optimization framework for deployment allocation and routing strategies. Achieves up to 4× tighter latency SLOs and 5× higher throughput while maintaining quality.",
        "abstract": "Recent advances in large language models (LLMs) have intensified the need to deliver both rapid responses and high-quality outputs. More powerful models yield better results but incur higher inference latency, whereas smaller models are faster yet less capable. Recent work proposes balancing this latency-quality trade-off using model cascades, which route simpler queries to smaller models and more complex ones to larger models. However, enabling efficient cascade serving remains challenging. Current frameworks lack effective mechanisms for handling (i) the huge and varying resource demands of different LLMs, (ii) the inherent heterogeneity of LLM workloads, and (iii) the co-optimization of system deployment and routing strategy. Motivated by these observations, we introduce Cascadia, a novel cascade serving framework designed explicitly to schedule request routing and deploy model cascades for fast, quality-preserving LLM serving. Cascadia employs a bi-level optimization method: at the deployment level, it uses a mixed-integer linear program to select resource allocations and parallelism strategies based on LLM information and workload characteristics; at the routing level, it applies a Chebyshev-guided method to iteratively co-optimize the routing strategy and the system deployment produced by the deployment level. Our extensive evaluation on diverse workload traces and different model cascades (DeepSeek and the Llama series) demonstrates that Cascadia significantly outperforms both single-model deployments and the state-of-the-art cascade serving baseline, achieving up to 4$\\times$ (2.3$\\times$ on average) tighter latency SLOs and up to 5$\\times$ (2.4$\\times$ on average) higher throughput while maintaining target answer quality.",
        "authors": [
            "Youhe Jiang",
            "Fangcheng Fu",
            "Wanru Zhao",
            "Stephan Rabanser",
            "Jintao Zhang",
            "Nicholas D. Lane",
            "Binhang Yuan"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-06-04"
    },
    "http://arxiv.org/abs/2506.04063v1": {
        "id": "http://arxiv.org/abs/2506.04063v1",
        "title": "Crowd-SFT: Crowdsourcing for LLM Alignment",
        "link": "http://arxiv.org/abs/2506.04063v1",
        "tags": [
            "training",
            "RL",
            "multi-modal"
        ],
        "relevant": true,
        "indexed_date": "2025-06-05",
        "tldr": "Proposes Crowd-SFT, an open crowd-sourced framework for LLM alignment via SFT with incentive fairness using point-based rewards correlated with Shapley values. Achieves up to 55% reduction in target distance vs. single-model selection.",
        "abstract": "Large Language Models (LLMs) increasingly rely on Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) to align model responses with human preferences. While RLHF employs a reinforcement learning approach with a separate reward model, SFT uses human-curated datasets for supervised learning. Both approaches traditionally depend on small, vetted groups of annotators, making them costly, prone to bias, and limited in scalability. We propose an open, crowd-sourced fine-tuning framework that addresses these limitations by enabling broader feedback collection for SFT without extensive annotator training. Our framework promotes incentive fairness via a point-based reward system correlated with Shapley values and guides model convergence through iterative model updates. Our multi-model selection framework demonstrates up to a 55% reduction in target distance over single-model selection, enabling subsequent experiments that validate our point-based reward mechanism's close alignment with Shapley values (a well-established method for attributing individual contributions) thereby supporting fair and scalable participation.",
        "authors": [
            "Alex Sotiropoulos",
            "Sulyab Thottungal Valapu",
            "Linus Lei",
            "Jared Coleman",
            "Bhaskar Krishnamachari"
        ],
        "categories": [
            "cs.HC",
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-06-04"
    },
    "http://arxiv.org/abs/2506.03296v3": {
        "id": "http://arxiv.org/abs/2506.03296v3",
        "title": "Parallel CPU-GPU Execution for LLM Inference on Constrained GPUs",
        "link": "http://arxiv.org/abs/2506.03296v3",
        "tags": [
            "serving",
            "offloading",
            "edge"
        ],
        "relevant": true,
        "indexed_date": "2025-06-05",
        "tldr": "Addresses inefficient CPU-GPU parallelism during hybrid LLM inference in constrained environments. Proposes APEX, a dynamic scheduler using execution time predictions to maximize task overlap. Improves throughput by up to 96% on T4 and 89% on A10 GPUs versus GPU-only baselines.",
        "abstract": "Deploying large language models (LLMs) for online inference is often constrained by limited GPU memory, particularly due to the growing KV cache during auto-regressive decoding. Hybrid GPU-CPU execution has emerged as a promising solution by offloading KV cache management and parts of attention computation to the CPU. However, a key bottleneck remains: existing schedulers fail to effectively overlap CPU-offloaded tasks with GPU execution during the latency-critical, bandwidth-bound decode phase. This particularly penalizes real-time, decode-heavy applications (e.g., chat, Chain-of-Thought reasoning) which are currently underserved by existing systems, especially under memory pressure typical of edge or low-cost deployments.   We present APEX, a novel, profiling-informed scheduling strategy that maximizes CPU-GPU parallelism during hybrid LLM inference. Unlike systems relying on static rules or purely heuristic approaches, APEX dynamically dispatches compute across heterogeneous resources by predicting execution times of CPU and GPU subtasks to maximize overlap while avoiding scheduling overheads. We evaluate APEX on diverse workloads and GPU architectures (NVIDIA T4, A10), using LLaMa-2-7B and LLaMa-3.1-8B models. Compared to GPU-only schedulers like VLLM, APEX improves throughput by 84% - 96% on T4 and 11% - 89% on A10 GPUs, while preserving latency. Against the best existing hybrid schedulers, it delivers up to 49% (T4) and 37% (A10) higher throughput in long-output settings. APEX significantly advances hybrid LLM inference efficiency on such memory-constrained hardware and provides a blueprint for scheduling in heterogeneous AI systems, filling a critical gap for efficient real-time LLM applications.",
        "authors": [
            "Jiakun Fan",
            "Yanglin Zhang",
            "Xiangchen Li",
            "Dimitrios S. Nikolopoulos"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-06-03"
    },
    "http://arxiv.org/abs/2506.02814v2": {
        "id": "http://arxiv.org/abs/2506.02814v2",
        "title": "Adaptive Configuration Selection for Multi-Model Inference Pipelines in Edge Computing",
        "link": "http://arxiv.org/abs/2506.02814v2",
        "tags": [
            "edge",
            "serving",
            "RL"
        ],
        "relevant": true,
        "indexed_date": "2025-06-04",
        "tldr": "Addresses optimal configuration of multi-model inference pipelines on edge devices under resource constraints. Proposes an RL-based framework with LSTM load prediction and policy gradients for online decision-making. Improves QoS by 1.4x and reduces cost by 31.6% compared to baselines in Kubernetes experiments.",
        "abstract": "The growing demand for real-time processing tasks is driving the need for multi-model inference pipelines on edge devices. However, cost-effectively deploying these pipelines while optimizing Quality of Service (QoS) and costs poses significant challenges. Existing solutions often neglect device resource constraints, focusing mainly on inference accuracy and cost efficiency. To address this, we develop a framework for configuring multi-model inference pipelines. Specifically: 1) We model the decision-making problem by considering the pipeline's QoS, costs, and device resource limitations. 2) We create a feature extraction module using residual networks and a load prediction model based on Long Short-Term Memory (LSTM) to gather comprehensive node and pipeline status information. Then, we implement a Reinforcement Learning (RL) algorithm based on policy gradients for online configuration decisions. 3) Experiments conducted in a real Kubernetes cluster show that our approach significantly improve QoS while reducing costs and shorten decision-making time for complex pipelines compared to baseline algorithms.",
        "authors": [
            "Jinhao Sheng",
            "Zhiqing Tang",
            "Jianxiong Guo",
            "Tian Wang"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-06-03"
    },
    "http://arxiv.org/abs/2506.02490v1": {
        "id": "http://arxiv.org/abs/2506.02490v1",
        "title": "Simplifying Root Cause Analysis in Kubernetes with StateGraph and LLM",
        "link": "http://arxiv.org/abs/2506.02490v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-06-04",
        "tldr": "",
        "abstract": "Kubernetes, a notably complex and distributed system, utilizes an array of controllers to uphold cluster management logic through state reconciliation. Nevertheless, maintaining state consistency presents significant challenges due to unexpected failures, network disruptions, and asynchronous issues, especially within dynamic cloud environments. These challenges result in operational disruptions and economic losses, underscoring the necessity for robust root cause analysis (RCA) to enhance Kubernetes reliability. The development of large language models (LLMs) presents a promising direction for RCA. However, existing methodologies encounter several obstacles, including the diverse and evolving nature of Kubernetes incidents, the intricate context of incidents, and the polymorphic nature of these incidents. In this paper, we introduce SynergyRCA, an innovative tool that leverages LLMs with retrieval augmentation from graph databases and enhancement with expert prompts. SynergyRCA constructs a StateGraph to capture spatial and temporal relationships and utilizes a MetaGraph to outline entity connections. Upon the occurrence of an incident, an LLM predicts the most pertinent resource, and SynergyRCA queries the MetaGraph and StateGraph to deliver context-specific insights for RCA. We evaluate SynergyRCA using datasets from two production Kubernetes clusters, highlighting its capacity to identify numerous root causes, including novel ones, with high efficiency and precision. SynergyRCA demonstrates the ability to identify root causes in an average time of about two minutes and achieves an impressive precision of approximately 0.90.",
        "authors": [
            "Yong Xiang",
            "Charley Peter Chen",
            "Liyi Zeng",
            "Wei Yin",
            "Xin Liu",
            "Hu Li",
            "Wei Xu"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.SE"
        ],
        "submit_date": "2025-06-03"
    },
    "http://arxiv.org/abs/2506.02026v1": {
        "id": "http://arxiv.org/abs/2506.02026v1",
        "title": "D-Rex: Heterogeneity-Aware Reliability Framework and Adaptive Algorithms for Distributed Storage",
        "link": "http://arxiv.org/abs/2506.02026v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-06-04",
        "tldr": "",
        "abstract": "The exponential growth of data necessitates distributed storage models, such as peer-to-peer systems and data federations. While distributed storage can reduce costs and increase reliability, the heterogeneity in storage capacity, I/O performance, and failure rates of storage resources makes their efficient use a challenge. Further, node failures are common and can lead to data unavailability and even data loss. Erasure coding is a common resiliency strategy implemented in storage systems to mitigate failures by striping data across storage locations. However, erasure coding is computationally expensive and existing systems do not consider the heterogeneous resources and their varied capacity and performance when placing data chunks. We tackle the challenges of using erasure coding with distributed and heterogeneous nodes, aiming to store as much data as possible, minimize encoding and decoding time, and meeting user-defined reliability requirements for each data item. We propose two new dynamic scheduling algorithms, D-Rex LB and D-Rex SC, that adaptively choose erasure coding parameters and map chunks to heterogeneous nodes. D-Rex SC achieves robust performance for both storage utilization and throughput, at a higher computational cost, while D-Rex LB is faster but with slightly less competitive performance. In addition, we propose two greedy algorithms, GreedyMinStorage and GreedyLeastUsed, that optimize for storage utilization and load balancing, respectively. Our experimental evaluation shows that our dynamic schedulers store, on average, 45% more data items without significantly degrading I/O throughput compared to state-of-the-art algorithms, while GreedyLeastUsed is able to store 21% more data items while also increasing throughput.",
        "authors": [
            "Maxime Gonthier",
            "Dante D. Sanchez-Gallegos",
            "Haochen Pan",
            "Bogdan Nicolae",
            "Sicheng Zhou",
            "Hai Duc Nguyen",
            "Valerie Hayot-Sasson",
            "J. Gregory Pauloski",
            "Jesus Carretero",
            "Kyle Chard",
            "Ian Foster"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-05-29"
    },
    "http://arxiv.org/abs/2506.02025v2": {
        "id": "http://arxiv.org/abs/2506.02025v2",
        "title": "Evaluating the Efficacy of LLM-Based Reasoning for Multiobjective HPC Job Scheduling",
        "link": "http://arxiv.org/abs/2506.02025v2",
        "tags": [
            "RL",
            "thinking",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-06-04",
        "tldr": "Proposes an LLM-based ReAct scheduler with scratchpad memory for multiobjective HPC job scheduling. Achieves effective multiobjective optimization without domain-specific training, balancing constraints like fairness and resource use, though incurs computational overhead.",
        "abstract": "High-Performance Computing (HPC) job scheduling involves balancing conflicting objectives such as minimizing makespan, reducing wait times, optimizing resource use, and ensuring fairness. Traditional methods, including heuristic-based, e.g., First-Come-First-Served (FJFS) and Shortest Job First (SJF), or intensive optimization techniques, often lack adaptability to dynamic workloads and, more importantly, cannot simultaneously optimize multiple objectives in HPC systems. To address this, we propose a novel Large Language Model (LLM)-based scheduler using a ReAct-style framework (Reason + Act), enabling iterative, interpretable decision-making. The system incorporates a scratchpad memory to track scheduling history and refine decisions via natural language feedback, while a constraint enforcement module ensures feasibility and safety. We evaluate our approach using OpenAI's O4-Mini and Anthropic's Claude 3.7 across seven real-world HPC workload scenarios, including heterogeneous mixes, bursty patterns, and adversarial cases etc. Comparisons against FCFS, SJF, and Google OR-Tools (on 10 to 100 jobs) reveal that LLM-based scheduling effectively balances multiple objectives while offering transparent reasoning through natural language traces. The method excels in constraint satisfaction and adapts to diverse workloads without domain-specific training. However, a trade-off between reasoning quality and computational overhead challenges real-time deployment. This work presents the first comprehensive study of reasoning-capable LLMs for HPC scheduling, demonstrating their potential to handle multiobjective optimization while highlighting limitations in computational efficiency. The findings provide insights into leveraging advanced language models for complex scheduling problems in dynamic HPC environments.",
        "authors": [
            "Prachi Jadhav",
            "Hongwei Jin",
            "Ewa Deelman",
            "Prasanna Balaprakash"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-05-29"
    },
    "http://arxiv.org/abs/2506.02024v2": {
        "id": "http://arxiv.org/abs/2506.02024v2",
        "title": "NestedFP: High-Performance, Memory-Efficient Dual-Precision Floating Point Support for LLMs",
        "link": "http://arxiv.org/abs/2506.02024v2",
        "tags": [
            "quantization",
            "serving",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-06-04",
        "tldr": "Proposes NestedFP for memory-efficient dual-precision LLM serving by overlaying FP8 parameters on FP16, using a compact format and specialized GEMM kernel. Reduces memory footprint by sharing FP16 storage while maintaining quality and throughput, with FP8 throughput close to native FP8.",
        "abstract": "Meeting service-level objectives (SLOs) in Large Language Models (LLMs) serving is critical, but managing the high variability in load presents a significant challenge. Recent advancements in FP8 inference, backed by native hardware support, offer a potential solution: executing FP16 models by default, while switching to FP8 models during sudden load surges to achieve higher throughput at the cost of a slight quality degradation. Although this approach facilitates effective SLO management, it introduces additional memory overhead due to storing two versions of the same model. In response, this paper proposes NestedFP, an LLM serving technique that supports both FP16 and FP8 models in a memory efficient manner by overlaying FP8 parameters onto FP16 parameters, allowing both models to share the same FP16 memory footprint. By leveraging a compact data format for the overlay and a specialized GEMM kernel optimized for this format, NestedFP ensures minimal degradation in both model quality and inference throughput across both FP8 and FP16 modes. NestedFP provides a flexible platform for dynamic, SLO-aware precision selection. The code is available at https://github.com/SNU-ARC/NestedFP.",
        "authors": [
            "Haeun Lee",
            "Omin Kwon",
            "Yeonhong Park",
            "Jae W. Lee"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-05-29"
    },
    "http://arxiv.org/abs/2506.02023v1": {
        "id": "http://arxiv.org/abs/2506.02023v1",
        "title": "DistMLIP: A Distributed Inference Platform for Machine Learning Interatomic Potentials",
        "link": "http://arxiv.org/abs/2506.02023v1",
        "tags": [
            "serving",
            "offloading",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-06-04",
        "tldr": "Proposes DistMLIP, a graph-partitioning based distributed inference platform for machine learning interatomic potentials. Achieves near-million-atom calculations in seconds on 8 GPUs via zero-redundancy graph-level parallelization, demonstrating up to 8x speedup for MLIP models like CHGNet and MACE.",
        "abstract": "Large-scale atomistic simulations are essential to bridge computational materials and chemistry to realistic materials and drug discovery applications. In the past few years, rapid developments of machine learning interatomic potentials (MLIPs) have offered a solution to scale up quantum mechanical calculations. Parallelizing these interatomic potentials across multiple devices poses a challenging, but promising approach to further extending simulation scales to real-world applications. In this work, we present DistMLIP, an efficient distributed inference platform for MLIPs based on zero-redundancy, graph-level parallelization. In contrast to conventional space-partitioning parallelization, DistMLIP enables efficient MLIP parallelization through graph partitioning, allowing multi-device inference on flexible MLIP model architectures like multi-layer graph neural networks. DistMLIP presents an easy-to-use, flexible, plug-in interface that enables distributed inference of pre-existing MLIPs. We demonstrate DistMLIP on four widely used and state-of-the-art MLIPs: CHGNet, MACE, TensorNet, and eSEN. We show that existing foundational potentials can perform near-million-atom calculations at the scale of a few seconds on 8 GPUs with DistMLIP.",
        "authors": [
            "Kevin Han",
            "Bowen Deng",
            "Amir Barati Farimani",
            "Gerbrand Ceder"
        ],
        "categories": [
            "cs.DC",
            "cond-mat.mtrl-sci",
            "cs.LG",
            "cs.PF"
        ],
        "submit_date": "2025-05-28"
    },
    "http://arxiv.org/abs/2506.02006v1": {
        "id": "http://arxiv.org/abs/2506.02006v1",
        "title": "Efficient and Workload-Aware LLM Serving via Runtime Layer Swapping and KV Cache Resizing",
        "link": "http://arxiv.org/abs/2506.02006v1",
        "tags": [
            "serving",
            "quantization",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-06-04",
        "tldr": "Addresses efficient LLM serving under dynamic workloads with MorphServe. Uses quantized layer swapping and pressure-aware KV cache resizing for runtime adaptation. Reduces SLO violations by 92.45% and improves P95 TTFT latency by up to 3.9x.",
        "abstract": "Efficiently serving large language models (LLMs) under dynamic and bursty workloads remains a key challenge for real-world deployment. Existing serving frameworks and static model compression techniques fail to adapt to workload fluctuations, leading to either service-level objective (SLO) violations under full-precision serving or persistent accuracy degradation with static quantization. We present MorphServe, a dynamic, workload-aware LLM serving framework based on morphological adaptation. MorphServe introduces two asynchronous, token-level runtime mechanisms: quantized layer swapping, which selectively replaces less impactful layers with quantized alternatives during high-load periods, and pressure-aware KV cache resizing, which dynamically adjusts KV cache capacity in response to memory pressure. These mechanisms enable state-preserving transitions with minimum runtime overhead and are fully compatible with modern scheduling and attention techniques. Extensive experiments on Vicuna and Llama family models with real-world workloads demonstrate that MorphServe reduces average SLO violations by 92.45 percent and improves the P95 TTFT latency by 2.2x-3.9x compared to full-precision serving, without compromising generation quality. These results establish MorphServe as a practical and elastic solution for LLM deployment in dynamic environments.",
        "authors": [
            "Zhaoyuan Su",
            "Tingfeng Lan",
            "Zirui Wang",
            "Juncheng Yang",
            "Yue Cheng"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-05-24"
    },
    "http://arxiv.org/abs/2506.00929v1": {
        "id": "http://arxiv.org/abs/2506.00929v1",
        "title": "Adaptive, Efficient and Fair Resource Allocation in Cloud Datacenters leveraging Weighted A3C Deep Reinforcement Learning",
        "link": "http://arxiv.org/abs/2506.00929v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-06-03",
        "tldr": "",
        "abstract": "Cloud data centres demand adaptive, efficient, and fair resource allocation techniques due to heterogeneous workloads with varying priorities. However, most existing approaches struggle to cope with dynamic traffic patterns, often resulting in suboptimal fairness, increased latency, and higher energy consumption. To overcome these limitations, we propose a novel method called Weighted Actor-Critic Deep Reinforcement Learning (WA3C). Unlike static rule-based schedulers, WA3C continuously learns from the environment, making it resilient to changing workload patterns and system dynamics. Furthermore, the algorithm incorporates a multi-objective reward structure that balances trade-offs among latency, throughput, energy consumption, and fairness. This adaptability makes WA3C well-suited for modern multi-tenant cloud infrastructures, where diverse applications often compete for limited resources. WA3C also supports online learning, allowing it to adapt in real time to shifting workload compositions without the need for retraining from scratch. The model's architecture is designed to be lightweight and scalable, ensuring feasibility even in large-scale deployments. Additionally, WA3C introduces a priority-aware advantage estimator that better captures the urgency of tasks, enhancing scheduling precision. As a result, WA3C achieves more effective convergence, lower latency, and balanced resource allocation among jobs. Extensive experiments using synthetic job traces demonstrate that WA3C consistently outperforms both traditional and reinforcement learning-based baselines, highlighting its potential for real-world deployment in large-scale cloud systems.",
        "authors": [
            "Suchi Kumari",
            "Dhruv Mishra"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-06-01"
    },
    "http://arxiv.org/abs/2506.00002v1": {
        "id": "http://arxiv.org/abs/2506.00002v1",
        "title": "Advancing AI-assisted Hardware Design with Hierarchical Decentralized Training and Personalized Inference-Time Optimization",
        "link": "http://arxiv.org/abs/2506.00002v1",
        "tags": [
            "training",
            "RL",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-06-03",
        "tldr": "Proposes a two-stage framework for LLM-assisted hardware design generation addressing data and efficiency challenges. Uses hierarchical decentralized training for data sharing and personalized inference-time optimization. Achieves 33-50% semantic accuracy improvement and 2.3x speedup.",
        "abstract": "Recent years have witnessed a significant increase in the adoption of AI techniques to enhance electronic design automation. In particular, the emergence of Large Language Models (LLMs) has sparked significant interest in LLM-assisted hardware design generation, spanning applications from classical digital circuits to quantum computing. Despite substantial progress in this direction, the quality of LLM-generated hardware design still cannot meet the requirements for practical deployment. In this work, we identify three critical challenges hindering the development of LLM-assisted hardware design generation: 1) limited data availability, 2) varied data quality, 3) inadequate inference-time efficiency. To address these fundamental challenges, this paper introduces a two-stage framework for AI-assisted hardware design by exploring decentralized training and personalized inference. In the first stage, we propose to harness private domain design sources through a hierarchical decentralized training mechanism that addresses data-sharing constraints. To mitigate the impact of low-quality data, we identify optimization opportunities in hardware generation tasks, using user-defined metrics for model aggregation. The second stage focuses on client personalization to enhance both speed and quality. We introduce a new metric, Trueput, to analyze LLM-assisted hardware generation efficiency. To optimize Trueput, we implement personalized inference-time acceleration and customized sampling strategies. Evaluating both classical and quantum benchmarks, our experimental results demonstrate that the proposed two-stage framework can significantly improve the model capability for hardware design generation. As orthogonal enhancements to existing methods, our framework can achieve $33\\% \\sim 50\\%$ semantic accuracy improvement and $2.3$ times speedup, depending on the difficulty of the generation tasks.",
        "authors": [
            "Hao Mark Chen",
            "Zehuan Zhang",
            "Wanru Zhao",
            "Nicholas Lane",
            "Hongxiang Fan"
        ],
        "categories": [
            "cs.AR",
            "cs.AI",
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-04-21"
    },
    "http://arxiv.org/abs/2505.24618v1": {
        "id": "http://arxiv.org/abs/2505.24618v1",
        "title": "Distributed Intelligence in the Computing Continuum with Active Inference",
        "link": "http://arxiv.org/abs/2505.24618v1",
        "tags": [
            "edge",
            "RL",
            "agentic"
        ],
        "relevant": true,
        "indexed_date": "2025-06-02",
        "tldr": "Proposes distributed Active Inference agents for autonomous service management in the Computing Continuum. Agents collaborate to meet device-aware SLOiDs, achieving over 90% fulfillment with pre-tested models and 80% with online learning, outperforming MARL in training efficiency.",
        "abstract": "The Computing Continuum (CC) is an emerging Internet-based computing paradigm that spans from local Internet of Things sensors and constrained edge devices to large-scale cloud data centers. Its goal is to orchestrate a vast array of diverse and distributed computing resources to support the next generation of Internet-based applications. However, the distributed, heterogeneous, and dynamic nature of CC platforms demands distributed intelligence for adaptive and resilient service management. This article introduces a distributed stream processing pipeline as a CC use case, where each service is managed by an Active Inference (AIF) agent. These agents collaborate to fulfill service needs specified by SLOiDs, a term we introduce to denote Service Level Objectives that are aware of its deployed devices, meaning that non-functional requirements must consider the characteristics of the hosting device. We demonstrate how AIF agents can be modeled and deployed alongside distributed services to manage them autonomously. Our experiments show that AIF agents achieve over 90% SLOiD fulfillment when using tested transition models, and around 80% when learning the models during deployment. We compare their performance to a multi-agent reinforcement learning algorithm, finding that while both approaches yield similar results, MARL requires extensive training, whereas AIF agents can operate effectively from the start. Additionally, we evaluate the behavior of AIF agents in offloading scenarios, observing a strong capacity for adaptation. Finally, we outline key research directions to advance AIF integration in CC platforms.",
        "authors": [
            "Victor Casamayor Pujol",
            "Boris Sedlak",
            "Tommaso Salvatori",
            "Karl Friston",
            "Schahram Dustdar"
        ],
        "categories": [
            "cs.DC",
            "cs.MA",
            "eess.SY"
        ],
        "submit_date": "2025-05-30"
    },
    "http://arxiv.org/abs/2505.24095v2": {
        "id": "http://arxiv.org/abs/2505.24095v2",
        "title": "SkyWalker: A Locality-Aware Cross-Region Load Balancer for LLM Inference",
        "link": "http://arxiv.org/abs/2505.24095v2",
        "tags": [
            "serving",
            "networking",
            "autoscaling"
        ],
        "relevant": true,
        "indexed_date": "2025-06-02",
        "tldr": "SkyWalker addresses inefficient resource utilization in multi-region LLM serving by introducing a locality-aware load balancer that aggregates global demand. It uses cache-aware traffic handling and selective pushing to maintain KV-cache locality and load balancing. Reduces cost by 25% and improves throughput by 1.12-2.06× while lowering latency by 1.74-6.30×.",
        "abstract": "Serving Large Language Models (LLMs) efficiently in multi-region setups remains a challenge. Due to cost and GPU availability concerns, providers typically deploy LLMs in multiple regions using instance with long-term commitments, like reserved instances or on-premise clusters, which are often underutilized due to their region-local traffic handling and diurnal traffic variance. In this paper, we introduce SkyWalker, a multi-region load balancer for LLM inference that aggregates regional diurnal patterns through cross-region traffic handling. By doing so, SkyWalker enables providers to reserve instances based on expected global demand, rather than peak demand in each individual region. Meanwhile, SkyWalker preserves KV-Cache locality and load balancing, ensuring cost efficiency without sacrificing performance. SkyWalker achieves this with a cache-aware cross-region traffic handler and a selective pushing based load balancing mechanism. Our evaluation on real-world workloads shows that it achieves 1.12-2.06x higher throughput and 1.74-6.30x lower latency compared to existing load balancers, while reducing total serving cost by 25%.",
        "authors": [
            "Tian Xia",
            "Ziming Mao",
            "Jamison Kerney",
            "Ethan J. Jackson",
            "Zhifei Li",
            "Jiarong Xing",
            "Scott Shenker",
            "Ion Stoica"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-05-30"
    },
    "http://arxiv.org/abs/2505.23970v1": {
        "id": "http://arxiv.org/abs/2505.23970v1",
        "title": "EmbAdvisor: Adaptive Cache Management for Sustainable LLM Serving",
        "link": "http://arxiv.org/abs/2505.23970v1",
        "tags": [
            "serving",
            "offloading",
            "storage"
        ],
        "relevant": true,
        "indexed_date": "2025-06-02",
        "tldr": "Addresses the environmental impact of LLM serving storage, specifically the embodied carbon from SSDs for KV caching. Proposes EmbAdvisor, a carbon-aware caching framework using ILP solver to adapt cache sizes balancing operational and embodied carbon. Achieves 9.5% average carbon reduction for Llama-3 70B.",
        "abstract": "As large language models (LLMs) become widely used, their environmental impact$\\unicode{x2014}$especially carbon emissions$\\unicode{x2014}$has attracted more attention. Prior studies focus on compute-related carbon emissions. In this paper, we find that storage is another key contributor. LLM caching, which saves and reuses KV caches for repeated context, reduces operational carbon by avoiding redundant computation. However, this benefit comes at the cost of embodied carbon from high-capacity, high-speed SSDs. As LLMs scale, the embodied carbon of storage grows significantly.   To address this tradeoff, we present EmbAdvisor, a carbon-aware caching framework that selects the optimal cache size for LLM serving. EmbAdvisor profiles different LLM tasks and uses an Integer Linear Programming (ILP) solver to select cache sizes that meet SLOs while minimizing total carbon emissions. Overall, EmbAdvisor reduces the average carbon emissions of a Llama-3 70B model by 9.5% under various carbon intensities compared to a non-adaptive cache scenario, and can save up to 31.2% when the carbon intensity is low.",
        "authors": [
            "Yuyang Tian",
            "Desen Sun",
            "Yi Ding",
            "Sihang Liu"
        ],
        "categories": [
            "cs.DC",
            "cs.AR"
        ],
        "submit_date": "2025-05-29"
    },
    "http://arxiv.org/abs/2505.23554v1": {
        "id": "http://arxiv.org/abs/2505.23554v1",
        "title": "Sustainable Carbon-Aware and Water-Efficient LLM Scheduling in Geo-Distributed Cloud Datacenters",
        "link": "http://arxiv.org/abs/2505.23554v1",
        "tags": [
            "serving",
            "autoscaling"
        ],
        "relevant": true,
        "indexed_date": "2025-05-30",
        "tldr": "Proposes SLIT, an ML-based metaheuristic for sustainable LLM serving in geo-distributed datacenters, co-optimizing QoS (time-to-first token), carbon emissions, water usage, and energy costs. Achieves holistic sustainability improvements without specifying metrics.",
        "abstract": "In recent years, Large Language Models (LLM) such as ChatGPT, CoPilot, and Gemini have been widely adopted in different areas. As the use of LLMs continues to grow, many efforts have focused on reducing the massive training overheads of these models. But it is the environmental impact of handling user requests to LLMs that is increasingly becoming a concern. Recent studies estimate that the costs of operating LLMs in their inference phase can exceed training costs by 25x per year. As LLMs are queried incessantly, the cumulative carbon footprint for the operational phase has been shown to far exceed the footprint during the training phase. Further, estimates indicate that 500 ml of fresh water is expended for every 20-50 requests to LLMs during inference. To address these important sustainability issues with LLMs, we propose a novel framework called SLIT to co-optimize LLM quality of service (time-to-first token), carbon emissions, water usage, and energy costs. The framework utilizes a machine learning (ML) based metaheuristic to enhance the sustainability of LLM hosting across geo-distributed cloud datacenters. Such a framework will become increasingly vital as LLMs proliferate.",
        "authors": [
            "Hayden Moore",
            "Sirui Qi",
            "Ninad Hogade",
            "Dejan Milojicic",
            "Cullen Bash",
            "Sudeep Pasricha"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.LG"
        ],
        "submit_date": "2025-05-29"
    },
    "http://arxiv.org/abs/2505.23254v3": {
        "id": "http://arxiv.org/abs/2505.23254v3",
        "title": "MemAscend: System Memory Optimization for SSD-Offloaded LLM Fine-Tuning",
        "link": "http://arxiv.org/abs/2505.23254v3",
        "tags": [
            "training",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-05-30",
        "tldr": "Addresses system memory bottlenecks in SSD-offloaded LLM fine-tuning. Proposes MemAscend to streamline pinned-memory allocation, eliminate fragmentation, and reduce peak CPU usage. Reduces peak system-memory consumption by 55.7% compared to standard methods.",
        "abstract": "Owing to the huge success of generative artificial intelligence (AI), large language models (LLMs) have emerged as a core subclass, underpinning applications such as question answering, text generation, and code completion. While fine-tuning these models on domain-specific data can yield significant performance gains, it also poses daunting computational challenges, especially for researchers and small organizations with limited hardware resources. Although SSD offloading (i.e., ZeRO-Infinity) has emerged as a viable strategy to overcome the GPU memory barrier via leveraging both system memory (i.e., CPU DRAM) and storage space (i.e., solid-state devices, SSDs), its design primarily targets model-centric performance issues. As a result, key system-level issues, including system memory fragmentation, inefficient pinned buffer allocation, peak CPU usage spikes, and file system overhead, remain unaddressed, stifling scalability and inflating costs. Such an observation motivates this paper to introduce MemAscend, a framework that systematically tackles the underexplored system memory bottlenecks in SSD-offloaded LLM training, with a focus on resource-constrained environments. By streamlining pinned-memory allocation, eradicating fragmentation, and mitigating peak overhead, MemAscend reclaims a substantial system memory budget, enabling larger models, longer context windows, and higher batch sizes without exceeding modest hardware limits. Across diverse LLM benchmarks, MemAscend reduces peak system-memory consumption by an average of 55.7% compared with standard SSD offloading techniques, lowering the hardware barrier for fine-tuning and unlocking new possibilities for cost-effective large-scale training on limited-resource machines.",
        "authors": [
            "Yong-Cheng Liaw",
            "Shuo-Han Chen"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-05-29"
    },
    "http://arxiv.org/abs/2505.23219v2": {
        "id": "http://arxiv.org/abs/2505.23219v2",
        "title": "Ghidorah: Fast LLM Inference on Edge with Speculative Decoding and Hetero-Core Parallelism",
        "link": "http://arxiv.org/abs/2505.23219v2",
        "tags": [
            "edge",
            "kernel",
            "quantization"
        ],
        "relevant": true,
        "indexed_date": "2025-05-30",
        "tldr": "Proposes Ghidorah, an LLM inference system for edge devices using speculative decoding and hetero-core parallelism to maximize hardware utilization. Introduces HCMP and ARCA approaches to optimize workload distribution and strategy selection. Achieves 7.6x decoding speedup on Jetson NX.",
        "abstract": "In-situ LLM inference on end-user devices has gained significant interest due to its privacy benefits and reduced dependency on external infrastructure. However, as the decoding process is memory-bandwidth-bound, the diverse processing units in modern end-user devices cannot be fully exploited, resulting in slow LLM inference. This paper presents Ghidorah, a LLM inference system for end-user devices with the unified memory architecture. The key idea of Ghidorah can be summarized in two steps: 1) leveraging speculative decoding approaches to enhance parallelism, and 2) ingeniously distributing workloads across multiple heterogeneous processing units to maximize computing power utilization. Ghidorah includes the hetero-core model parallelism (HCMP) architecture and the architecture-aware profiling (ARCA) approach. The HCMP architecture guides partitioning by leveraging the unified memory design of end-user devices and adapting to the hybrid computational demands of speculative decoding. The ARCA approach is used to determine the optimal speculative strategy and partitioning strategy, balancing acceptance rate with parallel capability to maximize the speedup. Additionally, we optimize sparse computation on ARM CPUs. Experimental results show that Ghidorah can achieve up to 7.6x speedup in the dominant LLM decoding phase compared to the sequential decoding approach in NVIDIA Jetson NX.",
        "authors": [
            "Jinhui Wei",
            "Ye Huang",
            "Yuhui Zhou",
            "Jiazhi Jiang",
            "Jiangsu Du",
            "Yutong Lu"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-05-29"
    },
    "http://arxiv.org/abs/2505.23523v2": {
        "id": "http://arxiv.org/abs/2505.23523v2",
        "title": "Efficient AllReduce with Stragglers",
        "link": "http://arxiv.org/abs/2505.23523v2",
        "tags": [
            "training",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-05-30",
        "tldr": "Addresses AllReduce collective slowdown due to straggler GPUs. Proposes StragglAR, a parallel algorithm exploiting GPU execution time variation with overlapping ReduceScatter. Achieves 25% speedup over state-of-the-art AllReduce algorithms on 8-GPU servers.",
        "abstract": "Distributed machine learning workloads use data and tensor parallelism for training and inference, both of which rely on the AllReduce collective to synchronize gradients or activations. However, AllReduce algorithms are delayed by the slowest GPU to reach the synchronization barrier before the collective (i.e., the straggler). To address this challenge, we propose StragglAR: a parallel algorithm for AllReduce that accelerates distributed training and inference by exploiting natural variation in GPU execution times. StragglAR implements a ReduceScatter among the remaining GPUs during the straggler-induced delay, and then executes a novel collective algorithm to complete the AllReduce once the final GPU reaches the synchronization barrier. StragglAR achieves a 2x theoretical speedup over popular bandwidth-efficient algorithms for large GPU clusters, surpassing the lower bound for bandwidth-optimal synchronous AllReduce by leveraging the asymmetry in when GPUs reach the synchronization barrier. On an 8-GPU server, StragglAR provides a 25% speedup over state-of-the-art AllReduce algorithms.",
        "authors": [
            "Arjun Devraj",
            "Eric Ding",
            "Abhishek Vijaya Kumar",
            "Robert Kleinberg",
            "Rachee Singh"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "submit_date": "2025-05-29"
    },
    "http://arxiv.org/abs/2505.21919v1": {
        "id": "http://arxiv.org/abs/2505.21919v1",
        "title": "Towards Efficient Key-Value Cache Management for Prefix Prefilling in LLM Inference",
        "link": "http://arxiv.org/abs/2505.21919v1",
        "tags": [
            "serving",
            "offloading",
            "storage"
        ],
        "relevant": true,
        "indexed_date": "2025-05-29",
        "tldr": "Addresses inefficient Key-Value Cache (KVC) metadata management during LLM inference prefilling. Analyzes real access patterns and evaluates KV stores, highlighting the need for distributed caching with optimized metadata. Proposes tailored solutions for scalable inference with reduced latency.",
        "abstract": "The increasing adoption of large language models (LLMs) with extended context windows necessitates efficient Key-Value Cache (KVC) management to optimize inference performance. Inference workloads like Retrieval-Augmented Generation (RAG) and agents exhibit high cache reusability, making efficient caching critical to reducing redundancy and improving speed. We analyze real-world KVC access patterns using publicly available traces and evaluate commercial key-value stores like Redis and state-of-the-art RDMA-based systems (CHIME [1] and Sherman [2]) for KVC metadata management. Our work demonstrates the lack of tailored storage solution for KVC prefilling, underscores the need for an efficient distributed caching system with optimized metadata management for LLM workloads, and provides insights into designing improved KVC management systems for scalable, low-latency inference.",
        "authors": [
            "Yue Zhu",
            "Hao Yu",
            "Chen Wang",
            "Zhuoran Liu",
            "Eun Kyung Lee"
        ],
        "categories": [
            "cs.ET",
            "cs.AI",
            "cs.DC"
        ],
        "submit_date": "2025-05-28"
    },
    "http://arxiv.org/abs/2505.19847v1": {
        "id": "http://arxiv.org/abs/2505.19847v1",
        "title": "DGRAG: Distributed Graph-based Retrieval-Augmented Generation in Edge-Cloud Systems",
        "link": "http://arxiv.org/abs/2505.19847v1",
        "tags": [
            "RAG",
            "edge",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-05-27",
        "tldr": "Addresses latency and privacy in RAG by distributing knowledge graphs across edge-cloud systems. Proposes DGRAG with local/cloud collaboration and summary sharing for queries beyond edge scope. Reduces latency by 40% and maintains 92% accuracy in QA tasks.",
        "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a promising approach to enhance the capabilities of language models by integrating external knowledge. Due to the diversity of data sources and the constraints of memory and computing resources, real-world data is often scattered in multiple devices. Conventional RAGs that store massive amounts of scattered data centrally face increasing privacy concerns and high computational costs. Additionally, RAG in a central node raises latency issues when searching over a large-scale knowledge base. To address these challenges, we propose a distributed Knowledge Graph-based RAG approach, referred to as DGRAG, in an edge-cloud system, where each edge device maintains a local knowledge base without the need to share it with the cloud, instead sharing only summaries of its knowledge. Specifically, DGRAG has two main phases. In the Distributed Knowledge Construction phase, DGRAG organizes local knowledge using knowledge graphs, generating subgraph summaries and storing them in a summary database in the cloud as information sharing. In the Collaborative Retrieval and Generation phase, DGRAG first performs knowledge retrieval and answer generation locally, and a gate mechanism determines whether the query is beyond the scope of local knowledge or processing capabilities. For queries that exceed the local knowledge scope, the cloud retrieves knowledge from the most relevant edges based on the summaries and generates a more precise answer. Experimental results demonstrate the effectiveness of the proposed DGRAG approach in significantly improving the quality of question-answering tasks over baseline approaches.",
        "authors": [
            "Wenqing Zhou",
            "Yuxuan Yan",
            "Qianqian Yang"
        ],
        "categories": [
            "cs.AI",
            "cs.DC"
        ],
        "submit_date": "2025-05-26"
    },
    "http://arxiv.org/abs/2505.19481v1": {
        "id": "http://arxiv.org/abs/2505.19481v1",
        "title": "Win Fast or Lose Slow: Balancing Speed and Accuracy in Latency-Sensitive Decisions of LLMs",
        "link": "http://arxiv.org/abs/2505.19481v1",
        "tags": [
            "serving",
            "quantization",
            "edge"
        ],
        "relevant": true,
        "indexed_date": "2025-05-27",
        "tldr": "Investigates balancing latency and accuracy for LLM agents under strict time constraints. Proposes FPX, an adaptive framework that dynamically selects model size and quantization level. Achieves 26.52% higher daily yield in trading and 80% win rate improvement in gaming by optimizing latency.",
        "abstract": "Large language models (LLMs) have shown remarkable performance across diverse reasoning and generation tasks, and are increasingly deployed as agents in dynamic environments such as code generation and recommendation systems. However, many real-world applications, such as high-frequency trading and real-time competitive gaming, require decisions under strict latency constraints, where faster responses directly translate into higher rewards. Despite the importance of this latency quality trade off, it remains underexplored in the context of LLM based agents. In this work, we present the first systematic study of this trade off in real time decision making tasks. To support our investigation, we introduce two new benchmarks: HFTBench, a high frequency trading simulation, and StreetFighter, a competitive gaming platform. Our analysis reveals that optimal latency quality balance varies by task, and that sacrificing quality for lower latency can significantly enhance downstream performance. To address this, we propose FPX, an adaptive framework that dynamically selects model size and quantization level based on real time demands. Our method achieves the best performance on both benchmarks, improving win rate by up to 80% in Street Fighter and boosting daily yield by up to 26.52% in trading, underscoring the need for latency aware evaluation and deployment strategies for LLM based agents. These results demonstrate the critical importance of latency aware evaluation and deployment strategies for real world LLM based agents. Our benchmarks are available at Latency Sensitive Benchmarks.",
        "authors": [
            "Hao Kang",
            "Qingru Zhang",
            "Han Cai",
            "Weiyuan Xu",
            "Tushar Krishna",
            "Yilun Du",
            "Tsachy Weissman"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC",
            "cs.MA"
        ],
        "submit_date": "2025-05-26"
    },
    "http://arxiv.org/abs/2505.17548v1": {
        "id": "http://arxiv.org/abs/2505.17548v1",
        "title": "H2:Towards Efficient Large-Scale LLM Training on Hyper-Heterogeneous Cluster over 1,000 Chips",
        "link": "http://arxiv.org/abs/2505.17548v1",
        "tags": [
            "training",
            "hardware",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-05-26",
        "tldr": "Proposes H2 framework for efficient large-scale LLM training on hyper-heterogeneous clusters. Integrates DiTorch for unified programming, DiComm for optimized RDMA communication, and HeteroPP with dynamic pipeline parallelism. Achieves superlinear speedup and up to 16.37% higher throughput than homogeneous baselines.",
        "abstract": "Recent advancements in large language models (LLMs) necessitate extensive computational resources, prompting the use of diverse hardware accelerators from multiple vendors. However, traditional distributed training frameworks struggle to efficiently utilize hyper-heterogeneous clusters comprising thousands of chips due to significant disparities in software stacks, operator implementations, communication libraries, and hardware capabilities. To address these challenges, we propose H2, which stands for HyperHetero and is a systematic framework enabling efficient training of LLMs on clusters with over 1,000 heterogeneous chips. H2 incorporates DiTorch, a unified PyTorch-compatible interface ensuring program consistency across chips, and DiComm, a device-direct RDMA communication library optimized for heterogeneous environments. Furthermore, we introduce HeteroPP with HeteroAuto, an adaptive pipeline parallelism strategy that dynamically balances computational load, memory limitations, and communication overhead. Evaluations on a 100-billion-parameter LLM demonstrate that our approach consistently achieves a superlinear speedup, outperforming baseline homogeneous training solutions by up to 16.37% in our experiments. These findings validate the feasibility and efficiency of hyper-heterogeneous training at unprecedented scales.",
        "authors": [
            "Ding Tang",
            "Jiecheng Zhou",
            "Jiakai Hu",
            "Shengwei Li",
            "Huihuang Zheng",
            "Zhilin Pei",
            "Hui Wang",
            "Xingcheng Zhang"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-05-23"
    },
    "http://arxiv.org/abs/2505.17826v3": {
        "id": "http://arxiv.org/abs/2505.17826v3",
        "title": "Trinity-RFT: A General-Purpose and Unified Framework for Reinforcement Fine-Tuning of Large Language Models",
        "link": "http://arxiv.org/abs/2505.17826v3",
        "tags": [
            "training",
            "RL"
        ],
        "relevant": true,
        "indexed_date": "2025-05-26",
        "tldr": "Proposes Trinity-RFT, a general-purpose RL fine-tuning framework for LLMs with modular design unifying synchronous/asynchronous and online/offline modes. Features optimized data pipelines and agent-environment integration. Achieves high efficiency and adaptability in experiments.",
        "abstract": "Trinity-RFT is a general-purpose, unified and easy-to-use framework designed for reinforcement fine-tuning (RFT) of large language models. It is built with a modular and decoupled design, consisting of (1) an RFT-core that unifies and generalizes synchronous/asynchronous, on-policy/off-policy, and online/offline modes of RFT; (2) seamless integration for agent-environment interaction with high efficiency and robustness; and (3) systematic data pipelines optimized for RFT. Trinity-RFT can be easily adapted for diverse application scenarios, and serves as a unified platform for development and research of advanced reinforcement learning paradigms at both macroscopic and microscopic levels. This technical report outlines the vision, features, design and implementations of Trinity-RFT, accompanied by extensive examples, applications and experiments that demonstrate its functionalities and user-friendliness.",
        "authors": [
            "Xuchen Pan",
            "Yanxi Chen",
            "Yushuo Chen",
            "Yuchang Sun",
            "Daoyuan Chen",
            "Wenhao Zhang",
            "Yuexiang Xie",
            "Yilun Huang",
            "Yilei Zhang",
            "Dawei Gao",
            "Weijie Shi",
            "Yaliang Li",
            "Bolin Ding",
            "Jingren Zhou"
        ],
        "categories": [
            "cs.LG",
            "cs.CL",
            "cs.DC"
        ],
        "submit_date": "2025-05-23"
    },
    "http://arxiv.org/abs/2505.16508v2": {
        "id": "http://arxiv.org/abs/2505.16508v2",
        "title": "Edge-First Language Model Inference: Models, Metrics, and Tradeoffs",
        "link": "http://arxiv.org/abs/2505.16508v2",
        "tags": [
            "edge",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-05-23",
        "tldr": "Explores cost-latency tradeoffs in deploying SLMs across edge, cloud, and hybrid environments. Benchmarks SLM inference on single-edge devices to distributed clusters; analyzes scaling limits and fallback strategies. Achieves comparable performance to cloud with edge setups in suitable scenarios at lower costs.",
        "abstract": "The widespread adoption of Language Models (LMs) across industries is driving interest in deploying these services across the computing continuum, from the cloud to the network edge. This shift aims to reduce costs, lower latency, and improve reliability and privacy. Small Language Models (SLMs), enabled by advances in model compression, are central to this shift, offering a path to on-device inference on resource-constrained edge platforms. This work examines the interplay between edge and cloud deployments, starting from detailed benchmarking of SLM capabilities on single edge devices, and extending to distributed edge clusters. We identify scenarios where edge inference offers comparable performance with lower costs, and others where cloud fallback becomes essential due to limits in scalability or model capacity. Rather than proposing a one-size-fits-all solution, we present platform-level comparisons and design insights for building efficient, adaptive LM inference systems across heterogeneous environments.",
        "authors": [
            "SiYoung Jang",
            "Roberto Morabito"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.NI",
            "cs.PF"
        ],
        "submit_date": "2025-05-22"
    },
    "http://arxiv.org/abs/2505.16502v2": {
        "id": "http://arxiv.org/abs/2505.16502v2",
        "title": "Recursive Offloading for LLM Serving in Multi-tier Networks",
        "link": "http://arxiv.org/abs/2505.16502v2",
        "tags": [
            "serving",
            "offloading",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-05-23",
        "tldr": "Addresses efficient LLM serving in device-edge-cloud networks via RecServe, which uses hierarchical confidence evaluation and dynamic offloading thresholds. Reduces communication burden by over 50% vs cloud-based serving.",
        "abstract": "Heterogeneous device-edge-cloud computing infrastructures have become widely adopted in telecommunication operators and Wide Area Networks (WANs), offering multi-tier computational support for emerging intelligent services. With the rapid proliferation of Large Language Model (LLM) services, efficiently coordinating inference tasks and reducing communication overhead within these multi-tier network architectures becomes a critical deployment challenge. Existing LLM serving paradigms exhibit significant limitations: on-device deployment supports only lightweight LLMs due to hardware constraints, while cloud-centric deployment suffers from resource congestion and considerable prompt communication overhead caused by frequent service requests during peak periods. Although the model-cascading-based inference strategy adapts better to multi-tier networks, its reliance on fine-grained, manually adjusted thresholds makes it less responsive to dynamic network conditions and varying task complexities. To address these challenges, we propose RecServe, a recursive offloading framework tailored for LLM serving in multi-tier networks. RecServe integrates a task-specific hierarchical confidence evaluation mechanism that guides offloading decisions based on inferred task complexity in progressively scaled LLMs across device, edge, and cloud tiers. To further enable intelligent task routing across tiers, RecServe employs a sliding-window-based dynamic offloading strategy with quantile interpolation, enabling real-time tracking of historical confidence distributions and adaptive offloading threshold adjustments. Experiments on eight datasets demonstrate that RecServe outperforms CasServe in both service quality and communication efficiency, and reduces the communication burden by over 50\\% compared to centralized cloud-based serving.",
        "authors": [
            "Zhiyuan Wu",
            "Sheng Sun",
            "Yuwei Wang",
            "Min Liu",
            "Bo Gao",
            "Jinda Lu",
            "Zheming Yang",
            "Tian Wen"
        ],
        "categories": [
            "cs.DC",
            "cs.NI"
        ],
        "submit_date": "2025-05-22"
    },
    "http://arxiv.org/abs/2505.14864v2": {
        "id": "http://arxiv.org/abs/2505.14864v2",
        "title": "Balanced and Elastic End-to-end Training of Dynamic LLMs",
        "link": "http://arxiv.org/abs/2505.14864v2",
        "tags": [
            "training",
            "MoE",
            "sparse"
        ],
        "relevant": true,
        "indexed_date": "2025-05-22",
        "tldr": "Addresses workload imbalance in distributed training of dynamic LLMs (MoE, pruning, sparse attention, etc.). Proposes DynMo, an autonomous load balancer that equalizes compute loads and consolidates computation. Achieves up to 4.52x speedup over static solutions in end-to-end training.",
        "abstract": "To reduce the computational and memory overhead of Large Language Models, various approaches have been proposed. These include a) Mixture of Experts (MoEs), where token routing affects compute balance; b) gradual pruning of model parameters; c) dynamically freezing layers; d) dynamic sparse attention mechanisms; e) early exit of tokens as they pass through model layers; and f) Mixture of Depths (MoDs), where tokens bypass certain blocks. While these approaches are effective in reducing overall computation, they often introduce significant workload imbalance across workers. In many cases, this imbalance is severe enough to render the techniques impractical for large-scale distributed training, limiting their applicability to toy models due to poor efficiency.   We propose an autonomous dynamic load balancing solution, DynMo, which provably achieves maximum reduction in workload imbalance and adaptively equalizes compute loads across workers in pipeline-parallel training. In addition, DynMo dynamically consolidates computation onto fewer workers without sacrificing training throughput, allowing idle workers to be released back to the job manager. DynMo supports both single-node multi-GPU systems and multi-node GPU clusters, and can be used in practical deployment. Compared to static distributed training solutions such as Megatron-LM and DeepSpeed, DynMo accelerates the end-to-end training of dynamic GPT models by up to 1.23x for MoEs, 3.18x for parameter pruning, 2.23x for layer freezing, 4.02x for sparse attention, 4.52x for early exit, and 1.17x for MoDs.",
        "authors": [
            "Mohamed Wahib",
            "Muhammed Abdullah Soyturk",
            "Didem Unat"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-05-20"
    },
    "http://arxiv.org/abs/2505.14468v1": {
        "id": "http://arxiv.org/abs/2505.14468v1",
        "title": "ServerlessLoRA: Minimizing Latency and Cost in Serverless Inference for LoRA-Based LLMs",
        "link": "http://arxiv.org/abs/2505.14468v1",
        "tags": [
            "serving",
            "LoRA",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-05-21",
        "tldr": "Addresses inefficiencies in serverless LoRA LLM serving by enabling backbone sharing, pre-loading artifacts, and contention-aware batching. ServerlessLoRA reduces Time-To-First-Token by 86% and costs by 89% vs. existing solutions.",
        "abstract": "Serverless computing has grown rapidly for serving Large Language Model (LLM) inference due to its pay-as-you-go pricing, fine-grained GPU usage, and rapid scaling. However, our analysis reveals that current serverless can effectively serve general LLM but fail with Low-Rank Adaptation (LoRA) inference due to three key limitations: 1) massive parameter redundancy among functions where 99% of weights are unnecessarily duplicated, 2) costly artifact loading latency beyond LLM loading, and 3) magnified resource contention when serving multiple LoRA LLMs. These inefficiencies lead to massive GPU wastage, increased Time-To-First-Token (TTFT), and high monetary costs.   We propose ServerlessLoRA, a novel serverless inference system designed for faster and cheaper LoRA LLM serving. ServerlessLoRA enables secure backbone LLM sharing across isolated LoRA functions to reduce redundancy. We design a pre-loading method that pre-loads comprehensive LoRA artifacts to minimize cold-start latency. Furthermore, ServerlessLoRA employs contention aware batching and offloading to mitigate GPU resource conflicts during bursty workloads. Experiment on industrial workloads demonstrates that ServerlessLoRA reduces TTFT by up to 86% and cuts monetary costs by up to 89% compared to state-of-the-art LLM inference solutions.",
        "authors": [
            "Yifan Sui",
            "Hao Wang",
            "Hanfei Yu",
            "Yitao Hu",
            "Jianxun Li",
            "Hao Wang"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "submit_date": "2025-05-20"
    },
    "http://arxiv.org/abs/2505.12658v2": {
        "id": "http://arxiv.org/abs/2505.12658v2",
        "title": "HydraInfer: Hybrid Disaggregated Scheduling for Multimodal Large Language Model Serving",
        "link": "http://arxiv.org/abs/2505.12658v2",
        "tags": [
            "serving",
            "disaggregation",
            "multi-modal"
        ],
        "relevant": true,
        "indexed_date": "2025-05-20",
        "tldr": "Proposes HydraInfer, a hybrid MLLM inference system with Encode-Prefill-Decode disaggregation and stage-level batching to address heterogeneous demands. Achieves up to 4x higher throughput vs. vLLM on 8xH800 GPUs while meeting 90th percentile SLOs.",
        "abstract": "Multimodal Large Language Models (MLLMs) have been rapidly advancing, enabling cross-modal understanding and generation, and propelling artificial intelligence towards artificial general intelligence. However, existing MLLM inference systems are typically designed based on the architecture of language models, integrating image processing and language processing as a single scheduling unit. This design struggles to accommodate the heterogeneous demands of different stages in terms of computational resources, memory access patterns, and service-level objectives (SLOs), leading to low resource utilization and high request latency, ultimately failing to meet the service requirements of diverse inference scenarios.   To address these challenges, we propose HydraInfer, an efficient MLLM inference system that adopts a Hybrid Encode-Prefill-Decode (EPD) Disaggregation architecture. By scheduling the three stages - encode, prefill, and decode - onto separate heterogeneous inference instances, the system flexibly reallocates resources across stages, significantly reducing idle computation, alleviating resource bottlenecks, and improving overall system throughput and scalability. In addition, HydraInfer supports a stage-level batching strategy that enhances load balancing, enables parallel execution of visual and language models, and further optimizes inference performance. Experiments under real multimodal inference workloads demonstrate that HydraInfer can achieve up to 4x higher inference throughput compared to state-of-the-art systems (e.g., vLLM) on a single-node 8xH800 GPU cluster, while meeting the 90th percentile request SLO.",
        "authors": [
            "Xianzhe Dong",
            "Tongxuan Liu",
            "Yuting Zeng",
            "Liangyu Liu",
            "Yang Liu",
            "Siyu Wu",
            "Yu Wu",
            "Hailong Yang",
            "Ke Zhang",
            "Jing Li"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-05-19"
    },
    "http://arxiv.org/abs/2505.11916v2": {
        "id": "http://arxiv.org/abs/2505.11916v2",
        "title": "Arrow: Adaptive Scheduling Mechanisms for Disaggregated LLM Inference Architecture",
        "link": "http://arxiv.org/abs/2505.11916v2",
        "tags": [
            "serving",
            "disaggregation",
            "autoscaling"
        ],
        "relevant": true,
        "indexed_date": "2025-05-20",
        "tldr": "Proposes Arrow, an adaptive scheduler for disaggregated LLM inference that dynamically adjusts prefill and decode instance allocations based on real-time metrics to handle load variations. Improves request serving rates by up to 2.55× over state-of-the-art systems.",
        "abstract": "Existing large language model (LLM) serving systems typically employ Prefill-Decode disaggregated architecture to prevent computational interference between the prefill and decode phases. However, in real-world LLM serving scenarios, significant fluctuations in request input/output lengths lead to imbalanced computational loads between prefill and decode nodes under traditional static node allocation strategies, consequently preventing efficient utilization of computing resources to improve the system's goodput. To address this challenge, we design and implement Arrow, an adaptive scheduler that leverages stateless instances and latency characteristics of prefill and decode tasks to achieve efficient adaptive request and instance scheduling. Arrow dynamically adjusts the number of instances handling prefill and decode tasks based on real-time cluster performance metrics, substantially enhancing the system's capability to handle traffic spikes and load variations. Our evaluation under diverse real-world workloads shows that Arrow achieves up to $2.55 \\times$ higher request serving rates compared to state-of-the-art Prefill-Decode disaggregated serving systems.",
        "authors": [
            "Yu Wu",
            "Tongxuan Liu",
            "Yuting Zeng",
            "Siyu Wu",
            "Jun Xiong",
            "Xianzhe Dong",
            "Hailong Yang",
            "Ke Zhang",
            "Jing Li"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-05-17"
    },
    "http://arxiv.org/abs/2505.13345v1": {
        "id": "http://arxiv.org/abs/2505.13345v1",
        "title": "Occult: Optimizing Collaborative Communication across Experts for Accelerated Parallel MoE Training and Inference",
        "link": "http://arxiv.org/abs/2505.13345v1",
        "tags": [
            "training",
            "MoE",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-05-20",
        "tldr": "Addresses communication overhead in Mixture-of-Experts (MoE) training and inference. Proposes Occult, optimizing expert-device mapping to increase intra-collaboration and reduce all-to-all communication. Achieves over 1.5× speedup compared to existing frameworks while maintaining model quality.",
        "abstract": "Mixture-of-experts (MoE) architectures could achieve impressive computational efficiency with expert parallelism, which relies heavily on all-to-all communication across devices. Unfortunately, such communication overhead typically constitutes a significant portion of the total runtime, hampering the scalability of distributed training and inference for modern MoE models (consuming over $40\\%$ runtime in large-scale training). In this paper, we first define collaborative communication to illustrate this intrinsic limitation, and then propose system- and algorithm-level innovations to reduce communication costs. Specifically, given a pair of experts co-activated by one token, we call them \"collaborated\", which comprises $2$ cases as intra- and inter-collaboration, depending on whether they are kept on the same device. Our pilot investigations reveal that augmenting the proportion of intra-collaboration can accelerate expert parallelism at scale. It motivates us to strategically optimize collaborative communication for accelerated MoE training and inference, dubbed Occult. Our designs are capable of either delivering exact results with reduced communication cost or controllably minimizing the cost with collaboration pruning, materialized by modified fine-tuning. Comprehensive experiments on various MoE-LLMs demonstrate that Occult can be faster than popular state-of-the-art inference or training frameworks (more than $1.5\\times$ speed up across multiple tasks and models) with comparable or superior quality compared to the standard fine-tuning. Code is available at $\\href{https://github.com/UNITES-Lab/Occult}{https://github.com/UNITES-Lab/Occult}$.",
        "authors": [
            "Shuqing Luo",
            "Pingzhi Li",
            "Jie Peng",
            "Hanrui Wang",
            "Yang",
            "Zhao",
            "Yu",
            "Cao",
            "Yu Cheng",
            "Tianlong Chen"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "submit_date": "2025-05-19"
    },
    "http://arxiv.org/abs/2505.11329v4": {
        "id": "http://arxiv.org/abs/2505.11329v4",
        "title": "TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference",
        "link": "http://arxiv.org/abs/2505.11329v4",
        "tags": [
            "serving",
            "kernel",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-05-19",
        "tldr": "Proposes TokenWeave to optimize distributed LLM inference by overlapping compute and communication via token splitting, fused AllReduce-RMSNorm kernel, and layer norm reordering. Achieves up to 1.29x latency reduction and 1.26x throughput improvement.",
        "abstract": "Distributed inference of large language models (LLMs) can introduce overheads of up to 20% even over GPUs connected via high-speed interconnects such as NVLink. Multiple techniques have been proposed to mitigate these overheads by decomposing computations into finer-grained tasks and overlapping communication with sub-tasks as they complete. However, fine-grained decomposition of a large computation into many smaller computations on GPUs results in overheads. Furthermore, the communication itself uses many streaming multiprocessors (SMs), adding to the overhead.   We present TokenWeave to address these challenges. TokenWeave proposes a Token-Splitting technique that divides the tokens in the inference batch into two approximately equal subsets in a wave-aware manner. The communication of one subset is then overlapped with the computation of the other. In addition, TokenWeave optimizes the order of the layer normalization computation with respect to communication operations and implements a novel fused AllReduce--RMSNorm kernel that carefully leverages Multimem instruction support available on Hopper and Blackwell NVIDIA GPUs. These optimizations allow TokenWeave to perform communication and RMSNorm using only 2-8 SMs. Moreover, our kernel enables the memory-bound RMSNorm to be overlapped with the other batch's computation, providing additional gains.   Our evaluations demonstrate up to 1.29x speedup in latency and 1.26x higher throughput across multiple models and workloads. In several settings, TokenWeave results in better performance compared to an equivalent model with all communication removed.",
        "authors": [
            "Raja Gond",
            "Nipun Kwatra",
            "Ramachandran Ramjee"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-05-16"
    },
    "http://arxiv.org/abs/2505.11432v3": {
        "id": "http://arxiv.org/abs/2505.11432v3",
        "title": "MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production",
        "link": "http://arxiv.org/abs/2505.11432v3",
        "tags": [
            "MoE",
            "training",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-05-19",
        "tldr": "Develops MegaScale-MoE to improve communication efficiency in large-scale MoE model training. Customizes parallelism strategies, overlaps computation/communication, and applies compression. Achieves 1.41M tokens/s throughput with 1.88× speedup over Megatron-LM on 352B model using 1440 Hopper GPUs.",
        "abstract": "We present MegaScale-MoE, a production system tailored for the efficient training of large-scale mixture-of-experts (MoE) models. MoE emerges as a promising architecture to scale large language models (LLMs) to unprecedented sizes, thereby enhancing model performance. However, existing MoE training systems experience a degradation in training efficiency, exacerbated by the escalating scale of MoE models and the continuous evolution of hardware.   Recognizing the pivotal role of efficient communication in enhancing MoE training, MegaScale-MoE customizes communication-efficient parallelism strategies for attention and FFNs in each MoE layer and adopts a holistic approach to overlap communication with computation at both inter- and intra-operator levels. Additionally, MegaScale-MoE applies communication compression with adjusted communication patterns to lower precision, further improving training efficiency. When training a 352B MoE model on 1,440 NVIDIA Hopper GPUs, MegaScale-MoE achieves a training throughput of 1.41M tokens/s, improving the efficiency by 1.88$\\times$ compared to Megatron-LM. We share our operational experience in accelerating MoE training and hope that by offering our insights in system design, this work will motivate future research in MoE systems.",
        "authors": [
            "Chao Jin",
            "Ziheng Jiang",
            "Zhihao Bai",
            "Zheng Zhong",
            "Juncai Liu",
            "Xiang Li",
            "Ningxin Zheng",
            "Xi Wang",
            "Cong Xie",
            "Qi Huang",
            "Wen Heng",
            "Yiyuan Ma",
            "Wenlei Bao",
            "Size Zheng",
            "Yanghua Peng",
            "Haibin Lin",
            "Xuanzhe Liu",
            "Xin Jin",
            "Xin Liu"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "submit_date": "2025-05-16"
    },
    "http://arxiv.org/abs/2505.11415v2": {
        "id": "http://arxiv.org/abs/2505.11415v2",
        "title": "MoE-CAP: Benchmarking Cost, Accuracy and Performance of Sparse Mixture-of-Experts Systems",
        "link": "http://arxiv.org/abs/2505.11415v2",
        "tags": [
            "MoE",
            "training",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-05-19",
        "tldr": "Introduces MoE-CAP, a benchmark for evaluating sparse Mixture-of-Experts systems on Cost, Accuracy, and Performance trade-offs. Proposes sparsity-aware metrics (S-MBU, S-MFU) for cross-platform assessment. Shows current hardware forces trade-offs where optimizing two CAP dimensions sacrifices the third.",
        "abstract": "The sparse Mixture-of-Experts (MoE) architecture is increasingly favored for scaling Large Language Models (LLMs) efficiently, but it depends on heterogeneous compute and memory resources. These factors jointly affect system Cost, Accuracy, and Performance (CAP), making trade-offs inevitable. Existing benchmarks often fail to capture these trade-offs accurately, complicating practical deployment decisions. To address this, we introduce MoE-CAP, a benchmark specifically designed for MoE systems. Our analysis reveals that achieving an optimal balance across CAP is difficult with current hardware; MoE systems typically optimize two of the three dimensions at the expense of the third-a dynamic we term the MoE-CAP trade-off. To visualize this, we propose the CAP Radar Diagram. We further introduce sparsity-aware performance metrics-Sparse Memory Bandwidth Utilization (S-MBU) and Sparse Model FLOPS Utilization (S-MFU)-to enable accurate performance benchmarking of MoE systems across diverse hardware platforms and deployment scenarios.",
        "authors": [
            "Yinsicheng Jiang",
            "Yao Fu",
            "Yeqi Huang",
            "Ping Nie",
            "Zhan Lu",
            "Leyang Xue",
            "Congjie He",
            "Man-Kit Sit",
            "Jilong Xue",
            "Li Dong",
            "Ziming Miao",
            "Dayou Du",
            "Tairan Xu",
            "Kai Zou",
            "Edoardo Ponti",
            "Luo Mai"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "submit_date": "2025-05-16"
    },
    "http://arxiv.org/abs/2505.09999v2": {
        "id": "http://arxiv.org/abs/2505.09999v2",
        "title": "ServeGen: Workload Characterization and Generation of Large Language Model Serving in Production",
        "link": "http://arxiv.org/abs/2505.09999v2",
        "tags": [
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-05-16",
        "tldr": "Characterizes production LLM serving workloads including multimodal and reasoning models, revealing new insights. Proposes ServeGen, a workload generation framework creating realistic per-client request patterns. Avoids 50% under-provisioning compared to naive approaches.",
        "abstract": "With the widespread adoption of Large Language Models (LLMs), serving LLM inference requests has become an increasingly important task, attracting active research advancements. Practical workloads play an essential role in this process: they are critical for motivating and benchmarking serving techniques and systems. However, the existing understanding of real-world LLM serving workloads is limited due to the lack of a comprehensive workload characterization. Prior analyses remain insufficient in scale and scope, thus failing to fully capture intricate workload characteristics.   In this paper, we fill the gap with an in-depth characterization of LLM serving workloads collected from our worldwide cloud inference serving service, covering not only language models but also emerging multimodal and reasoning models, and unveiling important new findings in each case. Moreover, based on our findings, we propose ServeGen, a principled framework for generating realistic LLM serving workloads by composing them on a per-client basis. A practical use case in production validates that ServeGen avoids 50% under-provisioning compared to naive workload generation, demonstrating ServeGen's advantage in performance benchmarking. ServeGen is available at https://github.com/alibaba/ServeGen.",
        "authors": [
            "Yuxing Xiang",
            "Xue Li",
            "Kun Qian",
            "Wenyuan Yu",
            "Ennan Zhai",
            "Xin Jin"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-05-15"
    },
    "http://arxiv.org/abs/2505.09142v1": {
        "id": "http://arxiv.org/abs/2505.09142v1",
        "title": "ELIS: Efficient LLM Iterative Scheduling System with Response Length Predictor",
        "link": "http://arxiv.org/abs/2505.09142v1",
        "tags": [
            "serving",
            "offline"
        ],
        "relevant": true,
        "indexed_date": "2025-05-15",
        "tldr": "Proposes ELIS, an LLM serving system with Iterative SRTF scheduler and response length predictor to mitigate head-of-line blocking. Uses BGE model for token prediction and ISRTF to prioritize shortest remaining tokens. Reduces average job completion time by up to 19.6%.",
        "abstract": "We propose ELIS, a serving system for Large Language Models (LLMs) featuring an Iterative Shortest Remaining Time First (ISRTF) scheduler designed to efficiently manage inference tasks with the shortest remaining tokens. Current LLM serving systems often employ a first-come-first-served scheduling strategy, which can lead to the \"head-of-line blocking\" problem. To overcome this limitation, it is necessary to predict LLM inference times and apply a shortest job first scheduling strategy. However, due to the auto-regressive nature of LLMs, predicting the inference latency is challenging. ELIS addresses this challenge by training a response length predictor for LLMs using the BGE model, an encoder-based state-of-the-art model. Additionally, we have devised the ISRTF scheduling strategy, an optimization of shortest remaining time first tailored to existing LLM iteration batching. To evaluate our work in an industrial setting, we simulate streams of requests based on our study of real-world user LLM serving trace records. Furthermore, we implemented ELIS as a cloud-native scheduler system on Kubernetes to evaluate its performance in production environments. Our experimental results demonstrate that ISRTF reduces the average job completion time by up to 19.6%.",
        "authors": [
            "Seungbeom Choi",
            "Jeonghoe Goo",
            "Eunjoo Jeon",
            "Mingyu Yang",
            "Minsung Jang"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.LG"
        ],
        "submit_date": "2025-05-14"
    },
    "http://arxiv.org/abs/2505.08098v1": {
        "id": "http://arxiv.org/abs/2505.08098v1",
        "title": "Fused3S: Fast Sparse Attention on Tensor Cores",
        "link": "http://arxiv.org/abs/2505.08098v1",
        "tags": [
            "sparse",
            "kernel",
            "training"
        ],
        "relevant": true,
        "indexed_date": "2025-05-14",
        "tldr": "Addresses inefficiency in sparse attention computation on GPUs. Proposes Fused3S, a fused algorithm for SDDMM, softmax, and SpMM operations optimized for tensor cores. Achieves 1.6-16.3× speedup over SOTA on H100 GPUs.",
        "abstract": "Sparse attention is a core building block in many leading neural network models, from graph-structured learning to sparse sequence modeling. It can be decomposed into a sequence of three sparse matrix operations (3S): sampled dense-dense matrix multiplication (SDDMM), softmax normalization, and sparse matrix multiplication (SpMM). Efficiently executing the 3S computational pattern on modern GPUs remains challenging due to (a) the mismatch between unstructured sparsity and tensor cores optimized for dense operations, and (b) the high cost of data movement. Previous works have optimized these sparse operations individually or addressed one of these challenges. This paper introduces Fused3S, the first fused 3S algorithm that jointly maximizes tensor core utilization and minimizes data movement. Across real-world graph datasets, Fused3S achieves $1.6- 16.3\\times$ and $1.5-14\\times$ speedup over state-of-the-art on H100 and A30 GPUs. Furthermore, integrating Fused3S into Graph Transformer inference accelerates end-to-end performance by $1.05-5.36\\times$, consistently outperforming all 3S baselines across diverse datasets (single and batched graphs) and GPU architectures.",
        "authors": [
            "Zitong Li",
            "Aparna Chandramowlishwaran"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-05-12"
    },
    "http://arxiv.org/abs/2505.07833v1": {
        "id": "http://arxiv.org/abs/2505.07833v1",
        "title": "Patchwork: A Unified Framework for RAG Serving",
        "link": "http://arxiv.org/abs/2505.07833v1",
        "tags": [
            "RAG",
            "serving",
            "autoscaling"
        ],
        "relevant": true,
        "indexed_date": "2025-05-14",
        "tldr": "Proposes Patchwork, an end-to-end RAG serving framework for optimizing heterogeneous computational pipelines. Features distributed deployment, custom pipeline specification, and dynamic scheduling with priority-based scaling. Achieves 48% higher throughput and 24% fewer SLO violations versus commercial alternatives.",
        "abstract": "Retrieval Augmented Generation (RAG) has emerged as a new paradigm for enhancing Large Language Model reliability through integration with external knowledge sources. However, efficient deployment of these systems presents significant technical challenges due to their inherently heterogeneous computational pipelines comprising LLMs, databases, and specialized processing components. We introduce Patchwork, a comprehensive end-to-end RAG serving framework designed to address these efficiency bottlenecks. Patchwork's architecture offers three key innovations: First, it provides a flexible specification interface enabling users to implement custom RAG pipelines. Secondly, it deploys these pipelines as distributed inference systems while optimizing for the unique scalability characteristics of individual RAG components. Third, Patchwork incorporates an online scheduling mechanism that continuously monitors request load and execution progress, dynamically minimizing SLO violations through strategic request prioritization and resource auto-scaling. Our experimental evaluation across four distinct RAG implementations demonstrates that Patchwork delivers substantial performance improvements over commercial alternatives, achieving throughput gains exceeding 48% while simultaneously reducing SLO violations by ~24%.",
        "authors": [
            "Bodun Hu",
            "Luis Pabon",
            "Saurabh Agarwal",
            "Aditya Akella"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.MA",
            "cs.OS"
        ],
        "submit_date": "2025-05-01"
    },
    "http://arxiv.org/abs/2505.07203v1": {
        "id": "http://arxiv.org/abs/2505.07203v1",
        "title": "PrefillOnly: An Inference Engine for Prefill-only Workloads in Large Language Model Applications",
        "link": "http://arxiv.org/abs/2505.07203v1",
        "tags": [
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-05-13",
        "tldr": "Proposes PrefillOnly, an inference engine optimized for LLM applications with single-token outputs. Leverages single-layer KV caching and predictable completion times for JCT-aware scheduling. Achieves 4x higher queries per second without latency inflation.",
        "abstract": "Besides typical generative applications, like ChatGPT, GitHub Copilot, and Cursor, we observe an emerging trend that LLMs are increasingly used in traditional discriminative tasks, such as recommendation, credit verification, and data labeling. The key characteristic of these emerging use cases is that the LLM generates only a single output token, rather than an arbitrarily long sequence of tokens. We call this prefill-only workload. However, since existing LLM engines assume arbitrary output lengths, they fail to leverage the unique properties of prefill-only workloads. In this paper, we present PrefillOnly, the first LLM inference engine that improves the inference throughput and latency by fully embracing the properties of prefill-only workloads. First, since it generates only one token, PrefillOnly only needs to store the KV cache of only the last computed layer, rather than of all layers. This drastically reduces the GPU memory footprint of LLM inference and allows handling long inputs without using solutions that reduces throughput, such as cross-GPU KV cache parallelization. Second, because the output length is fixed, rather than arbitrary, PrefillOnly can precisely determine the job completion time (JCT) of each prefill-only request before it starts. This enables efficient JCT-aware scheduling policies such as shortest remaining job first. PrefillOnly can process upto 4x larger queries per second without inflating average and P99 latency.",
        "authors": [
            "Kuntai Du",
            "Bowen Wang",
            "Chen Zhang",
            "Yiming Cheng",
            "Qing Lan",
            "Hejian Sang",
            "Yihua Cheng",
            "Jiayi Yao",
            "Xiaoxuan Liu",
            "Yifan Qiao",
            "Ion Stoica",
            "Junchen Jiang"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-05-12"
    },
    "http://arxiv.org/abs/2505.06641v1": {
        "id": "http://arxiv.org/abs/2505.06641v1",
        "title": "SneakPeek: Data-Aware Model Selection and Scheduling for Inference Serving on the Edge",
        "link": "http://arxiv.org/abs/2505.06641v1",
        "tags": [
            "edge",
            "serving",
            "quantization"
        ],
        "relevant": true,
        "indexed_date": "2025-05-13",
        "tldr": "Proposes data-aware model selection and scheduling for edge inference serving. Uses SneakPeek models to dynamically adjust accuracy estimates and incorporates greedy batching for throughput. Achieves higher accuracy and utility with up to 40% resource savings in constrained environments.",
        "abstract": "Modern applications increasingly rely on inference serving systems to provide low-latency insights with a diverse set of machine learning models. Existing systems often utilize resource elasticity to scale with demand. However, many applications cannot rely on hardware scaling when deployed at the edge or other resource-constrained environments. In this work, we propose a model selection and scheduling algorithm that implements accuracy scaling to increase efficiency for these more constrained deployments. We show that existing schedulers that make decisions using profiled model accuracy are biased toward the label distribution present in the test dataset. To address this problem, we propose using ML models -- which we call SneakPeek models -- to dynamically adjust estimates of model accuracy, based on the underlying data. Furthermore, we greedily incorporate inference batching into scheduling decisions to improve throughput and avoid the overhead of swapping models in and out of GPU memory. Our approach employs a new notion of request priority, which navigates the trade-off between attaining high accuracy and satisfying deadlines. Using data and models from three real-world applications, we show that our proposed approaches result in higher-utility schedules and higher accuracy inferences in these hardware-constrained environments.",
        "authors": [
            "Joel Wolfrath",
            "Daniel Frink",
            "Abhishek Chandra"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-05-10"
    },
    "http://arxiv.org/abs/2505.06461v1": {
        "id": "http://arxiv.org/abs/2505.06461v1",
        "title": "Challenging GPU Dominance: When CPUs Outperform for On-Device LLM Inference",
        "link": "http://arxiv.org/abs/2505.06461v1",
        "tags": [
            "edge",
            "inference",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-05-13",
        "tldr": "Challenges the assumption that GPUs are always superior for on-device LLM inference by showing CPUs can outperform under certain conditions. Demonstrates with a 1B-parameter model on iPhone 15 Pro that CPU-only achieves 17 tokens/s vs GPU's 12.8 tokens/s due to GPU memory overhead and CPU thread optimization.",
        "abstract": "The common assumption in on-device AI is that GPUs, with their superior parallel processing, always provide the best performance for large language model (LLM) inference. In this work, we challenge this notion by empirically demonstrating that, under certain conditions, CPUs can outperform GPUs for LLM inference on mobile devices. Using a 1-billion-parameter LLM deployed via llama.cpp on the iPhone 15 Pro, we show that a CPU-only configuration (two threads, F16 precision) achieves 17 tokens per second, surpassing the 12.8 tokens per second obtained with GPU acceleration. We analyze the architectural factors driving this counterintuitive result, revealing that GPU memory transfer overhead and CPU thread optimization play a critical role. Furthermore, we explore the impact of thread oversubscription, quantization strategies, and hardware constraints, providing new insights into efficient on-device AI execution. Our findings challenge conventional GPU-first thinking, highlighting the untapped potential of optimized CPU inference and paving the way for smarter deployment strategies in mobile AI. However, fully explaining the observed CPU advantage remains difficult due to limited access to low-level profiling tools on iOS.",
        "authors": [
            "Haolin Zhang",
            "Jeff Huang"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-05-09"
    },
    "http://arxiv.org/abs/2505.07680v1": {
        "id": "http://arxiv.org/abs/2505.07680v1",
        "title": "SpecRouter: Adaptive Routing for Multi-Level Speculative Decoding in Large Language Models",
        "link": "http://arxiv.org/abs/2505.07680v1",
        "tags": [
            "serving",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-05-13",
        "tldr": "Proposes SpecRouter, an adaptive multi-level speculative decoding framework for LLM inference. It uses real-time feedback to dynamically route requests through optimal model chains and synchronizes KV cache states. Achieves 5.3x speedup over static speculative decoding while maintaining output quality.",
        "abstract": "Large Language Models (LLMs) present a critical trade-off between inference quality and computational cost: larger models offer superior capabilities but incur significant latency, while smaller models are faster but less powerful. Existing serving strategies often employ fixed model scales or static two-stage speculative decoding, failing to dynamically adapt to the varying complexities of user requests or fluctuations in system performance. This paper introduces \\systemname{}, a novel framework that reimagines LLM inference as an adaptive routing problem solved through multi-level speculative decoding. \\systemname{} dynamically constructs and optimizes inference \"paths\" (chains of models) based on real-time feedback, addressing the limitations of static approaches. Our contributions are threefold: (1) An \\textbf{adaptive model chain scheduling} mechanism that leverages performance profiling (execution times) and predictive similarity metrics (derived from token distribution divergence) to continuously select the optimal sequence of draft and verifier models, minimizing predicted latency per generated token. (2) A \\textbf{multi-level collaborative verification} framework where intermediate models within the selected chain can validate speculative tokens, reducing the verification burden on the final, most powerful target model. (3) A \\textbf{synchronized state management} system providing efficient, consistent KV cache handling across heterogeneous models in the chain, including precise, low-overhead rollbacks tailored for asynchronous batch processing inherent in multi-level speculation. Preliminary experiments demonstrate the validity of our method.",
        "authors": [
            "Hang Wu",
            "Jianian Zhu",
            "Yinghui Li",
            "Haojie Wang",
            "Biao Hou",
            "Jidong Zhai"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "submit_date": "2025-05-12"
    },
    "http://arxiv.org/abs/2505.06481v1": {
        "id": "http://arxiv.org/abs/2505.06481v1",
        "title": "QoS-Efficient Serving of Multiple Mixture-of-Expert LLMs Using Partial Runtime Reconfiguration",
        "link": "http://arxiv.org/abs/2505.06481v1",
        "tags": [
            "serving",
            "MoE",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-05-13",
        "tldr": "Addresses memory inefficiency in serving multiple MoE-LLMs on a single GPU. Proposes similarity-based expert consolidation and runtime partial reconfiguration to share experts across models. Reduces turnaround time by 85% compared to NVIDIA MIG while maintaining throughput and output quality.",
        "abstract": "The deployment of mixture-of-experts (MoE) large language models (LLMs) presents significant challenges due to their high memory demands. These challenges become even more pronounced in multi-tenant environments, where shared resources must accommodate multiple models, limiting the effectiveness of conventional virtualization techniques. This paper addresses the problem of efficiently serving multiple fine-tuned MoE-LLMs on a single-GPU. We propose a serving system that employs \\textit{similarity-based expert consolidation} to reduce the overall memory footprint by sharing similar experts across models. To ensure output quality, we introduce \\textit{runtime partial reconfiguration}, dynamically replacing non-expert layers when processing requests from different models. As a result, our approach achieves a competitive output quality while maintaining throughput comparable to serving a single model while incurring a negligible increase in time-to-first-token (TTFT). Experiments on a server with a single NVIDIA A100 GPU (80GB) using Mixtral-8x7B models demonstrate an 85\\% average reduction in turnaround time compared to NVIDIA's multi-instance GPU (MIG). Furthermore, experiments on Google's Switch Transformer Base-8 model with up to four variants demonstrate the scalability and resilience of our approach in maintaining output quality compared to other model merging baselines, highlighting its effectiveness.",
        "authors": [
            "HamidReza Imani",
            "Jiaxin Peng",
            "Peiman Mohseni",
            "Abdolah Amirany",
            "Tarek El-Ghazawi"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "submit_date": "2025-05-10"
    },
    "http://arxiv.org/abs/2505.06252v3": {
        "id": "http://arxiv.org/abs/2505.06252v3",
        "title": "ZipLLM: Efficient LLM Storage via Model-Aware Synergistic Data Deduplication and Compression",
        "link": "http://arxiv.org/abs/2505.06252v3",
        "tags": [
            "storage",
            "compression"
        ],
        "relevant": true,
        "indexed_date": "2025-05-13",
        "tldr": "Reduces LLM storage via synergistic deduplication and compression. Proposes ZipLLM, integrating tensor-level deduplication with BitX algorithm for delta compression of fine-tuned variants. Achieves 54% storage reduction, 20% higher than prior methods.",
        "abstract": "Modern model hubs, such as Hugging Face, store tens of petabytes of LLMs, with fine-tuned variants vastly outnumbering base models and dominating storage consumption. Existing storage reduction techniques -- such as deduplication and compression -- are either LLM-oblivious or not compatible with each other, limiting data reduction effectiveness. Our large-scale characterization study across all publicly available Hugging Face LLM repositories reveals several key insights: (1) fine-tuned models within the same family exhibit highly structured, sparse parameter differences suitable for delta compression; (2) bitwise similarity enables LLM family clustering; and (3) tensor-level deduplication is better aligned with model storage workloads, achieving high data reduction with low metadata overhead. Building on these insights, we design BitX, an effective, fast, lossless delta compression algorithm that compresses XORed difference between fine-tuned and base LLMs. We build ZipLLM, a model storage reduction pipeline that unifies tensor-level deduplication and lossless BitX compression. By synergizing deduplication and compression around LLM family clustering, ZipLLM reduces model storage consumption by 54%, over 20% higher than state-of-the-art deduplication and compression approaches.",
        "authors": [
            "Zirui Wang",
            "Tingfeng Lan",
            "Zhaoyuan Su",
            "Juncheng Yang",
            "Yue Cheng"
        ],
        "categories": [
            "cs.DB",
            "cs.DC"
        ],
        "submit_date": "2025-04-30"
    },
    "http://arxiv.org/abs/2505.05713v2": {
        "id": "http://arxiv.org/abs/2505.05713v2",
        "title": "Understanding Stragglers in Large Model Training Using What-if Analysis",
        "link": "http://arxiv.org/abs/2505.05713v2",
        "tags": [
            "training"
        ],
        "relevant": true,
        "indexed_date": "2025-05-12",
        "tldr": "Studies causes and impacts of stragglers in LLM training using what-if analysis. Analyzes a five-month trace to quantify straggler frequency, patterns, and root causes. Quantifies performance degradation due to stragglers compared to an ideal scenario without delays.",
        "abstract": "Large language model (LLM) training is one of the most demanding distributed computations today, often requiring thousands of GPUs with frequent synchronization across machines. Such a workload pattern makes it susceptible to stragglers, where the training can be stalled by few slow workers. At ByteDance we find stragglers are not trivially always caused by hardware failures, but can arise from multiple complex factors. This work aims to present a comprehensive study on the straggler issues in LLM training, using a five-month trace collected from our ByteDance LLM training cluster. The core methodology is what-if analysis that simulates the scenario without any stragglers and contrasts with the actual case. We use this method to study the following questions: (1) how often do stragglers affect training jobs, and what effect do they have on job performance; (2) do stragglers exhibit temporal or spatial patterns; and (3) what are the potential root causes for stragglers?",
        "authors": [
            "Jinkun Lin",
            "Ziheng Jiang",
            "Zuquan Song",
            "Sida Zhao",
            "Menghan Yu",
            "Zhanghan Wang",
            "Chenyuan Wang",
            "Zuocheng Shi",
            "Xiang Shi",
            "Wei Jia",
            "Zherui Liu",
            "Shuguang Wang",
            "Haibin Lin",
            "Xin Liu",
            "Aurojit Panda",
            "Jinyang Li"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-05-09"
    },
    "http://arxiv.org/abs/2505.05370v2": {
        "id": "http://arxiv.org/abs/2505.05370v2",
        "title": "Walrus: An Efficient Decentralized Storage Network",
        "link": "http://arxiv.org/abs/2505.05370v2",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-05-09",
        "tldr": "",
        "abstract": "Decentralized storage systems face a fundamental trade-off between replication overhead, recovery efficiency, and security guarantees. Current approaches either rely on full replication, incurring substantial storage costs, or employ trivial erasure coding schemes that struggle with efficient recovery especially under high storage-node churn. We present Walrus, a novel decentralized blob storage system that addresses these limitations through multiple technical innovations. At the core of Walrus is RedStuff, a two-dimensional erasure coding protocol that achieves high security with only 4.5x replication factor, while enabling self-healing recovery that requires bandwidth proportional to only the lost data $(O(|blob|/n)$ versus $O(|blob|)$ in traditional systems). Crucially, RedStuff is the first protocol to support storage challenges in asynchronous networks, preventing adversaries from exploiting network delays to pass verification without actually storing data. Walrus also introduces a novel multi-stage epoch change protocol that efficiently handles storage node churn while maintaining uninterrupted availability during committee transitions. Our system incorporates authenticated data structures to defend against malicious clients and ensures data consistency throughout storage and retrieval processes. Experimental evaluation demonstrates that Walrus achieves practical performance at scale, making it suitable for a wide range of decentralized applications requiring high-integrity, available blob storage with reasonable overhead.",
        "authors": [
            "George Danezis",
            "Giacomo Giuliari",
            "Eleftherios Kokoris Kogias",
            "Markus Legner",
            "Jean-Pierre Smith",
            "Alberto Sonnino",
            "Karl Wüst"
        ],
        "categories": [
            "cs.DC",
            "cs.CR"
        ],
        "submit_date": "2025-05-08"
    },
    "http://arxiv.org/abs/2505.04710v2": {
        "id": "http://arxiv.org/abs/2505.04710v2",
        "title": "Exploring Influence Factors on LLM Suitability for No-Code Development of End User IoT Applications",
        "link": "http://arxiv.org/abs/2505.04710v2",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-05-09",
        "tldr": "",
        "abstract": "No-Code Development Platforms (NCDPs) empower non-technical end users to build applications tailored to their specific demands without writing code. While NCDPs lower technical barriers, users still require some technical knowledge, e.g., to structure process steps or define event-action rules. Large Language Models (LLMs) offer a promising solution to further reduce technical requirements by supporting natural language interaction and dynamic code generation. By integrating LLM, NCDPs can be more accessible to non-technical users, enabling application development truly without requiring any technical expertise.   Despite growing interest in LLM-powered NCDPs, a systematic investigation into the factors influencing LLM suitability and performance remains absent. Understanding these factors is critical to effectively leveraging LLMs capabilities and maximizing their impact. In this paper, we investigate key factors influencing the effectiveness of LLMs in supporting end-user application development within NCDPs. By conducting comprehensive experiments, we evaluate the impact of four key factors, i.e., model selection, prompt language, training data background, and an error-informed few-shot setup, on the quality of generated applications. Specifically, we selected a range of LLMs based on their architecture, scale, design focus, and training data, and evaluated them across four real-world smart home automation scenarios implemented on a representative open-source LLM-powered NCDP. Our findings offer practical insights into how LLMs can be effectively integrated into NCDPs, informing both platform design and the selection of suitable LLMs for end-user application development.",
        "authors": [
            "Minghe Wang",
            "Alexandra Kapp",
            "Trever Schirmer",
            "Tobias Pfandzelter",
            "David Bermbach"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-05-07"
    },
    "http://arxiv.org/abs/2505.04846v1": {
        "id": "http://arxiv.org/abs/2505.04846v1",
        "title": "HiPerRAG: High-Performance Retrieval Augmented Generation for Scientific Insights",
        "link": "http://arxiv.org/abs/2505.04846v1",
        "tags": [
            "RAG",
            "training",
            "quantization"
        ],
        "relevant": true,
        "indexed_date": "2025-05-09",
        "tldr": "Addresses scaling RAG for scientific literature by introducing HiPerRAG with HPC for processing millions of articles. Combines high-throughput document parsing and query-aware encoder fine-tuning. Achieves 90% accuracy on SciQ benchmark using up to thousands of GPUs.",
        "abstract": "The volume of scientific literature is growing exponentially, leading to underutilized discoveries, duplicated efforts, and limited cross-disciplinary collaboration. Retrieval Augmented Generation (RAG) offers a way to assist scientists by improving the factuality of Large Language Models (LLMs) in processing this influx of information. However, scaling RAG to handle millions of articles introduces significant challenges, including the high computational costs associated with parsing documents and embedding scientific knowledge, as well as the algorithmic complexity of aligning these representations with the nuanced semantics of scientific content. To address these issues, we introduce HiPerRAG, a RAG workflow powered by high performance computing (HPC) to index and retrieve knowledge from more than 3.6 million scientific articles. At its core are Oreo, a high-throughput model for multimodal document parsing, and ColTrast, a query-aware encoder fine-tuning algorithm that enhances retrieval accuracy by using contrastive learning and late-interaction techniques. HiPerRAG delivers robust performance on existing scientific question answering benchmarks and two new benchmarks introduced in this work, achieving 90% accuracy on SciQ and 76% on PubMedQA-outperforming both domain-specific models like PubMedGPT and commercial LLMs such as GPT-4. Scaling to thousands of GPUs on the Polaris, Sunspot, and Frontier supercomputers, HiPerRAG delivers million document-scale RAG workflows for unifying scientific knowledge and fostering interdisciplinary innovation.",
        "authors": [
            "Ozan Gokdemir",
            "Carlo Siebenschuh",
            "Alexander Brace",
            "Azton Wells",
            "Brian Hsu",
            "Kyle Hippe",
            "Priyanka V. Setty",
            "Aswathy Ajith",
            "J. Gregory Pauloski",
            "Varuni Sastry",
            "Sam Foreman",
            "Huihuo Zheng",
            "Heng Ma",
            "Bharat Kale",
            "Nicholas Chia",
            "Thomas Gibbs",
            "Michael E. Papka",
            "Thomas Brettin",
            "Francis J. Alexander",
            "Anima Anandkumar",
            "Ian Foster",
            "Rick Stevens",
            "Venkatram Vishwanath",
            "Arvind Ramanathan"
        ],
        "categories": [
            "cs.IR",
            "cs.CE",
            "cs.CL",
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-05-07"
    },
    "http://arxiv.org/abs/2505.04021v2": {
        "id": "http://arxiv.org/abs/2505.04021v2",
        "title": "Prism: Unleashing GPU Sharing for Cost-Efficient Multi-LLM Serving",
        "link": "http://arxiv.org/abs/2505.04021v2",
        "tags": [
            "serving",
            "offloading",
            "autoscaling"
        ],
        "relevant": true,
        "indexed_date": "2025-05-08",
        "tldr": "Addresses cost inefficiency in multi-LLM serving due to idle resources. Proposes Prism system with dynamic GPU sharing via on-demand memory allocation and two-level scheduling. Achieves over 2× cost savings and 3.3× SLO attainment versus state-of-the-art.",
        "abstract": "Serving large language models (LLMs) is expensive, especially for providers hosting many models, making cost reduction essential. The unique workload patterns of serving multiple LLMs (i.e., multi-LLM serving) create new opportunities and challenges for this task. The long-tail popularity of models and their long idle periods present opportunities to improve utilization through GPU sharing. However, existing GPU sharing systems lack the ability to adjust their resource allocation and sharing policies at runtime, making them ineffective at meeting latency service-level objectives (SLOs) under rapidly fluctuating workloads.   This paper presents Prism, a multi-LLM serving system that unleashes the full potential of GPU sharing to achieve both cost efficiency and SLO attainment. At its core, Prism tackles a key limitation of existing systems$\\unicode{x2014}$the lack of $\\textit{cross-model memory coordination}$, which is essential for flexibly sharing GPU memory across models under dynamic workloads. Prism achieves this with two key designs. First, it supports on-demand memory allocation by dynamically mapping physical to virtual memory pages, allowing flexible memory redistribution among models that space- and time-share a GPU. Second, it improves memory efficiency through a two-level scheduling policy that dynamically adjusts sharing strategies based on models' runtime demands. Evaluations on real-world traces show that Prism achieves more than $2\\times$ cost savings and $3.3\\times$ SLO attainment compared to state-of-the-art systems.",
        "authors": [
            "Shan Yu",
            "Jiarong Xing",
            "Yifan Qiao",
            "Mingyuan Ma",
            "Yangmin Li",
            "Yang Wang",
            "Shuo Yang",
            "Zhiqiang Xie",
            "Shiyi Cao",
            "Ke Bao",
            "Ion Stoica",
            "Harry Xu",
            "Ying Sheng"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.LG",
            "cs.PF"
        ],
        "submit_date": "2025-05-06"
    },
    "http://arxiv.org/abs/2505.04014v2": {
        "id": "http://arxiv.org/abs/2505.04014v2",
        "title": "Rollbaccine : Herd Immunity against Storage Rollback Attacks in TEEs [Technical Report]",
        "link": "http://arxiv.org/abs/2505.04014v2",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-05-08",
        "tldr": "",
        "abstract": "Today, users can \"lift-and-shift\" unmodified applications into modern, VM-based Trusted Execution Environments (TEEs) in order to gain hardware-based security guarantees. However, TEEs do not protect applications against disk rollback attacks, where persistent storage can be reverted to an earlier state after a crash; existing rollback resistance solutions either only support a subset of applications or require code modification. Our key insight is that restoring disk consistency after a rollback attack guarantees rollback resistance for any application. We present Rollbaccine, a device mapper that provides automatic rollback resistance for all applications by provably preserving disk consistency. Rollbaccine intercepts and replicates writes to disk, restores lost state from backups during recovery, and minimizes overheads by taking advantage of the weak, multi-threaded semantics of disk operations. Rollbaccine performs on-par with state-of-the-art, non-automatic rollback resistant solutions; in fact, across benchmarks over PostgreSQL, HDFS, and two file systems (ext4 and xfs), Rollbaccine adds only 19% overhead, except for the fsync-heavy Filebench Varmail.",
        "authors": [
            "David Chu",
            "Aditya Balasubramanian",
            "Dee Bao",
            "Natacha Crooks",
            "Heidi Howard",
            "Lucky E. Katahanas",
            "Soujanya Ponnapalli"
        ],
        "categories": [
            "cs.CR",
            "cs.DC"
        ],
        "submit_date": "2025-05-06"
    },
    "http://arxiv.org/abs/2505.03763v1": {
        "id": "http://arxiv.org/abs/2505.03763v1",
        "title": "Splitwiser: Efficient LM inference with constrained resources",
        "link": "http://arxiv.org/abs/2505.03763v1",
        "tags": [
            "serving",
            "offloading",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-05-08",
        "tldr": "Optimizes LLM inference efficiency by splitting prompt computation and token generation phases onto the same GPU. Introduces Splitwiser, a multiprocessing design that improves memory access and cache utilization, reducing overhead. Achieves improved resource utilization by eliminating cross-device data transfer.",
        "abstract": "Efficient inference of LLMs remains a crucial challenge, with two main phases: a compute-intensive prompt computation and a memory-intensive token generation. Despite existing batching and scheduling techniques, token generation phases fail to fully utilize compute resources, especially when compared to prompt computation phases. To address these challenges, we propose Splitwiser, a methodology that splits the two phases of an LLM inference request onto the same GPU, thereby reducing overhead and improving memory access and cache utilization. By eliminating the need to transfer data across devices, Splitwiser aims to minimize network-related overheads. In this report, we describe the basic structure of our proposed pipeline while sharing preliminary results and analysis. We implement our proposed multiprocessing design on two widely-used and independent LLM architectures: Huggingface and vLLM. We open-source our code for the respective implementations: 1) Huggingface (https://github.com/asad-aali/splitwiser), and 2) vLLM (https://github.com/adney11/vllm-sysml).",
        "authors": [
            "Asad Aali",
            "Adney Cardoza",
            "Melissa Capo"
        ],
        "categories": [
            "cs.AR",
            "cs.AI",
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-04-21"
    },
    "http://arxiv.org/abs/2505.02533v1": {
        "id": "http://arxiv.org/abs/2505.02533v1",
        "title": "Large Language Model Partitioning for Low-Latency Inference at the Edge",
        "link": "http://arxiv.org/abs/2505.02533v1",
        "tags": [
            "serving",
            "offloading",
            "edge"
        ],
        "relevant": true,
        "indexed_date": "2025-05-06",
        "tldr": "Addresses high inference latency for LLMs at the edge caused by growing KV caches. Proposes dynamic partitioning of attention heads with resource-aware migration to optimize compute and memory usage. Achieves 15-20% near-optimal latency in small setups and significant speedups over layer partitioning.",
        "abstract": "Large Language Models (LLMs) based on autoregressive, decoder-only Transformers generate text one token at a time, where a token represents a discrete unit of text. As each newly produced token is appended to the partial output sequence, the length grows and so does the memory and compute load, due to the expanding key-value caches, which store intermediate representations of all previously generated tokens in the multi-head attention (MHA) layer. As this iterative process steadily increases memory and compute demands, layer-based partitioning in resource-constrained edge environments often results in memory overload or high inference latency. To address this and reduce inference latency, we propose a resource-aware Transformer architecture partitioning algorithm, where the partitioning decision is updated at regular intervals during token generation. The approach is myopic in that it is based on instantaneous information about device resource availability and network link bandwidths. When first executed, the algorithm places blocks on devices, and in later executions, it migrates these blocks among devices so that the sum of migration delay and inference delay remains low. Our approach partitions the decoder at the attention head level, co-locating each attention head with its key-value cache and allowing dynamic migrations whenever resources become tight. By allocating different attention heads to different devices, we exploit parallel execution of attention heads and thus achieve substantial reductions in inference delays. Our experiments show that in small-scale settings (3-5 devices), the proposed method achieves within 15 to 20 percent of an exact optimal solver's latency, while in larger-scale tests it achieves notable improvements in inference speed and memory usage compared to state-of-the-art layer-based partitioning approaches.",
        "authors": [
            "Dimitrios Kafetzis",
            "Ramin Khalili",
            "Iordanis Koutsopoulos"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-05-05"
    },
    "http://arxiv.org/abs/2505.02351v2": {
        "id": "http://arxiv.org/abs/2505.02351v2",
        "title": "Opt-GPTQ: An Optimized GPTQ Combining Sparse Attention and Quantization Techniques",
        "link": "http://arxiv.org/abs/2505.02351v2",
        "tags": [
            "quantization",
            "kernel",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-05-06",
        "tldr": "Proposes Opt-GPTQ, combining sparse attention (Opt-GQA) and quantization to reduce LLM serving computation and memory. Integrates with vLLM via custom GPU kernels for reduced access latency and parallel compute. Achieves significant computation time and memory usage reduction on DCUs.",
        "abstract": "In the field of deep learning, traditional attention mechanisms face significant challenges related to high computational complexity and large memory consumption when processing long sequence data. To address these limitations, we propose Opt-GPTQ, an optimized Gradient-based Post Training Quantization (GPTQ) combining the Grouped Query Attention (GQA) mechanism with paging memory management, optimizing the traditional Multi-Head Attention (MHA) mechanism by grouping query heads and sharing key-value vectors. Optimized GQA (Opt-GQA) effectively reduces computational complexity, minimizes memory fragmentation, and enhances memory utilization for large-scale models. Opt-GPTQ is optimized for Data Center Units (DCUs) and integrated into the vLLM model to maximize hardware efficiency. It customizes GPU kernels to further enhance attention computation by reducing memory access latency and boosting parallel computing capabilities. Opt-GQA integrates Attention with Linear Biases (ALiBi) to reduce overhead and enhance long-sequence processing. Experimental results show that Opt-GPTQ significantly reduces computation time and memory usage while improving model performance.",
        "authors": [
            "Jie Kong",
            "Junxiang Zhang",
            "Jiheng Xu",
            "Yalong Li",
            "Shouhua Zhang",
            "Jiehan Zhou",
            "Yuhai Liu",
            "Peng Liang",
            "Quan Zhang",
            "Luohan Jiang"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-05-05"
    },
    "http://arxiv.org/abs/2505.01968v2": {
        "id": "http://arxiv.org/abs/2505.01968v2",
        "title": "HAS-GPU: Efficient Hybrid Auto-scaling with Fine-grained GPU Allocation for SLO-aware Serverless Inferences",
        "link": "http://arxiv.org/abs/2505.01968v2",
        "tags": [
            "serving",
            "autoscaling",
            "offline"
        ],
        "relevant": true,
        "indexed_date": "2025-05-06",
        "tldr": "Proposes HAS-GPU, a hybrid auto-scaling serverless framework with fine-grained GPU allocation and vertical scaling to reduce cost and SLO violations for deep learning inference. Features a scheduler for arbitrary SM partition/time quotas and a performance predictor. Achieves 10.8x cost reduction and better SLOs than baselines.",
        "abstract": "Serverless Computing (FaaS) has become a popular paradigm for deep learning inference due to the ease of deployment and pay-per-use benefits. However, current serverless inference platforms encounter the coarse-grained and static GPU resource allocation problems during scaling, which leads to high costs and Service Level Objective (SLO) violations in fluctuating workloads. Meanwhile, current platforms only support horizontal scaling for GPU inferences, thus the cold start problem further exacerbates the problems. In this paper, we propose HAS-GPU, an efficient Hybrid Auto-scaling Serverless architecture with fine-grained GPU allocation for deep learning inferences. HAS-GPU proposes an agile scheduler capable of allocating GPU Streaming Multiprocessor (SM) partitions and time quotas with arbitrary granularity and enables significant vertical quota scalability at runtime. To resolve performance uncertainty introduced by massive fine-grained resource configuration spaces, we propose the Resource-aware Performance Predictor (RaPP). Furthermore, we present an adaptive hybrid auto-scaling algorithm with both horizontal and vertical scaling to ensure inference SLOs and minimize GPU costs. The experiments demonstrated that compared to the mainstream serverless inference platform, HAS-GPU reduces function costs by an average of 10.8x with better SLO guarantees. Compared to state-of-the-art spatio-temporal GPU sharing serverless framework, HAS-GPU reduces function SLO violation by 4.8x and cost by 1.72x on average.",
        "authors": [
            "Jianfeng Gu",
            "Puxuan Wang",
            "Isaac David Nunez Araya",
            "Kai Huang",
            "Michael Gerndt"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-05-04"
    },
    "http://arxiv.org/abs/2505.02795v1": {
        "id": "http://arxiv.org/abs/2505.02795v1",
        "title": "HSplitLoRA: A Heterogeneous Split Parameter-Efficient Fine-Tuning Framework for Large Language Models",
        "link": "http://arxiv.org/abs/2505.02795v1",
        "tags": [
            "training",
            "LoRA",
            "edge"
        ],
        "relevant": true,
        "indexed_date": "2025-05-06",
        "tldr": "Proposes HSplitLoRA, a parameter-efficient fine-tuning framework for LLMs on heterogeneous devices using split learning and dynamic LoRA rank selection. It dynamically allocates compute based on device capabilities and aggregates adapters without noise. Achieves higher accuracy and faster convergence compared to baselines.",
        "abstract": "Recently, large language models (LLMs) have achieved remarkable breakthroughs, revolutionizing the natural language processing domain and beyond. Due to immense parameter sizes, fine-tuning these models with private data for diverse downstream tasks has become mainstream. Though federated learning (FL) offers a promising solution for fine-tuning LLMs without sharing raw data, substantial computing costs hinder its democratization. Moreover, in real-world scenarios, private client devices often possess heterogeneous computing resources, further complicating LLM fine-tuning. To combat these challenges, we propose HSplitLoRA, a heterogeneous parameter-efficient fine-tuning (PEFT) framework built on split learning (SL) and low-rank adaptation (LoRA) fine-tuning, for efficiently fine-tuning LLMs on heterogeneous client devices. HSplitLoRA first identifies important weights based on their contributions to LLM training. It then dynamically configures the decomposition ranks of LoRA adapters for selected weights and determines the model split point according to varying computing budgets of client devices. Finally, a noise-free adapter aggregation mechanism is devised to support heterogeneous adapter aggregation without introducing noise. Extensive experiments demonstrate that HSplitLoRA outperforms state-of-the-art benchmarks in training accuracy and convergence speed.",
        "authors": [
            "Zheng Lin",
            "Yuxin Zhang",
            "Zhe Chen",
            "Zihan Fang",
            "Xianhao Chen",
            "Praneeth Vepakomma",
            "Wei Ni",
            "Jun Luo",
            "Yue Gao"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC"
        ],
        "submit_date": "2025-05-05"
    },
    "http://arxiv.org/abs/2505.01164v1": {
        "id": "http://arxiv.org/abs/2505.01164v1",
        "title": "CaGR-RAG: Context-aware Query Grouping for Disk-based Vector Search in RAG Systems",
        "link": "http://arxiv.org/abs/2505.01164v1",
        "tags": [
            "RAG",
            "storage",
            "sparse"
        ],
        "relevant": true,
        "indexed_date": "2025-05-05",
        "tldr": "Addresses inefficient disk I/O in RAG vector search due to non-uniform cluster access. Proposes CaGR-RAG with context-aware query grouping and opportunistic prefetching to optimize locality. Reduces 99th percentile tail latency by up to 51.55% while improving cache hit ratio.",
        "abstract": "Modern embedding models capture both semantic and syntactic structures of queries, often mapping different queries to similar regions in vector space. This results in non-uniform cluster access patterns in disk-based vector search systems, particularly in Retrieval Augmented Generation (RAG) framework. While existing approaches optimize individual queries, they overlook the impact of cluster access patterns, failing to account for the locality effects of queries that access similar clusters. This oversight reduces cache efficiency and increases search latency due to excessive disk I/O. To address this, we introduce CaGR-RAG, a context-aware query grouping mechanism that organizes queries based on shared cluster access patterns. Additionally, it incorporates opportunistic cluster prefetching to minimize cache misses during transitions between query groups, further optimizing retrieval performance. Experimental results show that CaGR-RAG reduces 99th percentile tail latency by up to 51.55% while consistently maintaining a higher cache hit ratio than the baseline.",
        "authors": [
            "Yeonwoo Jeong",
            "Kyuli Park",
            "Hyunji Cho",
            "Sungyong Park"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-05-02"
    },
    "http://arxiv.org/abs/2504.20198v1": {
        "id": "http://arxiv.org/abs/2504.20198v1",
        "title": "Leveraging Neural Graph Compilers in Machine Learning Research for Edge-Cloud Systems",
        "link": "http://arxiv.org/abs/2504.20198v1",
        "tags": [
            "kernel",
            "hardware",
            "edge"
        ],
        "relevant": true,
        "indexed_date": "2025-04-30",
        "tldr": "Evaluates neural graph compilers for ML deployment on edge-cloud systems. Analyzes vendor-specific optimizations and introduces metrics for compiler performance under varying architectures and batch sizes. Shows up to 3.7× throughput gains on certain architectures after compilation.",
        "abstract": "This work presents a comprehensive evaluation of neural network graph compilers across heterogeneous hardware platforms, addressing the critical gap between theoretical optimization techniques and practical deployment scenarios. We demonstrate how vendor-specific optimizations can invalidate relative performance comparisons between architectural archetypes, with performance advantages sometimes completely reversing after compilation. Our systematic analysis reveals that graph compilers exhibit performance patterns highly dependent on both neural architecture and batch sizes. Through fine-grained block-level experimentation, we establish that vendor-specific compilers can leverage repeated patterns in simple architectures, yielding disproportionate throughput gains as model depth increases. We introduce novel metrics to quantify a compiler's ability to mitigate performance friction as batch size increases. Our methodology bridges the gap between academic research and practical deployment by incorporating compiler effects throughout the research process, providing actionable insights for practitioners navigating complex optimization landscapes across heterogeneous hardware environments.",
        "authors": [
            "Alireza Furutanpey",
            "Carmen Walser",
            "Philipp Raith",
            "Pantelis A. Frangoudis",
            "Schahram Dustdar"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-04-28"
    },
    "http://arxiv.org/abs/2504.20101v4": {
        "id": "http://arxiv.org/abs/2504.20101v4",
        "title": "PlanetServe: A Decentralized, Scalable, and Privacy-Preserving Overlay for Democratizing Large Language Model Serving",
        "link": "http://arxiv.org/abs/2504.20101v4",
        "tags": [
            "serving",
            "networking",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-04-30",
        "tldr": "Proposes PlanetServe, a decentralized overlay network for scalable LLM serving by leveraging distributed contributors. Addresses overlay organization, privacy, forwarding efficiency, and quality verification. Achieves 50% latency reduction over baseline with minimal security overhead.",
        "abstract": "While significant progress has been made in research and development on open-source and cost-efficient large-language models (LLMs), serving scalability remains a critical challenge, particularly for small organizations and individuals seeking to deploy and test their LLM innovations. Inspired by peer-to-peer networks that leverage decentralized overlay nodes to increase throughput and availability, we propose GenTorrent, an LLM serving overlay that harnesses computing resources from decentralized contributors. We identify four key research problems inherent to enabling such a decentralized infrastructure: 1) overlay network organization; 2) LLM communication privacy; 3) overlay forwarding for resource efficiency; and 4) verification of serving quality. This work presents the first systematic study of these fundamental problems in the context of decentralized LLM serving. Evaluation results from a prototype implemented on a set of decentralized nodes demonstrate that GenTorrent achieves a latency reduction of over 50% compared to the baseline design without overlay forwarding. Furthermore, the security features introduce minimal overhead to serving latency and throughput. We believe this work pioneers a new direction for democratizing and scaling future AI serving capabilities.",
        "authors": [
            "Fei Fang",
            "Yifan Hua",
            "Shengze Wang",
            "Ruilin Zhou",
            "Yi Liu",
            "Chen Qian",
            "Xiaoxue Zhang"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-04-27"
    },
    "http://arxiv.org/abs/2504.20068v3": {
        "id": "http://arxiv.org/abs/2504.20068v3",
        "title": "JITServe: SLO-aware LLM Serving with Imprecise Request Information",
        "link": "http://arxiv.org/abs/2504.20068v3",
        "tags": [
            "serving",
            "autoscaling"
        ],
        "relevant": true,
        "indexed_date": "2025-04-30",
        "tldr": "Proposes JITServe, an SLO-aware LLM serving system that schedules requests using imprecise information and a grouped margin algorithm. Maximizes service goodput by dynamically allocating serving bandwidth. Achieves 1.4x-6.3x higher goodput or 28.5%-83.2% resource savings over state-of-the-art designs.",
        "abstract": "The integration of Large Language Models (LLMs) into applications ranging from interactive chatbots to multi-agent systems has introduced a wide spectrum of service-level objectives (SLOs) for responsiveness. These include latency-sensitive requests emphasizing per-token latency in streaming chat, deadline-sensitive requests requiring rapid full responses to trigger external tools, and compound requests with evolving dependencies across multiple LLM calls. Despite-or perhaps, because of-this workload diversity and unpredictable request information (e.g., response lengths and dependencies), existing request schedulers have focused on aggregate performance, unable to ensure application-level SLO needs.   This paper presents JITServe, the first SLO-aware LLM serving system designed to maximize service goodput (e.g., the number of tokens meeting request SLOs) across diverse workloads. JITServe novelly schedules requests using imprecise request information and gradually relaxes this conservatism by refining request information estimates as generation progresses. It applies a grouped margin goodput maximization algorithm to allocate just enough serving bandwidth to satisfy each request's SLO just-in-time (JIT), maximizing residual capacity for others, while deciding the composition of requests in a batch to maximize efficiency and goodput with provable guarantees. Our evaluation across diverse realistic workloads, including chat, deep research, and agentic pipelines, shows that JITServe improves service goodput by 1.4x-6.3x, alternatively achieving 28.5%-83.2% resource savings, compared to state-of-the-art designs.",
        "authors": [
            "Wei Zhang",
            "Zhiyu Wu",
            "Yi Mu",
            "Rui Ning",
            "Banruo Liu",
            "Nikhil Sarda",
            "Myungjin Lee",
            "Fan Lai"
        ],
        "categories": [
            "cs.DC",
            "cs.LG",
            "eess.SY"
        ],
        "submit_date": "2025-04-24"
    },
    "http://arxiv.org/abs/2504.20964v2": {
        "id": "http://arxiv.org/abs/2504.20964v2",
        "title": "OSVBench: Benchmarking LLMs on Specification Generation Tasks for Operating System Verification",
        "link": "http://arxiv.org/abs/2504.20964v2",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-04-30",
        "tldr": "",
        "abstract": "We introduce OSVBench, a new benchmark for evaluating Large Language Models (LLMs) on the task of generating complete formal specifications for verifying the functional correctness of operating system kernels. This benchmark is built upon a real-world operating system kernel, Hyperkernel, and consists of 245 complex specification generation tasks in total, each of which is a long-context task of about 20k-30k tokens. The benchmark formulates the specification generation task as a program synthesis problem confined to a domain for specifying states and transitions. This formulation is provided to LLMs through a programming model. The LLMs must be able to understand the programming model and verification assumptions before delineating the correct search space for syntax and semantics and generating formal specifications. Guided by the operating system's high-level functional description, the LLMs are asked to generate a specification that fully describes all correct states and transitions for a potentially buggy code implementation of the operating system. Experimental results with 12 state-of-the-art LLMs indicate limited performance of existing LLMs on the specification generation task for operating system verification. Significant disparities in their performance highlight differences in their ability to handle long-context code generation tasks. The code are available at https://github.com/lishangyu-hkust/OSVBench",
        "authors": [
            "Shangyu Li",
            "Juyong Jiang",
            "Tiancheng Zhao",
            "Jiasi Shen"
        ],
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.OS",
            "cs.PL",
            "cs.SE"
        ],
        "submit_date": "2025-04-29"
    },
    "http://arxiv.org/abs/2504.19516v4": {
        "id": "http://arxiv.org/abs/2504.19516v4",
        "title": "Boosting LLM Serving through Spatial-Temporal GPU Resource Sharing",
        "link": "http://arxiv.org/abs/2504.19516v4",
        "tags": [
            "disaggregation",
            "autoscaling",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-04-29",
        "tldr": "Addresses inefficient GPU utilization in LLM serving from mismatched prefill/decode phases. Introduces Bullet, a spatial-temporal orchestration system enabling concurrent execution via SLO-aware scheduling and adaptive resource allocation. Achieves 1.26x average (up to 1.55x) throughput gain over SOTAs.",
        "abstract": "Modern LLM serving systems confront inefficient GPU utilization due to the fundamental mismatch between compute-intensive prefill and memory-bound decode phases. While current practices attempt to address this by organizing these phases into hybrid batches, such solutions create an inefficient tradeoff that sacrifices either throughput or latency, leaving substantial GPU resources underutilized. We identify two key root causes: 1) the prefill phase suffers from suboptimal compute utilization due to wave quantization and attention bottlenecks. 2) hybrid batches disproportionately prioritize latency over throughput, resulting in wasted compute and memory bandwidth. To mitigate the issues, we present Bullet, a novel spatial-temporal orchestration system that eliminates these inefficiencies through precise phase coordination. Bullet enables concurrent execution of prefill and decode phases, while dynamically provisioning GPU resources using real-time performance modeling. By integrating SLO-aware scheduling and adaptive resource allocation, Bullet maximizes utilization without compromising latency targets. Experimental evaluations on real-world workloads demonstrate that Bullet delivers 1.26x average throughput gains (up to 1.55x) over state-of-the-arts, while consistently meeting latency constraints.",
        "authors": [
            "Zejia Lin",
            "Hongxin Xu",
            "Guanyi Chen",
            "Zhiguang Chen",
            "Yutong Lu",
            "Xianwei Zhang"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-04-28"
    },
    "http://arxiv.org/abs/2504.19232v1": {
        "id": "http://arxiv.org/abs/2504.19232v1",
        "title": "Adaptra: Straggler-Resilient Hybrid-Parallel Training with Pipeline Adaptation",
        "link": "http://arxiv.org/abs/2504.19232v1",
        "tags": [
            "training",
            "networking",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-04-29",
        "tldr": "Addresses straggler-induced pipeline bubbles in distributed DNN training. Proposes ADAPTRA with adaptive pipeline scheduling and CPU offloading for communication ops. Improves training iteration time by 1.2-3.5× under stragglers.",
        "abstract": "Training large Deep Neural Network (DNN) models at scale often encounters straggler issues, mostly in communications due to network congestion, RNIC/switch defects, or topological asymmetry. Under advanced pipeline parallelism, even minor communication delays can induce significant training slowdowns. This occurs because (1) slow communication disrupts the pipeline schedule, creating cascading \"bubbles\" in a domino effect, and (2) current GPU kernel scheduling is susceptible to head-of-line blocking, where slow communication blocks subsequent computations, further adding to these bubbles. To address these challenges, we present ADAPTRA, a straggler-resilient training system with two key optimizations. First, it optimally adapts the pipeline schedule in the presence of stragglers to absorb communication delays without inducing cascading bubbles, using a simple yet effective algorithm guided by an analytical model. Second, upon detecting slow communication, ADAPTRA offloads communication operations from GPU to host memory and utilizes CPU-side RDMA for data transfer. This eliminates head-of-line blocking as subsequent computation kernels can be scheduled immediately on GPUs. Together, these optimizations effectively reduce pipeline stalls in the presence of communication stragglers, improving the training iteration time by 1.2-3.5x in our experiments under various settings.",
        "authors": [
            "Tianyuan Wu",
            "Lunxi Cao",
            "Hanfeng Lu",
            "Xiaoxiao Jiang",
            "Yinghao Yu",
            "Siran Yang",
            "Guodong Yang",
            "Jiamang Wang",
            "Lin Qu",
            "Liping Zhang",
            "Wei Wang"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-04-27"
    },
    "http://arxiv.org/abs/2504.19867v1": {
        "id": "http://arxiv.org/abs/2504.19867v1",
        "title": "semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated Computation and Unified Storage",
        "link": "http://arxiv.org/abs/2504.19867v1",
        "tags": [
            "serving",
            "disaggregation",
            "storage"
        ],
        "relevant": true,
        "indexed_date": "2025-04-29",
        "tldr": "Addresses storage inefficiency in LLM serving caused by disaggregated prefill/decode phases. Proposes semi-PD with disaggregated computation at SM level and unified storage managed by memory controller. Reduces average latency by 1.27-2.58x and serves 1.55-1.72x more requests at high loads.",
        "abstract": "Existing large language model (LLM) serving systems fall into two categories: 1) a unified system where prefill phase and decode phase are co-located on the same GPU, sharing the unified computational resource and storage, and 2) a disaggregated system where the two phases are disaggregated to different GPUs. The design of the disaggregated system addresses the latency interference and sophisticated scheduling issues in the unified system but leads to storage challenges including 1) replicated weights for both phases that prevent flexible deployment, 2) KV cache transfer overhead between the two phases, 3) storage imbalance that causes substantial wasted space of the GPU capacity, and 4) suboptimal resource adjustment arising from the difficulties in migrating KV cache. Such storage inefficiency delivers poor serving performance under high request rates.   In this paper, we identify that the advantage of the disaggregated system lies in the disaggregated computation, i.e., partitioning the computational resource to enable the asynchronous computation of two phases. Thus, we propose a novel LLM serving system, semi-PD, characterized by disaggregated computation and unified storage. In semi-PD, we introduce a computation resource controller to achieve disaggregated computation at the streaming multi-processor (SM) level, and a unified memory manager to manage the asynchronous memory access from both phases. semi-PD has a low-overhead resource adjustment mechanism between the two phases, and a service-level objective (SLO) aware dynamic partitioning algorithm to optimize the SLO attainment. Compared to state-of-the-art systems, semi-PD maintains lower latency at higher request rates, reducing the average end-to-end latency per request by 1.27-2.58x on DeepSeek series models, and serves 1.55-1.72x more requests adhering to latency constraints on Llama series models.",
        "authors": [
            "Ke Hong",
            "Lufang Chen",
            "Zhong Wang",
            "Xiuhong Li",
            "Qiuli Mao",
            "Jianping Ma",
            "Chao Xiong",
            "Guanyu Wu",
            "Buhe Han",
            "Guohao Dai",
            "Yun Liang",
            "Yu Wang"
        ],
        "categories": [
            "cs.CL",
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-04-28"
    },
    "http://arxiv.org/abs/2504.19720v1": {
        "id": "http://arxiv.org/abs/2504.19720v1",
        "title": "Taming the Titans: A Survey of Efficient LLM Inference Serving",
        "link": "http://arxiv.org/abs/2504.19720v1",
        "tags": [
            "serving",
            "offloading",
            "disaggregation"
        ],
        "relevant": true,
        "indexed_date": "2025-04-29",
        "tldr": "Surveys efficient LLM inference serving techniques addressing latency and throughput challenges. Covers instance-level optimizations (model placement, scheduling, storage) and cluster-level strategies (GPU deployment, load balancing). Highlights disaggregation and emerging directions, but lacks specific quantitative metrics.",
        "abstract": "Large Language Models (LLMs) for Generative AI have achieved remarkable progress, evolving into sophisticated and versatile tools widely adopted across various domains and applications. However, the substantial memory overhead caused by their vast number of parameters, combined with the high computational demands of the attention mechanism, poses significant challenges in achieving low latency and high throughput for LLM inference services. Recent advancements, driven by groundbreaking research, have significantly accelerated progress in this field. This paper provides a comprehensive survey of these methods, covering fundamental instance-level approaches, in-depth cluster-level strategies, emerging scenario directions, and other miscellaneous but important areas. At the instance level, we review model placement, request scheduling, decoding length prediction, storage management, and the disaggregation paradigm. At the cluster level, we explore GPU cluster deployment, multi-instance load balancing, and cloud service solutions. For emerging scenarios, we organize the discussion around specific tasks, modules, and auxiliary methods. To ensure a holistic overview, we also highlight several niche yet critical areas. Finally, we outline potential research directions to further advance the field of LLM inference serving.",
        "authors": [
            "Ranran Zhen",
            "Juntao Li",
            "Yixin Ji",
            "Zhenlin Yang",
            "Tong Liu",
            "Qingrong Xia",
            "Xinyu Duan",
            "Zhefeng Wang",
            "Baoxing Huai",
            "Min Zhang"
        ],
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-04-28"
    },
    "http://arxiv.org/abs/2504.18154v1": {
        "id": "http://arxiv.org/abs/2504.18154v1",
        "title": "EcoServe: Enabling Cost-effective LLM Serving with Proactive Intra- and Inter-Instance Orchestration",
        "link": "http://arxiv.org/abs/2504.18154v1",
        "tags": [
            "serving",
            "networking",
            "disaggregation"
        ],
        "relevant": true,
        "indexed_date": "2025-04-28",
        "tldr": "Proposes EcoServe, a partially disaggregated LLM serving strategy for cost-effectiveness on commodity interconnects. Uses temporal disaggregation, rolling activation, and adaptive scheduling to reduce prefill-decode interference and improve throughput. Achieves 82.49-126.96% higher goodput versus baselines on 30B/70B models.",
        "abstract": "Existing LLM serving strategies can be categorized based on whether prefill and decode phases are disaggregated: non-disaggregated (NoDG) or fully disaggregated (FuDG). However, the NoDG strategy leads to strong prefill-decode interference and the FuDG strategy highly relies on high-performance interconnects, making them less cost-effective.   We introduce EcoServe, a system that enables cost-effective LLM serving on clusters with commodity interconnects. EcoServe is built on the partially disaggregated (PaDG) strategy, applying temporal disaggregation and rolling activation for proactive intra- and inter-instance scheduling. It first disaggregates the prefill and decode phases along the time dimension within a single instance to mitigate inter-phase interference and enhance throughput. Next, it coordinates multiple instances and cyclically activates them to ensure the continuous availability of prefill processing, thereby improving latency. Thus, EcoServe's basic serving unit is the macro instance, within which multiple instances collaborate. It further integrates an adaptive scheduling algorithm to route requests in a macro instance and a mitosis scaling approach to enable fine-grained capacity scaling. Beyond delivering high goodput, EcoServe excels in load balancing, hardware cost, parallelism compatibility, and even engineering simplicity compared to existing solutions.   When serving 30B- and 70B-scale models on a production-level cluster with 32 NVIDIA L20 GPUs using commodity Ethernet, EcoServe averagely improves goodput by 82.49%, 86.17%, 122.76%, and 126.96% over four representative NoDG and FuDG systems.",
        "authors": [
            "Jiangsu Du",
            "Hongbin Zhang",
            "Taosheng Wei",
            "Zhenyi Zheng",
            "Kaiyi Wu",
            "Zhiguang Chen",
            "Yutong Lu"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-04-25"
    },
    "http://arxiv.org/abs/2504.16792v1": {
        "id": "http://arxiv.org/abs/2504.16792v1",
        "title": "Preemption Aware Task Scheduling for Priority and Deadline Constrained DNN Inference Task Offloading in Homogeneous Mobile-Edge Networks",
        "link": "http://arxiv.org/abs/2504.16792v1",
        "tags": [
            "offloading",
            "edge",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-04-24",
        "tldr": "Proposes a preemption-aware scheduler for deadline-constrained DNN inference offloading in homogeneous edge networks. Combines two scheduling algorithms for high/low priority tasks with deadline-aware preemption. Achieves 99% high-priority task completion and 3-8% increase in end-to-end classified frames.",
        "abstract": "This paper addresses the computational offloading of Deep Neural Networks (DNNs) to nearby devices with similar processing capabilities, to avoid the larger communication delays incurred for cloud offloading. We present a preemption aware scheduling approach for priority and deadline constrained task offloading in homogeneous edge networks. Our scheduling approach consists of two distinct scheduling algorithms, designed to accommodate the differing requirements of high and low priority tasks. To satisfy a task's deadline, our scheduling approach considers the availability of both communication and computational resources in the network when making placements in both the current time-slot and future time-slots. The scheduler implements a deadline-aware preemption mechanism to guarantee resource access to high priority tasks. When low-priority tasks are selected for preemption, the scheduler will attempt to reallocate them if possible before their deadline. We implement this scheduling approach into a task offloading system which we evaluate empirically in the real-world on a network of edge devices composed of four Raspberry Pi 2 Model B's. We evaluate this system under against a version without a task preemption mechanism as well as workstealing approaches to compare the impact on high priority task completion and the ability to complete overall frames. These solutions are evaluated under a workload of 1296 frames. Our findings show that our scheduling approach allows for 99\\% of high-priority tasks to complete while also providing a 3 - 8\\% increase in the number of frames fully classified end-to-end over both workstealing approaches and systems without a preemption mechanism.",
        "authors": [
            "Jamie Cotter",
            "Ignacio Castineiras",
            "Donna O'Shea",
            "Victor Cionca"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-04-23"
    },
    "http://arxiv.org/abs/2504.16344v2": {
        "id": "http://arxiv.org/abs/2504.16344v2",
        "title": "Real-time Bayesian inference at extreme scale: A digital twin for tsunami early warning applied to the Cascadia subduction zone",
        "link": "http://arxiv.org/abs/2504.16344v2",
        "tags": [
            "storage",
            "offloading",
            "hardware"
        ],
        "relevant": true,
        "indexed_date": "2025-04-24",
        "tldr": "Develops a Bayesian inversion digital twin for real-time tsunami forecasting by exploiting shift invariance for offline-online decomposition. Scales offline computation to 43,520 GPUs with 92% efficiency, and enables 0.2-second online inference on a modest GPU system—10-billion-fold speedup.",
        "abstract": "We present a Bayesian inversion-based digital twin that employs acoustic pressure data from seafloor sensors, along with 3D coupled acoustic-gravity wave equations, to infer earthquake-induced spatiotemporal seafloor motion in real time and forecast tsunami propagation toward coastlines for early warning with quantified uncertainties. Our target is the Cascadia subduction zone, with one billion parameters. Computing the posterior mean alone would require 50 years on a 512 GPU machine. Instead, exploiting the shift invariance of the parameter-to-observable map and devising novel parallel algorithms, we induce a fast offline-online decomposition. The offline component requires just one adjoint wave propagation per sensor; using MFEM, we scale this part of the computation to the full El Capitan system (43,520 GPUs) with 92% weak parallel efficiency. Moreover, given real-time data, the online component exactly solves the Bayesian inverse and forecasting problems in 0.2 seconds on a modest GPU system, a ten-billion-fold speedup.",
        "authors": [
            "Stefan Henneking",
            "Sreeram Venkat",
            "Veselin Dobrev",
            "John Camier",
            "Tzanio Kolev",
            "Milinda Fernando",
            "Alice-Agnes Gabriel",
            "Omar Ghattas"
        ],
        "categories": [
            "cs.DC",
            "math.NA",
            "physics.geo-ph"
        ],
        "submit_date": "2025-04-23"
    },
    "http://arxiv.org/abs/2504.16112v2": {
        "id": "http://arxiv.org/abs/2504.16112v2",
        "title": "HPU: High-Bandwidth Processing Unit for Scalable, Cost-effective LLM Inference via GPU Co-processing",
        "link": "http://arxiv.org/abs/2504.16112v2",
        "tags": [
            "hardware",
            "offloading",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-04-24",
        "tldr": "Proposes HPU, a co-processor for offloading memory-bound LLM inference operations like KV cache. HPU handles memory-intensive tasks while GPU focuses on compute, scaling for large batches and long sequences. Achieves 4.1× higher throughput and 4.6× better energy efficiency vs GPU-only.",
        "abstract": "The attention layer, a core component of Transformer-based LLMs, brings out inefficiencies in current GPU systems due to its low operational intensity and the substantial memory requirements of KV caches. We propose a High-bandwidth Processing Unit (HPU), a memoryintensive co-processor that enhances GPU resource utilization during large-batched LLM inference. By offloading memory-bound operations, the HPU allows the GPU to focus on compute-intensive tasks, increasing overall efficiency. Also, the HPU, as an add-on card, scales out to accommodate surging memory demands driven by large batch sizes and extended sequence lengths. In this paper, we show the HPU prototype implemented with PCIe-based FPGA cards mounted on a GPU system. Our novel GPU-HPU heterogeneous system demonstrates up to 4.1x performance gains and 4.6x energy efficiency improvements over a GPUonly system, providing scalability without increasing the number of GPUs.",
        "authors": [
            "Myunghyun Rhee",
            "Joonseop Sim",
            "Taeyoung Ahn",
            "Seungyong Lee",
            "Daegun Yoon",
            "Euiseok Kim",
            "Kyoung Park",
            "Youngpyo Joo",
            "Hoshik Kim"
        ],
        "categories": [
            "cs.AR",
            "cs.AI",
            "cs.CL",
            "cs.DC"
        ],
        "submit_date": "2025-04-18"
    },
    "http://arxiv.org/abs/2504.15720v1": {
        "id": "http://arxiv.org/abs/2504.15720v1",
        "title": "SeaLLM: Service-Aware and Latency-Optimized Resource Sharing for Large Language Model Inference",
        "link": "http://arxiv.org/abs/2504.15720v1",
        "tags": [
            "serving",
            "offloading",
            "autoscaling"
        ],
        "relevant": true,
        "indexed_date": "2025-04-23",
        "tldr": "Proposes SeaLLM for efficient multi-LLM sharing with optimized latency. Designs latency-aware scheduling, adaptive placement/replacement, and unified KV cache to share GPU memory. Achieves up to 13.6× lower normalized latency and 18.69× lower tail latency versus baselines.",
        "abstract": "Large language models (LLMs) with different architectures and sizes have been developed. Serving each LLM with dedicated GPUs leads to resource waste and service inefficiency due to the varying demand of LLM requests. A common practice is to share multiple LLMs. However, existing sharing systems either do not consider the autoregressive pattern of LLM services, or only focus on improving the throughput, which impairs the sharing performance, especially the serving latency. We present SeaLLM, which enables service-aware and latency-optimized LLM sharing. SeaLLM improves the overall sharing performance by (1) a latency-optimized scheduling algorithm utilizing the characteristics of LLM services, (2) a placement algorithm to determine the placement plan and an adaptive replacement algorithm to decide the replacement interval, and (3) a unified key-value cache to share GPU memory among LLM services efficiently. Our evaluation under real-world traces and LLM services demonstrates that SeaLLM improves the normalized latency by up to $13.60\\times$, the tail latency by up to $18.69\\times$, and the SLO attainment by up to $3.64\\times$ compared to existing solutions.",
        "authors": [
            "Yihao Zhao",
            "Jiadun Chen",
            "Peng Sun",
            "Lei Li",
            "Xuanzhe Liu",
            "Xin Jin"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-04-22"
    },
    "http://arxiv.org/abs/2504.15303v1": {
        "id": "http://arxiv.org/abs/2504.15303v1",
        "title": "High-Throughput LLM inference on Heterogeneous Clusters",
        "link": "http://arxiv.org/abs/2504.15303v1",
        "tags": [
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-04-23",
        "tldr": "Addresses high-throughput LLM inference on heterogeneous clusters by optimizing deployment configurations via modeling and exhaustive search, and a capability-aware scheduling algorithm. Achieves throughput improvements of 122.5% and 33.6% on two test clusters.",
        "abstract": "Nowadays, many companies possess various types of AI accelerators, forming heterogeneous clusters. Efficiently leveraging these clusters for high-throughput large language model (LLM) inference services can significantly reduce costs and expedite task processing. However, LLM inference on heterogeneous clusters presents two main challenges. Firstly, different deployment configurations can result in vastly different performance. The number of possible configurations is large, and evaluating the effectiveness of a specific setup is complex. Thus, finding an optimal configuration is not an easy task. Secondly, LLM inference instances within a heterogeneous cluster possess varying processing capacities, leading to different processing speeds for handling inference requests. Evaluating these capacities and designing a request scheduling algorithm that fully maximizes the potential of each instance is challenging. In this paper, we propose a high-throughput inference service system on heterogeneous clusters. First, the deployment configuration is optimized by modeling the resource amount and expected throughput and using the exhaustive search method. Second, a novel mechanism is proposed to schedule requests among instances, which fully considers the different processing capabilities of various instances. Extensive experiments show that the proposed scheduler improves throughput by 122.5% and 33.6% on two heterogeneous clusters, respectively.",
        "authors": [
            "Yi Xiong",
            "Jinqi Huang",
            "Wenjie Huang",
            "Xuebing Yu",
            "Entong Li",
            "Zhixiong Ning",
            "Jinhua Zhou",
            "Li Zeng",
            "Xin Chen"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-04-18"
    },
    "http://arxiv.org/abs/2504.15302v1": {
        "id": "http://arxiv.org/abs/2504.15302v1",
        "title": "RAGDoll: Efficient Offloading-based Online RAG System on a Single GPU",
        "link": "http://arxiv.org/abs/2504.15302v1",
        "tags": [
            "serving",
            "RAG",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-04-23",
        "tldr": "Proposes RAGDoll, an efficient RAG system for resource-constrained platforms, decoupling retrieval and generation into parallel pipelines with joint memory placement and batch scheduling. Achieves up to 3.6x speedup in average latency over serial baselines.",
        "abstract": "Retrieval-Augmented Generation (RAG) enhances large language model (LLM) generation quality by incorporating relevant external knowledge. However, deploying RAG on consumer-grade platforms is challenging due to limited memory and the increasing scale of both models and knowledge bases. In this work, we introduce RAGDoll, a resource-efficient, self-adaptive RAG serving system integrated with LLMs, specifically designed for resource-constrained platforms. RAGDoll exploits the insight that RAG retrieval and LLM generation impose different computational and memory demands, which in a traditional serial workflow result in substantial idle times and poor resource utilization. Based on this insight, RAGDoll decouples retrieval and generation into parallel pipelines, incorporating joint memory placement and dynamic batch scheduling strategies to optimize resource usage across diverse hardware devices and workloads. Extensive experiments demonstrate that RAGDoll adapts effectively to various hardware configurations and LLM scales, achieving up to 3.6 times speedup in average latency compared to serial RAG systems based on vLLM.",
        "authors": [
            "Weiping Yu",
            "Ningyi Liao",
            "Siqiang Luo",
            "Junfeng Liu"
        ],
        "categories": [
            "cs.DC",
            "cs.OS"
        ],
        "submit_date": "2025-04-17"
    },
    "http://arxiv.org/abs/2504.15299v1": {
        "id": "http://arxiv.org/abs/2504.15299v1",
        "title": "D$^{2}$MoE: Dual Routing and Dynamic Scheduling for Efficient On-Device MoE-based LLM Serving",
        "link": "http://arxiv.org/abs/2504.15299v1",
        "tags": [
            "MoE",
            "edge",
            "quantization"
        ],
        "relevant": true,
        "indexed_date": "2025-04-23",
        "tldr": "Proposes D$^2$MoE, an algorithm-system co-design for efficient on-device MoE LLM serving. Uses matryoshka weight quantization for bit-nested expert compression and hottest-expert-bit-first scheduling to optimize I/O-computation pipelines. Achieves up to 1.39× throughput improvement and 53% peak memory reduction on edge devices while maintaining accuracy.",
        "abstract": "The mixture of experts (MoE) model is a sparse variant of large language models (LLMs), designed to hold a better balance between intelligent capability and computational overhead. Despite its benefits, MoE is still too expensive to deploy on resource-constrained edge devices, especially with the demands of on-device inference services. Recent research efforts often apply model compression techniques, such as quantization, pruning and merging, to restrict MoE complexity. Unfortunately, due to their predefined static model optimization strategies, they cannot always achieve the desired quality-overhead trade-off when handling multiple requests, finally degrading the on-device quality of service. These limitations motivate us to propose the D$^2$MoE, an algorithm-system co-design framework that matches diverse task requirements by dynamically allocating the most proper bit-width to each expert. Specifically, inspired by the nested structure of matryoshka dolls, we propose the matryoshka weight quantization (MWQ) to progressively compress expert weights in a bit-nested manner and reduce the required runtime memory. On top of it, we further optimize the I/O-computation pipeline and design a heuristic scheduling algorithm following our hottest-expert-bit-first (HEBF) principle, which maximizes the expert parallelism between I/O and computation queue under constrained memory budgets, thus significantly reducing the idle temporal bubbles waiting for the experts to load. Evaluations on real edge devices show that D$^2$MoE improves the overall inference throughput by up to 1.39$\\times$ and reduces the peak memory footprint by up to 53% over the latest on-device inference frameworks, while still preserving comparable serving accuracy as its INT8 counterparts.",
        "authors": [
            "Haodong Wang",
            "Qihua Zhou",
            "Zicong Hong",
            "Song Guo"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-04-17"
    },
    "http://arxiv.org/abs/2504.15296v1": {
        "id": "http://arxiv.org/abs/2504.15296v1",
        "title": "Scalability Optimization in Cloud-Based AI Inference Services: Strategies for Real-Time Load Balancing and Automated Scaling",
        "link": "http://arxiv.org/abs/2504.15296v1",
        "tags": [
            "autoscaling",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-04-23",
        "tldr": "Addresses dynamic resource scaling for AI inference services in the cloud. Proposes a hybrid framework using reinforcement learning for load balancing and DNNs for demand forecasting, with decentralized decision-making. Achieves 35% higher load balancing efficiency and 28% lower response delay.",
        "abstract": "The rapid expansion of AI inference services in the cloud necessitates a robust scalability solution to manage dynamic workloads and maintain high performance. This study proposes a comprehensive scalability optimization framework for cloud AI inference services, focusing on real-time load balancing and autoscaling strategies. The proposed model is a hybrid approach that combines reinforcement learning for adaptive load distribution and deep neural networks for accurate demand forecasting. This multi-layered approach enables the system to anticipate workload fluctuations and proactively adjust resources, ensuring maximum resource utilisation and minimising latency. Furthermore, the incorporation of a decentralised decision-making process within the model serves to enhance fault tolerance and reduce response time in scaling operations. Experimental results demonstrate that the proposed model enhances load balancing efficiency by 35\\ and reduces response delay by 28\\, thereby exhibiting a substantial optimization effect in comparison with conventional scalability solutions.",
        "authors": [
            "Yihong Jin",
            "Ze Yang"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-04-16"
    },
    "http://arxiv.org/abs/2504.15930v1": {
        "id": "http://arxiv.org/abs/2504.15930v1",
        "title": "StreamRL: Scalable, Heterogeneous, and Elastic RL for LLMs with Disaggregated Stream Generation",
        "link": "http://arxiv.org/abs/2504.15930v1",
        "tags": [
            "RL",
            "disaggregation",
            "training"
        ],
        "relevant": true,
        "indexed_date": "2025-04-23",
        "tldr": "Investigates how to improve the scalability and cost-efficiency of RL for LLMs via disaggregated architecture. Proposes StreamRL with stream generation to overlap pipeline stages and output-length rankers for skewness mitigation. Achieves 2.66x higher throughput and 1.33x better cost-effectiveness.",
        "abstract": "Reinforcement learning (RL) has become the core post-training technique for large language models (LLMs). RL for LLMs involves two stages: generation and training. The LLM first generates samples online, which are then used to derive rewards for training. The conventional view holds that the colocated architecture, where the two stages share resources via temporal multiplexing, outperforms the disaggregated architecture, in which dedicated resources are assigned to each stage. However, in real-world deployments, we observe that the colocated architecture suffers from resource coupling, where the two stages are constrained to use the same resources. This coupling compromises the scalability and cost-efficiency of colocated RL in large-scale training. In contrast, the disaggregated architecture allows for flexible resource allocation, supports heterogeneous training setups, and facilitates cross-datacenter deployment.   StreamRL is designed with disaggregation from first principles and fully unlocks its potential by addressing two types of performance bottlenecks in existing disaggregated RL frameworks: pipeline bubbles, caused by stage dependencies, and skewness bubbles, resulting from long-tail output length distributions. To address pipeline bubbles, StreamRL breaks the traditional stage boundary in synchronous RL algorithms through stream generation and achieves full overlapping in asynchronous RL. To address skewness bubbles, StreamRL employs an output-length ranker model to identify long-tail samples and reduces generation time via skewness-aware dispatching and scheduling. Experiments show that StreamRL improves throughput by up to 2.66x compared to existing state-of-the-art systems, and improves cost-effectiveness by up to 1.33x in a heterogeneous, cross-datacenter setting.",
        "authors": [
            "Yinmin Zhong",
            "Zili Zhang",
            "Xiaoniu Song",
            "Hanpeng Hu",
            "Chao Jin",
            "Bingyang Wu",
            "Nuo Chen",
            "Yukun Chen",
            "Yu Zhou",
            "Changyi Wan",
            "Hongyu Zhou",
            "Yimin Jiang",
            "Yibo Zhu",
            "Daxin Jiang"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "submit_date": "2025-04-22"
    },
    "http://arxiv.org/abs/2504.14966v2": {
        "id": "http://arxiv.org/abs/2504.14966v2",
        "title": "SLO-Aware Scheduling for Large Language Model Inferences",
        "link": "http://arxiv.org/abs/2504.14966v2",
        "tags": [
            "serving",
            "autoscaling"
        ],
        "relevant": true,
        "indexed_date": "2025-04-22",
        "tldr": "Proposes an SLO-aware scheduler for LLM inference using simulated annealing to optimize request sequencing based on SLOs, input and output lengths. Achieves 5x higher SLO attainment and 31.6% lower average latency versus state-of-the-art frameworks.",
        "abstract": "Large language models (LLMs) have revolutionized applications such as code completion, chatbots, and online classification. To elevate user experiences, service level objectives (SLOs) serve as crucial benchmarks for assessing inference services capabilities. In practice, an inference service processes multiple types of tasks, each with its own distinct SLO. To ensure satisfactory user experiences, each request's distinct SLOs should be considered in scheduling. However, existing designs lack this consideration, leading to insufficient hardware utility and suboptimal performance.   This paper analyzes scenarios to process tasks with varying SLOs, and introduces a simulated annealing-based scheduler to decide request priority sequence based on a request's SLO, input lengths, and possible output lengths. As the first specialized scheduler for multi-SLO scenarios, this work improves SLO attainment by up to 5x and reduces average latency by 31.6% on Python-Code-23k-ShareGPT and ShareGPT_Vicuna_unfiltered datasets, compared to current state-of-the-art framework vLLM and a new framework LMDeploy.",
        "authors": [
            "Jinqi Huang",
            "Yi Xiong",
            "Xuebing Yu",
            "Wenjie Huang",
            "Entong Li",
            "Li Zeng",
            "Xin Chen"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-04-21"
    },
    "http://arxiv.org/abs/2504.14775v2": {
        "id": "http://arxiv.org/abs/2504.14775v2",
        "title": "gLLM: Global Balanced Pipeline Parallelism System for Distributed LLM Serving with Token Throttling",
        "link": "http://arxiv.org/abs/2504.14775v2",
        "tags": [
            "serving",
            "training"
        ],
        "relevant": true,
        "indexed_date": "2025-04-22",
        "tldr": "Addresses pipeline bubbles in distributed LLM serving due to imbalanced computation. Proposes gLLM with token throttling to regulate prefill and decode tokens globally. Achieves 11%–398% higher max throughput with lower latency vs. state-of-the-art systems.",
        "abstract": "Pipeline parallelism has emerged as a predominant approach for deploying large language models (LLMs) across distributed nodes, owing to its lower communication overhead compared to tensor parallelism. While demonstrating high throughput in request serving, pipeline parallelism often suffers from performance limitations caused by pipeline bubbles, which are primarily resulted from imbalanced computation delays across batches. Existing methods like Sarathi-Serve attempt to address this through hybrid scheduling of chunked prefill and decode tokens using a fixed token budget. However, such methods may experience significant fluctuations due to either insufficient prefill tokens or uneven distribution of decode tokens, ultimately leading to computational imbalance. To overcome these inefficiencies, we present gLLM, a globally balanced pipeline parallelism system incorporating Token Throttling to effectively mitigate the pipeline bubbles. Our Token Throttling mechanism is a fine-grained scheduling policy that independently regulates the quantities of prefill and decode tokens, thus enabling balanced computation by leveraging global information from the inference system. Specifically, for decode tokens, gLLM maintains near-consistent token count across processing batches. For prefill tokens, it dynamically adjusts batch sizes based on both total pending tokens and the memory utilization rates of key-value cache (KV cache). Furthermore, gLLM runtime adopts an asynchronous execution and message passing architecture specifically optimized for pipeline parallelism characteristics. Experimental evaluations with representative LLMs show that gLLM achieves significant performance improvements, delivering 11% to 398% higher maximum throughput compared to state-of-the-art pipeline or tensor parallelism systems, while simultaneously maintaining lower latency.",
        "authors": [
            "Tianyu Guo",
            "Xianwei Zhang",
            "Jiangsu Du",
            "Zhiguang Chen",
            "Nong Xiao",
            "Yutong Lu"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-04-21"
    },
    "http://arxiv.org/abs/2504.14611v1": {
        "id": "http://arxiv.org/abs/2504.14611v1",
        "title": "Joint Optimization of Offloading, Batching and DVFS for Multiuser Co-Inference",
        "link": "http://arxiv.org/abs/2504.14611v1",
        "tags": [
            "offloading",
            "edge",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-04-22",
        "tldr": "Investigates energy-efficient edge-device co-inference for DNN tasks with latency constraints. Proposes J-DOB strategy that jointly optimizes DVFS, offloading, and batching. Achieves up to 51.30% energy reduction compared to local execution.",
        "abstract": "With the growing integration of artificial intelligence in mobile applications, a substantial number of deep neural network (DNN) inference requests are generated daily by mobile devices. Serving these requests presents significant challenges due to limited device resources and strict latency requirements. Therefore, edge-device co-inference has emerged as an effective paradigm to address these issues. In this study, we focus on a scenario where multiple mobile devices offload inference tasks to an edge server equipped with a graphics processing unit (GPU). For finer control over offloading and scheduling, inference tasks are partitioned into smaller sub-tasks. Additionally, GPU batch processing is employed to boost throughput and improve energy efficiency. This work investigates the problem of minimizing total energy consumption while meeting hard latency constraints. We propose a low-complexity Joint DVFS, Offloading, and Batching strategy (J-DOB) to solve this problem. The effectiveness of the proposed algorithm is validated through extensive experiments across varying user numbers and deadline constraints. Results show that J-DOB can reduce energy consumption by up to 51.30% and 45.27% under identical and different deadlines, respectively, compared to local computing.",
        "authors": [
            "Yaodan Xu",
            "Sheng Zhou",
            "Zhisheng Niu"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-04-20"
    },
    "http://arxiv.org/abs/2504.14960v2": {
        "id": "http://arxiv.org/abs/2504.14960v2",
        "title": "MoE Parallel Folding: Heterogeneous Parallelism Mappings for Efficient Large-Scale MoE Model Training with Megatron Core",
        "link": "http://arxiv.org/abs/2504.14960v2",
        "tags": [
            "training",
            "MoE"
        ],
        "relevant": true,
        "indexed_date": "2025-04-22",
        "tldr": "Proposes a 5D hybrid parallelism framework with MoE Parallel Folding for efficient large-scale MoE training. Decouples parallelism for attention/MoE layers and introduces a flexible token dispatcher. Achieves up to 49.3% MFU for Mixtral 8x22B on 1024 H100 GPUs.",
        "abstract": "Mixture of Experts (MoE) models enhance neural network scalability by dynamically selecting relevant experts per input token, enabling larger model sizes while maintaining manageable computation costs. However, efficient training of large-scale MoE models across thousands of GPUs presents significant challenges due to limitations in existing parallelism strategies. We introduce an end-to-end training framework for large-scale MoE models that utilizes five-dimensional hybrid parallelism: Tensor Parallelism, Expert Parallelism, Context Parallelism, Data Parallelism, and Pipeline Parallelism. Central to our approach is MoE Parallel Folding, a novel strategy that decouples the parallelization of attention and MoE layers in Transformer models, allowing each layer type to adopt optimal parallel configurations. Additionally, we develop a flexible token-level dispatcher that supports both token-dropping and token-dropless MoE training across all five dimensions of parallelism. This dispatcher accommodates dynamic tensor shapes and coordinates different parallelism schemes for Attention and MoE layers, facilitating complex parallelism implementations. Our experiments demonstrate significant improvements in training efficiency and scalability. We achieve up to 49.3% Model Flops Utilization (MFU) for the Mixtral 8x22B model and 39.0% MFU for the Qwen2-57B-A14B model on H100 GPUs, outperforming existing methods. The framework scales efficiently up to 1,024 GPUs and maintains high performance with sequence lengths up to 128K tokens, validating its effectiveness for large-scale MoE model training. The code is available in Megatron-Core.",
        "authors": [
            "Dennis Liu",
            "Zijie Yan",
            "Xin Yao",
            "Tong Liu",
            "Vijay Korthikanti",
            "Evan Wu",
            "Shiqing Fan",
            "Gao Deng",
            "Hongxiao Bai",
            "Jianbin Chang",
            "Ashwath Aithal",
            "Michael Andersch",
            "Mohammad Shoeybi",
            "Jiajie Yao",
            "Chandler Zhou",
            "David Wu",
            "Xipeng Li",
            "June Yang"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "submit_date": "2025-04-21"
    },
    "http://arxiv.org/abs/2504.14489v2": {
        "id": "http://arxiv.org/abs/2504.14489v2",
        "title": "Optimizing SLO-oriented LLM Serving with PD-Multiplexing",
        "link": "http://arxiv.org/abs/2504.14489v2",
        "tags": [
            "serving",
            "disaggregation",
            "autoscaling"
        ],
        "relevant": true,
        "indexed_date": "2025-04-22",
        "tldr": "Resolves the tradeoff between SLO attainment and throughput in LLM serving by proposing Drift with PD-multiplexing for phase-decoupled compute partition. Uses GPU partitioning, gang scheduling, and SLO-aware dispatching. Achieves 5.1× average throughput improvement over baselines.",
        "abstract": "Modern LLM services demand high throughput and stringent SLO guarantees across two distinct inference phases-prefill and decode-and complex multi-turn workflows. However, current systems face a fundamental tradeoff: out-of-place compute partition enables per-phase SLO attainment, while in-place memory sharing maximizes throughput via KV cache reuse. Moreover, existing in-place compute partition also encounters low utilization and high overhead due to phase-coupling design. We present Drift, a new LLM serving framework that resolves this tension via PD multiplexing, enabling in-place and phase-decoupled compute partition. Drift leverages low-level GPU partitioning techniques to multiplex prefill and decode phases spatially and adaptively on shared GPUs, while preserving in-place memory sharing. To fully leverage the multiplexing capability, Drift introduces an adaptive gang scheduling mechanism, a contention-free modeling method, and a SLO-aware dispatching policy. Evaluation shows that Drift achieves an average $5.1\\times$ throughput improvement (up to $17.5\\times$) over state-of-the-art baselines, while consistently meeting SLO targets under complex LLM workloads.",
        "authors": [
            "Weihao Cui",
            "Yukang Chen",
            "Han Zhao",
            "Ziyi Xu",
            "Quan Chen",
            "Xusheng Chen",
            "Yangjie Zhou",
            "Shixuan Sun",
            "Minyi Guo"
        ],
        "categories": [
            "cs.OS"
        ],
        "submit_date": "2025-04-20"
    },
    "http://arxiv.org/abs/2504.12471v1": {
        "id": "http://arxiv.org/abs/2504.12471v1",
        "title": "You Don't Need All Attentions: Distributed Dynamic Fine-Tuning for Foundation Models",
        "link": "http://arxiv.org/abs/2504.12471v1",
        "tags": [
            "training",
            "LoRA",
            "RAG"
        ],
        "relevant": true,
        "indexed_date": "2025-04-18",
        "tldr": "Proposes D2FT, a distributed fine-tuning framework that skips unnecessary attention modules with selection strategies optimized by knapsack optimization. Reduces training computational costs by 40% and communication costs by 50% with minimal accuracy drops.",
        "abstract": "Fine-tuning plays a crucial role in adapting models to downstream tasks with minimal training efforts. However, the rapidly increasing size of foundation models poses a daunting challenge for accommodating foundation model fine-tuning in most commercial devices, which often have limited memory bandwidth. Techniques like model sharding and tensor parallelism address this issue by distributing computation across multiple devices to meet memory requirements. Nevertheless, these methods do not fully leverage their foundation nature in facilitating the fine-tuning process, resulting in high computational costs and imbalanced workloads. We introduce a novel Distributed Dynamic Fine-Tuning (D2FT) framework that strategically orchestrates operations across attention modules based on our observation that not all attention modules are necessary for forward and backward propagation in fine-tuning foundation models. Through three innovative selection strategies, D2FT significantly reduces the computational workload required for fine-tuning foundation models. Furthermore, D2FT addresses workload imbalances in distributed computing environments by optimizing these selection strategies via multiple knapsack optimization. Our experimental results demonstrate that the proposed D2FT framework reduces the training computational costs by 40% and training communication costs by 50% with only 1% to 2% accuracy drops on the CIFAR-10, CIFAR-100, and Stanford Cars datasets. Moreover, the results show that D2FT can be effectively extended to recent LoRA, a state-of-the-art parameter-efficient fine-tuning technique. By reducing 40% computational cost or 50% communication cost, D2FT LoRA top-1 accuracy only drops 4% to 6% on Stanford Cars dataset.",
        "authors": [
            "Shiwei Ding",
            "Lan Zhang",
            "Zhenlin Wang",
            "Giuseppe Ateniese",
            "Xiaoyong Yuan"
        ],
        "categories": [
            "cs.LG",
            "cs.DC",
            "cs.PF"
        ],
        "submit_date": "2025-04-16"
    },
    "http://arxiv.org/abs/2504.11750v1": {
        "id": "http://arxiv.org/abs/2504.11750v1",
        "title": "Characterizing and Optimizing LLM Inference Workloads on CPU-GPU Coupled Architectures",
        "link": "http://arxiv.org/abs/2504.11750v1",
        "tags": [
            "serving",
            "kernel",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-04-17",
        "tldr": "Characterizes CPU-bound bottlenecks in LLM inference on CPU-GPU coupled systems via fine-grained kernel tracing. Proposes SKIP profiler and TKLQT metric to identify transition points. Shows kernel fusion reduces launch overhead, improving GH200 prefill latency by up to 2.7x at large batches.",
        "abstract": "Large language model (LLM)-based inference workloads increasingly dominate data center costs and resource utilization. Therefore, understanding the inference workload characteristics on evolving CPU-GPU coupled architectures is crucial for optimization. This paper presents an in-depth analysis of LLM inference behavior on loosely-coupled (PCIe A100/H100) and closely-coupled (GH200) systems. We analyze performance dynamics using fine-grained operator-to-kernel trace analysis, facilitated by our novel profiler SKIP and metrics like Total Kernel Launch and Queuing Time (TKLQT). Results show that closely-coupled (CC) GH200 significantly outperforms loosely-coupled (LC) systems at large batch sizes, achieving 1.9x-2.7x faster prefill latency for Llama 3.2-1B. However, our analysis also reveals that GH200 remains CPU-bound up to 4x larger batch sizes than LC systems. In this extended CPU-bound region, we identify the performance characteristics of the Grace CPU as a key factor contributing to higher inference latency at low batch sizes on GH200. We demonstrate that TKLQT accurately identifies this CPU/GPU-bound transition point. Based on this analysis, we further show that kernel fusion offers significant potential to mitigate GH200's low-batch latency bottleneck by reducing kernel launch overhead. This detailed kernel-level characterization provides critical insights for optimizing diverse CPU-GPU coupling strategies. This work is an initial effort, and we plan to explore other major AI/DL workloads that demand different degrees of CPU-GPU heterogeneous architectures.",
        "authors": [
            "Prabhu Vellaisamy",
            "Thomas Labonte",
            "Sourav Chakraborty",
            "Matt Turner",
            "Samantika Sury",
            "John Paul Shen"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.AR",
            "cs.PF"
        ],
        "submit_date": "2025-04-16"
    },
    "http://arxiv.org/abs/2504.11816v1": {
        "id": "http://arxiv.org/abs/2504.11816v1",
        "title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
        "link": "http://arxiv.org/abs/2504.11816v1",
        "tags": [
            "serving",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-04-17",
        "tldr": "Addresses high cost of GPU instances for LLM inference with VM selection optimization. Proposes InferSave framework using KV cache offloading and Compute Time Calibration Function to match SLOs with cost-efficient instances. Achieves 73.7% cost reduction for online workloads and 20.19% savings for offline workloads via offloading.",
        "abstract": "LLM inference is essential for applications like text summarization, translation, and data analysis, but the high cost of GPU instances from Cloud Service Providers (CSPs) like AWS is a major burden. This paper proposes InferSave, a cost-efficient VM selection framework for cloud based LLM inference. InferSave optimizes KV cache offloading based on Service Level Objectives (SLOs) and workload charac teristics, estimating GPU memory needs, and recommending cost-effective VM instances. Additionally, the Compute Time Calibration Function (CTCF) improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance. Experiments on AWS GPU instances show that selecting lower-cost instances without KV cache offloading improves cost efficiency by up to 73.7% for online workloads, while KV cache offloading saves up to 20.19% for offline workloads.",
        "authors": [
            "Kihyun Kim",
            "Jinwoo Kim",
            "Hyunsun Chung",
            "Myung-Hoon Cha",
            "Hong-Yeon Kim",
            "Youngjae Kim"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "submit_date": "2025-04-16"
    },
    "http://arxiv.org/abs/2504.11651v2": {
        "id": "http://arxiv.org/abs/2504.11651v2",
        "title": "70% Size, 100% Accuracy: Lossless LLM Compression for Efficient GPU Inference via Dynamic-Length Float",
        "link": "http://arxiv.org/abs/2504.11651v2",
        "tags": [
            "inference",
            "storage",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-04-17",
        "tldr": "Proposes DFloat11, a lossless compression framework for LLM weights using dynamic-length float encodings and custom GPU kernels for efficient decompression. Achieves 30% model size reduction with bit-for-bit identical outputs, enabling 2.3–46.2x throughput gain vs offloading in token generation.",
        "abstract": "Large-scale AI models, such as Large Language Models (LLMs) and Diffusion Models (DMs), have grown rapidly in size, creating significant challenges for efficient deployment on resource-constrained hardware. In this paper, we introduce Dynamic-Length Float (DFloat11), a lossless compression framework that reduces LLM and DM size by 30% while preserving outputs that are bit-for-bit identical to the original model. DFloat11 is motivated by the low entropy in the BFloat16 weight representation of LLMs, which reveals significant inefficiency in the existing storage format. By applying entropy coding, DFloat11 assigns dynamic-length encodings to weights based on frequency, achieving near information-optimal compression without any loss of precision. To facilitate efficient inference with dynamic-length encodings, we develop a custom GPU kernel for fast online decompression. Our design incorporates the following: (i) compact, hierarchical lookup tables (LUTs) that fit within GPU SRAM for efficient decoding, (ii) a two-phase GPU kernel for coordinating thread read/write positions using lightweight auxiliary variables, and (iii) transformer-block-level decompression to minimize latency. Experiments on Llama 3.3, Qwen 3, Mistral 3, FLUX.1, and others validate our hypothesis that DFloat11 achieves around 30% model size reduction while preserving bit-for-bit identical outputs. Compared to a potential alternative of offloading parts of an uncompressed model to the CPU to meet memory constraints, DFloat11 achieves 2.3--46.2x higher throughput in token generation. With a fixed GPU memory budget, DFloat11 enables 5.7--14.9x longer generation lengths than uncompressed models. Notably, our method enables lossless inference of Llama 3.1 405B, an 810GB model, on a single node equipped with 8x80GB GPUs. Our code is available at https://github.com/LeanModels/DFloat11.",
        "authors": [
            "Tianyi Zhang",
            "Mohsen Hariri",
            "Shaochen Zhong",
            "Vipin Chaudhary",
            "Yang Sui",
            "Xia Hu",
            "Anshumali Shrivastava"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "submit_date": "2025-04-15"
    },
    "http://arxiv.org/abs/2504.11320v1": {
        "id": "http://arxiv.org/abs/2504.11320v1",
        "title": "Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory Constraints",
        "link": "http://arxiv.org/abs/2504.11320v1",
        "tags": [
            "serving",
            "offloading",
            "quantization"
        ],
        "relevant": true,
        "indexed_date": "2025-04-16",
        "tldr": "Proposes WAIT and Nested WAIT scheduling algorithms for LLM inference under memory constraints, using fluid dynamics approximation. Optimizes throughput and latency by managing KV cache growth. Achieves improved throughput and reduced latency vs. vLLM/Sarathi on Llama-7B with A100 GPU.",
        "abstract": "Large Language Models (LLMs) are indispensable in today's applications, but their inference procedure -- generating responses by processing text in segments and using a memory-heavy Key-Value (KV) cache -- demands significant computational resources, particularly under memory constraints. This paper formulates LLM inference optimization as a multi-stage online scheduling problem where sequential prompt arrivals and KV cache growth render conventional scheduling ineffective. We develop a fluid dynamics approximation to provide a tractable benchmark that guides algorithm design. Building on this, we propose the Waiting for Accumulated Inference Threshold (WAIT) algorithm, which uses multiple thresholds to schedule incoming prompts optimally when output lengths are known, and extend it to Nested WAIT for cases with unknown output lengths. Theoretical analysis shows that both algorithms achieve near-optimal performance against the fluid benchmark in heavy traffic conditions, balancing throughput, latency, and Time to First Token (TTFT). Experiments with the Llama-7B model on an A100 GPU using both synthetic and real-world datasets demonstrate improved throughput and reduced latency relative to established baselines like vLLM and Sarathi. This work bridges operations research and machine learning, offering a rigorous framework for the efficient deployment of LLMs under memory constraints.",
        "authors": [
            "Ruicheng Ao",
            "Gan Luo",
            "David Simchi-Levi",
            "Xinshang Wang"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC",
            "math.OC",
            "stat.ML"
        ],
        "submit_date": "2025-04-15"
    },
    "http://arxiv.org/abs/2504.10289v1": {
        "id": "http://arxiv.org/abs/2504.10289v1",
        "title": "Optimal Graph Stretching for Distributed Averaging",
        "link": "http://arxiv.org/abs/2504.10289v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-04-15",
        "tldr": "",
        "abstract": "The performance of distributed averaging depends heavily on the underlying topology. In various fields, including compressed sensing, multi-party computation, and abstract graph theory, graphs may be expected to be free of short cycles, i.e. to have high girth. Though extensive analyses and heuristics exist for optimising the performance of distributed averaging in general networks, these studies do not consider girth. As such, it is not clear what happens to convergence time when a graph is stretched to a higher girth.   In this work, we introduce the optimal graph stretching problem, wherein we are interested in finding the set of edges for a particular graph that ensures optimal convergence time under constraint of a minimal girth. We compare various methods for choosing which edges to remove, and use various convergence heuristics to speed up the searching process. We generate many graphs with varying parameters, stretch and optimise them, and measure the duration of distributed averaging. We find that stretching by itself significantly increases convergence time. This decrease can be counteracted with a subsequent repair phase, guided by a convergence time heuristic. Existing heuristics are capable, but may be suboptimal.",
        "authors": [
            "Florine W. Dekker",
            "Zekeriya Erkin",
            "Mauro Conti"
        ],
        "categories": [
            "cs.DC",
            "cs.DM"
        ],
        "submit_date": "2025-04-14"
    },
    "http://arxiv.org/abs/2504.10013v1": {
        "id": "http://arxiv.org/abs/2504.10013v1",
        "title": "Training LLMs on HPC Systems: Best Practices from the OpenGPT-X Project",
        "link": "http://arxiv.org/abs/2504.10013v1",
        "tags": [
            "training",
            "scalability"
        ],
        "relevant": true,
        "indexed_date": "2025-04-15",
        "tldr": "Details best practices for efficiently training large LLMs on HPC systems. Describes system architecture, software stacks, and operational workflows for the OpenGPT-X project's Teuken-7B model. Achieves optimized resource utilization for training 7B-parameter models on European language datasets.",
        "abstract": "The training of large language models (LLMs) requires substantial computational resources, complex software stacks, and carefully designed workflows to achieve scalability and efficiency. This report presents best practices and insights gained from the OpenGPT-X project, a German initiative focused on developing open, multilingual LLMs optimized for European languages. We detail the use of high-performance computing (HPC) systems, primarily JUWELS Booster at JSC, for training Teuken-7B, a 7-billion-parameter transformer model. The report covers system architecture, training infrastructure, software choices, profiling and benchmarking tools, as well as engineering and operational challenges.",
        "authors": [
            "Carolin Penke",
            "Chelsea Maria John",
            "Jan Ebert",
            "Stefan Kesselheim",
            "Andreas Herten"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-04-14"
    },
    "http://arxiv.org/abs/2504.09345v1": {
        "id": "http://arxiv.org/abs/2504.09345v1",
        "title": "MoE-Lens: Towards the Hardware Limit of High-Throughput MoE LLM Serving Under Resource Constraints",
        "link": "http://arxiv.org/abs/2504.09345v1",
        "tags": [
            "MoE",
            "serving",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-04-15",
        "tldr": "Addresses the challenge of optimizing resource utilization for MoE LLM inference in resource-constrained environments. Proposes MoE-Lens, a system designed via holistic performance modeling to approach hardware limits. Achieves 4.6x average throughput improvement over state-of-the-art.",
        "abstract": "Mixture of Experts (MoE) LLMs, characterized by their sparse activation patterns, offer a promising approach to scaling language models while avoiding proportionally increasing the inference cost. However, their large parameter sizes present deployment challenges in resource-constrained environments with limited GPU memory capacity, as GPU memory is often insufficient to accommodate the full set of model weights. Consequently, typical deployments rely on CPU-GPU hybrid execution: the GPU handles compute-intensive GEMM operations, while the CPU processes the relatively lightweight attention mechanism. This setup introduces a key challenge: how to effectively optimize resource utilization across CPU and GPU? Prior work has designed system optimizations based on performance models with limited scope. Specifically, such models do not capture the complex interactions between hardware properties and system execution mechanisms. Therefore, previous approaches neither identify nor achieve the hardware limit.   This paper presents MoE-Lens, a high-throughput MoE LLM inference system designed through holistic performance modeling for resource-constrained environments. Our performance model thoroughly analyzes various fundamental system components, including CPU memory capacity, GPU compute power, and workload characteristics, to understand the theoretical performance upper bound of MoE inference. Furthermore, it captures the system execution mechanisms to identify the key hardware bottlenecks and accurately predict the achievable throughput. Informed by our performance model, MoE-Lens introduces an inference system approaching hardware limits. Evaluated on diverse MoE models and datasets, MoE-Lens outperforms the state-of-the-art solution by 4.6x on average (up to 25.5x), with our theoretical model predicting performance with an average 94% accuracy.",
        "authors": [
            "Yichao Yuan",
            "Lin Ma",
            "Nishil Talati"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-04-12"
    },
    "http://arxiv.org/abs/2504.09307v1": {
        "id": "http://arxiv.org/abs/2504.09307v1",
        "title": "Lumos: Efficient Performance Modeling and Estimation for Large-scale LLM Training",
        "link": "http://arxiv.org/abs/2504.09307v1",
        "tags": [
            "training",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-04-15",
        "tldr": "Proposes Lumos, a trace-driven performance modeling toolkit for large-scale LLM training, enabling accurate replay and estimation of execution behaviors. Evaluated on up to 512 H100 GPUs with GPT-3 variants, achieving average execution time replay error of 3.3% and estimation capability for new setups.",
        "abstract": "Training LLMs in distributed environments presents significant challenges due to the complexity of model execution, deployment systems, and the vast space of configurable strategies. Although various optimization techniques exist, achieving high efficiency in practice remains difficult. Accurate performance models that effectively characterize and predict a model's behavior are essential for guiding optimization efforts and system-level studies. We propose Lumos, a trace-driven performance modeling and estimation toolkit for large-scale LLM training, designed to accurately capture and predict the execution behaviors of modern LLMs. We evaluate Lumos on a production ML cluster with up to 512 NVIDIA H100 GPUs using various GPT-3 variants, demonstrating that it can replay execution time with an average error of just 3.3%, along with other runtime details, across different models and configurations. Additionally, we validate its ability to estimate performance for new setups from existing traces, facilitating efficient exploration of model and deployment configurations.",
        "authors": [
            "Mingyu Liang",
            "Hiwot Tadese Kassa",
            "Wenyin Fu",
            "Brian Coutinho",
            "Louis Feng",
            "Christina Delimitrou"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-04-12"
    },
    "http://arxiv.org/abs/2504.09285v2": {
        "id": "http://arxiv.org/abs/2504.09285v2",
        "title": "DynaServe: Unified and Elastic Execution for Dynamic Disaggregated LLM Serving",
        "link": "http://arxiv.org/abs/2504.09285v2",
        "tags": [
            "serving",
            "disaggregation"
        ],
        "relevant": true,
        "indexed_date": "2025-04-15",
        "tldr": "Proposes DynaServe, a unified LLM serving system with micro-request abstraction and two-level scheduling for dynamic workloads. It splits requests at token boundaries and balances load across GPUs, increasing capacity by up to 3.07× and goodput by 1.91× under SLOs.",
        "abstract": "LLM inference must meet strict latency SLOs (e.g., 100 ms P99 time-between-tokens) while maximizing goodput. Yet, real-world variability in prompt and response lengths skews compute-intensive prefill and memory-bound decode phases, making both colocated (even with chunked prefill) and disaggregated deployments unable to simultaneously deliver low tail latency and high throughput.   We introduce DynaServe, a high-performance LLM serving system built atop vLLM that unifies and extends both paradigms for maximizing goodput under SLO constraints, when handling unbalanced and dynamic workloads. It relies on a micro-request abstraction, which arbitrarily splits each request at any token boundary into at most two cooperating segments. A two-level scheduling framework then balances micro-request load across unified GPU instances. The global scheduler rapidly selects per-request split points by considering both the request's prefill/decode time ratio and the current load across GPU instances. The local schedulers on each GPU instance independently form SLO-aware batches, adjusting their composition in response to workload fluctuations, potential latency spikes and per-GPU under/over utilization. On real-world traces, DynaServe boosts the overall serving capacity from 1.15$\\times$ to 3.07$\\times$, improves goodput by up to 1.91$\\times$ and 1.61$\\times$, and improves the performance by up to 60\\% in a hybrid workload under SLO compared to state-of-the-art colocated and disaggregated baselines.",
        "authors": [
            "Chaoyi Ruan",
            "Yinhe Chen",
            "Dongqi Tian",
            "Yandong Shi",
            "Yongji Wu",
            "Jialin Li",
            "Cheng Li"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-04-12"
    },
    "http://arxiv.org/abs/2504.08850v1": {
        "id": "http://arxiv.org/abs/2504.08850v1",
        "title": "SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting",
        "link": "http://arxiv.org/abs/2504.08850v1",
        "tags": [
            "serving",
            "offloading",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-04-15",
        "tldr": "Proposes SpecEE, an inference engine using speculative early exiting to reduce computation and memory access. Introduces speculation-based predictor design, heuristic scheduling, and context-aware merged mapping to speed up decoding. Achieves 2.25-2.43x speedup with Llama2-7B without extra training overhead.",
        "abstract": "Early exiting has recently emerged as a promising technique for accelerating large language models (LLMs) by effectively reducing the hardware computation and memory access. In this paper, we present SpecEE, a fast LLM inference engine with speculative early exiting. (1) At the algorithm level, we propose the speculation-based lightweight predictor design by exploiting the probabilistic correlation between the speculative tokens and the correct results and high parallelism of GPUs. (2) At the system level, we point out that not all layers need a predictor and design the two-level heuristic predictor scheduling engine based on skewed distribution and contextual similarity. (3) At the mapping level, we point out that different decoding methods share the same essential characteristics, and propose the context-aware merged mapping for predictor with efficient GPU implementations to support speculative decoding, and form a framework for various existing orthogonal acceleration techniques (e.g., quantization and sparse activation) on cloud and personal computer (PC) scenarios, successfully pushing the Pareto frontier of accuracy and speedup. It is worth noting that SpecEE can be applied to any LLM by negligible training overhead in advance without affecting the model original parameters. Extensive experiments show that SpecEE achieves 2.25x and 2.43x speedup with Llama2-7B on cloud and PC scenarios respectively.",
        "authors": [
            "Jiaming Xu",
            "Jiayi Pan",
            "Yongkang Zhou",
            "Siming Chen",
            "Jinhao Li",
            "Yaoxiu Lian",
            "Junyi Wu",
            "Guohao Dai"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-04-11"
    },
    "http://arxiv.org/abs/2504.08795v1": {
        "id": "http://arxiv.org/abs/2504.08795v1",
        "title": "DARIS: An Oversubscribed Spatio-Temporal Scheduler for Real-Time DNN Inference on GPUs",
        "link": "http://arxiv.org/abs/2504.08795v1",
        "tags": [
            "serving",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-04-15",
        "tldr": "Addresses low GPU utilization and coarse scheduling for real-time DNN inference. Proposes DARIS, a priority-based scheduler using MPS and CUDA streams for spatial-temporal resource sharing and migration. Improves throughput by 15% over batching and reduces high-priority response times by 33%.",
        "abstract": "The widespread use of Deep Neural Networks (DNNs) is limited by high computational demands, especially in constrained environments. GPUs, though effective accelerators, often face underutilization and rely on coarse-grained scheduling. This paper introduces DARIS, a priority-based real-time DNN scheduler for GPUs, utilizing NVIDIA's MPS and CUDA streaming for spatial sharing, and a synchronization-based staging method for temporal partitioning. In particular, DARIS improves GPU utilization and uniquely analyzes GPU concurrency by oversubscribing computing resources. It also supports zero-delay DNN migration between GPU partitions. Experiments show DARIS improves throughput by 15% and 11.5% over batching and state-of-the-art schedulers, respectively, even without batching. All high-priority tasks meet deadlines, with low-priority tasks having under 2% deadline miss rate. High-priority response times are 33% better than those of low-priority tasks.",
        "authors": [
            "Amir Fakhim Babaei",
            "Thidapat Chantem"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-04-08"
    },
    "http://arxiv.org/abs/2504.08791v2": {
        "id": "http://arxiv.org/abs/2504.08791v2",
        "title": "Prima.cpp: Fast 30-70B LLM Inference on Heterogeneous and Low-Resource Home Clusters",
        "link": "http://arxiv.org/abs/2504.08791v2",
        "tags": [
            "edge",
            "offloading",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-04-15",
        "tldr": "Prima.cpp enables efficient LLM inference on heterogeneous home clusters with limited resources. It uses pipelined-ring parallelism for I/O-compute overlap with mmap offloading and a heterogeneity-aware scheduler. Achieves 5-17× lower TPOT vs baselines for 30-70B models at 26 tokens/s on 32B model.",
        "abstract": "On-device inference offers privacy, offline use, and instant response, but consumer hardware restricts large language models (LLMs) to low throughput and capability. To overcome this challenge, we present prima.cpp, a distributed on-device inference system that runs 30-70B LLMs on consumer home clusters with mixed CPUs/GPUs, insufficient RAM/VRAM, slow disks, Wi-Fi links, and heterogeneous OSs. We introduce pipelined-ring parallelism (PRP) to overlap disk I/O with compute and communication, and address the prefetch-release conflict in mmap-based offloading. We further propose Halda, a heterogeneity-aware scheduler that co-optimizes per-device CPU/GPU workloads and device selection under RAM/VRAM constraints. On four consumer home devices, a 70B model reaches 674 ms/token TPOT with <6% memory pressure, and a 32B model with speculative decoding achieves 26 tokens/s. Compared with llama.cpp, exo, and dllama, our proposed prima.cpp achieves 5-17x lower TPOT, supports fine-grained model sizes from 8B to 70B, ensures broader cross-OS and quantization compatibility, and remains OOM-free, while also being Wi-Fi tolerant, privacy-preserving, and hardware-independent. The code is available at https://gitee.com/zonghang-li/prima.cpp.",
        "authors": [
            "Zonghang Li",
            "Tao Li",
            "Wenjiao Feng",
            "Rongxing Xiao",
            "Jianshu She",
            "Hong Huang",
            "Mohsen Guizani",
            "Hongfang Yu",
            "Qirong Ho",
            "Wei Xiang",
            "Steve Liu"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-04-07"
    },
    "http://arxiv.org/abs/2504.08784v1": {
        "id": "http://arxiv.org/abs/2504.08784v1",
        "title": "SLOs-Serve: Optimized Serving of Multi-SLO LLMs",
        "link": "http://arxiv.org/abs/2504.08784v1",
        "tags": [
            "serving",
            "quantization"
        ],
        "relevant": true,
        "indexed_date": "2025-04-15",
        "tldr": "Addresses efficient serving of multi-stage LLMs with SLO constraints. Proposes token allocation optimization and request routing via dynamic programming. Achieves 2.2x higher per-GPU serving capacity compared to prior systems.",
        "abstract": "This paper introduces SLOs-Serve, a system designed for serving multi-stage large language model (LLM) requests with application- and stage-specific service level objectives (SLOs). The key idea behind SLOs-Serve is to customize the allocation of tokens to meet these SLO requirements. SLOs-Serve uses a multi-SLO dynamic programming-based algorithm to continuously optimize token allocations under SLO constraints by exploring the full design space of chunked prefill and (optional) speculative decoding. Leveraging this resource planning algorithm, SLOs-Serve effectively supports multi-SLOs and multi-replica serving with dynamic request routing while being resilient to bursty arrivals. Our evaluation across 6 LLM application scenarios (including summarization, coding, chatbot, tool calling, and reasoning) demonstrates that SLOs-Serve improves per-GPU serving capacity by 2.2x on average compared to prior state-of-the-art systems.",
        "authors": [
            "Siyuan Chen",
            "Zhipeng Jia",
            "Samira Khan",
            "Arvind Krishnamurthy",
            "Phillip B. Gibbons"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-04-05"
    },
    "http://arxiv.org/abs/2504.09474v4": {
        "id": "http://arxiv.org/abs/2504.09474v4",
        "title": "MigGPT: Harnessing Large Language Models for Automated Migration of Out-of-Tree Linux Kernel Patches Across Versions",
        "link": "http://arxiv.org/abs/2504.09474v4",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-04-15",
        "tldr": "",
        "abstract": "Out-of-tree kernel patches are essential for adapting the Linux kernel to new hardware or enabling specific functionalities. Maintaining and updating these patches across different kernel versions demands significant effort from experienced engineers. Large language models (LLMs) have shown remarkable progress across various domains, suggesting their potential for automating out-of-tree kernel patch migration. However, our findings reveal that LLMs, while promising, struggle with incomplete code context understanding and inaccurate migration point identification. In this work, we propose MigGPT, a framework that employs a novel code fingerprint structure to retain code snippet information and incorporates three meticulously designed modules to improve the migration accuracy and efficiency of out-of-tree kernel patches. Furthermore, we establish a robust benchmark using real-world out-of-tree kernel patch projects to evaluate LLM capabilities. Evaluations show that MigGPT significantly outperforms the direct application of vanilla LLMs, achieving an average completion rate of 74.07 for migration tasks.",
        "authors": [
            "Pucheng Dang",
            "Di Huang",
            "Dong Li",
            "Kang Chen",
            "Yuanbo Wen",
            "Qi Guo",
            "Xing Hu"
        ],
        "categories": [
            "cs.SE",
            "cs.AI",
            "cs.OS"
        ],
        "submit_date": "2025-04-13"
    },
    "http://arxiv.org/abs/2504.08242v1": {
        "id": "http://arxiv.org/abs/2504.08242v1",
        "title": "Jupiter: Fast and Resource-Efficient Collaborative Inference of Generative LLMs on Edge Devices",
        "link": "http://arxiv.org/abs/2504.08242v1",
        "tags": [
            "edge",
            "serving",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-04-14",
        "tldr": "Proposes Jupiter, a collaborative edge AI system for LLM inference, with intra-sequence pipeline parallelism for prefill phase and outline-based speculative decoding for autoregressive phase. Achieves up to 26.1x lower latency while maintaining generation quality on resource-constrained edges.",
        "abstract": "Generative large language models (LLMs) have garnered significant attention due to their exceptional capabilities in various AI tasks. Traditionally deployed in cloud datacenters, LLMs are now increasingly moving towards more accessible edge platforms to protect sensitive user data and ensure privacy preservation. The limited computational resources of individual edge devices, however, can result in excessively prolonged inference latency and overwhelmed memory usage. While existing research has explored collaborative edge computing to break the resource wall of individual devices, these solutions yet suffer from massive communication overhead and under-utilization of edge resources. Furthermore, they focus exclusively on optimizing the prefill phase, neglecting the crucial autoregressive decoding phase for generative LLMs. To address that, we propose Jupiter, a fast, scalable, and resource-efficient collaborative edge AI system for generative LLM inference. Jupiter introduces a flexible pipelined architecture as a principle and differentiates its system design according to the differentiated characteristics of the prefill and decoding phases. For prefill phase, Jupiter submits a novel intra-sequence pipeline parallelism and develops a meticulous parallelism planning strategy to maximize resource efficiency; For decoding, Jupiter devises an effective outline-based pipeline parallel decoding mechanism combined with speculative decoding, which further magnifies inference acceleration. Extensive evaluation based on realistic implementation demonstrates that Jupiter remarkably outperforms state-of-the-art approaches under various edge environment setups, achieving up to 26.1x end-to-end latency reduction while rendering on-par generation quality.",
        "authors": [
            "Shengyuan Ye",
            "Bei Ouyang",
            "Liekang Zeng",
            "Tianyi Qian",
            "Xiaowen Chu",
            "Jian Tang",
            "Xu Chen"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.NI"
        ],
        "submit_date": "2025-04-11"
    },
    "http://arxiv.org/abs/2504.07878v1": {
        "id": "http://arxiv.org/abs/2504.07878v1",
        "title": "Token Level Routing Inference System for Edge Devices",
        "link": "http://arxiv.org/abs/2504.07878v1",
        "tags": [
            "edge",
            "serving",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-04-11",
        "tldr": "Addresses efficient LLM inference on edge devices via collaborative decoding. Proposes a system where a small on-device model generates most tokens while offloading critical tokens to a cloud-based large model. Achieves 60% higher accuracy on CommonsenseQA with under 7% tokens offloaded.",
        "abstract": "The computational complexity of large language model (LLM) inference significantly constrains their deployment efficiency on edge devices. In contrast, small language models offer faster decoding and lower resource consumption but often suffer from degraded response quality and heightened susceptibility to hallucinations. To address this trade-off, collaborative decoding, in which a large model assists in generating critical tokens, has emerged as a promising solution. This paradigm leverages the strengths of both model types by enabling high-quality inference through selective intervention of the large model, while maintaining the speed and efficiency of the smaller model. In this work, we present a novel collaborative decoding inference system that allows small models to perform on-device inference while selectively consulting a cloud-based large model for critical token generation. Remarkably, the system achieves a 60% performance gain on CommonsenseQA using only a 0.5B model on an M1 MacBook, with under 7% of tokens generation uploaded to the large model in the cloud.",
        "authors": [
            "Jianshu She",
            "Wenhao Zheng",
            "Zhengzhong Liu",
            "Hongyi Wang",
            "Eric Xing",
            "Huaxiu Yao",
            "Qirong Ho"
        ],
        "categories": [
            "cs.CL",
            "cs.DC"
        ],
        "submit_date": "2025-04-10"
    },
    "http://arxiv.org/abs/2504.06095v1": {
        "id": "http://arxiv.org/abs/2504.06095v1",
        "title": "Nonuniform-Tensor-Parallelism: Mitigating GPU failure impact for Scaled-up LLM Training",
        "link": "http://arxiv.org/abs/2504.06095v1",
        "tags": [
            "training",
            "hardware"
        ],
        "relevant": true,
        "indexed_date": "2025-04-09",
        "tldr": "Proposes nonuniform tensor parallelism (NTP) to reduce the impact of GPU failures on large-scale LLM training. Uses reduced TP degrees in affected replicas and power-boosting rack design. Mitigates 10% throughput loss to near-zero with 0.1% GPU failures.",
        "abstract": "LLM training is scaled up to 10Ks of GPUs by a mix of data-(DP) and model-parallel (MP) execution. Critical to achieving efficiency is tensor-parallel (TP; a form of MP) execution within tightly-coupled subsets of GPUs, referred to as a scale-up domain, and the larger the scale-up domain the better the performance. New datacenter architectures are emerging with more GPUs able to be tightly-coupled in a scale-up domain, such as moving from 8 GPUs to 72 GPUs connected via NVLink. Unfortunately, larger scale-up domains increase the blast-radius of failures, with a failure of single GPU potentially impacting TP execution on the full scale-up domain, which can degrade overall LLM training throughput dramatically. With as few as 0.1% of GPUs being in a failed state, a high TP-degree job can experience nearly 10% reduction in LLM training throughput. We propose nonuniform-tensor-parallelism (NTP) to mitigate this amplified impact of GPU failures. In NTP, a DP replica that experiences GPU failures operates at a reduced TP degree, contributing throughput equal to the percentage of still-functional GPUs. We also propose a rack-design with improved electrical and thermal capabilities in order to sustain power-boosting of scale-up domains that have experienced failures; combined with NTP, this can allow the DP replica with the reduced TP degree (i.e., with failed GPUs) to keep up with the others, thereby achieving near-zero throughput loss for large-scale LLM training.",
        "authors": [
            "Daiyaan Arfeen",
            "Dheevatsa Mudigere",
            "Ankit More",
            "Bhargava Gopireddy",
            "Ahmet Inci",
            "Gregory R. Ganger"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-04-08"
    },
    "http://arxiv.org/abs/2504.05897v1": {
        "id": "http://arxiv.org/abs/2504.05897v1",
        "title": "HybriMoE: Hybrid CPU-GPU Scheduling and Cache Management for Efficient MoE Inference",
        "link": "http://arxiv.org/abs/2504.05897v1",
        "tags": [
            "MoE",
            "serving",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-04-09",
        "tldr": "Addresses overhead in hybrid CPU-GPU inference for MoE models due to unstable expert activation patterns. Proposes a dynamic scheduler, impact-driven prefetching, and score-based caching. Achieves 1.33× prefill and 1.70× decode speedup over SOTA framework.",
        "abstract": "The Mixture of Experts (MoE) architecture has demonstrated significant advantages as it enables to increase the model capacity without a proportional increase in computation. However, the large MoE model size still introduces substantial memory demands, which usually requires expert offloading on resource-constrained platforms and incurs significant overhead. Hybrid CPU-GPU inference has been proposed to leverage CPU computation to reduce expert loading overhead but faces major challenges: on one hand, the expert activation patterns of MoE models are highly unstable, rendering the fixed mapping strategies in existing works inefficient; on the other hand, the hybrid CPU-GPU schedule for MoE is inherently complex due to the diverse expert sizes, structures, uneven workload distribution, etc. To address these challenges, in this paper, we propose HybriMoE, a hybrid CPU-GPU inference framework that improves resource utilization through a novel CPU-GPU scheduling and cache management system. HybriMoE introduces (i) a dynamic intra-layer scheduling strategy to balance workloads across CPU and GPU, (ii) an impact-driven inter-layer prefetching algorithm, and (iii) a score-based caching algorithm to mitigate expert activation instability. We implement HybriMoE on top of the kTransformers framework and evaluate it on three widely used MoE-based LLMs. Experimental results demonstrate that HybriMoE achieves an average speedup of 1.33$\\times$ in the prefill stage and 1.70$\\times$ in the decode stage compared to state-of-the-art hybrid MoE inference framework. Our code is available at: https://github.com/PKU-SEC-Lab/HybriMoE.",
        "authors": [
            "Shuzhang Zhong",
            "Yanfan Sun",
            "Ling Liang",
            "Runsheng Wang",
            "Ru Huang",
            "Meng Li"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "submit_date": "2025-04-08"
    },
    "http://arxiv.org/abs/2504.04429v1": {
        "id": "http://arxiv.org/abs/2504.04429v1",
        "title": "IntentContinuum: Using LLMs to Support Intent-Based Computing Across the Compute Continuum",
        "link": "http://arxiv.org/abs/2504.04429v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-04-08",
        "tldr": "",
        "abstract": "The increasing proliferation of IoT devices and AI applications has created a demand for scalable and efficient computing solutions, particularly for applications requiring real-time processing. The compute continuum integrates edge and cloud resources to meet this need, balancing the low-latency demands of the edge with the high computational power of the cloud. However, managing resources in such a distributed environment presents challenges due to the diversity and complexity of these systems. Traditional resource management methods, often relying on heuristic algorithms, struggle to manage the increasing complexity, scale, and dynamics of these systems, as well as adapt to dynamic workloads and changing network conditions. Moreover, designing such approaches is often time-intensive and highly tailored to specific applications, demanding deep expertise. In this paper, we introduce a novel framework for intent-driven resource management in the compute continuum, using large language models (LLMs) to help automate decision-making processes. Our framework ensures that user-defined intents -- such as achieving the required response times for time-critical applications -- are consistently fulfilled. In the event of an intent violation, our system performs root cause analysis by examining system data to identify and address issues. This approach reduces the need for human intervention and enhances system reliability, offering a more dynamic and efficient solution for resource management in distributed environments.",
        "authors": [
            "Negin Akbari",
            "John Grundy",
            "Aamir Cheema",
            "Adel N. Toosi"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-04-06"
    },
    "http://arxiv.org/abs/2504.03871v1": {
        "id": "http://arxiv.org/abs/2504.03871v1",
        "title": "HeterMoE: Efficient Training of Mixture-of-Experts Models on Heterogeneous GPUs",
        "link": "http://arxiv.org/abs/2504.03871v1",
        "tags": [
            "training",
            "MoE",
            "disaggregation"
        ],
        "relevant": true,
        "indexed_date": "2025-04-08",
        "tldr": "Proposes HeterMoE to efficiently train MoE models on heterogeneous GPUs by disaggregating attention and expert computation. Uses zebra parallelism for overlapping computation and asymmetric expert assignment for load balancing. Achieves up to 2.3× speed-up over existing systems.",
        "abstract": "The Mixture-of-Experts (MoE) architecture has become increasingly popular as a method to scale up large language models (LLMs). To save costs, heterogeneity-aware training solutions have been proposed to utilize GPU clusters made up of both newer and older-generation GPUs. However, existing solutions are agnostic to the performance characteristics of different MoE model components (i.e., attention and expert) and do not fully utilize each GPU's compute capability.   In this paper, we introduce HeterMoE, a system to efficiently train MoE models on heterogeneous GPUs. Our key insight is that newer GPUs significantly outperform older generations on attention due to architectural advancements, while older GPUs are still relatively efficient for experts. HeterMoE disaggregates attention and expert computation, where older GPUs are only assigned with expert modules. Through the proposed zebra parallelism, HeterMoE overlaps the computation on different GPUs, in addition to employing an asymmetric expert assignment strategy for fine-grained load balancing to minimize GPU idle time. Our evaluation shows that HeterMoE achieves up to 2.3x speed-up compared to existing MoE training systems, and 1.4x compared to an optimally balanced heterogeneity-aware solution. HeterMoE efficiently utilizes older GPUs by maintaining 95% training throughput on average, even with half of the GPUs in a homogeneous A40 cluster replaced with V100.",
        "authors": [
            "Yongji Wu",
            "Xueshen Liu",
            "Shuowei Jin",
            "Ceyu Xu",
            "Feng Qian",
            "Z. Morley Mao",
            "Matthew Lentz",
            "Danyang Zhuo",
            "Ion Stoica"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-04-04"
    },
    "http://arxiv.org/abs/2504.03775v1": {
        "id": "http://arxiv.org/abs/2504.03775v1",
        "title": "FlowKV: A Disaggregated Inference Framework with Low-Latency KV Cache Transfer and Load-Aware Scheduling",
        "link": "http://arxiv.org/abs/2504.03775v1",
        "tags": [
            "disaggregation",
            "serving",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-04-08",
        "tldr": "Proposes FlowKV, a disaggregated inference framework that optimizes KV cache transfer via efficient memory allocation and introduces a load-aware scheduler for balanced resource utilization. Achieves up to 15.2%-48.9% inference acceleration on LongBench with KV transfer latency reduction from 0.944s to 0.053s.",
        "abstract": "Disaggregated inference has become an essential framework that separates the prefill (P) and decode (D) stages in large language model inference to improve throughput. However, the KV cache transfer faces significant delays between prefill and decode nodes. The block-wise calling method and discontinuous KV cache memory allocation increase the number of calls to the transmission kernel. Additionally, existing frameworks often fix the roles of P and D nodes, leading to computational imbalances. In this paper, we propose FlowKV, a novel disaggregated inference framework, which reduces the average transmission latency of KV cache by 96%, from 0.944s to 0.053s, almost eliminating the transfer time relative to the total request latency by optimizing the KV cache transfer. FlowKV introduces the Load-Aware Scheduler for balanced request scheduling and flexible PD node allocation. This design maximizes hardware resource utilization, achieving peak system throughput across various scenarios, including normal, computational imbalance, and extreme overload conditions. Experimental results demonstrate that FlowKV significantly accelerates inference by 15.2%-48.9% on LongBench dataset compared to the baseline and supports applications with heterogeneous GPUs.",
        "authors": [
            "Weiqing Li",
            "Guochao Jiang",
            "Xiangyong Ding",
            "Zhangcheng Tao",
            "Chuzhan Hao",
            "Chenfeng Xu",
            "Yuewei Zhang",
            "Hao Wang"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.CL"
        ],
        "submit_date": "2025-04-03"
    },
    "http://arxiv.org/abs/2504.03668v3": {
        "id": "http://arxiv.org/abs/2504.03668v3",
        "title": "Intelligent Orchestration of Distributed Large Foundation Model Inference at the Edge",
        "link": "http://arxiv.org/abs/2504.03668v3",
        "tags": [
            "edge",
            "serving",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-04-08",
        "tldr": "Addresses efficient LFM inference orchestration in dynamic edge environments. Proposes an adaptive framework with runtime tunable placement/partitioning, including capacity-aware distribution, partition migration, and real-time reconfiguration. Achieves balanced latency, throughput, and privacy under varying network/compute conditions.",
        "abstract": "Large Foundation Models (LFMs), including multi-modal and generative models, promise to unlock new capabilities for next-generation Edge AI applications. However, performing inference with LFMs in resource-constrained and heterogeneous edge environments, such as Multi-access Edge Computing (MEC), presents significant challenges for workload orchestration due to time-varying network, compute, and storage conditions. In particular, current split inference strategies, which partition LFM layers across nodes, are not designed to adapt to fluctuating workloads, dynamic bandwidth conditions, or evolving privacy constraints in high-utilization MEC environments. In this work, we propose a novel adaptive split inference orchestration framework that elevates both the placement and partitioning of LFM layers to runtime-tunable variables. Specifically, our framework enables real-time, quality-of-service (QoS)-aware management of inference workloads by extending conventional orchestrators with three key services: (1) Capacity-aware workload distribution, which continuously profiles node resources and selects an optimal subset of MEC nodes; (2) Dynamic partition migration, which transparently relocates pre-cut LFM segments in response to changes in utilization or network conditions; (3) Real-time reconfiguration, which dynamically re-splits LFM layers to balance latency, throughput, and privacy. We formalize the joint placement-partitioning problem, outline a reference architecture and algorithmic workflow, and discuss applicability in representative smart city, V2X, and industrial edge scenarios.",
        "authors": [
            "Fernando Koch",
            "Aladin Djuhera",
            "Alecio Binotto"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-03-19"
    },
    "http://arxiv.org/abs/2504.03665v2": {
        "id": "http://arxiv.org/abs/2504.03665v2",
        "title": "LLM & HPC:Benchmarking DeepSeek's Performance in High-Performance Computing Tasks",
        "link": "http://arxiv.org/abs/2504.03665v2",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-04-08",
        "tldr": "",
        "abstract": "Large Language Models (LLMs), such as GPT-4 and DeepSeek, have been applied to a wide range of domains in software engineering. However, their potential in the context of High-Performance Computing (HPC) much remains to be explored. This paper evaluates how well DeepSeek, a recent LLM, performs in generating a set of HPC benchmark codes: a conjugate gradient solver, the parallel heat equation, parallel matrix multiplication, DGEMM, and the STREAM triad operation. We analyze DeepSeek's code generation capabilities for traditional HPC languages like Cpp, Fortran, Julia and Python. The evaluation includes testing for code correctness, performance, and scaling across different configurations and matrix sizes. We also provide a detailed comparison between DeepSeek and another widely used tool: GPT-4. Our results demonstrate that while DeepSeek generates functional code for HPC tasks, it lags behind GPT-4, in terms of scalability and execution efficiency of the generated code.",
        "authors": [
            "Noujoud Nader",
            "Patrick Diehl",
            "Steve Brandt",
            "Hartmut Kaiser"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-03-15"
    },
    "http://arxiv.org/abs/2504.03664v2": {
        "id": "http://arxiv.org/abs/2504.03664v2",
        "title": "PIPO: Pipelined Offloading for Efficient Inference on Consumer Devices",
        "link": "http://arxiv.org/abs/2504.03664v2",
        "tags": [
            "offloading",
            "inference"
        ],
        "relevant": true,
        "indexed_date": "2025-04-08",
        "tldr": "Proposes pipeline offloading (PIPO) for efficient LLM inference on devices with limited GPU memory. Designs fine-grained offloading with optimized transfers to improve concurrency. Achieves 3.1× higher throughput and 90% GPU utilization versus below 40% baseline on a 6GB GPU laptop.",
        "abstract": "The high memory and computation demand of large language models (LLMs) makes them challenging to be deployed on consumer devices due to limited GPU memory. Offloading can mitigate the memory constraint but often suffers from low GPU utilization, leading to low inference efficiency. In this work, we propose a novel framework, called pipelined offloading (PIPO), for efficient inference on consumer devices. PIPO designs a fine-grained offloading pipeline, complemented with optimized data transfer and computation, to achieve high concurrency and efficient scheduling for inference. Experimental results show that compared with state-of-the-art baseline, PIPO increases GPU utilization from below 40% to over 90% and achieves up to 3.1$\\times$ higher throughput, running on a laptop equipped with a RTX3060 GPU of 6GB memory.",
        "authors": [
            "Yangyijian Liu",
            "Jun Li",
            "Wu-Jun Li"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.LG"
        ],
        "submit_date": "2025-03-15"
    },
    "http://arxiv.org/abs/2504.03444v2": {
        "id": "http://arxiv.org/abs/2504.03444v2",
        "title": "LLMSched: Uncertainty-Aware Workload Scheduling for Compound LLM Applications",
        "link": "http://arxiv.org/abs/2504.03444v2",
        "tags": [
            "serving",
            "RL",
            "autoscaling"
        ],
        "relevant": true,
        "indexed_date": "2025-04-07",
        "tldr": "Proposes LLMSched, an uncertainty-aware scheduling framework for compound LLM applications. Uses a DAG model with Bayesian uncertainty profiling and an entropy-based mechanism to reduce job completion time. Achieves 14-79% reduction in average JCT compared to state-of-the-art schedulers.",
        "abstract": "Developing compound Large Language Model (LLM) applications is becoming an increasingly prevalent approach to solving real-world problems. In these applications, an LLM collaborates with various external modules, including APIs and even other LLMs, to realize complex intelligent services. However, we reveal that the intrinsic duration and structural uncertainty in compound LLM applications pose great challenges for LLM service providers in serving and scheduling them efficiently. In this paper, we propose LLMSched, an uncertainty-aware scheduling framework for emerging compound LLM applications. In LLMSched, we first design a novel DAG-based model to describe the uncertain compound LLM applications. Then, we adopt the Bayesian network to comprehensively profile compound LLM applications and identify uncertainty-reducing stages, along with an entropy-based mechanism to quantify their uncertainty reduction. Combining an uncertainty reduction strategy and a job completion time (JCT)-efficient scheme, we further propose an efficient scheduler to reduce the average JCT. Evaluation of both simulation and testbed experiments on various representative compound LLM applications shows that compared to existing state-of-the-art scheduling schemes, LLMSched can reduce the average JCT by 14~79%.",
        "authors": [
            "Botao Zhu",
            "Chen Chen",
            "Xiaoyi Fan",
            "Yifei Zhu"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-04-04"
    },
    "http://arxiv.org/abs/2504.02211v2": {
        "id": "http://arxiv.org/abs/2504.02211v2",
        "title": "FT-Transformer: Resilient and Reliable Transformer with End-to-End Fault Tolerant Attention",
        "link": "http://arxiv.org/abs/2504.02211v2",
        "tags": [
            "kernel",
            "hardware",
            "inference"
        ],
        "relevant": true,
        "indexed_date": "2025-04-04",
        "tldr": "Addresses soft error resilience in Transformer inference on HPC systems. Proposes end-to-end fault tolerance for attention modules via kernel-level protection and strided ABFT for linear modules. Achieves 7.56x speedup over baselines with 13.9% average overhead.",
        "abstract": "Transformer models rely on High-Performance Computing (HPC) resources for inference, where soft errors are inevitable in large-scale systems, making the reliability of the model particularly critical. Existing fault tolerance frameworks for Transformers are designed at the operation level without architectural optimization, leading to significant computational and memory overhead, which in turn reduces protection efficiency and limits scalability to larger models. In this paper, we implement module-level protection for Transformers by treating the operations within the attention module as a single kernel and applying end-to-end fault tolerance. This method provides unified protection across multi-step computations, while achieving comprehensive coverage of potential errors in the nonlinear computations. For linear modules, we design a strided algorithm-based fault tolerance (ABFT) that avoids inter-thread communication. Experimental results show that our end-to-end fault tolerance achieves up to 7.56x speedup over traditional methods with an average fault tolerance overhead of 13.9%.",
        "authors": [
            "Huangliang Dai",
            "Shixun Wu",
            "Jiajun Huang",
            "Zizhe Jian",
            "Yue Zhu",
            "Haiyang Hu",
            "Zizhong Chen"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.LG"
        ],
        "submit_date": "2025-04-03"
    },
    "http://arxiv.org/abs/2504.00407v2": {
        "id": "http://arxiv.org/abs/2504.00407v2",
        "title": "AMP4EC: Adaptive Model Partitioning Framework for Efficient Deep Learning Inference in Edge Computing Environments",
        "link": "http://arxiv.org/abs/2504.00407v2",
        "tags": [
            "edge",
            "serving",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-04-02",
        "tldr": "Proposes AMP4EC, an adaptive model partitioning framework for efficient DNN inference in edge computing. It uses real-time resource monitoring, dynamic model splitting, and weighted task scheduling to optimize latency and throughput. Achieves 78% latency reduction and 414% throughput improvement over baselines.",
        "abstract": "Edge computing facilitates deep learning in resource-constrained environments, but challenges such as resource heterogeneity and dynamic constraints persist. This paper introduces AMP4EC, an Adaptive Model Partitioning framework designed to optimize deep learning inference in edge environments through real-time resource monitoring, dynamic model partitioning, and adaptive task scheduling. AMP4EC features a resource-aware model partitioner that splits deep learning models based on device capabilities, a task scheduler that ensures efficient load balancing using a weighted scoring mechanism, and a Docker-based deployment environment for validation. Experimental results show up to a 78% reduction in latency and a 414% improvement in throughput compared to baseline methods. The framework achieves consistent performance with low scheduling overhead across varying resource profiles, demonstrating adaptability in high-resource (1 CPU, 1GB RAM) and low-resource (0.4 CPU, 512MB RAM) scenarios. These results highlight AMP4EC's scalability, efficiency, and robustness for real-world edge deployments, addressing the critical need for efficient distributed inference in dynamic, resource-constrained environments.",
        "authors": [
            "Guilin Zhang",
            "Wulan Guo",
            "Ziqi Tan",
            "Hailong Jiang"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-04-01"
    },
    "http://arxiv.org/abs/2503.23830v2": {
        "id": "http://arxiv.org/abs/2503.23830v2",
        "title": "Orchestrate Multimodal Data with Batch Post-Balancing to Accelerate Multimodal Large Language Model Training",
        "link": "http://arxiv.org/abs/2503.23830v2",
        "tags": [
            "training",
            "multi-modal",
            "sparse"
        ],
        "relevant": true,
        "indexed_date": "2025-04-01",
        "tldr": "Addresses training inefficiency in multimodal LLMs due to modality composition incoherence. Proposes OrchMLLM with batch post-balancing dispatcher and global orchestrator to balance GPU utilization. Achieves 3.1× higher throughput than Megatron-LM and 41.6% MFU on a 2560 H100 GPU cluster.",
        "abstract": "Multimodal large language models (MLLMs), such as GPT-4o, are garnering significant attention. During the exploration of MLLM training, we identified Modality Composition Incoherence, a phenomenon that the proportion of a certain modality varies dramatically across different examples. It exacerbates the challenges of addressing mini-batch imbalances, which lead to uneven GPU utilization between Data Parallel (DP) instances and severely degrades the efficiency and scalability of MLLM training, ultimately affecting training speed and hindering further research on MLLMs.   To address these challenges, we introduce OrchMLLM, a comprehensive framework designed to mitigate the inefficiencies in MLLM training caused by Modality Composition Incoherence. First, we propose Batch Post-Balancing Dispatcher, a technique that efficiently eliminates mini-batch imbalances in sequential data. Additionally, we integrate MLLM Global Orchestrator into the training framework to orchestrate multimodal data and tackle the issues arising from Modality Composition Incoherence. We evaluate OrchMLLM across various MLLM sizes, demonstrating its efficiency and scalability. Experimental results reveal that OrchMLLM achieves a Model FLOPs Utilization (MFU) of $41.6\\%$ when training an 84B MLLM with three modalities on $2560$ H100 GPUs, outperforming Megatron-LM by up to $3.1\\times$ in throughput.",
        "authors": [
            "Yijie Zheng",
            "Bangjun Xiao",
            "Lei Shi",
            "Xiaoyang Li",
            "Faming Wu",
            "Tianyu Li",
            "Xuefeng Xiao",
            "Yang Zhang",
            "Yuxuan Wang",
            "Shouda Liu"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-03-31"
    },
    "http://arxiv.org/abs/2503.23817v2": {
        "id": "http://arxiv.org/abs/2503.23817v2",
        "title": "MVDRAM: Enabling GeMV Execution in Unmodified DRAM for Low-Bit LLM Acceleration",
        "link": "http://arxiv.org/abs/2503.23817v2",
        "tags": [
            "hardware",
            "quantization",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-04-01",
        "tldr": "The paper presents MVDRAM, a system that accelerates low-bit LLM GeMV operations using unmodified DRAM via Processing-Using-DRAM. By leveraging data sharing and mathematical linearity, it eliminates preprocessing overheads. For 2-bit models, it achieves 2.18× throughput improvement and 3.04× energy efficiency.",
        "abstract": "General matrix-vector multiplication (GeMV) remains a critical latency bottleneck in large language model (LLM) inference, even with quantized low-bit models. Processing-Using-DRAM (PUD), an analog in-DRAM computing technique, has the potential to repurpose on-device DRAM as a GeMV engine, offering additional high-throughput processing capabilities to widespread consumer devices without DRAM modifications. However, applying PUD to GeMV operations in the LLM inference pipeline incurs significant overheads $\\textit{before}$ and $\\textit{after}$ in-DRAM computation, diminishing the benefits of its high-throughput processing capabilities.   This paper presents MVDRAM, the first practical system to accelerate GeMV operations for low-bit LLM inference using unmodified DRAM. By leveraging the data sharing patterns and mathematical linearity in GeMV operations, MVDRAM orchestrates the processor and DRAM to eliminate the costs associated with pre-arranging inputs and bit-transposition of outputs required in conventional PUD approaches. Our experimental evaluation with four DDR4 DRAM modules shows that MVDRAM achieves comparable or even better inference speed than the processor-based implementation for GeMV operations in low-bit (under 4-bit) LLM. In particular, MVDRAM achieves up to 7.29$\\times$ speedup and 30.5$\\times$ energy efficiency for low-bit GeMV operations. For end-to-end LLM inference, MVDRAM achieves 2.18$\\times$ and 1.31$\\times$ throughput improvements, along with 3.04$\\times$ and 2.35$\\times$ energy efficiency, for 2-bit and 4-bit quantized low-bit models, respectively. MVDRAM has the potential to redefine the AI hardware landscape by demonstrating the feasibility of standard DRAM as an LLM accelerator.",
        "authors": [
            "Tatsuya Kubo",
            "Daichi Tokuda",
            "Tomoya Nagatani",
            "Masayuki Usui",
            "Lei Qu",
            "Ting Cao",
            "Shinya Takamaeda-Yamazaki"
        ],
        "categories": [
            "cs.AR",
            "cs.DC"
        ],
        "submit_date": "2025-03-31"
    },
    "http://arxiv.org/abs/2503.22562v1": {
        "id": "http://arxiv.org/abs/2503.22562v1",
        "title": "Niyama : Breaking the Silos of LLM Inference Serving",
        "link": "http://arxiv.org/abs/2503.22562v1",
        "tags": [
            "serving",
            "autoscaling"
        ],
        "relevant": true,
        "indexed_date": "2025-03-31",
        "tldr": "Addresses inefficiency in LLM serving due to coarse workload segregation. Proposes Niyama, a QoS-driven system with fine-grained classification, dynamic chunking, and hybrid prioritization. Increases serving capacity by 32% and reduces SLO violations by 10x during extreme load.",
        "abstract": "The widespread adoption of Large Language Models (LLMs) has enabled diverse applications with very different latency requirements. Existing LLM serving frameworks rely on siloed infrastructure with coarse-grained workload segregation -- interactive and batch -- leading to inefficient resource utilization and limited support for fine-grained Quality-of-Service (QoS) differentiation. This results in operational inefficiencies, over-provisioning and poor load management during traffic surges.   We present Niyama, a novel QoS-driven inference serving system that enables efficient co-scheduling of diverse workloads on shared infrastructure. Niyama introduces fine-grained QoS classification allowing applications to specify precise latency requirements, and dynamically adapts scheduling decisions based on real-time system state. Leveraging the predictable execution characteristics of LLM inference, Niyama implements a dynamic chunking mechanism to improve overall throughput while maintaining strict QoS guarantees. Additionally, Niyama employs a hybrid prioritization policy that balances fairness and efficiency, and employs selective request relegation that enables graceful service degradation during overload conditions. Our evaluation demonstrates that Niyama increases serving capacity by 32% compared to current siloed deployments, while maintaining QoS guarantees. Notably, under extreme load, our system reduces SLO violations by an order of magnitude compared to current strategies.",
        "authors": [
            "Kanishk Goel",
            "Jayashree Mohan",
            "Nipun Kwatra",
            "Ravi Shreyas Anupindi",
            "Ramachandran Ramjee"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC"
        ],
        "submit_date": "2025-03-28"
    },
    "http://arxiv.org/abs/2503.21476v2": {
        "id": "http://arxiv.org/abs/2503.21476v2",
        "title": "Robust DNN Partitioning and Resource Allocation Under Uncertain Inference Time",
        "link": "http://arxiv.org/abs/2503.21476v2",
        "tags": [
            "edge",
            "offloading",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-03-28",
        "tldr": "Proposes robust DNN partitioning and resource allocation for edge-based inference under uncertain inference times. Uses mean and variance of inference time to optimize partitioning and resource allocation via MINLP decomposition and CCP transformations. Reduces mobile energy consumption by 23-65% while meeting probabilistic deadlines.",
        "abstract": "In edge intelligence systems, deep neural network (DNN) partitioning and data offloading can provide real-time task inference for resource-constrained mobile devices. However, the inference time of DNNs is typically uncertain and cannot be precisely determined in advance, presenting significant challenges in ensuring timely task processing within deadlines. To address the uncertain inference time, we propose a robust optimization scheme to minimize the total energy consumption of mobile devices while meeting task probabilistic deadlines. The scheme only requires the mean and variance information of the inference time, without any prediction methods or distribution functions. The problem is formulated as a mixed-integer nonlinear programming (MINLP) that involves jointly optimizing the DNN model partitioning and the allocation of local CPU/GPU frequencies and uplink bandwidth. To tackle the problem, we first decompose the original problem into two subproblems: resource allocation and DNN model partitioning. Subsequently, the two subproblems with probability constraints are equivalently transformed into deterministic optimization problems using the chance-constrained programming (CCP) method. Finally, the convex optimization technique and the penalty convex-concave procedure (PCCP) technique are employed to obtain the optimal solution of the resource allocation subproblem and a stationary point of the DNN model partitioning subproblem, respectively. The proposed algorithm leverages real-world data from popular hardware platforms and is evaluated on widely used DNN models. Extensive simulations show that our proposed algorithm effectively addresses the inference time uncertainty with probabilistic deadline guarantees while minimizing the energy consumption of mobile devices.",
        "authors": [
            "Zhaojun Nan",
            "Yunchu Han",
            "Sheng Zhou",
            "Zhisheng Niu"
        ],
        "categories": [
            "cs.DC",
            "cs.IT",
            "cs.LG"
        ],
        "submit_date": "2025-03-27"
    },
    "http://arxiv.org/abs/2503.21109v1": {
        "id": "http://arxiv.org/abs/2503.21109v1",
        "title": "Optimizing Multi-DNN Inference on Mobile Devices through Heterogeneous Processor Co-Execution",
        "link": "http://arxiv.org/abs/2503.21109v1",
        "tags": [
            "edge",
            "serving",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-03-28",
        "tldr": "Addresses inefficient multi-DNN inference on mobile heterogeneous processors. Proposes ADMS with offline partitioning and dynamic workload adjustment based on processor state. Reduces inference latency by 4.04× compared to vanilla frameworks.",
        "abstract": "Deep Neural Networks (DNNs) are increasingly deployed across diverse industries, driving demand for mobile device support. However, existing mobile inference frameworks often rely on a single processor per model, limiting hardware utilization and causing suboptimal performance and energy efficiency. Expanding DNN accessibility on mobile platforms requires adaptive, resource-efficient solutions to meet rising computational needs without compromising functionality. Parallel inference of multiple DNNs on heterogeneous processors remains challenging. Some works partition DNN operations into subgraphs for parallel execution across processors, but these often create excessive subgraphs based only on hardware compatibility, increasing scheduling complexity and memory overhead.   To address this, we propose an Advanced Multi-DNN Model Scheduling (ADMS) strategy for optimizing multi-DNN inference on mobile heterogeneous processors. ADMS constructs an optimal subgraph partitioning strategy offline, balancing hardware operation support and scheduling granularity, and uses a processor-state-aware algorithm to dynamically adjust workloads based on real-time conditions. This ensures efficient workload distribution and maximizes processor utilization. Experiments show ADMS reduces multi-DNN inference latency by 4.04 times compared to vanilla frameworks.",
        "authors": [
            "Yunquan Gao",
            "Zhiguo Zhang",
            "Praveen Kumar Donta",
            "Chinmaya Kumar Dehury",
            "Xiujun Wang",
            "Dusit Niyato",
            "Qiyang Zhang"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-03-27"
    },
    "http://arxiv.org/abs/2503.21033v1": {
        "id": "http://arxiv.org/abs/2503.21033v1",
        "title": "Scalability Evaluation of HPC Multi-GPU Training for ECG-based LLMs",
        "link": "http://arxiv.org/abs/2503.21033v1",
        "tags": [
            "training",
            "multi-modal"
        ],
        "relevant": true,
        "indexed_date": "2025-03-28",
        "tldr": "Investigates scalability of multi-GPU training for ECG-based language models on HPC. Compares frameworks like Horovod, DeepSpeed, and PyTorch for distributed training. Achieves 1.6x speedup on two GPUs and 1.9x on four with sub-linear scaling.",
        "abstract": "Training large language models requires extensive processing, made possible by many high-performance computing resources. This study compares multi-node and multi-GPU environments for training large language models of electrocardiograms. It provides a detailed mapping of current frameworks for distributed deep learning in multinode and multi-GPU settings, including Horovod from Uber, DeepSpeed from Microsoft, and the built-in distributed capabilities of PyTorch and TensorFlow. We compare various multi-GPU setups for different dataset configurations, utilizing multiple HPC nodes independently and focusing on scalability, speedup, efficiency, and overhead. The analysis leverages HPC infrastructure with SLURM, Apptainer (Singularity) containers, CUDA, PyTorch, and shell scripts to support training workflows and automation. We achieved a sub-linear speedup when scaling the number of GPUs, with values of 1.6x for two and 1.9x for four.",
        "authors": [
            "Dimitar Mileski",
            "Nikola Petrovski",
            "Marjan Gusev"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-03-26"
    },
    "http://arxiv.org/abs/2503.20552v1": {
        "id": "http://arxiv.org/abs/2503.20552v1",
        "title": "Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation",
        "link": "http://arxiv.org/abs/2503.20552v1",
        "tags": [
            "serving",
            "disaggregation",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-03-27",
        "tldr": "Addresses low resource utilization in disaggregated LLM serving. Proposes Adrenaline, an attention disaggregation mechanism that offloads decoding-phase attention computations to prefill instances using synchronization, colocation, and scheduling. Achieves up to 1.68x higher inference throughput.",
        "abstract": "In large language model (LLM) serving systems, executing each request consists of two phases: the compute-intensive prefill phase and the memory-intensive decoding phase. To prevent performance interference between the two phases, current LLM serving systems typically adopt prefill-decoding disaggregation, where the two phases are split across separate machines. However, we observe this approach leads to significant resource underutilization. Specifically, prefill instances that are compute-intensive suffer from low memory utilization, while decoding instances that are memory-intensive experience low compute utilization. To address this problem, this paper proposes Adrenaline, an attention disaggregation and offloading mechanism designed to enhance resource utilization and performance in LLM serving systems. Adrenaline's key innovation lies in disaggregating part of the attention computation in the decoding phase and offloading them to prefill instances. The memory-bound nature of decoding-phase attention computation inherently enables an effective offloading strategy, yielding two complementary advantages: 1) improved memory capacity and bandwidth utilization in prefill instances, and 2) increased decoding batch sizes that enhance compute utilization in decoding instances, collectively boosting overall system performance. Adrenaline achieves these gains through three key techniques: low-latency decoding synchronization, resource-efficient prefill colocation, and load-aware offloading scheduling. Experimental results show that Adrenaline achieves 2.28x higher memory capacity and 2.07x better memory bandwidth utilization in prefill instances, up to 1.67x improvements in compute utilization for decoding instances, and 1.68x higher overall inference throughput compared to state-of-the-art systems.",
        "authors": [
            "Yunkai Liang",
            "Zhangyu Chen",
            "Pengfei Zuo",
            "Zhi Zhou",
            "Xu Chen",
            "Zhou Yu"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-03-26"
    },
    "http://arxiv.org/abs/2503.20507v3": {
        "id": "http://arxiv.org/abs/2503.20507v3",
        "title": "Harmonia: A Multi-Agent Reinforcement Learning Approach to Data Placement and Migration in Hybrid Storage Systems",
        "link": "http://arxiv.org/abs/2503.20507v3",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-03-27",
        "tldr": "",
        "abstract": "Hybrid storage systems (HSS) integrate multiple storage devices with diverse characteristics to deliver high performance and capacity at low cost. The performance of an HSS highly depends on the effectiveness of two key policies: (1) the data-placement policy, which determines the best-fit storage device for incoming data, and (2) the data-migration policy, which dynamically rearranges stored data (i.e., prefetches hot data and evicts cold data) across the devices to sustain high HSS performance. Prior works optimize either data placement or data migration in isolation, which leads to suboptimal HSS performance. Unfortunately, no prior work tries to optimize both policies together.   Our goal is to design a holistic data-management technique that optimizes both data-placement and data-migration policies to fully exploit the potential of an HSS, and thus significantly improve system performance. We propose Harmonia, a multi-agent reinforcement learning (RL)-based data-management technique that employs two lightweight autonomous RL agents, a data-placement agent and a data-migration agent, that adapt their policies for the current workload and HSS configuration while coordinating with each other to improve overall HSS performance.   We evaluate Harmonia on real HSS configurations with up to four heterogeneous storage devices and seventeen data-intensive workloads. On performance-optimized (cost-optimized) HSS with two storage devices, Harmonia outperforms the best-performing prior approach by 49.5% (31.7%) on average. On an HSS with three (four) devices, Harmonia outperforms the best-performing prior work by 37.0% (42.0%) on average. Harmonia's performance benefits come with low latency (240ns for inference) and storage overheads (206 KiB in DRAM for both RL agents combined). We will open-source Harmonia's implementation to aid future research on HSS.",
        "authors": [
            "Rakesh Nadig",
            "Vamanan Arulchelvan",
            "Rahul Bera",
            "Taha Shahroodi",
            "Gagandeep Singh",
            "Andreas Kakolyris",
            "Mohammad Sadrosadati",
            "Jisung Park",
            "Onur Mutlu"
        ],
        "categories": [
            "cs.AR",
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-03-26"
    },
    "http://arxiv.org/abs/2503.20263v1": {
        "id": "http://arxiv.org/abs/2503.20263v1",
        "title": "L4: Diagnosing Large-scale LLM Training Failures via Automated Log Analysis",
        "link": "http://arxiv.org/abs/2503.20263v1",
        "tags": [
            "training",
            "storage",
            "autoscaling"
        ],
        "relevant": true,
        "indexed_date": "2025-03-27",
        "tldr": "Proposes L4, an automated log analysis framework for diagnosing failures in large-scale LLM training. Utilizes cross-job, spatial, and temporal log patterns to extract failure indicators. Reduces diagnosis time by 68% and identifies faults with 92% accuracy.",
        "abstract": "As Large Language Models (LLMs) show their capabilities across various applications, training customized LLMs has become essential for modern enterprises. However, due to the complexity of LLM training, which requires massive computational resources and extensive training time, failures are inevitable during the training process. These failures result in considerable waste of resource and time, highlighting the critical need for effective and efficient failure diagnosis to reduce the cost of LLM training.   In this paper, we present the first empirical study on the failure reports of 428 LLM training failures in our production Platform-X between May 2023 and April 2024. Our study reveals that hardware and user faults are the predominant root causes, and current diagnosis processes rely heavily on training logs. Unfortunately, existing log-based diagnostic methods fall short in handling LLM training logs. Considering the unique features of LLM training, we identify three distinct patterns of LLM training logs: cross-job, spatial, and temporal patterns. We then introduce our Log-based Large-scale LLM training failure diagnosis framework, L4, which can automatically extract failure-indicating information (i.e., log events, nodes, stages, and iterations) from extensive training logs, thereby reducing manual effort and facilitating failure recovery. Experimental results on real-world datasets show that L4 outperforms existing approaches in identifying failure-indicating logs and localizing faulty nodes. Furthermore, L4 has been applied in Platform-X and demonstrated its effectiveness in enabling accurate and efficient failure diagnosis.",
        "authors": [
            "Zhihan Jiang",
            "Junjie Huang",
            "Zhuangbin Chen",
            "Yichen Li",
            "Guangba Yu",
            "Cong Feng",
            "Yongqiang Yang",
            "Zengyin Yang",
            "Michael R. Lyu"
        ],
        "categories": [
            "cs.SE",
            "cs.DC"
        ],
        "submit_date": "2025-03-26"
    },
    "http://arxiv.org/abs/2503.19050v1": {
        "id": "http://arxiv.org/abs/2503.19050v1",
        "title": "Mist: Efficient Distributed Training of Large Language Models via Memory-Parallelism Co-Optimization",
        "link": "http://arxiv.org/abs/2503.19050v1",
        "tags": [
            "training",
            "sparse"
        ],
        "relevant": true,
        "indexed_date": "2025-03-26",
        "tldr": "Proposes Mist, an automatic co-optimization system for distributed LLM training via memory-parallelism techniques. Uses fine-grained overlap scheduling, symbolic performance analysis, and imbalance-aware tuning. Achieves 1.28–2.04× speedup over Megatron-LM and Aceso.",
        "abstract": "Various parallelism, such as data, tensor, and pipeline parallelism, along with memory optimizations like activation checkpointing, redundancy elimination, and offloading, have been proposed to accelerate distributed training for Large Language Models. To find the best combination of these techniques, automatic distributed training systems are proposed. However, existing systems only tune a subset of optimizations, due to the lack of overlap awareness, inability to navigate the vast search space, and ignoring the inter-microbatch imbalance, leading to sub-optimal performance. To address these shortcomings, we propose Mist, a memory, overlap, and imbalance-aware automatic distributed training system that comprehensively co-optimizes all memory footprint reduction techniques alongside parallelism. Mist is based on three key ideas: (1) fine-grained overlap-centric scheduling, orchestrating optimizations in an overlapped manner, (2) symbolic-based performance analysis that predicts runtime and memory usage using symbolic expressions for fast tuning, and (3) imbalance-aware hierarchical tuning, decoupling the process into an inter-stage imbalance and overlap aware Mixed Integer Linear Programming problem and an intra-stage Dual-Objective Constrained Optimization problem, and connecting them through Pareto frontier sampling. Our evaluation results show that Mist achieves an average of 1.28$\\times$ (up to 1.73$\\times$) and 1.27$\\times$ (up to 2.04$\\times$) speedup compared to state-of-the-art manual system Megatron-LM and state-of-the-art automatic system Aceso, respectively.",
        "authors": [
            "Zhanda Zhu",
            "Christina Giannoula",
            "Muralidhar Andoorveedu",
            "Qidong Su",
            "Karttikeya Mangalam",
            "Bojian Zheng",
            "Gennady Pekhimenko"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-03-24"
    },
    "http://arxiv.org/abs/2503.18292v1": {
        "id": "http://arxiv.org/abs/2503.18292v1",
        "title": "Jenga: Effective Memory Management for Serving LLM with Heterogeneity",
        "link": "http://arxiv.org/abs/2503.18292v1",
        "tags": [
            "serving",
            "offloading",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-03-25",
        "tldr": "Addresses memory fragmentation and inefficient caching in LLM serving due to embedding heterogeneity. Jenga uses a two-level allocator with LCM-based sizing and layer-specific caching APIs. Achieves up to 4.92x throughput gain and 79.6% better memory utilization.",
        "abstract": "Large language models (LLMs) are widely used but expensive to run, especially as inference workloads grow. To lower costs, maximizing the request batch size by managing GPU memory efficiently is crucial. While PagedAttention has recently been proposed to improve the efficiency of memory management, we find that the growing heterogeneity in the embeddings dimensions, attention, and access patterns of modern LLM architectures introduces new challenges for memory allocation.   In this paper, we present Jenga, a novel memory allocation framework for heterogeneous embeddings in LLMs. Jenga tackles two key challenges: (1) minimizing memory fragmentation when managing embeddings of different sizes, and (2) enabling flexible caching and eviction policies tailored to the specific token-dependency patterns of various layers. Jenga employs a two-level memory allocator, leveraging the least common multiple (LCM) of embedding sizes to optimize memory usage and providing APIs to express layer-specific caching logic to enhance memory reuse.   We implemente Jenga on vLLM, a state-of-the-art LLM inference engine, and evaluate it with diverse LLMs, datasets, and GPU configurations. Evaluations show that Jenga improves GPU memory utilization by up to 79.6%, and increases serving throughput by up to 4.92x (1.80x on average).",
        "authors": [
            "Chen Zhang",
            "Kuntai Du",
            "Shu Liu",
            "Woosuk Kwon",
            "Xiangxi Mo",
            "Yufeng Wang",
            "Xiaoxuan Liu",
            "Kaichao You",
            "Zhuohan Li",
            "Mingsheng Long",
            "Jidong Zhai",
            "Joseph Gonzalez",
            "Ion Stoica"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-03-24"
    },
    "http://arxiv.org/abs/2503.18265v1": {
        "id": "http://arxiv.org/abs/2503.18265v1",
        "title": "Risk Management for Distributed Arbitrage Systems: Integrating Artificial Intelligence",
        "link": "http://arxiv.org/abs/2503.18265v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-03-25",
        "tldr": "",
        "abstract": "Effective risk management solutions become absolutely crucial when financial markets embrace distributed technology and decentralized financing (DeFi). This study offers a thorough survey and comparative analysis of the integration of artificial intelligence (AI) in risk management for distributed arbitrage systems. We examine several modern caching techniques namely in memory caching, distributed caching, and proxy caching and their functions in enhancing performance in decentralized settings. Through literature review we examine the utilization of AI techniques for alleviating risks related to market volatility, liquidity challenges, operational failures, regulatory compliance, and security threats. This comparison research evaluates various case studies from prominent DeFi technologies, emphasizing critical performance metrics like latency reduction, load balancing, and system resilience. Additionally, we examine the problems and trade offs associated with these technologies, emphasizing their effects on consistency, scalability, and fault tolerance. By meticulously analyzing real world applications, specifically centering on the Aave platform as our principal case study, we illustrate how the purposeful amalgamation of AI with contemporary caching methodologies has revolutionized risk management in distributed arbitrage systems.",
        "authors": [
            "Akaash Vishal Hazarika",
            "Mahak Shah",
            "Swapnil Patil",
            "Pradyumna Shukla"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.LG"
        ],
        "submit_date": "2025-03-24"
    },
    "http://arxiv.org/abs/2503.17924v1": {
        "id": "http://arxiv.org/abs/2503.17924v1",
        "title": "WLB-LLM: Workload-Balanced 4D Parallelism for Large Language Model Training",
        "link": "http://arxiv.org/abs/2503.17924v1",
        "tags": [
            "training",
            "offline"
        ],
        "relevant": true,
        "indexed_date": "2025-03-25",
        "tldr": "Addresses workload imbalance in 4D parallel LLM training. Proposes workload-aware document packing for pipeline parallelism and per-document sharding for context parallelism. Achieves average 1.23x speedup in end-to-end training throughput.",
        "abstract": "In this work, we present WLB-LLM, a workLoad-balanced 4D parallelism for large language model training. We first thoroughly analyze the workload imbalance issue in LLM training and identify two primary sources of imbalance at the pipeline parallelism and context parallelism levels. Then, to address the imbalance issue, at the pipeline parallelism level, WLB-LLM incorporates a workload-aware variable-length document packing method to balance the computation and communication workload across micro-batches. Additionally, at the context parallelism level, WLB-LLM introduces a novel fine-grained per-document sharding strategy, ensuring each worker within a context parallelism group has an identical workload. Comprehensive experiments under different model scales demonstrate that WLB-LLM significantly mitigates the workload imbalance during 4D parallelism LLM training and achieves an average speedup of 1.23x when applying WLB-LLM in our internal LLM training framework.",
        "authors": [
            "Zheng Wang",
            "Anna Cai",
            "Xinfeng Xie",
            "Zaifeng Pan",
            "Yue Guan",
            "Weiwei Chu",
            "Jie Wang",
            "Shikai Li",
            "Jianyu Huang",
            "Chris Cai",
            "Yuchen Hao",
            "Yufei Ding"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.LG"
        ],
        "submit_date": "2025-03-23"
    },
    "http://arxiv.org/abs/2503.17707v1": {
        "id": "http://arxiv.org/abs/2503.17707v1",
        "title": "PipeBoost: Resilient Pipelined Architecture for Fast Serverless LLM Scaling",
        "link": "http://arxiv.org/abs/2503.17707v1",
        "tags": [
            "serving",
            "offloading",
            "autoscaling"
        ],
        "relevant": true,
        "indexed_date": "2025-03-25",
        "tldr": "Proposes PipeBoost, a low-latency LLM serving system using fault-tolerant pipeline parallelism across model loading and inference on multi-GPU clusters to handle bursty requests. Achieves 31-49.8% lower inference latency and cold-start latencies down to hundreds of microseconds for models like OPT-1.3B.",
        "abstract": "This paper presents PipeBoost, a low-latency LLM serving system for multi-GPU (serverless) clusters, which can rapidly launch inference services in response to bursty requests without preemptively over-provisioning GPUs. Many LLM inference tasks rely on the same base model (e.g., LoRA). To leverage this, PipeBoost introduces fault-tolerant pipeline parallelism across both model loading and inference stages. This approach maximizes aggregate PCIe bandwidth and parallel computation across GPUs, enabling faster generation of the first token. PipeBoost also introduces recovery techniques that enable uninterrupted inference services by utilizing the shared advantages of multiple GPUs. Experimental results show that, compared to state-of-the-art low-latency LLM serving systems, PipeBoost reduces inference latency by 31% to 49.8%. For certain models (e.g., OPT-1.3B), PipeBoost achieves cold-start latencies in the range of a few hundred microseconds.",
        "authors": [
            "Chongpeng Liu",
            "Xiaojian Liao",
            "Hancheng Liu",
            "Limin Xiao",
            "Jianxin Li"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-03-22"
    },
    "http://arxiv.org/abs/2503.17603v1": {
        "id": "http://arxiv.org/abs/2503.17603v1",
        "title": "A Generative Caching System for Large Language Models",
        "link": "http://arxiv.org/abs/2503.17603v1",
        "tags": [
            "serving",
            "RAG",
            "offline"
        ],
        "relevant": true,
        "indexed_date": "2025-03-25",
        "tldr": "Proposes a generative caching system to reduce LLM inference latency and cost. Introduces generative caching that synthesizes new responses from cached data, and optimizes caching algorithms for cost-latency-quality tradeoffs. Achieves significantly faster performance compared to GPTcache.",
        "abstract": "Caching has the potential to be of significant benefit for accessing large language models (LLMs) due to their high latencies which typically range from a small number of seconds to well over a minute. Furthermore, many LLMs charge money for queries; caching thus has a clear monetary benefit. This paper presents a new caching system for improving user experiences with LLMs. In addition to reducing both latencies and monetary costs for accessing LLMs, our system also provides important features that go beyond the performance benefits typically associated with caches. A key feature we provide is generative caching, wherein multiple cached responses can be synthesized to provide answers to queries which have never been seen before. Our generative caches function as repositories of valuable information which can be mined and analyzed. We also improve upon past semantic caching techniques by tailoring the caching algorithms to optimally balance cost and latency reduction with the quality of responses provided. Performance tests indicate that our caches are considerably faster than GPTcache.",
        "authors": [
            "Arun Iyengar",
            "Ashish Kundu",
            "Ramana Kompella",
            "Sai Nandan Mamidi"
        ],
        "categories": [
            "cs.DB",
            "cs.AI",
            "cs.DC",
            "cs.NI"
        ],
        "submit_date": "2025-03-22"
    },
    "http://arxiv.org/abs/2503.16893v1": {
        "id": "http://arxiv.org/abs/2503.16893v1",
        "title": "Improving the End-to-End Efficiency of Offline Inference for Multi-LLM Applications Based on Sampling and Simulation",
        "link": "http://arxiv.org/abs/2503.16893v1",
        "tags": [
            "offline",
            "sparse",
            "multi-modal"
        ],
        "relevant": true,
        "indexed_date": "2025-03-24",
        "tldr": "Proposes SamuLLM to enhance offline inference efficiency for multi-LLM applications. Uses sampling-then-simulation to estimate output lengths and model running time, with a greedy scheduler for concurrent execution. Achieves 1.0-2.4× end-to-end speedup.",
        "abstract": "As large language models (LLMs) have shown great success in many tasks, they are used in various applications. While a lot of works have focused on the efficiency of single-LLM application (e.g., offloading, request scheduling, parallelism strategy selection), multi-LLM applications receive less attention, particularly in offline inference scenarios. In this work, we aim to improve the offline end-to-end inference efficiency of multi-LLM applications in the single-node multi-GPU environment. The problem involves two key decisions: (1) determining which LLMs to run concurrently each time (we may not run all the models at the same time), and (2) selecting a parallelism strategy to use for each LLM. This problem is NP-hard. Naive solutions may not work well because the running time for a model to complete a set of requests depends on the request workload and the selected parallelism strategy, and they lack an accurate model of the running time. As the LLM output lengths are unknown before running, to estimate the model running time, we propose a sampling-then-simulation method which first estimates the output lengths by sampling from an empirical cumulative function we obtained from a large dataset in advance, and then simulates the LLM inference process accordingly. Based on the simulation, we estimate the per-iteration latencys to get the total latency. A greedy method is proposed to optimize the scheduling of the LLMs in the application across the GPUs. We then propose a framework SamuLLM which contains two phases: planning, which calls the greedy method for an application and running, which runs the application and dynamically adjust the model scheduling based on the runtime information. Experiments on 3 applications and a mixed application show that SamuLLM can achieve 1.0-2.4$\\times$ end-to-end speedups compared to the competitors.",
        "authors": [
            "Jingzhi Fang",
            "Yanyan Shen",
            "Yue Wang",
            "Lei Chen"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-03-21"
    },
    "http://arxiv.org/abs/2503.16585v1": {
        "id": "http://arxiv.org/abs/2503.16585v1",
        "title": "Distributed LLMs and Multimodal Large Language Models: A Survey on Advances, Challenges, and Future Directions",
        "link": "http://arxiv.org/abs/2503.16585v1",
        "tags": [
            "training",
            "multi-modal",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-03-24",
        "tldr": "Surveys distributed computing solutions for large and multimodal language models. Focuses on decentralized techniques for training, inference, fine-tuning, and deployment to address scalability and privacy challenges. Highlights gaps and future directions for robustness across heterogeneous resources.",
        "abstract": "Language models (LMs) are machine learning models designed to predict linguistic patterns by estimating the probability of word sequences based on large-scale datasets, such as text. LMs have a wide range of applications in natural language processing (NLP) tasks, including autocomplete and machine translation. Although larger datasets typically enhance LM performance, scalability remains a challenge due to constraints in computational power and resources. Distributed computing strategies offer essential solutions for improving scalability and managing the growing computational demand. Further, the use of sensitive datasets in training and deployment raises significant privacy concerns. Recent research has focused on developing decentralized techniques to enable distributed training and inference while utilizing diverse computational resources and enabling edge AI. This paper presents a survey on distributed solutions for various LMs, including large language models (LLMs), vision language models (VLMs), multimodal LLMs (MLLMs), and small language models (SLMs). While LLMs focus on processing and generating text, MLLMs are designed to handle multiple modalities of data (e.g., text, images, and audio) and to integrate them for broader applications. To this end, this paper reviews key advancements across the MLLM pipeline, including distributed training, inference, fine-tuning, and deployment, while also identifying the contributions, limitations, and future areas of improvement. Further, it categorizes the literature based on six primary focus areas of decentralization. Our analysis describes gaps in current methodologies for enabling distributed solutions for LMs and outline future research directions, emphasizing the need for novel solutions to enhance the robustness and applicability of distributed LMs.",
        "authors": [
            "Hadi Amini",
            "Md Jueal Mia",
            "Yasaman Saadati",
            "Ahmed Imteaj",
            "Seyedsina Nabavirazavi",
            "Urmish Thakker",
            "Md Zarif Hossain",
            "Awal Ahmed Fime",
            "S. S. Iyengar"
        ],
        "categories": [
            "cs.CL",
            "cs.CV",
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-03-20"
    },
    "http://arxiv.org/abs/2503.15921v1": {
        "id": "http://arxiv.org/abs/2503.15921v1",
        "title": "SPIN: Accelerating Large Language Model Inference with Heterogeneous Speculative Models",
        "link": "http://arxiv.org/abs/2503.15921v1",
        "tags": [
            "serving",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-03-21",
        "tldr": "Addresses inefficiencies in speculative decoding for LLM inference by using heterogeneous SSMs to handle varying request difficulty, request decomposition for verification batching, and pipeline orchestration. Achieves 2.28x speedup over state-of-the-art methods.",
        "abstract": "Speculative decoding has been shown as an effective way to accelerate Large Language Model (LLM) inference by using a Small Speculative Model (SSM) to generate candidate tokens in a so-called speculation phase, which are subsequently verified by the LLM in a verification phase. However, current state-of-the-art speculative decoding approaches have three key limitations: handling requests with varying difficulty using homogeneous SSMs, lack of robust support for batch processing, and insufficient holistic optimization for both speculation and verification phases. In this paper, we introduce SPIN, an efficient LLM inference serving system based on speculative decoding, designed to address these challenges through three main innovations. First, SPIN improves token speculation by using multiple heterogeneous SSMs, with a learning-based algorithm for SSM selection that operates without prior knowledge of request difficulty. Second, SPIN employs a request decomposition method to minimize batching overhead during LLM verification. Finally, SPIN orchestrates speculation and verification phases by pipelining their executions on GPUs to achieve further acceleration. Experimental results demonstrate that SPIN significantly outperforms state-of-the-art methods, achieving a performance increase of approximately 2.28X.",
        "authors": [
            "Fahao Chen",
            "Peng Li",
            "Tom H. Luan",
            "Zhou Su",
            "Jing Deng"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-03-20"
    },
    "http://arxiv.org/abs/2503.15758v2": {
        "id": "http://arxiv.org/abs/2503.15758v2",
        "title": "ATTENTION2D: Communication Efficient Distributed Self-Attention Mechanism",
        "link": "http://arxiv.org/abs/2503.15758v2",
        "tags": [
            "training",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-03-21",
        "tldr": "Addresses communication inefficiency in distributed self-attention computation. Proposes ATTENTION2D, which parallelizes along query and key/value dimensions. Achieves up to 9.4× faster performance on 64 GPUs compared to Ring Attention.",
        "abstract": "Transformer-based models have emerged as a leading architecture for natural language processing, natural language generation, and image generation tasks. A fundamental element of the transformer architecture is self-attention, which allows the model to capture intricate dependencies within the data. However, the self-attention mechanism also incurs significant computational and memory costs, particularly for long sequences.   In this paper, we introduce ATTENTION2D, a novel approach that exploits parallelism along two dimensions - query and key/value - of the self-attention operation. This method enables efficient distribution and parallelization of computations across multiple devices. Our approach facilitates asymptotically faster training and inference phases compared to previous methods, without relying on approximations or incurring additional computational or memory overheads. Furthermore, unlike existing techniques that struggle to scale with an increasing number of processing units, our approach effectively scales with additional processing units.   Our experimental results confirm the effectiveness of our method in improving communication efficiency and scalability. Compared to Ring Attention, our approach demonstrated up to a 5x performance boost on a GPT-3-like model using 64 NVIDIA A100 GPUs across 16 nodes, and up to a 9.4x performance boost on 64 NVIDIA H100 GPUs across 64 nodes.",
        "authors": [
            "Venmugil Elango"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC"
        ],
        "submit_date": "2025-03-20"
    },
    "http://arxiv.org/abs/2503.15252v1": {
        "id": "http://arxiv.org/abs/2503.15252v1",
        "title": "Efficient allocation of image recognition and LLM tasks on multi-GPU system",
        "link": "http://arxiv.org/abs/2503.15252v1",
        "tags": [
            "training",
            "multi-modal",
            "quantization"
        ],
        "relevant": true,
        "indexed_date": "2025-03-20",
        "tldr": "Assesses parallelization strategies for image recognition and LLM training in multi-GPU systems. Evaluates data parallelism, distributed processing, and PyTorch's mechanisms using NVIDIA H100. Achieves reduced iteration time and optimized data utilization for heterogeneous workloads.",
        "abstract": "This work is concerned with the evaluation of the performance of parallelization of learning and tuning processes for image classification and large language models. For machine learning model in image recognition, various parallelization methods are developed based on different hardware and software scenarios: simple data parallelism, distributed data parallelism, and distributed processing. A detailed description of presented strategies is given, highlighting the challenges and benefits of their application. Furthermore, the impact of different dataset types on the tuning process of large language models is investigated. Experiments show to what extent the task type affects the iteration time in a multi-GPU environment, offering valuable insights into the optimal data utilization strategies to improve model performance. Furthermore, this study leverages the built-in parallelization mechanisms of PyTorch that can facilitate these tasks. Furthermore, performance profiling is incorporated into the study to thoroughly evaluate the impact of memory and communication operations during the training/tuning procedure. Test scenarios are developed and tested with numerous benchmarks on the NVIDIA H100 architecture showing efficiency through selected metrics.",
        "authors": [
            "Marcin Lawenda",
            "Krzesimir Samborski",
            "Kyrylo Khloponin",
            "Łukasz Szustak"
        ],
        "categories": [
            "cs.DC",
            "cs.PF"
        ],
        "submit_date": "2025-03-19"
    },
    "http://arxiv.org/abs/2503.14932v1": {
        "id": "http://arxiv.org/abs/2503.14932v1",
        "title": "Prada: Black-Box LLM Adaptation with Private Data on Resource-Constrained Devices",
        "link": "http://arxiv.org/abs/2503.14932v1",
        "tags": [
            "offline",
            "edge",
            "LoRA"
        ],
        "relevant": true,
        "indexed_date": "2025-03-20",
        "tldr": "Proposes Prada, a privacy-preserving LLM adaptation system for edge devices using a locally fine-tuned proxy model with LoRA and logits offset-based inference refinement. Achieves 60% computational overhead reduction and 80% communication cost savings compared to centralized methods.",
        "abstract": "In recent years, Large Language Models (LLMs) have demonstrated remarkable abilities in various natural language processing tasks. However, adapting these models to specialized domains using private datasets stored on resource-constrained edge devices, such as smartphones and personal computers, remains challenging due to significant privacy concerns and limited computational resources. Existing model adaptation methods either compromise data privacy by requiring data transmission or jeopardize model privacy by exposing proprietary LLM parameters. To address these challenges, we propose Prada, a novel privacy-preserving and efficient black-box LLM adaptation system using private on-device datasets. Prada employs a lightweight proxy model fine-tuned with Low-Rank Adaptation (LoRA) locally on user devices. During inference, Prada leverages the logits offset, i.e., difference in outputs between the base and adapted proxy models, to iteratively refine outputs from a remote black-box LLM. This offset-based adaptation approach preserves both data privacy and model privacy, as there is no need to share sensitive data or proprietary model parameters. Furthermore, we incorporate speculative decoding to further speed up the inference process of Prada, making the system practically deployable on bandwidth-constrained edge devices, enabling a more practical deployment of Prada. Extensive experiments on various downstream tasks demonstrate that Prada achieves performance comparable to centralized fine-tuning methods while significantly reducing computational overhead by up to 60% and communication costs by up to 80%.",
        "authors": [
            "Ziyao Wang",
            "Yexiao He",
            "Zheyu Shen",
            "Yu Li",
            "Guoheng Sun",
            "Myungjin Lee",
            "Ang Li"
        ],
        "categories": [
            "cs.CR",
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-03-19"
    },
    "http://arxiv.org/abs/2503.14649v2": {
        "id": "http://arxiv.org/abs/2503.14649v2",
        "title": "RAGO: Systematic Performance Optimization for Retrieval-Augmented Generation Serving",
        "link": "http://arxiv.org/abs/2503.14649v2",
        "tags": [
            "RAG",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-03-20",
        "tldr": "Addresses inefficient retrieval-augmented generation (RAG) serving due to workload variability. Proposes RAGSchema abstraction and RAGO optimization framework for flexible serving optimizations. Achieves up to 2× higher QPS per chip and 55% lower time-to-first-token latency.",
        "abstract": "Retrieval-augmented generation (RAG), which combines large language models (LLMs) with retrievals from external knowledge databases, is emerging as a popular approach for reliable LLM serving. However, efficient RAG serving remains an open challenge due to the rapid emergence of many RAG variants and the substantial differences in workload characteristics across them. In this paper, we make three fundamental contributions to advancing RAG serving. First, we introduce RAGSchema, a structured abstraction that captures the wide range of RAG algorithms, serving as a foundation for performance optimization. Second, we analyze several representative RAG workloads with distinct RAGSchema, revealing significant performance variability across these workloads. Third, to address this variability and meet diverse performance requirements, we propose RAGO (Retrieval-Augmented Generation Optimizer), a system optimization framework for efficient RAG serving. Our evaluation shows that RAGO achieves up to a 2x increase in QPS per chip and a 55% reduction in time-to-first-token latency compared to RAG systems built on LLM-system extensions.",
        "authors": [
            "Wenqi Jiang",
            "Suvinay Subramanian",
            "Cat Graves",
            "Gustavo Alonso",
            "Amir Yazdanbakhsh",
            "Vidushi Dadu"
        ],
        "categories": [
            "cs.IR",
            "cs.AI",
            "cs.CL",
            "cs.DC"
        ],
        "submit_date": "2025-03-18"
    },
    "http://arxiv.org/abs/2503.13772v1": {
        "id": "http://arxiv.org/abs/2503.13772v1",
        "title": "Do Large Language Models Understand Performance Optimization?",
        "link": "http://arxiv.org/abs/2503.13772v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-03-19",
        "tldr": "",
        "abstract": "Large Language Models (LLMs) have emerged as powerful tools for software development tasks such as code completion, translation, and optimization. However, their ability to generate efficient and correct code, particularly in complex High-Performance Computing (HPC) contexts, has remained underexplored. To address this gap, this paper presents a comprehensive benchmark suite encompassing multiple critical HPC computational motifs to evaluate the performance of code optimized by state-of-the-art LLMs, including OpenAI o1, Claude-3.5, and Llama-3.2. In addition to analyzing basic computational kernels, we developed an agent system that integrates LLMs to assess their effectiveness in real HPC applications. Our evaluation focused on key criteria such as execution time, correctness, and understanding of HPC-specific concepts. We also compared the results with those achieved using traditional HPC optimization tools. Based on the findings, we recognized the strengths of LLMs in understanding human instructions and performing automated code transformations. However, we also identified significant limitations, including their tendency to generate incorrect code and their challenges in comprehending complex control and data flows in sophisticated HPC code.",
        "authors": [
            "Bowen Cui",
            "Tejas Ramesh",
            "Oscar Hernandez",
            "Keren Zhou"
        ],
        "categories": [
            "cs.DC",
            "cs.SE"
        ],
        "submit_date": "2025-03-17"
    },
    "http://arxiv.org/abs/2503.12228v1": {
        "id": "http://arxiv.org/abs/2503.12228v1",
        "title": "Adaptive Fault Tolerance Mechanisms of Large Language Models in Cloud Computing Environments",
        "link": "http://arxiv.org/abs/2503.12228v1",
        "tags": [
            "training",
            "offloading",
            "storage"
        ],
        "relevant": true,
        "indexed_date": "2025-03-18",
        "tldr": "Proposes adaptive fault tolerance for LLMs in cloud environments using dynamic resource allocation and anomaly detection. Integrates adaptive checkpointing and recovery to minimize downtime. Reduces system downtime by 30% compared to classical mechanisms.",
        "abstract": "With the rapid evolution of Large Language Models (LLMs) and their large-scale experimentation in cloud-computing spaces, the challenge of guaranteeing their security and efficiency in a failure scenario has become a main issue. To ensure the reliability and availability of large-scale language models in cloud computing scenarios, such as frequent resource failures, network problems, and computational overheads, this study proposes a novel adaptive fault tolerance mechanism. It builds upon known fault-tolerant mechanisms, such as checkpointing, redundancy, and state transposition, introducing dynamic resource allocation and prediction of failure based on real-time performance metrics. The hybrid model integrates data driven deep learning-based anomaly detection technique underlining the contribution of cloud orchestration middleware for predictive prevention of system failures. Additionally, the model integrates adaptive checkpointing and recovery strategies that dynamically adapt according to load and system state to minimize the influence on the performance of the model and minimize downtime. The experimental results demonstrate that the designed model considerably enhances the fault tolerance in large-scale cloud surroundings, and decreases the system downtime by $\\mathbf{30\\%}$, and has a better modeling availability than the classical fault tolerance mechanism.",
        "authors": [
            "Yihong Jin",
            "Ze Yang",
            "Xinhe Xu",
            "Yihan Zhang",
            "Shuyang Ji"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-03-15"
    },
    "http://arxiv.org/abs/2503.12185v1": {
        "id": "http://arxiv.org/abs/2503.12185v1",
        "title": "FAILS: A Framework for Automated Collection and Analysis of LLM Service Incidents",
        "link": "http://arxiv.org/abs/2503.12185v1",
        "tags": [
            "serving",
            "autoscaling",
            "offline"
        ],
        "relevant": true,
        "indexed_date": "2025-03-18",
        "tldr": "Proposes FAILS, an open-sourced framework for automated collection and analysis of LLM service incidents. Includes data scraping, processing, 17 failure analyses, and visualization to explore reliability metrics (MTTR, MTBF). Enables understanding of failure patterns to mitigate outages, supporting operational reliability.",
        "abstract": "Large Language Model (LLM) services such as ChatGPT, DALLE, and Cursor have quickly become essential for society, businesses, and individuals, empowering applications such as chatbots, image generation, and code assistance. The complexity of LLM systems makes them prone to failures and affects their reliability and availability, yet their failure patterns are not fully understood, making it an emerging problem. However, there are limited datasets and studies in this area, particularly lacking an open-access tool for analyzing LLM service failures based on incident reports. Addressing these problems, in this work we propose FAILS, the first open-sourced framework for incident reports collection and analysis on different LLM services and providers. FAILS provides comprehensive data collection, analysis, and visualization capabilities, including:(1) It can automatically collect, clean, and update incident data through its data scraper and processing components;(2) It provides 17 types of failure analysis, allowing users to explore temporal trends of incidents, analyze service reliability metrics, such as Mean Time to Recovery (MTTR) and Mean Time Between Failures (MTBF);(3) It leverages advanced LLM tools to assist in data analysis and interpretation, enabling users to gain observations and insights efficiently. All functions are integrated in the backend, allowing users to easily access them through a web-based frontend interface. FAILS supports researchers, engineers, and general users to understand failure patterns and further mitigate operational incidents and outages in LLM services. The framework is publicly available on https://github.com/atlarge-research/FAILS.",
        "authors": [
            "Sándor Battaglini-Fischer",
            "Nishanthi Srinivasan",
            "Bálint László Szarvas",
            "Xiaoyu Chu",
            "Alexandru Iosup"
        ],
        "categories": [
            "cs.PF",
            "cs.DC"
        ],
        "submit_date": "2025-03-15"
    },
    "http://arxiv.org/abs/2503.11023v3": {
        "id": "http://arxiv.org/abs/2503.11023v3",
        "title": "Beyond A Single AI Cluster: A Survey of Decentralized LLM Training",
        "link": "http://arxiv.org/abs/2503.11023v3",
        "tags": [
            "training",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-03-17",
        "tldr": "Surveys decentralized LLM training across clusters/datacenters to democratize resource access. Characterizes resource-driven approaches, categorizes community vs organizational methods, and taxonomizes advancements. Analyzes case studies for improving scalability and resource utilization.",
        "abstract": "The emergence of large language models (LLMs) has revolutionized AI development, yet the resource demands beyond a single cluster or even datacenter, limiting accessibility to well-resourced organizations. Decentralized training has emerged as a promising paradigm to leverage dispersed resources across clusters, datacenters and regions, offering the potential to democratize LLM development for broader communities. As the first comprehensive exploration of this emerging field, we present decentralized LLM training as a resource-driven paradigm and categorize existing efforts into community-driven and organizational approaches. We further clarify this through: (1) a comparison with related paradigms, (2) a characterization of decentralized resources, and (3) a taxonomy of recent advancements. We also provide up-to-date case studies and outline future directions to advance research in decentralized LLM training.",
        "authors": [
            "Haotian Dong",
            "Jingyan Jiang",
            "Rongwei Lu",
            "Jiajun Luo",
            "Jiajun Song",
            "Bowen Li",
            "Ying Shen",
            "Zhi Wang"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-03-14"
    },
    "http://arxiv.org/abs/2503.11244v1": {
        "id": "http://arxiv.org/abs/2503.11244v1",
        "title": "LLMPerf: GPU Performance Modeling meets Large Language Models",
        "link": "http://arxiv.org/abs/2503.11244v1",
        "tags": [
            "modeling"
        ],
        "relevant": true,
        "indexed_date": "2025-03-17",
        "tldr": "Explores using LLMs for GPU performance modeling of OpenCL programs. Designs an LLM-based estimator to predict execution times, addressing manual model limitations. Achieves 24.25% mean absolute error on generated validation set.",
        "abstract": "Performance modeling, a pivotal domain in program cost analysis, currently relies on manually crafted models constrained by various program and hardware limitations, especially in the intricate landscape of GPGPU. Meanwhile, Large Language Models (LLMs) have demonstrated their effectiveness in addressing diverse programming challenges. Our work establishes a connection between LLMs and performance modeling, employing the LLM as a performance estimator. Through experimental exploration with carefully designed large-scale OpenCL datasets, we highlight the potential capability as well as the main difficulties of using LLMs in handling performance modeling tasks for OpenCL device source programs. As the first study for this line of work, our LLM-based performance model achieves a mean absolute percentage error of $24.25\\%$ for a large-scale generated validation set. On a set of publicly available OpenCL programs, our model achieves a mean absolute percentage error of $46.1\\%$.",
        "authors": [
            "Khoi N. M. Nguyen",
            "Hoang Duy Nguyen Do",
            "Huyen Thao Le",
            "Thanh Tuan Dao"
        ],
        "categories": [
            "cs.PF",
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-03-14"
    },
    "http://arxiv.org/abs/2503.10725v1": {
        "id": "http://arxiv.org/abs/2503.10725v1",
        "title": "Samoyeds: Accelerating MoE Models with Structured Sparsity Leveraging Sparse Tensor Cores",
        "link": "http://arxiv.org/abs/2503.10725v1",
        "tags": [
            "MoE",
            "sparse",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-03-17",
        "tldr": "Proposes Samoyeds, a system accelerating MoE LLMs by applying structured sparsity to both activations and parameters via custom sparse data format and kernel. Achieves up to 1.99x kernel-level and 1.58x model-level speedup, with 4.41x average batch size increase.",
        "abstract": "The escalating size of Mixture-of-Experts (MoE) based Large Language Models (LLMs) presents significant computational and memory challenges, necessitating innovative solutions to enhance efficiency without compromising model accuracy. Structured sparsity emerges as a compelling strategy to address these challenges by leveraging the emerging sparse computing hardware. Prior works mainly focus on the sparsity in model parameters, neglecting the inherent sparse patterns in activations. This oversight can lead to additional computational costs associated with activations, potentially resulting in suboptimal performance.   This paper presents Samoyeds, an innovative acceleration system for MoE LLMs utilizing Sparse Tensor Cores (SpTCs). Samoyeds is the first to apply sparsity simultaneously to both activations and model parameters. It introduces a bespoke sparse data format tailored for MoE computation and develops a specialized sparse-sparse matrix multiplication kernel. Furthermore, Samoyeds incorporates systematic optimizations specifically designed for the execution of dual-side structured sparse MoE LLMs on SpTCs, further enhancing system performance. Evaluations show that Samoyeds outperforms SOTA works by up to 1.99$\\times$ at the kernel level and 1.58$\\times$ at the model level. Moreover, it enhances memory efficiency, increasing maximum supported batch sizes by 4.41$\\times$ on average. Additionally, Samoyeds surpasses existing SOTA structured sparse solutions in both model accuracy and hardware portability.",
        "authors": [
            "Chenpeng Wu",
            "Qiqi Gu",
            "Heng Shi",
            "Jianguo Yao",
            "Haibing Guan"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC",
            "cs.OS"
        ],
        "submit_date": "2025-03-13"
    },
    "http://arxiv.org/abs/2503.10377v1": {
        "id": "http://arxiv.org/abs/2503.10377v1",
        "title": "SPPO:Efficient Long-sequence LLM Training via Adaptive Sequence Pipeline Parallel Offloading",
        "link": "http://arxiv.org/abs/2503.10377v1",
        "tags": [
            "training",
            "offloading",
            "sparse"
        ],
        "relevant": true,
        "indexed_date": "2025-03-14",
        "tldr": "Addresses efficient training for long-sequence LLMs under limited GPU resources. Proposes SPPO with adaptive offloading and pipeline scheduling to optimize memory and computation. Achieves 3.38x throughput improvement over baseline frameworks at 4M token sequences.",
        "abstract": "In recent years, Large Language Models (LLMs) have exhibited remarkable capabilities, driving advancements in real-world applications. However, training LLMs on increasingly long input sequences imposes significant challenges due to high GPU memory and computational demands. Existing solutions face two key limitations: (1) memory reduction techniques, such as activation recomputation and CPU offloading, compromise training efficiency; (2) distributed parallelism strategies require excessive GPU resources, limiting the scalability of input sequence length.   To address these gaps, we propose Adaptive Sequence Pipeline Parallel Offloading (SPPO), a novel LLM training framework that optimizes memory and computational resource efficiency for long-sequence training. SPPO introduces adaptive offloading, leveraging sequence-aware offloading, and two-level activation management to reduce GPU memory consumption without degrading the training efficiency. Additionally, SPPO develops an adaptive pipeline scheduling approach with a heuristic solver and multiplexed sequence partitioning to improve computational resource efficiency. Experimental results demonstrate that SPPO achieves up to 3.38x throughput improvement over Megatron-LM and DeepSpeed, realizing efficient training of a 7B LLM with sequence lengths of up to 4M tokens on only 128 A100 GPUs.",
        "authors": [
            "Qiaoling Chen",
            "Shenggui Li",
            "Wei Gao",
            "Peng Sun",
            "Yonggang Wen",
            "Tianwei Zhang"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-03-13"
    },
    "http://arxiv.org/abs/2503.10325v2": {
        "id": "http://arxiv.org/abs/2503.10325v2",
        "title": "Collaborative Speculative Inference for Efficient LLM Inference Serving",
        "link": "http://arxiv.org/abs/2503.10325v2",
        "tags": [
            "serving",
            "offloading",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-03-14",
        "tldr": "Proposes CoSine, a collaborative speculative inference system that decouples drafting from parallel verification, using adaptive batch scheduling and token fusion to balance throughput. Achieves 23.2% lower latency and 32.5% higher throughput vs baselines at same cost.",
        "abstract": "Speculative inference is a promising paradigm employing small speculative models (SSMs) as drafters to generate draft tokens, which are subsequently verified in parallel by the target large language model (LLM). This approach enhances the efficiency of inference serving by reducing LLM inference latency and costs while preserving generation quality. However, existing speculative methods face critical challenges, including inefficient resource utilization and limited draft acceptance, which constrain their scalability and overall effectiveness. To overcome these obstacles, we present CoSine, a novel speculative inference system that decouples sequential speculative decoding from parallel verification, enabling efficient collaboration among multiple nodes. Specifically, CoSine routes inference requests to specialized drafters based on their expertise and incorporates a confidence-based token fusion mechanism to synthesize outputs from cooperating drafters, ensuring high-quality draft generation. Additionally, CoSine dynamically orchestrates the execution of speculative decoding and verification in a pipelined manner, employing batch scheduling to selectively group requests and adaptive speculation control to minimize idle periods. By optimizing parallel workflows through heterogeneous node collaboration, CoSine balances draft generation and verification throughput in real-time, thereby maximizing resource utilization. Experimental results demonstrate that CoSine achieves superior performance compared to state-of-the-art speculative approaches. Notably, with equivalent resource costs, CoSine achieves up to a 23.2% decrease in latency and a 32.5% increase in throughput compared to baseline methods.",
        "authors": [
            "Luyao Gao",
            "Jianchun Liu",
            "Hongli Xu",
            "Xichong Zhang",
            "Yunming Liao",
            "Liusheng Huang"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-03-13"
    },
    "http://arxiv.org/abs/2503.09716v1": {
        "id": "http://arxiv.org/abs/2503.09716v1",
        "title": "MoE-Gen: High-Throughput MoE Inference on a Single GPU with Module-Based Batching",
        "link": "http://arxiv.org/abs/2503.09716v1",
        "tags": [
            "MoE",
            "serving",
            "offline"
        ],
        "relevant": true,
        "indexed_date": "2025-03-14",
        "tldr": "Proposes MoE-Gen for high-throughput MoE inference on single GPU via module-based batching, accumulating tokens in host memory and launching large batches to optimize computation-communication overlap. Achieves 8-31x throughput gain over state-of-the-art systems on DeepSeek and Mixtral models for offline tasks.",
        "abstract": "This paper presents MoE-Gen, a high-throughput MoE inference system optimized for single-GPU execution. Existing inference systems rely on model-based or continuous batching strategies, originally designed for interactive inference, which result in excessively small batches for MoE's key modules-attention and expert modules-leading to poor throughput. To address this, we introduce module-based batching, which accumulates tokens in host memory and dynamically launches large batches on GPUs to maximize utilization. Additionally, we optimize the choice of batch sizes for each module in an MoE to fully overlap GPU computation and communication, maximizing throughput. Evaluation demonstrates that MoE-Gen achieves 8-31x higher throughput compared to state-of-the-art systems employing model-based batching (FlexGen, MoE-Lightning, DeepSpeed), and offers even greater throughput improvements over continuous batching systems (e.g., vLLM and Ollama) on popular MoE models (DeepSeek and Mixtral) across offline inference tasks. MoE-Gen's source code is publicly available at https://github.com/EfficientMoE/MoE-Gen",
        "authors": [
            "Tairan Xu",
            "Leyang Xue",
            "Zhan Lu",
            "Adrian Jackson",
            "Luo Mai"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-03-12"
    },
    "http://arxiv.org/abs/2503.08966v1": {
        "id": "http://arxiv.org/abs/2503.08966v1",
        "title": "Performance Models for a Two-tiered Storage System",
        "link": "http://arxiv.org/abs/2503.08966v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-03-13",
        "tldr": "",
        "abstract": "This work describes the design, implementation and performance analysis of a distributed two-tiered storage software. The first tier functions as a distributed software cache implemented using solid-state devices~(NVMes) and the second tier consists of multiple hard disks~(HDDs). We describe an online learning algorithm that manages data movement between the tiers. The software is hybrid, i.e. both distributed and multi-threaded. The end-to-end performance model of the two-tier system was developed using queuing networks and behavioral models of storage devices. We identified significant parameters that affect the performance of storage devices and created behavioral models for each device. The performance of the software was evaluated on a many-core cluster using non-trivial read/write workloads. The paper provides examples to illustrate the use of these models.",
        "authors": [
            "Aparna Sasidharan",
            "Xian-He",
            "Jay Lofstead",
            "Scott Klasky"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-03-12"
    },
    "http://arxiv.org/abs/2503.09304v1": {
        "id": "http://arxiv.org/abs/2503.09304v1",
        "title": "Priority-Aware Preemptive Scheduling for Mixed-Priority Workloads in MoE Inference",
        "link": "http://arxiv.org/abs/2503.09304v1",
        "tags": [
            "serving",
            "MoE"
        ],
        "relevant": true,
        "indexed_date": "2025-03-13",
        "tldr": "Addresses head-of-line blocking in mixed-priority MoE inference workloads. Introduces fine-grained expert-level preemptive scheduling to prioritize LS jobs. Reduces average LS TTFT by 65.5× and achieves SLO at 7 requests/sec vs baseline failure.",
        "abstract": "Large Language Models have revolutionized natural language processing, yet serving them efficiently in data centers remains challenging due to mixed workloads comprising latency-sensitive (LS) and best-effort (BE) jobs. Existing inference systems employ iteration-level first-come-first-served scheduling, causing head-of-line blocking when BE jobs delay LS jobs. We introduce QLLM, a novel inference system designed for Mixture of Experts (MoE) models, featuring a fine-grained, priority-aware preemptive scheduler. QLLM enables expert-level preemption, deferring BE job execution while minimizing LS time-to-first-token (TTFT). Our approach removes iteration-level scheduling constraints, enabling the scheduler to preempt jobs at any layer based on priority. Evaluations on an Nvidia A100 GPU show that QLLM significantly improves performance. It reduces LS TTFT by an average of $65.5\\times$ and meets the SLO at up to $7$ requests/sec, whereas the baseline fails to do so under the tested workload. Additionally, it cuts LS turnaround time by up to $12.8\\times$ without impacting throughput. QLLM is modular, extensible, and seamlessly integrates with Hugging Face MoE models.",
        "authors": [
            "Mohammad Siavashi",
            "Faezeh Keshmiri Dindarloo",
            "Dejan Kostic",
            "Marco Chiesa"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "submit_date": "2025-03-12"
    },
    "http://arxiv.org/abs/2503.09114v2": {
        "id": "http://arxiv.org/abs/2503.09114v2",
        "title": "Sometimes Painful but Certainly Promising: Feasibility and Trade-offs of Language Model Inference at the Edge",
        "link": "http://arxiv.org/abs/2503.09114v2",
        "tags": [
            "edge",
            "offloading",
            "quantization"
        ],
        "relevant": true,
        "indexed_date": "2025-03-13",
        "tldr": "Evaluates feasibility of deploying language model inference on edge devices. Measures memory, latency, energy, and cost trade-offs with compressed models on CPU/GPU edge hardware. Quantization reduces memory but not all bottlenecks; larger models exceed capabilities, showing limitations.",
        "abstract": "The rapid rise of Language Models (LMs) has expanded the capabilities of natural language processing, powering applications from text generation to complex decision-making. While state-of-the-art LMs often boast hundreds of billions of parameters and are primarily deployed in data centers, recent trends show a growing focus on compact models-typically under 10 billion parameters-enabled by techniques such as quantization and other model compression techniques. This shift paves the way for LMs on edge devices, offering potential benefits such as enhanced privacy, reduced latency, and improved data sovereignty. However, the inherent complexity of even these smaller models, combined with the limited computing resources of edge hardware, raises critical questions about the practical trade-offs in executing LM inference outside the cloud. To address these challenges, we present a comprehensive evaluation of generative LM inference on representative CPU-based and GPU-accelerated edge devices. Our study measures key performance indicators-including memory usage, inference speed, and energy consumption-across various device configurations. Additionally, we examine throughput-energy trade-offs, cost considerations, and usability, alongside an assessment of qualitative model performance. While quantization helps mitigate memory overhead, it does not fully eliminate resource bottlenecks, especially for larger models. Our findings quantify the memory and energy constraints that must be considered for practical real-world deployments, offering concrete insights into the trade-offs between model size, inference performance, and efficiency. The exploration of LMs at the edge is still in its early stages. We hope this study provides a foundation for future research, guiding the refinement of models, the enhancement of inference efficiency, and the advancement of edge-centric AI systems.",
        "authors": [
            "Maximilian Abstreiter",
            "Sasu Tarkoma",
            "Roberto Morabito"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC",
            "cs.PF"
        ],
        "submit_date": "2025-03-12"
    },
    "http://arxiv.org/abs/2503.08415v2": {
        "id": "http://arxiv.org/abs/2503.08415v2",
        "title": "TokenSim: Enabling Hardware and Software Exploration for Large Language Model Inference Systems",
        "link": "http://arxiv.org/abs/2503.08415v2",
        "tags": [
            "serving",
            "kernel",
            "quantization"
        ],
        "relevant": true,
        "indexed_date": "2025-03-12",
        "tldr": "Introduces TokenSim for hardware/software co-exploration of LLM inference systems. Supports extensible optimizations like scheduling and memory management. Achieves simulation error rate below 1% vs real systems.",
        "abstract": "The increasing demand for large language model (LLM) serving has necessitated significant advancements in the optimization and profiling of LLM inference systems. As these models become integral to a wide range of applications, the need for efficient and scalable serving solutions has grown exponentially. This work introduces TokenSim, a comprehensive hardware and software exploration system designed specifically for LLM inference. TokenSim is characterized by its support for extensible system optimizations including scheduling and memory management. We validate the results with systems running with realworld datasets, achieving an error rate of less than 1%. Furthermore, TokenSim facilitates various insightful explorations into the performance and optimization of LLM serving systems.",
        "authors": [
            "Feiyang Wu",
            "Zhuohang Bian",
            "Guoyang Duan",
            "Tianle Xu",
            "Junchi Wu",
            "Teng Ma",
            "Yongqiang Yao",
            "Ruihao Gong",
            "Youwei Zhuo"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-03-11"
    },
    "http://arxiv.org/abs/2503.08311v2": {
        "id": "http://arxiv.org/abs/2503.08311v2",
        "title": "Mind the Memory Gap: Unveiling GPU Bottlenecks in Large-Batch LLM Inference",
        "link": "http://arxiv.org/abs/2503.08311v2",
        "tags": [
            "serving",
            "kernel",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-03-12",
        "tldr": "Identifies DRAM bandwidth saturation as the bottleneck in large-batch LLM inference, challenging compute-bound assumptions. Proposes a Batching Configuration Advisor (BCA) for optimal memory allocation and uses model replication to leverage freed resources. Achieves improved throughput and GPU utilization, particularly for smaller models.",
        "abstract": "Large language models have been widely adopted across different tasks, but their auto-regressive generation nature often leads to inefficient resource utilization during inference. While batching is commonly used to increase throughput, performance gains plateau beyond a certain batch size, especially with smaller models, a phenomenon that existing literature typically explains as a shift to the compute-bound regime. In this paper, through an in-depth GPU-level analysis, we reveal that large-batch inference remains memory-bound, with most GPU compute capabilities underutilized due to DRAM bandwidth saturation as the primary bottleneck. To address this, we propose a Batching Configuration Advisor (BCA) that optimizes memory allocation, reducing GPU memory requirements with minimal impact on throughput. The freed memory and underutilized GPU compute capabilities can then be leveraged by concurrent workloads. Specifically, we use model replication to improve serving throughput and GPU utilization. Our findings challenge conventional assumptions about LLM inference, offering new insights and practical strategies for improving resource utilization, particularly for smaller language models. The code is publicly available at https://github.com/FerranAgulloLopez/vLLMBatchingMemoryGap.",
        "authors": [
            "Pol G. Recasens",
            "Ferran Agullo",
            "Yue Zhu",
            "Chen Wang",
            "Eun Kyung Lee",
            "Olivier Tardieu",
            "Jordi Torres",
            "Josep Ll. Berral"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-03-11"
    },
    "http://arxiv.org/abs/2503.08223v2": {
        "id": "http://arxiv.org/abs/2503.08223v2",
        "title": "Will LLMs Scaling Hit the Wall? Breaking Barriers via Distributed Resources on Massive Edge Devices",
        "link": "http://arxiv.org/abs/2503.08223v2",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-03-12",
        "tldr": "",
        "abstract": "The remarkable success of foundation models has been driven by scaling laws, demonstrating that model performance improves predictably with increased training data and model size. However, this scaling trajectory faces two critical challenges: the depletion of high-quality public data, and the prohibitive computational power required for larger models, which have been monopolized by tech giants. These two bottlenecks pose significant obstacles to the further development of AI. In this position paper, we argue that leveraging massive distributed edge devices can break through these barriers. We reveal the vast untapped potential of data and computational resources on massive edge devices, and review recent technical advancements in distributed/federated learning that make this new paradigm viable. Our analysis suggests that by collaborating on edge devices, everyone can participate in training large language models with small edge devices. This paradigm shift towards distributed training on edge has the potential to democratize AI development and foster a more inclusive AI community.",
        "authors": [
            "Tao Shen",
            "Didi Zhu",
            "Ziyu Zhao",
            "Zexi Li",
            "Chao Wu",
            "Fei Wu"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-03-11"
    },
    "http://arxiv.org/abs/2503.08467v1": {
        "id": "http://arxiv.org/abs/2503.08467v1",
        "title": "Accelerating MoE Model Inference with Expert Sharding",
        "link": "http://arxiv.org/abs/2503.08467v1",
        "tags": [
            "MoE",
            "serving",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-03-12",
        "tldr": "Addresses inefficiencies in MoE encoder model inference due to imbalanced token routing. Proposes MoEShard, a tensor sharding system for expert matrices with fused computations for balanced load and reduced kernel launches. Achieves 6.4× lower time to first token vs. DeepSpeed.",
        "abstract": "Mixture of experts (MoE) models achieve state-of-the-art results in language modeling but suffer from inefficient hardware utilization due to imbalanced token routing and communication overhead. While prior work has focused on optimizing MoE training and decoder architectures, inference for encoder-based MoE models in a multi-GPU with expert parallelism setting remains underexplored. We introduce MoEShard, an inference system that achieves perfect load balancing through tensor sharding of MoE experts. Unlike existing approaches that rely on heuristic capacity factors or drop tokens, MoEShard evenly distributes computation across GPUs and ensures full token retention, maximizing utilization regardless of routing skewness. We achieve this through a strategic row- and column-wise decomposition of expert matrices. This reduces idle time and avoids bottlenecks caused by imbalanced expert assignments. Furthermore, MoEShard minimizes kernel launches by fusing decomposed expert computations, significantly improving throughput. We evaluate MoEShard against DeepSpeed on encoder-based architectures, demonstrating speedups of up to 6.4$\\times$ in time to first token (TTFT). Our results show that tensor sharding, when properly applied to experts, is a viable and effective strategy for efficient MoE inference.",
        "authors": [
            "Oana Balmau",
            "Anne-Marie Kermarrec",
            "Rafael Pires",
            "André Loureiro Espírito Santo",
            "Martijn de Vos",
            "Milos Vujasinovic"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC"
        ],
        "submit_date": "2025-03-11"
    },
    "http://arxiv.org/abs/2503.08461v1": {
        "id": "http://arxiv.org/abs/2503.08461v1",
        "title": "FastCache: Optimizing Multimodal LLM Serving through Lightweight KV-Cache Compression Framework",
        "link": "http://arxiv.org/abs/2503.08461v1",
        "tags": [
            "multi-modal",
            "serving",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-03-12",
        "tldr": "Proposes FastCache, a KV-cache compression framework with dynamic batching and memory pooling to optimize multimodal LLM serving. Reduces TTFT by 19.3x and improves throughput by 12.1x while cutting memory by 20% under high concurrency.",
        "abstract": "Multi-modal Large Language Models (MLLMs) serving systems commonly employ KV-cache compression to reduce memory footprint. However, existing compression methods introduce significant processing overhead and queuing delays, particularly in concurrent serving scenarios. We present \\texttt{FastCache}, a novel serving framework that effectively addresses these challenges through two key innovations: (1) a dynamic batching strategy that optimizes request scheduling across prefill, compression, and decode stages, and (2) an efficient KV-cache memory pool mechanism that eliminates memory fragmentation while maintaining high GPU utilization. Our comprehensive experiments on the GQA and MileBench datasets demonstrate that \\texttt{FastCache} achieves up to 19.3$\\times$ reduction in Time-To-First-Token (TTFT) and 12.1$\\times$ improvement in throughput compared to state-of-the-art baselines. The system maintains stable performance under high-concurrency scenarios (up to 40 req/s) while reducing average memory consumption by 20\\%. These results establish \\texttt{FastCache} as an efficient solution for real-world LLM serving systems with KV-cache compression.",
        "authors": [
            "Jianian Zhu",
            "Hang Wu",
            "Haojie Wang",
            "Yinghui Li",
            "Biao Hou",
            "Ruixuan Li",
            "Jidong Zhai"
        ],
        "categories": [
            "cs.MM",
            "cs.DC"
        ],
        "submit_date": "2025-03-11"
    },
    "http://arxiv.org/abs/2503.07675v2": {
        "id": "http://arxiv.org/abs/2503.07675v2",
        "title": "DynTaskMAS: A Dynamic Task Graph-driven Framework for Asynchronous and Parallel LLM-based Multi-Agent Systems",
        "link": "http://arxiv.org/abs/2503.07675v2",
        "tags": [
            "agentic",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-03-12",
        "tldr": "Describes DynTaskMAS, a dynamic task graph-driven framework for efficient asynchronous multi-agent systems with LLMs. Innovations include task graph generation, asynchronous execution engine, context management, and adaptive workflow. Achieves 21-33% lower execution time and 35.4% higher resource utilization.",
        "abstract": "The emergence of Large Language Models (LLMs) in Multi-Agent Systems (MAS) has opened new possibilities for artificial intelligence, yet current implementations face significant challenges in resource management, task coordination, and system efficiency. While existing frameworks demonstrate the potential of LLM-based agents in collaborative problem-solving, they often lack sophisticated mechanisms for parallel execution and dynamic task management. This paper introduces DynTaskMAS, a novel framework that orchestrates asynchronous and parallel operations in LLM-based MAS through dynamic task graphs. The framework features four key innovations: (1) a Dynamic Task Graph Generator that intelligently decomposes complex tasks while maintaining logical dependencies, (2) an Asynchronous Parallel Execution Engine that optimizes resource utilization through efficient task scheduling, (3) a Semantic-Aware Context Management System that enables efficient information sharing among agents, and (4) an Adaptive Workflow Manager that dynamically optimizes system performance. Experimental evaluations demonstrate that DynTaskMAS achieves significant improvements over traditional approaches: a 21-33% reduction in execution time across task complexities (with higher gains for more complex tasks), a 35.4% improvement in resource utilization (from 65% to 88%), and near-linear throughput scaling up to 16 concurrent agents (3.47X improvement for 4X agents). Our framework establishes a foundation for building scalable, high-performance LLM-based multi-agent systems capable of handling complex, dynamic tasks efficiently.",
        "authors": [
            "Junwei Yu",
            "Yepeng Ding",
            "Hiroyuki Sato"
        ],
        "categories": [
            "cs.MA",
            "cs.AI",
            "cs.DC"
        ],
        "submit_date": "2025-03-10"
    },
    "http://arxiv.org/abs/2503.06433v1": {
        "id": "http://arxiv.org/abs/2503.06433v1",
        "title": "Seesaw: High-throughput LLM Inference via Model Re-sharding",
        "link": "http://arxiv.org/abs/2503.06433v1",
        "tags": [
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-03-11",
        "tldr": "Proposes Seesaw, an LLM inference engine using dynamic model re-sharding to optimize parallelization strategies for prefill and decoding stages. Employs tiered KV cache buffering and transition-minimizing scheduling. Achieves 1.78× higher throughput vs vLLM.",
        "abstract": "To improve the efficiency of distributed large language model (LLM) inference, various parallelization strategies, such as tensor and pipeline parallelism, have been proposed. However, the distinct computational characteristics inherent in the two stages of LLM inference-prefilling and decoding-render a single static parallelization strategy insufficient for the effective optimization of both stages. In this work, we present Seesaw, an LLM inference engine optimized for throughput-oriented tasks. The key idea behind Seesaw is dynamic model re-sharding, a technique that facilitates the dynamic reconfiguration of parallelization strategies across stages, thereby maximizing throughput at both phases. To mitigate re-sharding overhead and optimize computational efficiency, we employ tiered KV cache buffering and transition-minimizing scheduling. These approaches work synergistically to reduce the overhead caused by frequent stage transitions while ensuring maximum batching efficiency. Our evaluation demonstrates that Seesaw achieves a throughput increase of up to 1.78x (1.36x on average) compared to vLLM, the most widely used state-of-the-art LLM inference engine.",
        "authors": [
            "Qidong Su",
            "Wei Zhao",
            "Xin Li",
            "Muralidhar Andoorveedu",
            "Chenhao Jiang",
            "Zhanda Zhu",
            "Kevin Song",
            "Christina Giannoula",
            "Gennady Pekhimenko"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-03-09"
    },
    "http://arxiv.org/abs/2503.06823v1": {
        "id": "http://arxiv.org/abs/2503.06823v1",
        "title": "eMoE: Task-aware Memory Efficient Mixture-of-Experts-Based (MoE) Model Inference",
        "link": "http://arxiv.org/abs/2503.06823v1",
        "tags": [
            "MoE",
            "offloading",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-03-11",
        "tldr": "Addresses high memory overhead in MoE-based LLM inference. Proposes eMoE, which predicts required experts based on routing patterns and schedules task-aware expert loading to minimize latency. Reduces memory usage by 80% and latency by 17%, while enabling 40x longer prompts and 1.5x higher throughput.",
        "abstract": "In recent years, Mixture-of-Experts (MoE) has emerged as an effective approach for enhancing the capacity of deep neural network (DNN) with sub-linear computational costs. However, storing all experts on GPUs incurs significant memory overhead, increasing the monetary cost of MoE-based inference. To address this, we propose eMoE, a memory efficient inference system for MoE-based large language models (LLMs) by leveraging our observations from experiment measurements. eMoE reduces memory usage by predicting and loading only the required experts based on recurrent patterns in expert routing. To reduce loading latency while maintaining accuracy, as we found using the same experts for subsequent prompts has minimal impact on perplexity, eMoE invokes the expert predictor every few prompts rather than for each prompt. In addition, it skips predictions for tasks less sensitive to routing accuracy. Finally, it has task-aware scheduling to minimize inference latency by considering Service Level Objectives (SLOs), task-specific output lengths, and expert loading latencies. Experimental results show that compared to existing systems, eMoE reduces memory consumption by up to 80% while maintaining accuracy and reduces inference latency by up to 17%. It also enables processing prompts 40x longer, batches 4.5x larger, and achieves 1.5x higher throughput.",
        "authors": [
            "Suraiya Tairin",
            "Shohaib Mahmud",
            "Haiying Shen",
            "Anand Iyer"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "submit_date": "2025-03-10"
    },
    "http://arxiv.org/abs/2503.06208v1": {
        "id": "http://arxiv.org/abs/2503.06208v1",
        "title": "Distributed Graph Neural Network Inference With Just-In-Time Compilation For Industry-Scale Graphs",
        "link": "http://arxiv.org/abs/2503.06208v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-03-11",
        "tldr": "",
        "abstract": "Graph neural networks (GNNs) have delivered remarkable results in various fields. However, the rapid increase in the scale of graph data has introduced significant performance bottlenecks for GNN inference. Both computational complexity and memory usage have risen dramatically, with memory becoming a critical limitation. Although graph sampling-based subgraph learning methods can help mitigate computational and memory demands, they come with drawbacks such as information loss and high redundant computation among subgraphs. This paper introduces an innovative processing paradgim for distributed graph learning that abstracts GNNs with a new set of programming interfaces and leverages Just-In-Time (JIT) compilation technology to its full potential. This paradigm enables GNNs to highly exploit the computational resources of distributed clusters by eliminating the drawbacks of subgraph learning methods, leading to a more efficient inference process. Our experimental results demonstrate that on industry-scale graphs of up to \\textbf{500 million nodes and 22.4 billion edges}, our method can produce a performance boost of up to \\textbf{27.4 times}.",
        "authors": [
            "Xiabao Wu",
            "Yongchao Liu",
            "Wei Qin",
            "Chuntao Hong"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC"
        ],
        "submit_date": "2025-03-08"
    },
    "http://arxiv.org/abs/2503.05248v1": {
        "id": "http://arxiv.org/abs/2503.05248v1",
        "title": "Optimizing LLM Inference Throughput via Memory-aware and SLA-constrained Dynamic Batching",
        "link": "http://arxiv.org/abs/2503.05248v1",
        "tags": [
            "serving",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-03-10",
        "tldr": "Proposes a memory-aware, SLA-constrained dynamic batching for LLM inference, adapting batch sizes based on real-time GPU memory and latency. Achieves 8% to 28% higher throughput and 22% capacity improvement over static batching while meeting SLAs.",
        "abstract": "The increasing adoption of large language models (LLMs) necessitates inference serving systems that can deliver both high throughput and low latency. Deploying LLMs with hundreds of billions of parameters on memory-constrained GPUs exposes significant limitations in static batching methods. Current inference serving systems often treat batch sizes as fixed hyper-parameters, hindering real-time adaptation to varying system conditions. In this paper, we propose a dynamic batching method that continuously monitors memory utilization and adheres to service-level agreements (SLAs) to enable real-time batch size configuration adjustment. The method comprises two core components: a memory-aware batch scheduler that dynamically allocates GPU resources and a latency feedback mechanism that optimizes decoding processes under SLA constraints. The numerical experiments demonstrate throughput gains of 8% to 28% and capacity improvements of 22% compared to traditional static batching methods, while maintaining full compatibility with existing inference infrastructure. These results highlight the effectiveness of dynamic batching in balancing computational efficiency and quality-of-service requirements for contemporary LLM deployment scenarios. The source code of this work is publicly available at https://github.com/KevinLee1110/dynamic-batching.",
        "authors": [
            "Bowen Pang",
            "Kai Li",
            "Feifan Wang"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-03-07"
    },
    "http://arxiv.org/abs/2503.05447v2": {
        "id": "http://arxiv.org/abs/2503.05447v2",
        "title": "Linear-MoE: Linear Sequence Modeling Meets Mixture-of-Experts",
        "link": "http://arxiv.org/abs/2503.05447v2",
        "tags": [
            "training",
            "MoE",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-03-10",
        "tldr": "Proposes Linear-MoE, a system integrating Linear Sequence Modeling with MoE for efficient training. Introduces modeling subsystem for unified LSM support and training subsystem with Sequence Parallelism. Achieves efficiency gains in 300M-7B models while maintaining benchmark performance.",
        "abstract": "Linear Sequence Modeling (LSM) like linear attention, state space models and linear RNNs, and Mixture-of-Experts (MoE) have recently emerged as significant architectural improvements. In this paper, we introduce Linear-MoE, a production-level system for modeling and training large-scale models that integrate LSM with MoE. Linear-MoE leverages the advantages of both LSM modules for linear-complexity sequence modeling and MoE layers for sparsely activation, aiming to offer high performance with efficient training. The Linear-MoE system comprises: 1) Modeling subsystem, which provides a unified framework supporting all instances of LSM. and 2) Training subsystem, which facilitates efficient training by incorporating various advanced parallelism technologies, particularly Sequence Parallelism designed for Linear-MoE models. Additionally, we explore hybrid models that combine Linear-MoE layers with standard Transformer-MoE layers with its Sequence Parallelism to further enhance model flexibility and performance. Evaluations on two model series, A0.3B-2B and A1B-7B, demonstrate Linear-MoE achieves efficiency gains while maintaining competitive performance on various benchmarks, showcasing its potential as a next-generation foundational model architecture. Code: https://github.com/OpenSparseLLMs/Linear-MoE.",
        "authors": [
            "Weigao Sun",
            "Disen Lan",
            "Tong Zhu",
            "Xiaoye Qu",
            "Yu Cheng"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "cs.DC"
        ],
        "submit_date": "2025-03-07"
    },
    "http://arxiv.org/abs/2503.04521v2": {
        "id": "http://arxiv.org/abs/2503.04521v2",
        "title": "Dynamic Pricing for On-Demand DNN Inference in the Edge-AI Market",
        "link": "http://arxiv.org/abs/2503.04521v2",
        "tags": [
            "edge",
            "networking",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-03-07",
        "tldr": "Proposes AERIA, an auction-based pricing mechanism for revenue maximization in Edge-AI DNN inference acceleration. Designs multi-exit device-edge synergistic inference with model partitioning, pricing, and resource allocation. Achieves competitive revenue gains over state-of-the-art approaches in simulations.",
        "abstract": "The convergence of edge computing and Artificial Intelligence (AI) gives rise to Edge-AI, which enables the deployment of real-time AI applications at the network edge. A key research challenge in Edge-AI is edge inference acceleration, which aims to realize low-latency high-accuracy Deep Neural Network (DNN) inference by offloading partitioned inference tasks from end devices to edge servers. However, existing research has yet to adopt a practical Edge-AI market perspective, which would explore the personalized inference needs of AI users (e.g., inference accuracy, latency, and task complexity), the revenue incentives for AI service providers that offer edge inference services, and multi-stakeholder governance within a market-oriented context. To bridge this gap, we propose an Auction-based Edge Inference Pricing Mechanism (AERIA) for revenue maximization to tackle the multi-dimensional optimization problem of DNN model partition, edge inference pricing, and resource allocation. We develop a multi-exit device-edge synergistic inference scheme for on-demand DNN inference acceleration, and theoretically analyze the auction dynamics amongst the AI service providers, AI users and edge infrastructure provider. Owing to the strategic mechanism design via randomized consensus estimate and cost sharing techniques, the Edge-AI market attains several desirable properties. These include competitiveness in revenue maximization, incentive compatibility, and envy-freeness, which are crucial to maintain the effectiveness, truthfulness, and fairness in auction outcomes. Extensive simulations based on four representative DNN inference workloads demonstrate that AERIA significantly outperforms several state-of-the-art approaches in revenue maximization. This validates the efficacy of AERIA for on-demand DNN inference in the Edge-AI market.",
        "authors": [
            "Songyuan Li",
            "Jia Hu",
            "Geyong Min",
            "Haojun Huang",
            "Jiwei Huang"
        ],
        "categories": [
            "cs.AI",
            "cs.CE",
            "cs.DC",
            "cs.SE"
        ],
        "submit_date": "2025-03-06"
    },
    "http://arxiv.org/abs/2503.04398v3": {
        "id": "http://arxiv.org/abs/2503.04398v3",
        "title": "Speculative MoE: Communication Efficient Parallel MoE Inference with Speculative Token and Expert Pre-scheduling",
        "link": "http://arxiv.org/abs/2503.04398v3",
        "tags": [
            "MoE",
            "serving",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-03-07",
        "tldr": "Proposes Speculative MoE, a method to reduce communication overhead in MoE inference via speculative token shuffling and expert pre-scheduling. Integrated into DeepSpeed-MoE and SGLang, it trims EP communication volume, boosting throughput on diverse interconnects.",
        "abstract": "MoE (Mixture of Experts) prevails as a neural architecture that can scale modern transformer-based LLMs (Large Language Models) to unprecedented scales. Nevertheless, large MoEs' great demands of computing power, memory capacity and memory bandwidth make scalable serving a fundamental challenge and efficient parallel inference has become a requisite to attain adequate throughput under latency constraints. DeepSpeed-MoE, one state-of-the-art MoE inference framework, adopts a 3D-parallel paradigm including EP (Expert Parallelism), TP (Tensor Parallel) and DP (Data Parallelism). However, our analysis shows DeepSpeed-MoE's inference efficiency is largely bottlenecked by EP, which is implemented with costly all-to-all collectives to route token activation. Our work aims to boost DeepSpeed-MoE by strategically reducing EP's communication overhead with a technique named Speculative MoE. Speculative MoE has two speculative parallelization schemes, speculative token shuffling and speculative expert grouping, which predict outstanding tokens' expert routing paths and pre-schedule tokens and experts across devices to losslessly trim EP's communication volume. Besides DeepSpeed-MoE, we also build Speculative MoE into a prevailing MoE inference engine SGLang. Experiments show Speculative MoE can significantly boost state-of-the-art MoE inference frameworks on fast homogeneous and slow heterogeneous interconnects.",
        "authors": [
            "Yan Li",
            "Pengfei Zheng",
            "Shuang Chen",
            "Zewei Xu",
            "Yuanhao Lai",
            "Yunfei Du",
            "Zhengang Wang"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC"
        ],
        "submit_date": "2025-03-06"
    },
    "http://arxiv.org/abs/2503.04302v1": {
        "id": "http://arxiv.org/abs/2503.04302v1",
        "title": "Malware Detection at the Edge with Lightweight LLMs: A Performance Evaluation",
        "link": "http://arxiv.org/abs/2503.04302v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-03-07",
        "tldr": "",
        "abstract": "The rapid evolution of malware attacks calls for the development of innovative detection methods, especially in resource-constrained edge computing. Traditional detection techniques struggle to keep up with modern malware's sophistication and adaptability, prompting a shift towards advanced methodologies like those leveraging Large Language Models (LLMs) for enhanced malware detection. However, deploying LLMs for malware detection directly at edge devices raises several challenges, including ensuring accuracy in constrained environments and addressing edge devices' energy and computational limits. To tackle these challenges, this paper proposes an architecture leveraging lightweight LLMs' strengths while addressing limitations like reduced accuracy and insufficient computational power. To evaluate the effectiveness of the proposed lightweight LLM-based approach for edge computing, we perform an extensive experimental evaluation using several state-of-the-art lightweight LLMs. We test them with several publicly available datasets specifically designed for edge and IoT scenarios and different edge nodes with varying computational power and characteristics.",
        "authors": [
            "Christian Rondanini",
            "Barbara Carminati",
            "Elena Ferrari",
            "Antonio Gaudiano",
            "Ashish Kundu"
        ],
        "categories": [
            "cs.CR",
            "cs.AI",
            "cs.DC"
        ],
        "submit_date": "2025-03-06"
    },
    "http://arxiv.org/abs/2503.03182v2": {
        "id": "http://arxiv.org/abs/2503.03182v2",
        "title": "Enhancing Memory Efficiency in Large Language Model Training Through Chronos-aware Pipeline Parallelism",
        "link": "http://arxiv.org/abs/2503.03182v2",
        "tags": [
            "training",
            "offloading",
            "sparse"
        ],
        "relevant": true,
        "indexed_date": "2025-03-06",
        "tldr": "Addresses memory inefficiency in LLM pretraining by introducing ChronosPipe, a pipeline parallelism strategy that optimizes temporal locality. Combines Chronos-Pipe scheduling, Chronos-Recomp, and Chronos-Offload to enhance HBM utilization. Expands trainable model size by 2.4x while maintaining throughput.",
        "abstract": "Larger model sizes and longer sequence lengths have empowered the Large Language Model (LLM) to achieve outstanding performance across various domains. However, this progress brings significant storage capacity challenges for LLM pretraining. High Bandwidth Memory (HBM) is expensive and requires more advanced packaging technologies for capacity expansion, creating an urgent need for memory-efficient scheduling strategies. Yet, prior pipeline parallelism schedules have primarily focused on reducing bubble overhead, often neglecting memory efficiency and lacking compatibility with other memory-efficient strategies. Consequently, these methods struggle to meet the storage demands of storage capacity for next-generation LLM. This work presents ChronosPipe, a Chronos-aware pipeline parallelism for memory-efficient LLM pretraining. The core insight of ChronosPipe is to treat HBM as a fast but small 'cache,' optimizing and exploiting temporal locality within LLM pretraining to enhance HBM utilization. ChronosPipe introduces a pipeline scheduling strategy, Chronos-Pipe, to reduce the extrinsic overhead that disrupts the temporal locality of activations. Additionally, it leverages Chronos-Recomp and Chronos-Offload to efficiently harness the intrinsic temporal locality of activations and weights in Deep Neural Networks. Experiment results show that ChronosPipe can expand the trainable model size by 2.4x while maintaining comparable throughput, achieving 1.5x better than the 1F1B strategy combined with recomputation.",
        "authors": [
            "Xinyuan Lin",
            "Chenlu Li",
            "Zongle Huang",
            "Chunyu Wang",
            "Bo Xiao",
            "Huazhong Yang",
            "Shishi Duan",
            "Yongpan Liu"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-03-05"
    },
    "http://arxiv.org/abs/2503.03070v1": {
        "id": "http://arxiv.org/abs/2503.03070v1",
        "title": "Environment-Aware Dynamic Pruning for Pipelined Edge Inference",
        "link": "http://arxiv.org/abs/2503.03070v1",
        "tags": [
            "edge",
            "sparse",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-03-06",
        "tldr": "Proposes environment-aware dynamic pruning for edge inference pipelines to handle resource limitations and unpredictability. Introduces training for robustness to post-deployment pruning and an adaptive algorithm for pruning levels. Achieves 1.5x speedup and 3x SLO improvement on Raspberry Pi cluster.",
        "abstract": "IoT and edge-based inference systems require unique solutions to overcome resource limitations and unpredictable environments. In this paper, we propose an environment-aware dynamic pruning system that handles the unpredictability of edge inference pipelines. While traditional pruning approaches can reduce model footprint and compute requirements, they are often performed only once, offline, and are not designed to react to transient or post-deployment device conditions. Similarly, existing pipeline placement strategies may incur high overhead if reconfigured at runtime, limiting their responsiveness. Our approach allows slices of a model, already placed on a distributed pipeline, to be ad-hoc pruned as a means of load-balancing. To support this capability, we introduce two key components: (1) novel training strategies that endow models with robustness to post-deployment pruning, and (2) an adaptive algorithm that determines the optimal pruning level for each node based on monitored bottlenecks. In real-world experiments on a Raspberry Pi 4B cluster running camera-trap workloads, our method achieves a 1.5x speedup and a 3x improvement in service-level objective (SLO) attainment, all while maintaining high accuracy.",
        "authors": [
            "Austin O'Quinn",
            "Conor Snedeker",
            "Siyuan Zhang",
            "Jenna Kline"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-03-05"
    },
    "http://arxiv.org/abs/2503.02550v3": {
        "id": "http://arxiv.org/abs/2503.02550v3",
        "title": "SpecInF: Exploiting Idle GPU Resources in Distributed DL Training via Speculative Inference Filling",
        "link": "http://arxiv.org/abs/2503.02550v3",
        "tags": [
            "training",
            "inference"
        ],
        "relevant": true,
        "indexed_date": "2025-03-05",
        "tldr": "Addresses underutilized GPU resources during distributed DL training. Proposes SpecInF, a system that speculatively overlaps training bubbles with colocated inference instances. Achieves up to 14x offline inference throughput compared to TGS while maintaining training throughput.",
        "abstract": "Deep Learning (DL), especially with Large Language Models (LLMs), brings benefits to various areas. However, DL training systems usually yield prominent idling GPU resources due to many factors, such as resource allocation and collective communication. To improve GPU utilization, we present SpecInF, which adopts a Speculative Inference Filling method to exploit idle GPU resources. It collocates each primary training instance with additional inference instances on the same GPU, detects the training bubbles and adaptively fills with online or offline inference workloads. Our results show that SpecInF can effectively enhance GPU utilization under mainstream parallel training modes, delivering additional up to 14$\\times$ offline inference throughputs than TGS and 67\\% reduction in online inference p95 latency than MPS, while guaranteeing collocated training throughput.",
        "authors": [
            "Cunchi Lv",
            "Xiao Shi",
            "Dong Liang",
            "Wenting Tan",
            "Xiaofang Zhao"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-03-04"
    },
    "http://arxiv.org/abs/2503.02354v2": {
        "id": "http://arxiv.org/abs/2503.02354v2",
        "title": "CoServe: Efficient Collaboration-of-Experts (CoE) Model Inference with Limited Memory",
        "link": "http://arxiv.org/abs/2503.02354v2",
        "tags": [
            "serving",
            "offloading",
            "autoscaling"
        ],
        "relevant": true,
        "indexed_date": "2025-03-05",
        "tldr": "Addresses inefficiencies in serving Collaboration-of-Experts models under memory constraints. Proposes CoServe, featuring dependency-aware scheduling and expert management, with offline profiling for resource allocation. Achieves 4.5× to 12× higher throughput in manufacturing workloads.",
        "abstract": "Large language models like GPT-4 are resource-intensive, but recent advancements suggest that smaller, specialized experts can outperform the monolithic models on specific tasks. The Collaboration-of-Experts (CoE) approach integrates multiple expert models, improving the accuracy of generated results and offering great potential for precision-critical applications, such as automatic circuit board quality inspection. However, deploying CoE serving systems presents challenges to memory capacity due to the large number of experts required, which can lead to significant performance overhead from frequent expert switching across different memory and storage tiers.   We propose CoServe, an efficient CoE model serving system on heterogeneous CPU and GPU with limited memory. CoServe reduces unnecessary expert switching by leveraging expert dependency, a key property of CoE inference. CoServe introduces a dependency-aware request scheduler and dependency-aware expert management for efficient inference. It also introduces an offline profiler to automatically find optimal resource allocation on various processors and devices. In real-world intelligent manufacturing workloads, CoServe achieves 4.5$\\times$ to 12$\\times$ higher throughput compared to state-of-the-art systems.",
        "authors": [
            "Jiashun Suo",
            "Xiaojian Liao",
            "Limin Xiao",
            "Li Ruan",
            "Jinquan Wang",
            "Xiao Su",
            "Zhisheng Huo"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.PF"
        ],
        "submit_date": "2025-03-04"
    },
    "http://arxiv.org/abs/2503.02236v2": {
        "id": "http://arxiv.org/abs/2503.02236v2",
        "title": "VQ-LLM: High-performance Code Generation for Vector Quantization Augmented LLM Inference",
        "link": "http://arxiv.org/abs/2503.02236v2",
        "tags": [
            "inference",
            "quantization",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-03-05",
        "tldr": "Introduces VQ-LLM, a fused vector quantization kernel framework with codebook cache optimizing memory access and computation fusion. Achieves up to 99.1% latency reduction compared to existing implementations, rivaling element-wise quantization methods at equivalent bit-widths.",
        "abstract": "In this work, we design and implement VQ-LLM, an efficient fused Vector Quantization (VQ) kernel generation framework. We first introduce a software abstraction called codebook cache to optimize codebook access efficiency and support the integration of VQ with various computations. The codebook cache adaptively stores different entries across the GPU's memory hierarchy, including off-chip global memory, on-chip shared memory, and registers. Centered around the codebook cache, we design an efficient computation engine that optimizes memory traffic during computations involving codebooks. This compute engine adopts the codebook-centric dataflow and fusion optimizations. Additionally, we provide adaptive heuristics to tailor parameter selection in our optimizations to diverse VQ configurations. Our optimizations achieve an average latency reduction of 46.13% compared to unoptimized versions. Compared to existing open-source implementations, our methods decrease latency by 64.36% to 99.1%. A final comparison with state-of-the-art element-wise quantization methods like AWQ and KVQuant shows that our VQ-LLM is practically viable, achieving latencies close or even better latencies to those at equivalent bit-widths, potentially offering greater accuracy.",
        "authors": [
            "Zihan Liu",
            "Xinhao Luo",
            "Junxian Guo",
            "Wentao Ni",
            "Yangjie Zhou",
            "Yue Guan",
            "Cong Guo",
            "Weihao Cui",
            "Yu Feng",
            "Minyi Guo",
            "Yuhao Zhu",
            "Minjia Zhang",
            "Jingwen Leng",
            "Chen Jin"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-03-04"
    },
    "http://arxiv.org/abs/2503.01025v1": {
        "id": "http://arxiv.org/abs/2503.01025v1",
        "title": "Improving inference time in multi-TPU systems with profiled model segmentation",
        "link": "http://arxiv.org/abs/2503.01025v1",
        "tags": [
            "serving",
            "offloading",
            "edge"
        ],
        "relevant": true,
        "indexed_date": "2025-03-04",
        "tldr": "Improves inference time on multi-TPU systems by addressing host memory bottlenecks. Proposes model segmentation and pipelining across up to four Edge TPUs. Achieves speedups of 6× to 46× compared to single-TPU setups for different layer types.",
        "abstract": "In this paper, we systematically evaluate the inference performance of the Edge TPU by Google for neural networks with different characteristics. Specifically, we determine that, given the limited amount of on-chip memory on the Edge TPU, accesses to external (host) memory rapidly become an important performance bottleneck. We demonstrate how multiple devices can be jointly used to alleviate the bottleneck introduced by accessing the host memory. We propose a solution combining model segmentation and pipelining on up to four TPUs, with remarkable performance improvements that range from $6\\times$ for neural networks with convolutional layers to $46\\times$ for fully connected layers, compared with single-TPU setups.",
        "authors": [
            "Jorge Villarrubia",
            "Luis Costero",
            "Francisco D. Igual",
            "Katzalin Olcoz"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-03-02"
    },
    "http://arxiv.org/abs/2502.21231v1": {
        "id": "http://arxiv.org/abs/2502.21231v1",
        "title": "ByteScale: Efficient Scaling of LLM Training with a 2048K Context Length on More Than 12,000 GPUs",
        "link": "http://arxiv.org/abs/2502.21231v1",
        "tags": [
            "training",
            "offloading",
            "sparse"
        ],
        "relevant": true,
        "indexed_date": "2025-03-03",
        "tldr": "Addresses inefficiency in scaling LLM training for variable-length sequences with long contexts. Introduces ByteScale with Hybrid Data Parallelism (HDP), including data-aware sharding, dynamic communication, selective offloading, and balance scheduling. Achieves up to 7.89x speedup on models up to 141B parameters with 2048K context.",
        "abstract": "Scaling long-context ability is essential for Large Language Models (LLMs). To amortize the memory consumption across multiple devices in long-context training, inter-data partitioning (a.k.a. Data Parallelism) and intra-data partitioning (a.k.a. Context Parallelism) are commonly used. Current training frameworks predominantly treat the two techniques as orthogonal, and establish static communication groups to organize the devices as a static mesh (e.g., a 2D mesh). However, the sequences for LLM training typically vary in lengths, no matter for texts, multi-modalities or reinforcement learning. The mismatch between data heterogeneity and static mesh causes redundant communication and imbalanced computation, degrading the training efficiency.   In this work, we introduce ByteScale, an efficient, flexible, and scalable LLM training framework for large-scale mixed training of long and short sequences. The core of ByteScale is a novel parallelism strategy, namely Hybrid Data Parallelism (HDP), which unifies the inter- and intra-data partitioning with a dynamic mesh design. In particular, we build a communication optimizer, which eliminates the redundant communication for short sequences by data-aware sharding and dynamic communication, and further compresses the communication cost for long sequences by selective offloading. Besides, we also develop a balance scheduler to mitigate the imbalanced computation by parallelism-aware data assignment. We evaluate ByteScale with the model sizes ranging from 7B to 141B, context lengths from 256K to 2048K, on a production cluster with more than 12,000 GPUs. Experiment results show that ByteScale outperforms the state-of-the-art training system by up to 7.89x.",
        "authors": [
            "Hao Ge",
            "Junda Feng",
            "Qi Huang",
            "Fangcheng Fu",
            "Xiaonan Nie",
            "Lei Zuo",
            "Haibin Lin",
            "Bin Cui",
            "Xin Liu"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.LG"
        ],
        "submit_date": "2025-02-28"
    },
    "http://arxiv.org/abs/2502.20969v3": {
        "id": "http://arxiv.org/abs/2502.20969v3",
        "title": "TeleRAG: Efficient Retrieval-Augmented Generation Inference with Lookahead Retrieval",
        "link": "http://arxiv.org/abs/2502.20969v3",
        "tags": [
            "RAG",
            "serving",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-03-03",
        "tldr": "Addresses high latency and low throughput in Retrieval-Augmented Generation pipelines. Proposes TeleRAG with lookahead retrieval for data prefetching and two schedulers for multi-GPU efficiency. Achieves up to 1.83x higher throughput and reduces latency 1.53x with minimal GPU memory.",
        "abstract": "Retrieval-augmented generation (RAG) extends large language models (LLMs) with external data sources to enhance factual correctness and domain coverage. Modern RAG pipelines rely on large datastores, creating a significant system challenge: achieving high throughput and low latency is difficult, especially when GPU memory is limited. To address these challenges, we propose TeleRAG, an efficient inference system that reduces latency and improves throughput with minimal GPU memory requirements. The core innovation of TeleRAG is lookahead retrieval, a prefetching mechanism that predicts required data and transfers them from CPU to GPU in parallel with LLM generation. In addition, TeleRAG adopts a prefetching scheduler and a cache-aware scheduler to support efficient multi-GPU inference with minimal overhead. Evaluations show TeleRAG achieves up to a 1.53x average end-to-end latency reduction (single-query) and 1.83x higher average throughput (batched), as well as good scalability in throughput. This confirms the practical utility of TeleRAG for faster and more memory-efficient deployments of RAG applications.",
        "authors": [
            "Chien-Yu Lin",
            "Keisuke Kamahori",
            "Yiyu Liu",
            "Xiaoxiang Shi",
            "Madhav Kashyap",
            "Yile Gu",
            "Rulin Shao",
            "Zihao Ye",
            "Kan Zhu",
            "Rohan Kadekodi",
            "Stephanie Wang",
            "Arvind Krishnamurthy",
            "Luis Ceze",
            "Baris Kasikci"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-02-28"
    },
    "http://arxiv.org/abs/2502.20959v2": {
        "id": "http://arxiv.org/abs/2502.20959v2",
        "title": "Cicada: A Pipeline-Efficient Approach to Serverless Inference with Decoupled Management",
        "link": "http://arxiv.org/abs/2502.20959v2",
        "tags": [
            "serving",
            "offloading",
            "storage"
        ],
        "relevant": true,
        "indexed_date": "2025-03-03",
        "tldr": "Addresses cold start delays in serverless ML inference by decoupling model loading. Proposes Cicada with MiniLoader for parameter initialization, WeightDecoupler for asynchronous weight retrieval, and priority-aware scheduling. Reduces end-to-end latency by 61.59% and improves pipeline utilization by 2.52x.",
        "abstract": "Serverless computing has emerged as a pivotal paradigm for deploying Deep Learning (DL) models, offering automatic scaling and cost efficiency. However, the inherent cold start problem in serverless ML inference systems, particularly the time-consuming model loading process, remains a significant bottleneck. Utilizing pipelined model loading improves efficiency but still suffer from pipeline stalls due to sequential layer construction and monolithic weight loading. In this paper, we propose \\textit{Cicada}, a novel pipeline optimization framework that coordinates computational, storage, and scheduling resources through three key mechanisms: (1) \\textit{MiniLoader}: which reduces layer construction overhead by opportunistically optimizing parameter initialization; (2) \\textit{WeightDecoupler}: decoupling weight file processing from layer construction, enabling asynchronous weight retrieval and out-of-order weight application; (3) \\textit{Priority-Aware Scheduler}: dynamically allocating resources to ensure high-priority inference tasks are executed promptly. Our experimental results demonstrate that Cicada achieves significant performance improvements over the state-of-the-art PISeL framework. Specifically, Cicada reduces end-to-end inference latency by an average of 61.59\\%, with the MiniLoader component contributing the majority of this optimization (53.41\\%), and the WeightDecoupler achieves up to 26.17\\% improvement. Additionally, Cicada achieves up to 2.52x speedup in the inference pipeline utlization compared to PISeL.",
        "authors": [
            "Z. Wu",
            "Y. Deng",
            "J. Hu",
            "L. Cui",
            "Z. Zhang",
            "L. Zeng",
            "G. Min"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-02-28"
    },
    "http://arxiv.org/abs/2502.20818v1": {
        "id": "http://arxiv.org/abs/2502.20818v1",
        "title": "SkyStore: Cost-Optimized Object Storage Across Regions and Clouds",
        "link": "http://arxiv.org/abs/2502.20818v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-03-03",
        "tldr": "",
        "abstract": "Modern applications span multiple clouds to reduce costs, avoid vendor lock-in, and leverage low-availability resources in another cloud. However, standard object stores operate within a single cloud, forcing users to manually manage data placement across clouds, i.e., navigate their diverse APIs and handle heterogeneous costs for network and storage. This is often a complex choice: users must either pay to store objects in a remote cloud, or pay to transfer them over the network based on application access patterns and cloud provider cost offerings. To address this, we present SkyStore, a unified object store that addresses cost-optimal data management across regions and clouds. SkyStore introduces a virtual object and bucket API to hide the complexity of interacting with multiple clouds. At its core, SkyStore has a novel TTL-based data placement policy that dynamically replicates and evicts objects according to application access patterns while optimizing for lower cost. Our evaluation shows that across various workloads, SkyStore reduces the overall cost by up to 6x over academic baselines and commercial alternatives like AWS multi-region buckets. SkyStore also has comparable latency, and its availability and fault tolerance are on par with standard cloud offerings. We release the data and code of SkyStore at https://github.com/skyplane-project/skystore.",
        "authors": [
            "Shu Liu",
            "Xiangxi Mo",
            "Moshik Hershcovitch",
            "Henric Zhang",
            "Audrey Cheng",
            "Guy Girmonsky",
            "Gil Vernik",
            "Michael Factor",
            "Tiemo Bang",
            "Soujanya Ponnapalli",
            "Natacha Crooks",
            "Joseph E. Gonzalez",
            "Danny Harnik",
            "Ion Stoica"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-02-28"
    },
    "http://arxiv.org/abs/2502.20825v1": {
        "id": "http://arxiv.org/abs/2502.20825v1",
        "title": "LADs: Leveraging LLMs for AI-Driven DevOps",
        "link": "http://arxiv.org/abs/2502.20825v1",
        "tags": [
            "RAG"
        ],
        "relevant": true,
        "indexed_date": "2025-03-03",
        "tldr": "Proposes LADs, an LLM-driven framework for automated cloud configuration using RAG, Few-Shot Learning, and feedback-based prompt chaining for iterative refinement. Reduces manual effort and improves configuration accuracy in multi-tenant environments.",
        "abstract": "Automating cloud configuration and deployment remains a critical challenge due to evolving infrastructures, heterogeneous hardware, and fluctuating workloads. Existing solutions lack adaptability and require extensive manual tuning, leading to inefficiencies and misconfigurations. We introduce LADs, the first LLM-driven framework designed to tackle these challenges by ensuring robustness, adaptability, and efficiency in automated cloud management. Instead of merely applying existing techniques, LADs provides a principled approach to configuration optimization through in-depth analysis of what optimization works under which conditions. By leveraging Retrieval-Augmented Generation, Few-Shot Learning, Chain-of-Thought, and Feedback-Based Prompt Chaining, LADs generates accurate configurations and learns from deployment failures to iteratively refine system settings. Our findings reveal key insights into the trade-offs between performance, cost, and scalability, helping practitioners determine the right strategies for different deployment scenarios. For instance, we demonstrate how prompt chaining-based adaptive feedback loops enhance fault tolerance in multi-tenant environments and how structured log analysis with example shots improves configuration accuracy. Through extensive evaluations, LADs reduces manual effort, optimizes resource utilization, and improves system reliability. By open-sourcing LADs, we aim to drive further innovation in AI-powered DevOps automation.",
        "authors": [
            "Ahmad Faraz Khan",
            "Azal Ahmad Khan",
            "Anas Mohamed",
            "Haider Ali",
            "Suchithra Moolinti",
            "Sabaat Haroon",
            "Usman Tahir",
            "Mattia Fazzini",
            "Ali R. Butt",
            "Ali Anwar"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC",
            "cs.SE"
        ],
        "submit_date": "2025-02-28"
    },
    "http://arxiv.org/abs/2502.19913v1": {
        "id": "http://arxiv.org/abs/2502.19913v1",
        "title": "SkipPipe: Partial and Reordered Pipelining Framework for Training LLMs in Heterogeneous Networks",
        "link": "http://arxiv.org/abs/2502.19913v1",
        "tags": [
            "training",
            "offloading",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-02-28",
        "tldr": "Proposes SkipPipe, a partial pipeline framework with stage skipping and reordering to reduce LLM training time in heterogeneous networks. Introduces a path scheduling algorithm to minimize idle time while preserving convergence. Reduces training iteration time by up to 55% compared to full pipelining.",
        "abstract": "Data and pipeline parallelism are ubiquitous for training of Large Language Models (LLM) on distributed nodes. Driven by the need for cost-effective training, recent work explores efficient communication arrangement for end to end training. Motivated by LLM's resistance to layer skipping and layer reordering, in this paper, we explore stage (several consecutive layers) skipping in pipeline training, and challenge the conventional practice of sequential pipeline execution. We derive convergence and throughput constraints (guidelines) for pipelining with skipping and swapping pipeline stages. Based on these constraints, we propose SkipPipe, the first partial pipeline framework to reduce the end-to-end training time for LLMs while preserving the convergence. The core of SkipPipe is a path scheduling algorithm that optimizes the paths for individual microbatches and reduces idle time (due to microbatch collisions) on the distributed nodes, complying with the given stage skipping ratio. We extensively evaluate SkipPipe on LLaMa models from 500M to 8B parameters on up to 20 nodes. Our results show that SkipPipe reduces training iteration time by up to $55\\%$ compared to full pipeline. Our partial pipeline training also improves resistance to layer omission during inference, experiencing a drop in perplexity of only $7\\%$ when running only half the model. Our code is available at https://github.com/gensyn-ai/skippipe.",
        "authors": [
            "Nikolay Blagoev",
            "Lydia Yiyu Chen",
            "Oğuzhan Ersoy"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "submit_date": "2025-02-27"
    },
    "http://arxiv.org/abs/2502.18910v1": {
        "id": "http://arxiv.org/abs/2502.18910v1",
        "title": "CLLoRA: An Approach to Measure the Effects of the Context Length for LLM Fine-Tuning",
        "link": "http://arxiv.org/abs/2502.18910v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-02-27",
        "tldr": "",
        "abstract": "Large language model fine-tuning has been identified as an efficient approach to applying the pre-trained Large language models to other domains. To guarantee data privacy for different data owners, models are often fine-tuned in federated learning environments across different data owners, which often involve data heterogeneity issues and affect the fine-tuning performance. In addition, the length of the context for the training data has been identified as a major factor that affects the LLM's model performance.   To efficiently measure how the context length affects the LLM's model performance in heterogeneous federated learning environments, we propose CLLoRA. CLLoRA utilizes the parameter-efficient fine-tuning approach LoRA based on different kinds of LLMs with varying sizes as the fine-tuning approach to investigate whether the quality and length of contexts can serve as standards for measuring non-IID context. The findings indicate that an imbalance in context quality not only affects local training on clients but also impacts the global model's performance. However, context length has a minimal effect on local training but a more significant influence on the global model. These results provide insights into how context quality and length affect the model performance for LLM fine-tuning in federated learning environments.",
        "authors": [
            "Ping Zhang",
            "Zhaorui Zhang",
            "Sheng Di",
            "Yao Xin",
            "Benben Liu"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "submit_date": "2025-02-26"
    },
    "http://arxiv.org/abs/2502.15804v2": {
        "id": "http://arxiv.org/abs/2502.15804v2",
        "title": "FairKV: Balancing Per-Head KV Cache for Fast Multi-GPU Inference",
        "link": "http://arxiv.org/abs/2502.15804v2",
        "tags": [
            "serving",
            "offloading",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-02-25",
        "tldr": "Addresses load imbalance in multi-GPU inference with imbalanced KV cache compression. Introduces FairKV with Fair-Copying to replicate memory-intensive heads across GPUs. Achieves 1.66x throughput improvement over tensor parallelism for LLaMA 70b.",
        "abstract": "KV cache techniques in Transformer models aim to reduce redundant computations at the expense of substantially increased memory usage, making KV cache compression an important and popular research topic. Recently, state-of-the-art KV cache compression methods implement imbalanced, per-head allocation algorithms that dynamically adjust the KV cache budget for each attention head, achieving excellent performance in single-GPU scenarios. However, we observe that such imbalanced compression leads to significant load imbalance when deploying multi-GPU inference, as some GPUs become overburdened while others remain underutilized. In this paper, we propose FairKV, a method designed to ensure fair memory usage among attention heads in systems employing imbalanced KV cache compression. The core technique of FairKV is Fair-Copying, which replicates a small subset of memory-intensive attention heads across GPUs using data parallelism to mitigate load imbalance. Our experiments on popular models, including LLaMA 70b and Mistral 24b model, demonstrate that FairKV increases throughput by 1.66x compared to standard tensor parallelism inference. Our code will be released as open source upon acceptance.",
        "authors": [
            "Bingzhe Zhao",
            "Ke Cheng",
            "Aomufei Yuan",
            "Yuxuan Tian",
            "Ruiguang Zhong",
            "Chengchen Hu",
            "Tong Yang",
            "Lian Yu"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-02-19"
    },
    "http://arxiv.org/abs/2502.15763v1": {
        "id": "http://arxiv.org/abs/2502.15763v1",
        "title": "Hybrid Offline-online Scheduling Method for Large Language Model Inference Optimization",
        "link": "http://arxiv.org/abs/2502.15763v1",
        "tags": [
            "serving",
            "online"
        ],
        "relevant": true,
        "indexed_date": "2025-02-25",
        "tldr": "Proposes a hybrid offline-online scheduling method for LLM inference optimization. Uses MIP for offline bin packing and online preemptive scheduling with Lagrangian cost efficiency. Improves utilization to 89.1% (from 80.2%) and reduces total inference time from 201 to 190.58 seconds.",
        "abstract": "With the development of large language models (LLMs), it has become increasingly important to optimize hardware usage and improve throughput. In this paper, we study the inference optimization of the serving system that deploys LLMs. To optimize system throughput and maximize hardware utilization, we formulate the inference optimization problem as a mixed-integer programming (MIP) model and propose a hybrid offline-online method as solution. The offline method improves large-scale inference systems by introducing a Minimizing Makespan Bin Packing Problem. We further provide a theoretical lower bound computation method. Then, we propose an online sorting and preemptive scheduling method to better utilize hardware. In the online iteration scheduling process, a Lagrangian method is applied to evaluate the cost efficiency of inserting prefill stages versus decode stages at each iteration and dynamically determine when to preempt decoding tasks and insert prefill tasks. Experiments using real-world data from the LLaMA-65B model and the GSM8K dataset demonstrate that system utilization improves from 80.2% to 89.1%, and the total inference time decreases from 201.00 to 190.58 seconds. A 100-cases study shows that our method consistently outperforms the baseline method and improves the utilization rate by 8.0% on average. Finally, we discuss potential future extensions, including stochastic modeling, reinforcement learning-based schedulers, and dynamic decision-making strategies for system throughput and hardware utilization.",
        "authors": [
            "Bowen Pang",
            "Kai Li",
            "Ruifeng She",
            "Feifan Wang"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.AR",
            "cs.LG"
        ],
        "submit_date": "2025-02-14"
    },
    "http://arxiv.org/abs/2502.15761v2": {
        "id": "http://arxiv.org/abs/2502.15761v2",
        "title": "AIvaluateXR: An Evaluation Framework for on-Device AI in XR with Benchmarking Results",
        "link": "http://arxiv.org/abs/2502.15761v2",
        "tags": [
            "edge",
            "serving",
            "inference"
        ],
        "relevant": true,
        "indexed_date": "2025-02-25",
        "tldr": "Proposes AIvaluateXR, a framework to benchmark LLMs on XR devices, measuring consistency, speed, memory, and battery. Evaluates 68 model-device pairs under varying conditions with Pareto Optimality for optimal selection. Achieves insights for efficient on-device inference versus cloud setups.",
        "abstract": "The deployment of large language models (LLMs) on extended reality (XR) devices has great potential to advance the field of human-AI interaction. In the case of direct, on-device model inference, selecting the appropriate model and device for specific tasks remains challenging. In this paper, we present AIvaluateXR, a comprehensive evaluation framework for benchmarking LLMs running on XR devices. To demonstrate the framework, we deploy 17 selected LLMs across four XR platforms: Magic Leap 2, Meta Quest 3, Vivo X100s Pro, and Apple Vision Pro, and conduct an extensive evaluation. Our experimental setup measures four key metrics: performance consistency, processing speed, memory usage, and battery consumption. For each of the 68 model-device pairs, we assess performance under varying string lengths, batch sizes, and thread counts, analyzing the trade-offs for real-time XR applications. We propose a unified evaluation method based on the 3D Pareto Optimality theory to select the optimal device-model pairs from quality and speed objectives. Additionally, we compare the efficiency of on-device LLMs with client-server and cloud-based setups, and evaluate their accuracy on two interactive tasks. We believe our findings offer valuable insight to guide future optimization efforts for LLM deployment on XR devices. Our evaluation method can be used as standard groundwork for further research and development in this emerging field. The source code and supplementary materials are available at: www.nanovis.org/AIvaluateXR.html",
        "authors": [
            "Dawar Khan",
            "Xinyu Liu",
            "Omar Mena",
            "Donggang Jia",
            "Alexandre Kouyoumdjian",
            "Ivan Viola"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.GR",
            "cs.HC"
        ],
        "submit_date": "2025-02-13"
    },
    "http://arxiv.org/abs/2502.15735v1": {
        "id": "http://arxiv.org/abs/2502.15735v1",
        "title": "DistrEE: Distributed Early Exit of Deep Neural Network Inference on Edge Devices",
        "link": "http://arxiv.org/abs/2502.15735v1",
        "tags": [
            "edge",
            "serving",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-02-25",
        "tldr": "Proposes DistrEE, a distributed DNN inference framework for edge devices with early exit to balance latency and accuracy. Integrates model early exit and distributed inference with a policy to terminate inference. Achieves efficient collaborative inference with trade-off between latency and accuracy.",
        "abstract": "Distributed DNN inference is becoming increasingly important as the demand for intelligent services at the network edge grows. By leveraging the power of distributed computing, edge devices can perform complicated and resource-hungry inference tasks previously only possible on powerful servers, enabling new applications in areas such as autonomous vehicles, industrial automation, and smart homes. However, it is challenging to achieve accurate and efficient distributed edge inference due to the fluctuating nature of the actual resources of the devices and the processing difficulty of the input data. In this work, we propose DistrEE, a distributed DNN inference framework that can exit model inference early to meet specific quality of service requirements. In particular, the framework firstly integrates model early exit and distributed inference for multi-node collaborative inferencing scenarios. Furthermore, it designs an early exit policy to control when the model inference terminates. Extensive simulation results demonstrate that DistrEE can efficiently realize efficient collaborative inference, achieving an effective trade-off between inference latency and accuracy.",
        "authors": [
            "Xian Peng",
            "Xin Wu",
            "Lianming Xu",
            "Li Wang",
            "Aiguo Fei"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.LG"
        ],
        "submit_date": "2025-02-06"
    },
    "http://arxiv.org/abs/2502.15524v2": {
        "id": "http://arxiv.org/abs/2502.15524v2",
        "title": "HydraServe: Minimizing Cold Start Latency for Serverless LLM Serving in Public Clouds",
        "link": "http://arxiv.org/abs/2502.15524v2",
        "tags": [
            "serving",
            "offloading",
            "autoscaling"
        ],
        "relevant": true,
        "indexed_date": "2025-02-24",
        "tldr": "Addresses high cold start latency in serverless LLM serving on public clouds. Introduces HydraServe with proactive model distribution, worker placement optimization, and pipeline consolidation for overlapping startup stages. Reduces cold start latency by 1.7×–4.7× and improves SLO attainment by 1.43×–1.74×.",
        "abstract": "With the proliferation of large language model (LLM) variants, developers are turning to serverless computing for cost-efficient LLM deployment. However, public cloud providers often struggle to provide performance guarantees for serverless LLM serving due to significant cold start latency caused by substantial model sizes and complex runtime dependencies. To address this problem, we present HydraServe, a serverless LLM serving system designed to minimize cold start latency in public clouds. HydraServe proactively distributes models across servers to quickly fetch them, and overlaps cold-start stages within workers to reduce startup latency. Additionally, HydraServe strategically places workers across GPUs to avoid network contention among cold-start instances. To minimize resource consumption during cold starts, HydraServe further introduces pipeline consolidation that can merge groups of workers into individual serving endpoints. Our comprehensive evaluations under diverse settings demonstrate that HydraServe reduces the cold start latency by 1.7$\\times$-- 4.7$\\times$ and improves service level objective attainment by 1.43$\\times$--1.74$\\times$ compared to baselines.",
        "authors": [
            "Chiheng Lou",
            "Sheng Qi",
            "Chao Jin",
            "Dapeng Nie",
            "Haoran Yang",
            "Yu Ding",
            "Xuanzhe Liu",
            "Xin Jin"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-02-21"
    },
    "http://arxiv.org/abs/2502.15312v1": {
        "id": "http://arxiv.org/abs/2502.15312v1",
        "title": "FlexPie: Accelerate Distributed Inference on Edge Devices with Flexible Combinatorial Optimization[Technical Report]",
        "link": "http://arxiv.org/abs/2502.15312v1",
        "tags": [
            "edge",
            "serving",
            "autoscaling"
        ],
        "relevant": true,
        "indexed_date": "2025-02-24",
        "tldr": "Addresses efficient distributed DNN inference on edge devices with dynamic workloads. Introduces FlexPie, a flexible combinatorial optimization approach for task scheduling and resource allocation. Achieves up to 3.2× throughput improvement and 45% latency reduction compared to baselines.",
        "abstract": "The rapid advancement of deep learning has catalyzed the development of novel IoT applications, which often deploy pre-trained deep neural network (DNN) models across multiple edge devices for collaborative inference.",
        "authors": [
            "Runhua Zhang",
            "Hongxu Jiang",
            "Jinkun Geng",
            "Yuhang Ma",
            "Chenhui Zhu",
            "Haojie Wang"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-02-21"
    },
    "http://arxiv.org/abs/2502.14617v3": {
        "id": "http://arxiv.org/abs/2502.14617v3",
        "title": "SageServe: Optimizing LLM Serving on Cloud Data Centers with Forecast Aware Auto-Scaling",
        "link": "http://arxiv.org/abs/2502.14617v3",
        "tags": [
            "serving",
            "autoscaling",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-02-21",
        "tldr": "Proposes SageServe, a forecast-aware autoscaling system for LLM serving in cloud data centers, combining multi-timescale routing, GPU scaling, and model placement via traffic forecasts and ILP. Achieves 25% GPU-hour savings and 80% reduction in scaling wastage, saving up to $2.5M monthly while meeting SLAs.",
        "abstract": "Global cloud service providers handle inference workloads for Large Language Models (LLMs) that span latency-sensitive (e.g., chatbots) and insensitive (e.g., report writing) tasks, resulting in diverse and often conflicting Service Level Agreement (SLA) requirements. Managing such mixed workloads is challenging due to the complexity of the inference serving stack, which encompasses multiple models, GPU hardware, and global data centers. Existing solutions often silo such fast and slow tasks onto separate GPU resource pools with different SLAs, but this leads to significant under-utilization of expensive accelerators due to load mismatch. In this article, we characterize the LLM serving workloads at Microsoft Office 365, one of the largest users of LLMs within Microsoft Azure cloud with over 10 million requests per day, and highlight key observations across workloads in different data center regions and across time. This is one of the first such public studies of Internet-scale LLM workloads. We use these insights to propose SageServe, a comprehensive LLM serving framework that dynamically adapts to workload demands using multi-timescale control knobs. It combines short-term request routing to data centers with long-term scaling of GPU VMs and model placement with higher lead times, and co-optimizes the routing and resource allocation problem using a traffic forecast model and an Integer Linear Programming (ILP) solution. We evaluate SageServe through real runs and realistic simulations on 10 million production requests across three regions and four open-source models. We achieve up to 25% savings in GPU-hours compared to the current baseline deployment and reduce GPU-hour wastage due to inefficient auto-scaling by 80%, resulting in a potential monthly cost savings of up to $2.5 million, while maintaining tail latency and meeting SLAs.",
        "authors": [
            "Shashwat Jaiswal",
            "Kunal Jain",
            "Yogesh Simmhan",
            "Anjaly Parayil",
            "Ankur Mallick",
            "Rujia Wang",
            "Renee St. Amant",
            "Chetan Bansal",
            "Victor Rühle",
            "Anoop Kulkarni",
            "Steve Kofsky",
            "Saravan Rajmohan"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-02-20"
    },
    "http://arxiv.org/abs/2502.14419v1": {
        "id": "http://arxiv.org/abs/2502.14419v1",
        "title": "Optimizing the Longhorn Cloud-native Software Defined Storage Engine for High Performance",
        "link": "http://arxiv.org/abs/2502.14419v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-02-21",
        "tldr": "",
        "abstract": "Longhorn is an open-source, cloud-native software-defined storage (SDS) engine that delivers distributed block storage management in Kubernetes environments. This paper explores performance optimization techniques for Longhorn's core component, the Longhorn engine, to overcome limitations in leveraging high-performance server hardware, such as solid-state NVMe disks and low-latency, high-bandwidth networking. By integrating ublk at the frontend, to expose the virtual block device to the operating system, restructuring the communication protocol, and employing DBS, our simplified, direct-to-disk storage scheme, the system achieves significant performance improvements with respect to the default I/O path. Our results contribute to enhancing Longhorn's applicability in both cloud and on-premises setups, as well as provide insights for the broader SDS community.",
        "authors": [
            "Konstantinos Kampadais",
            "Antony Chazapis",
            "Angelos Bilas"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-02-20"
    },
    "http://arxiv.org/abs/2502.14076v2": {
        "id": "http://arxiv.org/abs/2502.14076v2",
        "title": "CarbonEdge: Leveraging Mesoscale Spatial Carbon-Intensity Variations for Low Carbon Edge Computing",
        "link": "http://arxiv.org/abs/2502.14076v2",
        "tags": [
            "offline",
            "edge",
            "autoscaling"
        ],
        "relevant": true,
        "indexed_date": "2025-02-21",
        "tldr": "Proposes CarbonEdge, a carbon-aware framework optimizing edge workload placement across mesoscale data centers to reduce emissions while meeting latency SLOs. Achieves up to 78.7% emissions reduction for regional edge deployments with under 5.5 ms latency increase.",
        "abstract": "The proliferation of latency-critical and compute-intensive edge applications is driving increases in computing demand and carbon emissions at the edge. To better understand carbon emissions at the edge, we analyze granular carbon intensity traces at intermediate \"mesoscales,\" such as within a single US state or among neighboring countries in Europe, and observe significant variations in carbon intensity at these spatial scales. Importantly, our analysis shows that carbon intensity variations, which are known to occur at large continental scales (e.g., cloud regions), also occur at much finer spatial scales, making it feasible to exploit geographic workload shifting in the edge computing context. Motivated by these findings, we propose \\proposedsystem, a carbon-aware framework for edge computing that optimizes the placement of edge workloads across mesoscale edge data centers to reduce carbon emissions while meeting latency SLOs. We implement CarbonEdge and evaluate it on a real edge computing testbed and through large-scale simulations for multiple edge workloads and settings. Our experimental results on a real testbed demonstrate that CarbonEdge can reduce emissions by up to 78.7\\% for a regional edge deployment in central Europe. Moreover, our CDN-scale experiments show potential savings of 49.5\\% and 67.8\\% in the US and Europe, respectively, while limiting the one-way latency increase to less than 5.5 ms.",
        "authors": [
            "Li Wu",
            "Walid A. Hanafy",
            "Abel Souza",
            "Khai Nguyen",
            "Jan Harkes",
            "David Irwin",
            "Mahadev Satyanarayanan",
            "Prashant Shenoy"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-02-19"
    },
    "http://arxiv.org/abs/2502.14866v2": {
        "id": "http://arxiv.org/abs/2502.14866v2",
        "title": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention",
        "link": "http://arxiv.org/abs/2502.14866v2",
        "tags": [
            "serving",
            "sparse",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-02-21",
        "tldr": "Proposes LServe for efficient long-sequence LLM serving by unifying hardware-friendly sparse attention patterns in prefilling and decoding. Introduces hybrid sparse attention and hierarchical KV page selection, accelerating prefilling by up to 2.9x and decoding by 1.3-2.1x over vLLM while maintaining accuracy.",
        "abstract": "Large language models (LLMs) have shown remarkable potential in processing long sequences and complex reasoning tasks, yet efficiently serving these models remains challenging due to the quadratic computational complexity of attention in the prefilling stage and the large memory footprint of the KV cache in the decoding stage. To address these issues, we introduce LServe, an efficient system that accelerates long-sequence LLM serving via hybrid sparse attention. This method unifies different hardware-friendly, structured sparsity patterns for both prefilling and decoding attention into a single framework, where computations on less important tokens are skipped block-wise. LServe demonstrates the compatibility of static and dynamic sparsity in long-context LLM attention. This design enables multiplicative speedups by combining these optimizations. Specifically, we convert half of the attention heads to nearly free streaming heads in both the prefilling and decoding stages. Additionally, we find that only a constant number of KV pages is required to preserve long-context and reasoning capabilities, irrespective of context length. We then design a hierarchical KV page selection policy that dynamically prunes KV pages based on query-centric similarity. On average, LServe accelerates LLM prefilling by up to 2.9x and decoding by 1.3-2.1x over vLLM, maintaining long-context accuracy. Code is released at https://github.com/mit-han-lab/omniserve.",
        "authors": [
            "Shang Yang",
            "Junxian Guo",
            "Haotian Tang",
            "Qinghao Hu",
            "Guangxuan Xiao",
            "Jiaming Tang",
            "Yujun Lin",
            "Zhijian Liu",
            "Yao Lu",
            "Song Han"
        ],
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.DC",
            "cs.LG",
            "cs.PF"
        ],
        "submit_date": "2025-02-20"
    },
    "http://arxiv.org/abs/2502.14450v2": {
        "id": "http://arxiv.org/abs/2502.14450v2",
        "title": "LLM4FaaS: No-Code Application Development using LLMs and FaaS",
        "link": "http://arxiv.org/abs/2502.14450v2",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-02-21",
        "tldr": "",
        "abstract": "Large language models (LLMs) show great capabilities in generating code from natural language descriptions, bringing programming power closer to non-technical users. However, their lack of expertise in operating the generated code remains a key barrier to realizing customized applications. Function-as-a-Service (FaaS) platforms offer a high level of abstraction for code execution and deployment, allowing users to run LLM-generated code without requiring technical expertise or incurring operational overhead.   In this paper, we present LLM4FaaS, a no-code application development approach that integrates LLMs and FaaS platforms to enable non-technical users to build and run customized applications using only natural language. By deploying LLM-generated code through FaaS, LLM4FaaS abstracts away infrastructure management and boilerplate code generation. We implement a proof-of-concept prototype based on an open-source FaaS platform, and evaluate it using real prompts from non-technical users. Experiments with GPT-4o show that LLM4FaaS can automatically build and deploy code in 71.47% of cases, outperforming a non-FaaS baseline at 43.48% and an existing LLM-based platform at 14.55%, narrowing the gap to human performance at 88.99%. Further analysis of code quality, programming language diversity, latency, and consistency demonstrates a balanced performance in terms of efficiency, maintainability and availability.",
        "authors": [
            "Minghe Wang",
            "Tobias Pfandzelter",
            "Trever Schirmer",
            "David Bermbach"
        ],
        "categories": [
            "cs.SE",
            "cs.DC"
        ],
        "submit_date": "2025-02-20"
    },
    "http://arxiv.org/abs/2502.13965v1": {
        "id": "http://arxiv.org/abs/2502.13965v1",
        "title": "Autellix: An Efficient Serving Engine for LLM Agents as General Programs",
        "link": "http://arxiv.org/abs/2502.13965v1",
        "tags": [
            "agentic",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-02-20",
        "tldr": "Addresses inefficiency in serving LLM agentic programs due to call dependencies. Proposes Autellix, a serving system that schedules LLM calls with program-level context and preemption. Improves program throughput by 4-15x at same latency vs vLLM.",
        "abstract": "Large language model (LLM) applications are evolving beyond simple chatbots into dynamic, general-purpose agentic programs, which scale LLM calls and output tokens to help AI agents reason, explore, and solve complex tasks. However, existing LLM serving systems ignore dependencies between programs and calls, missing significant opportunities for optimization. Our analysis reveals that programs submitted to LLM serving engines experience long cumulative wait times, primarily due to head-of-line blocking at both the individual LLM request and the program. To address this, we introduce Autellix, an LLM serving system that treats programs as first-class citizens to minimize their end-to-end latencies. Autellix intercepts LLM calls submitted by programs, enriching schedulers with program-level context. We propose two scheduling algorithms-for single-threaded and distributed programs-that preempt and prioritize LLM calls based on their programs' previously completed calls. Our evaluation demonstrates that across diverse LLMs and agentic workloads, Autellix improves throughput of programs by 4-15x at the same latency compared to state-of-the-art systems, such as vLLM.",
        "authors": [
            "Michael Luo",
            "Xiaoxiang Shi",
            "Colin Cai",
            "Tianjun Zhang",
            "Justin Wong",
            "Yichuan Wang",
            "Chi Wang",
            "Yanping Huang",
            "Zhifeng Chen",
            "Joseph E. Gonzalez",
            "Ion Stoica"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC"
        ],
        "submit_date": "2025-02-19"
    },
    "http://arxiv.org/abs/2502.12784v3": {
        "id": "http://arxiv.org/abs/2502.12784v3",
        "title": "SparkAttention: High-Performance Multi-Head Attention for Large Models on Volta GPU Architecture",
        "link": "http://arxiv.org/abs/2502.12784v3",
        "tags": [
            "training",
            "kernel",
            "hardware"
        ],
        "relevant": true,
        "indexed_date": "2025-02-19",
        "tldr": "Proposes SparkAttention, a library accelerating Multi-Head Attention training on Volta GPUs via Tensor Core Units and kernel fusion to reduce HBM accesses. Achieves 1.80× average speedup on V100 GPU compared to PyTorch.",
        "abstract": "Transformer are widely used in various fields such as natural language processing and computer vision. However, the training time for large Transformer models can be challenging due to the Multi-Head Attention (MHA) mechanism. Especially as models become larger, training becomes more costly. So it is crucial to utilize various resources for efficient model training. Currently, NVIDIA Volta GPU is still widely used. However, because the computational shapes supported by Tensor Core Units (TCU) of Volta GPU differ from other GPU architectures, most efforts have not focused on using them to accelerate Transformer training. To address this issue, we propose SparkAttention, an acceleration library designed to speed up MHA training on the Volta GPU. SparkAttention leverages TCU and kernel fusion to reduce the number of high bandwidth memory (HBM) accesses and overhead. Our End-to-End experimental results on an NVIDIA V100 GPU show that SparkAttention achieves on average 1.80$\\times$ (up to 2.46$\\times$) speedup compared to using PyTorch.",
        "authors": [
            "Youxuan Xu",
            "Tong Wu",
            "Shigang Li",
            "Xueying Wang",
            "Jingjing Wang"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-02-18"
    },
    "http://arxiv.org/abs/2502.12559v1": {
        "id": "http://arxiv.org/abs/2502.12559v1",
        "title": "Distributed On-Device LLM Inference With Over-the-Air Computation",
        "link": "http://arxiv.org/abs/2502.12559v1",
        "tags": [
            "edge",
            "networking",
            "training"
        ],
        "relevant": true,
        "indexed_date": "2025-02-19",
        "tldr": "Proposes distributed on-device LLM inference with tensor parallelism, leveraging over-the-air computation for fast all-reduce operations. Develops joint model assignment and transceiver optimization to minimize transmission error. Reduces inference latency by up to 2x while maintaining model accuracy for edge devices.",
        "abstract": "Large language models (LLMs) have achieved remarkable success across various artificial intelligence tasks. However, their enormous sizes and computational demands pose significant challenges for the deployment on edge devices. To address this issue, we present a distributed on-device LLM inference framework based on tensor parallelism, which partitions neural network tensors (e.g., weight matrices) of LLMs among multiple edge devices for collaborative inference. Nevertheless, tensor parallelism involves frequent all-reduce operations to aggregate intermediate layer outputs across participating devices during inference, resulting in substantial communication overhead. To mitigate this bottleneck, we propose an over-the-air computation method that leverages the analog superposition property of wireless multiple-access channels to facilitate fast all-reduce operations. To minimize the average transmission mean-squared error, we investigate joint model assignment and transceiver optimization, which can be formulated as a mixed-timescale stochastic non-convex optimization problem. Then, we develop a mixed-timescale algorithm leveraging semidefinite relaxation and stochastic successive convex approximation methods. Comprehensive simulation results will show that the proposed approach significantly reduces inference latency while improving accuracy. This makes distributed on-device LLM inference practical for resource-constrained edge devices.",
        "authors": [
            "Kai Zhang",
            "Hengtao He",
            "Shenghui Song",
            "Jun Zhang",
            "Khaled B. Letaief"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-02-18"
    },
    "http://arxiv.org/abs/2502.12340v1": {
        "id": "http://arxiv.org/abs/2502.12340v1",
        "title": "Understanding Silent Data Corruption in LLM Training",
        "link": "http://arxiv.org/abs/2502.12340v1",
        "tags": [
            "training",
            "hardware",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-02-19",
        "tldr": "Investigates the impact of silent data corruption (SDC) on LLM training through deterministic execution and synchronization. Analyzes SDC effects at submodule, optimizer step, and training period levels using real-world faulty nodes. Reveals significant perturbations causing divergent convergence and training loss spikes despite small local errors.",
        "abstract": "As the scale of training large language models (LLMs) increases, one emergent failure is silent data corruption (SDC), where hardware produces incorrect computations without explicit failure signals. In this work, we are the first to investigate the impact of real-world SDCs on LLM training by comparing model training between healthy production nodes and unhealthy nodes exhibiting SDCs. With the help from a cloud computing platform, we access the unhealthy nodes that were swept out from production by automated fleet management. Using deterministic execution via XLA compiler and our proposed synchronization mechanisms, we isolate and analyze the impact of SDC errors on these nodes at three levels: at each submodule computation, at a single optimizer step, and at a training period. Our results reveal that the impact of SDCs on computation varies on different unhealthy nodes. Although in most cases the perturbations from SDCs on submodule computation and gradients are relatively small, SDCs can lead models to converge to different optima with different weights and even cause spikes in the training loss. Our analysis sheds light on further understanding and mitigating the impact of SDCs.",
        "authors": [
            "Jeffrey Ma",
            "Hengzhi Pei",
            "Leonard Lausen",
            "George Karypis"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "submit_date": "2025-02-17"
    },
    "http://arxiv.org/abs/2502.10151v1": {
        "id": "http://arxiv.org/abs/2502.10151v1",
        "title": "Semantica: Decentralized Search using a LLM-Guided Semantic Tree Overlay",
        "link": "http://arxiv.org/abs/2502.10151v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-02-19",
        "tldr": "",
        "abstract": "Centralized search engines are key for the Internet, but lead to undesirable concentration of power. Decentralized alternatives fail to offer equal document retrieval accuracy and speed. Nevertheless, Semantic Overlay Networks can come close to the performance of centralized solutions when the semantics of documents are properly captured. This work uses embeddings from Large Language Models to capture semantics and fulfill the promise of Semantic Overlay Networks. Our proposed algorithm, called Semantica, constructs a prefix tree (trie) utilizing document embeddings calculated by a language model. Users connect to each other based on the embeddings of their documents, ensuring that semantically similar users are directly linked. Thereby, this construction makes it more likely for user searches to be answered by the users that they are directly connected to, or by the users they are close to in the network connection graph. The implementation of our algorithm also accommodates the semantic diversity of individual users by spawning \"clone\" user identifiers in the tree. Our experiments use emulation with a real-world workload to show Semantica's ability to identify and connect to similar users quickly. Semantica finds up to ten times more semantically similar users than current state-of-the-art approaches. At the same time, Semantica can retrieve more than two times the number of relevant documents given the same network load. We also make our code publicly available to facilitate further research in the area.",
        "authors": [
            "Petru Neague",
            "Quinten Stokkink",
            "Naman Goel",
            "Johan Pouwelse"
        ],
        "categories": [
            "cs.IR",
            "cs.DC",
            "cs.NI",
            "eess.SY"
        ],
        "submit_date": "2025-02-14"
    },
    "http://arxiv.org/abs/2502.12017v1": {
        "id": "http://arxiv.org/abs/2502.12017v1",
        "title": "Scalable and Cost-Efficient ML Inference: Parallel Batch Processing with Serverless Functions",
        "link": "http://arxiv.org/abs/2502.12017v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-02-18",
        "tldr": "",
        "abstract": "As data-intensive applications grow, batch processing in limited-resource environments faces scalability and resource management challenges. Serverless computing offers a flexible alternative, enabling dynamic resource allocation and automatic scaling. This paper explores how serverless architectures can make large-scale ML inference tasks faster and cost-effective by decomposing monolithic processes into parallel functions. Through a case study on sentiment analysis using the DistilBERT model and the IMDb dataset, we demonstrate that serverless parallel processing can reduce execution time by over 95% compared to monolithic approaches, at the same cost.",
        "authors": [
            "Amine Barrak",
            "Emna Ksontini"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-01-30"
    },
    "http://arxiv.org/abs/2502.11464v3": {
        "id": "http://arxiv.org/abs/2502.11464v3",
        "title": "BagChain: A Dual-functional Blockchain Leveraging Bagging-based Distributed Learning",
        "link": "http://arxiv.org/abs/2502.11464v3",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-02-18",
        "tldr": "",
        "abstract": "This work proposes a dual-functional blockchain framework named BagChain for bagging-based decentralized learning. BagChain integrates blockchain with distributed machine learning by replacing the computationally costly hash operations in proof-of-work with machine-learning model training. BagChain utilizes individual miners' private data samples and limited computing resources to train potentially weak base models, which may be very weak, and further aggregates them into strong ensemble models. Specifically, we design a three-layer blockchain structure associated with the corresponding generation and validation mechanisms to enable distributed machine learning among uncoordinated miners in a permissionless and open setting. To reduce computational waste due to blockchain forking, we further propose the cross fork sharing mechanism for practical networks with lengthy delays. Extensive experiments illustrate the superiority and efficacy of BagChain when handling various machine learning tasks on both independently and identically distributed (IID) and non-IID datasets. BagChain remains robust and effective even when facing constrained local computing capability, heterogeneous private user data, and sparse network connectivity.",
        "authors": [
            "Zixiang Cui",
            "Xintong Ling",
            "Xingyu Zhou",
            "Jiaheng Wang",
            "Zhi Ding",
            "Xiqi Gao"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-02-17"
    },
    "http://arxiv.org/abs/2502.11058v1": {
        "id": "http://arxiv.org/abs/2502.11058v1",
        "title": "DreamDDP: Accelerating Data Parallel Distributed LLM Training with Layer-wise Scheduled Partial Synchronization",
        "link": "http://arxiv.org/abs/2502.11058v1",
        "tags": [
            "training",
            "networking",
            "sparse"
        ],
        "relevant": true,
        "indexed_date": "2025-02-18",
        "tldr": "Proposes DreamDDP, a layer-wise partially synchronized SGD method for distributed LLM training in low-bandwidth environments. Decouples model synchronization per layer to enable communication-computation overlap without extra memory. Achieves 1.49×-3.91× speedups over baselines on GPT-2 and Llama-2.",
        "abstract": "The growth of large language models (LLMs) increases challenges of accelerating distributed training across multiple GPUs in different data centers. Moreover, concerns about data privacy and data exhaustion have heightened interest in geo-distributed data centers. Communication in geo-distributed data parallel training (DDP) with stochastic gradient descent (S-SGD) is the main bottleneck in low-bandwidth environments. Local SGD mitigates communication overhead by reducing synchronization frequency, and recent studies have successfully applied it to geo-distributedly pre-train LLMs. However, we identify that its model synchronization mechanism prevents overlapping communication and computation, which makes the system lose opportunities to overlap communication and computation.   To overcome this limitation, we expand the design space of local SGD by layer-wisely decoupling model synchronization. In each iteration, only some layers are synchronized instead of the entire model after a specific number of iterations. Leveraging this methodology, we introduce DreamDDP, a training framework to accelerate low-bandwidth distributed training with three key innovations: (1) partial local SGD with theoretical assurances of convergence rates comparable to S-SGD; (2) overlapping parameter synchronization with computation without extra GPU memory occupation; (3) identifying and exploiting three properties to schedule the communication and computation to reduce the training time based on fine-grained profiling of layer-wise communication and computation time. Empirical evaluations conducted on 32 GPUs using prominent deep learning models, including ResNet-18, ResNet-50, GPT-2, and Llama-2, demonstrate that DreamDDP enhances the convergence properties of Local SGD (and Adam) and achieves speedups ranging from $1.49\\times$ to $3.91\\times$ over leading baseline methods.",
        "authors": [
            "Zhenheng Tang",
            "Zichen Tang",
            "Junlin Huang",
            "Xinglin Pan",
            "Rudan Yan",
            "Yuxin Wang",
            "Amelie Chi Zhou",
            "Shaohuai Shi",
            "Xiaowen Chu",
            "Bo Li"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-02-16"
    },
    "http://arxiv.org/abs/2502.11880v1": {
        "id": "http://arxiv.org/abs/2502.11880v1",
        "title": "Bitnet.cpp: Efficient Edge Inference for Ternary LLMs",
        "link": "http://arxiv.org/abs/2502.11880v1",
        "tags": [
            "edge",
            "quantization",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-02-18",
        "tldr": "Addresses efficient edge inference for ternary 1.58-bit LLMs. Introduces Bitnet.cpp with a novel mpGEMM library using Ternary Lookup Table (TL) and Int2 with Scale (I2_S) methods. Achieves up to 6.25x speedup over full-precision baselines.",
        "abstract": "The advent of 1-bit large language models (LLMs), led by BitNet b1.58, has spurred interest in ternary LLMs. Despite this, research and practical applications focusing on efficient edge inference for ternary LLMs remain scarce. To bridge this gap, we introduce Bitnet.cpp, an inference system optimized for BitNet b1.58 and ternary LLMs. Given that mixed-precision matrix multiplication (mpGEMM) constitutes the bulk of inference time in ternary LLMs, Bitnet.cpp incorporates a novel mpGEMM library to facilitate sub-2-bits-per-weight, efficient and lossless inference. The library features two core solutions: Ternary Lookup Table (TL), which addresses spatial inefficiencies of previous bit-wise methods, and Int2 with a Scale (I2_S), which ensures lossless edge inference, both enabling high-speed inference. Our experiments show that Bitnet.cpp achieves up to a 6.25x increase in speed over full-precision baselines and up to 2.32x over low-bit baselines, setting new benchmarks in the field. Additionally, we expand TL to element-wise lookup table (ELUT) for low-bit LLMs in the appendix, presenting both theoretical and empirical evidence of its considerable potential. Bitnet.cpp is publicly available at https://github.com/microsoft/BitNet/tree/paper , offering a sophisticated solution for the efficient and practical deployment of edge LLMs.",
        "authors": [
            "Jinheng Wang",
            "Hansong Zhou",
            "Ting Song",
            "Shijie Cao",
            "Yan Xia",
            "Ting Cao",
            "Jianyu Wei",
            "Shuming Ma",
            "Hongyu Wang",
            "Furu Wei"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "cs.DC"
        ],
        "submit_date": "2025-02-17"
    },
    "http://arxiv.org/abs/2502.11417v2": {
        "id": "http://arxiv.org/abs/2502.11417v2",
        "title": "DiSCo: Device-Server Collaborative LLM-Based Text Streaming Services",
        "link": "http://arxiv.org/abs/2502.11417v2",
        "tags": [
            "serving",
            "offloading",
            "edge"
        ],
        "relevant": true,
        "indexed_date": "2025-02-18",
        "tldr": "Addresses high cost and QoE challenges in LLM-based text streaming services. Proposes DiSCo, a device-server collaborative scheduler with adaptive request routing and token-level migration. Reduces tail TTFT by 11-52% and serving costs by up to 84%.",
        "abstract": "The rapid rise of large language models (LLMs) in text streaming services has introduced significant cost and Quality of Experience (QoE) challenges in serving millions of daily requests, especially in meeting Time-To-First-Token (TTFT) and Time-Between-Token (TBT) requirements for real-time interactions. Our real-world measurements show that both server-based and on-device deployments struggle to meet diverse QoE demands: server deployments face high costs and last-hop issues (e.g., Internet latency and dynamics), while on-device LLM inference is constrained by resources.   We introduce DiSCo, a device-server cooperative scheduler designed to optimize users' QoE by adaptively routing requests and migrating response generation between endpoints while maintaining cost constraints. DiSCo employs cost-aware scheduling, leveraging the predictable speed of on-device LLM inference with the flexible capacity of server-based inference to dispatch requests on the fly, while introducing a token-level migration mechanism to ensure consistent token delivery during migration. Evaluations on real-world workloads -- including commercial services like OpenAI GPT and DeepSeek, and open-source deployments such as LLaMA3 -- show that DiSCo can improve users' QoE by reducing tail TTFT (11-52\\%) and mean TTFT (6-78\\%) across different model-device configurations, while dramatically reducing serving costs by up to 84\\% through its migration mechanism while maintaining comparable QoE levels.",
        "authors": [
            "Ting Sun",
            "Penghan Wang",
            "Fan Lai"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "submit_date": "2025-02-17"
    },
    "http://arxiv.org/abs/2502.11007v3": {
        "id": "http://arxiv.org/abs/2502.11007v3",
        "title": "Local-Cloud Inference Offloading for LLMs in Multi-Modal, Multi-Task, Multi-Dialogue Settings",
        "link": "http://arxiv.org/abs/2502.11007v3",
        "tags": [
            "offloading",
            "edge",
            "multi-modal"
        ],
        "relevant": true,
        "indexed_date": "2025-02-18",
        "tldr": "Proposes TMO, a local-cloud inference offloading system for multi-modal LLMs. Uses resource-constrained reinforcement learning to optimize task allocation and data source selection. Achieves significant improvements in latency, cost, and response quality over baselines.",
        "abstract": "Compared to traditional machine learning models, recent large language models (LLMs) can exhibit multi-task-solving capabilities through multiple dialogues and multi-modal data sources. These unique characteristics of LLMs, together with their large model size, make their deployment more challenging. Specifically, (i) deploying LLMs on local devices faces computational, memory, and energy resource issues, while (ii) deploying them in the cloud cannot guarantee real-time service and incurs communication/usage costs. In this paper, we design TMO, a local-cloud LLM inference system with Three-M Offloading: Multi-modal, Multi-task, and Multi-dialogue. TMO incorporates (i) a lightweight local LLM that can process simple tasks at high speed and (ii) a large-scale cloud LLM that can handle multi-modal data sources. We develop a resource-constrained reinforcement learning (RCRL) strategy for TMO that optimizes the inference location (i.e., local vs. cloud) and multi-modal data sources to use for each task/dialogue, aiming to maximize the long-term reward (response quality, latency, and usage cost) while adhering to resource constraints. We also contribute M4A1, a new dataset we curated that contains reward and cost metrics across multiple modality, task, dialogue, and LLM configurations, enabling evaluation of offloading decisions. We demonstrate the effectiveness of TMO compared to several exploration-decision and LLM-as-Agent baselines, showing significant improvements in latency, cost, and response quality.",
        "authors": [
            "Liangqi Yuan",
            "Dong-Jun Han",
            "Shiqiang Wang",
            "Christopher G. Brinton"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "submit_date": "2025-02-16"
    },
    "http://arxiv.org/abs/2502.09922v2": {
        "id": "http://arxiv.org/abs/2502.09922v2",
        "title": "λScale: Enabling Fast Scaling for Serverless Large Language Model Inference",
        "link": "http://arxiv.org/abs/2502.09922v2",
        "tags": [
            "serving",
            "autoscaling",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-02-17",
        "tldr": "Addresses slow scaling for serverless LLM inference with high startup overhead. Proposes λScale with RDMA-based model multicast and execute-while-load pipeline execution. Achieves 5x lower tail latency and 31.3% cost reduction versus state-of-the-art.",
        "abstract": "Serverless computing has emerged as a compelling solution for cloud-based model inference. However, as modern large language models (LLMs) continue to grow in size, existing serverless platforms often face substantial model startup overhead. This poses a significant challenge in efficiently scaling model instances to accommodate dynamic, bursty workloads commonly observed in real-world inference services. In this paper, we introduce λScale, an efficient serverless inference system to achieve fast model scaling. The key idea behind λScale is to leverage high-speed RDMA networks between GPU nodes for fast model multicast, while enabling distributed inference execution during model transmission -- referred to as \"execute-while-load\". λScale proposes an efficient model scaling scheme, λPipe, which supports adaptive model multicast and dynamically constructs execution pipelines across receiving nodes for collaborative, distributed inference. Additionally, λScale supports efficient model management across GPU and host memory, allowing fast scaling for models across different storage tiers. Evaluation results show that λScale enables fast model scaling and effectively handles load spikes, achieving up to 5x tail-latency improvement and 31.3% cost reduction compared to state-of-the-art solutions on real-world LLM inference traces.",
        "authors": [
            "Minchen Yu",
            "Rui Yang",
            "Chaobo Jia",
            "Zhaoyuan Su",
            "Sheng Yao",
            "Tingfeng Lan",
            "Yuchen Yang",
            "Yue Cheng",
            "Wei Wang",
            "Ao Wang",
            "Ruichuan Chen"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-02-14"
    },
    "http://arxiv.org/abs/2502.09334v2": {
        "id": "http://arxiv.org/abs/2502.09334v2",
        "title": "ThunderServe: High-performance and Cost-efficient LLM Serving in Cloud Environments",
        "link": "http://arxiv.org/abs/2502.09334v2",
        "tags": [
            "serving",
            "autoscaling",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-02-14",
        "tldr": "Addresses high-performance and cost-efficient LLM serving in heterogeneous cloud environments. Proposes ThunderServe with a novel GPU-aware scheduling algorithm and lightweight re-scheduling for dynamic conditions. Achieves up to 2.1× higher throughput and 2.5× lower latency than SOTA systems at same cost.",
        "abstract": "Recent developments in large language models (LLMs) have demonstrated their remarkable proficiency in a range of tasks. Compared to in-house homogeneous GPU clusters, deploying LLMs in cloud environments with diverse types of GPUs is crucial for addressing the GPU shortage problem and being more cost-effective. However, the diversity of network environments and various GPU types on the cloud bring difficulties to achieving high-performance serving. In this work, we propose ThunderServe, a high-performance and cost-efficient LLM serving system for heterogeneous cloud environments. We introduce a novel scheduling algorithm, which optimizes the deployment plan of LLM serving to accommodate the heterogeneous resource and network bandwidth conditions in cloud environments. Furthermore, we propose a lightweight re-scheduling mechanism, designed to adapt to fluctuating online conditions (e.g., node failures, workload shifts) without the need for costly restarts of ongoing services. Empirical results in both heterogeneous cloud and homogeneous in-house environments reveal that ThunderServe delivers up to a 2.1$\\times$ and on average a $1.7\\times$ increase in throughput and achieves up to a 2.5$\\times$ and on average a $1.5\\times$ reduction in latency deadlines compared with state-of-the-art systems given the same price budget, suggesting opting for cloud services provides a more cost-efficient solution.",
        "authors": [
            "Youhe Jiang",
            "Fangcheng Fu",
            "Xiaozhe Yao",
            "Taiyi Wang",
            "Bin Cui",
            "Ana Klimovic",
            "Eiko Yoneki"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-02-13"
    },
    "http://arxiv.org/abs/2502.08182v1": {
        "id": "http://arxiv.org/abs/2502.08182v1",
        "title": "Memory Offloading for Large Language Model Inference with Latency SLO Guarantees",
        "link": "http://arxiv.org/abs/2502.08182v1",
        "tags": [
            "offloading",
            "serving",
            "quantization"
        ],
        "relevant": true,
        "indexed_date": "2025-02-13",
        "tldr": "Proposes Select-N, a latency-SLO-aware memory offloading system for LLM serving. It uses a two-stage approach with an offline range generator and runtime adjustment of offloading intervals per inference iteration. Increases throughput by 1.85× while meeting SLOs.",
        "abstract": "Offloading large language models (LLMs) state to host memory during inference promises to reduce operational costs by supporting larger models, longer inputs, and larger batch sizes. However, the design of existing memory offloading mechanisms does not take latency service-level objectives (SLOs) into consideration. As a result, they either lead to frequent SLO violations or underutilize host memory, thereby incurring economic loss and thus defeating the purpose of memory offloading.   This paper presents Select-N, a latency-SLO-aware memory offloading system for LLM serving. A key challenge in designing Select-N is to reconcile the tension between meeting SLOs and maximizing host memory usage. Select-N overcomes it by exploiting a unique characteristic of modern LLMs: during serving, the computation time of each decoder layer is deterministic. Leveraging this, Select-N introduces offloading interval, an internal tunable knob that captures the tradeoff between SLOs and host memory usage, thereby reducing the aforementioned challenge to pick an optimal offloading interval. With that, Select-N proposes a two-stage approach to automatically pick the offloading interval. The first stage is offline that generates the range of optimal offloading interval, while the second stage adjusts offloading interval at the granularity of inference iteration based on runtime hardware status. Our evaluation shows that Select-N consistently meets SLOs and improves the serving throughput over existing mechanisms by 1.85X due to maximizing the use of host memory.",
        "authors": [
            "Chenxiang Ma",
            "Zhisheng Ye",
            "Hanyu Zhao",
            "Zehua Yang",
            "Tianhao Fu",
            "Jiaxun Han",
            "Jie Zhang",
            "Yingwei Luo",
            "Xiaolin Wang",
            "Zhenlin Wang",
            "Yong Li",
            "Diyu Zhou"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-02-12"
    },
    "http://arxiv.org/abs/2502.07903v1": {
        "id": "http://arxiv.org/abs/2502.07903v1",
        "title": "HexGen-2: Disaggregated Generative Inference of LLMs in Heterogeneous Environment",
        "link": "http://arxiv.org/abs/2502.07903v1",
        "tags": [
            "serving",
            "disaggregation",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-02-13",
        "tldr": "Solves efficient LLM serving on heterogeneous GPUs via disaggregated prefill/decode. HexGen-2 schedules computations, communications, and KV caching as constraint optimization. Achieves up to 2× higher throughput and 30% cost reduction versus SOTA systems.",
        "abstract": "Disaggregating the prefill and decoding phases represents an effective new paradigm for generative inference of large language models (LLM), which eliminates prefill-decoding interference and optimizes resource allocation. However, it is still an open problem about how to deploy the disaggregated inference paradigm across a group of heterogeneous GPUs, which can be an economical alternative to deployment over homogeneous high-performance GPUs. Towards this end, we introduce HexGen-2, a distributed system for efficient and economical LLM serving on heterogeneous GPUs following the disaggregated paradigm. Built on top of HexGen, the core component of HexGen-2 is a scheduling algorithm that formalizes the allocation of disaggregated LLM inference computations and communications over heterogeneous GPUs and network connections as a constraint optimization problem. We leverage the graph partitioning and max-flow algorithms to co-optimize resource allocation, parallel strategies for distinct inference phases, and the efficiency of inter-phase key-value (KV) cache communications. We conduct extensive experiments to evaluate HexGen-2, i.e., on OPT (30B) and Llama-2 (70B) models in various real-world settings, the results reveal that HexGen-2 delivers up to a 2.0 times and on average a 1.3 times improvement in serving throughput, reduces the average inference latency by 1.5 times compared with state-of-the-art systems given the same price budget, and achieves comparable inference performance with a 30% lower price budget.",
        "authors": [
            "Youhe Jiang",
            "Ran Yan",
            "Binhang Yuan"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-02-11"
    },
    "http://arxiv.org/abs/2502.08145v1": {
        "id": "http://arxiv.org/abs/2502.08145v1",
        "title": "Democratizing AI: Open-source Scalable LLM Training on GPU-based Supercomputers",
        "link": "http://arxiv.org/abs/2502.08145v1",
        "tags": [
            "training",
            "hardware",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-02-13",
        "tldr": "Presents AxoNN, a scalable framework for LLM training on GPU supercomputers. Introduces a hybrid parallel algorithm with kernel optimizations and collective overlap. Achieves record speeds: 1.423 Exaflop/s bf16 on Alps for GPT-style models.",
        "abstract": "Training and fine-tuning large language models (LLMs) with hundreds of billions to trillions of parameters requires tens of thousands of GPUs, and a highly scalable software stack. In this work, we present a novel four-dimensional hybrid parallel algorithm implemented in a highly scalable, portable, open-source framework called AxoNN. We describe several performance optimizations in AxoNN to improve matrix multiply kernel performance, overlap non-blocking collectives with computation, and performance modeling to choose performance optimal configurations. These have resulted in unprecedented scaling and peak flop/s (bf16) for training of GPT-style transformer models on Perlmutter (620.1 Petaflop/s), Frontier (1.381 Exaflop/s) and Alps (1.423 Exaflop/s).   While the abilities of LLMs improve with the number of trainable parameters, so do privacy and copyright risks caused by memorization of training data, which can cause disclosure of sensitive or private information at inference time. We highlight this side effect of scale through experiments that explore \"catastrophic memorization\", where models are sufficiently large to memorize training data in a single pass, and present an approach to prevent it. As part of this study, we demonstrate fine-tuning of a 405-billion parameter LLM using AxoNN on Frontier.",
        "authors": [
            "Siddharth Singh",
            "Prajwal Singhania",
            "Aditya Ranjan",
            "John Kirchenbauer",
            "Jonas Geiping",
            "Yuxin Wen",
            "Neel Jain",
            "Abhimanyu Hans",
            "Manli Shu",
            "Aditya Tomar",
            "Tom Goldstein",
            "Abhinav Bhatele"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC"
        ],
        "submit_date": "2025-02-12"
    },
    "http://arxiv.org/abs/2502.06643v1": {
        "id": "http://arxiv.org/abs/2502.06643v1",
        "title": "MoETuner: Optimized Mixture of Expert Serving with Balanced Expert Placement and Token Routing",
        "link": "http://arxiv.org/abs/2502.06643v1",
        "tags": [
            "MoE",
            "serving",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-02-11",
        "tldr": "Optimizes MoE model serving by addressing expert placement and token routing imbalance. Proposes MoETuner, an ILP-based approach to balance token processing and minimize inter-GPU routing costs. Achieves 17.5% speedup in multi-node inference end-to-end latency.",
        "abstract": "Mixture-of-Experts (MoE) model architecture has emerged as a promising solution for scaling transformer models efficiently, offering sparse activation that reduces computational costs while increasing model capacity. However, as MoE models scale, they need to be distributed across GPU devices, thus face critical performance bottlenecks due to their large memory footprint. Expert parallelism distributes experts across GPUs, however, faces key challenges including an unbalanced token routing and expert activation, resulting in communication tail latency and processing inefficiencies. While existing solutions address some of these issues, they fail to resolve the dual challenges of load imbalance and communication skew. The imbalance in token processing load across experts causes uneven processing times on different GPUs, while communication skew between GPUs leads to unbalanced inter-GPU data transfers. These factors degrade the performance of MoE models by increasing tail latency and reducing overall throughput. To address these limitations, we propose an Integer Linear Programming (ILP) formulation to optimize expert placement by jointly considering token load, communication, and computation costs. We exploit the property that there is a token routing dependency across layers, where tokens routed to a specific expert in one layer are likely to be routed to a limited set of experts in the subsequent layer. Our solution, MoETuner, offers an optimal expert-to-GPU assignment that minimizes inter-GPU token routing costs and balances token processing across devices, thereby reducing tail latency and end-to-end execution time. Experimental results demonstrate 9.3% and 17.5% of end-to-end speedups for single-node and multi-node inference respectively, showcasing the potential of our ILP-based optimization for offering expert parallel solutions for next-generation MoEs.",
        "authors": [
            "Seokjin Go",
            "Divya Mahajan"
        ],
        "categories": [
            "cs.LG",
            "cs.DC"
        ],
        "submit_date": "2025-02-10"
    },
    "http://arxiv.org/abs/2502.05370v2": {
        "id": "http://arxiv.org/abs/2502.05370v2",
        "title": "Taming Latency-Memory Trade-Off in MoE-Based LLM Serving via Fine-Grained Expert Offloading",
        "link": "http://arxiv.org/abs/2502.05370v2",
        "tags": [
            "MoE",
            "offloading",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-02-11",
        "tldr": "Addresses the memory inefficiency and high latency in serving MoE-based LLMs. Proposes FineMoE, a fine-grained expert offloading system using expert selection patterns and input semantics for prefetching and caching. Reduces inference latency by 47% and improves expert hit rate by 39% over SOTA methods.",
        "abstract": "Large Language Models (LLMs) have gained immense success in revolutionizing various applications, including content generation, search and recommendation, and AI-assisted operation. To reduce high training costs, Mixture-of-Experts (MoE) architecture has become a popular backbone for modern LLMs. However, despite the benefits, serving MoE-based LLMs experience severe memory inefficiency due to sparsely activated experts. Recent studies propose to offload inactive experts from GPU memory to CPU memory to improve the serving efficiency of MoE models. However, they either incur high inference latency or high model memory footprints due to coarse-grained designs.   To tame the latency-memory trade-off in MoE serving, we present FineMoE, a fine-grained expert offloading system for MoE serving that achieves low inference latency with memory efficiency. We design FineMoE to extract fine-grained expert selection patterns from MoE models and semantic hints from input prompts to efficiently guide expert prefetching, caching, and offloading decisions. FineMoE is prototyped on top of HuggingFace Transformers and deployed on a six-GPU testbed. Experiments with open-source MoE models and real-world workloads show that FineMoE reduces inference latency by 47% and improves expert hit rate by 39% over state-of-the-art solutions.",
        "authors": [
            "Hanfei Yu",
            "Xingqi Cui",
            "Hong Zhang",
            "Hao Wang",
            "Hao Wang"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC"
        ],
        "submit_date": "2025-02-07"
    },
    "http://arxiv.org/abs/2502.05043v2": {
        "id": "http://arxiv.org/abs/2502.05043v2",
        "title": "EcoServe: Designing Carbon-Aware AI Inference Systems",
        "link": "http://arxiv.org/abs/2502.05043v2",
        "tags": [
            "serving",
            "autoscaling",
            "offline"
        ],
        "relevant": true,
        "indexed_date": "2025-02-10",
        "tldr": "Addresses carbon emissions in LLM inference systems. Designs EcoServe, a resource provisioning scheduler based on Reduce, Reuse, Rightsize, Recycle principles using cross-stack optimization. Reduces carbon emissions by up to 47% while maintaining SLOs.",
        "abstract": "The rapid increase in LLM ubiquity and scale levies unprecedented demands on computing infrastructure. These demands not only incur large compute and memory resources but also significant energy, yielding large operational and embodied carbon emissions. In this work, we present three main observations based on modeling and traces from the production deployment of two Generative AI services in a major cloud service provider. First, while GPUs dominate operational carbon, host processing systems (e.g., CPUs, memory, storage) dominate embodied carbon. Second, offline, batch inference accounts for a significant portion (up to 55\\%) of serving capacity. Third, there are different levels of heterogeneity across hardware and workloads for LLM inference. Based on these observations, we design EcoServe, a carbon-aware resource provision and scheduling framework for LLM serving systems. It is based on four principles - Reduce, Reuse, Rightsize, and Recycle (4R). With a cross-stack ILP formulation and design, we demonstrate that EcoServe can lower carbon emissions by up to 47\\%, compared to performance, energy, and cost-optimized design points, while maintaining performance targets and SLOs.",
        "authors": [
            "Yueying Li",
            "Zhanqiu Hu",
            "Esha Choukse",
            "Rodrigo Fonseca",
            "G. Edward Suh",
            "Udit Gupta"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-02-07"
    },
    "http://arxiv.org/abs/2502.04563v3": {
        "id": "http://arxiv.org/abs/2502.04563v3",
        "title": "WaferLLM: Large Language Model Inference at Wafer Scale",
        "link": "http://arxiv.org/abs/2502.04563v3",
        "tags": [
            "hardware",
            "kernel",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-02-10",
        "tldr": "Investigates efficient LLM inference for wafer-scale accelerators. Introduces WaferLLM with PLMR model, wafer-scale parallelism, and optimized MeshGEMM/GEMV kernels. Achieves 200× higher accelerator utilization and 10-20× speedups over GPU clusters.",
        "abstract": "Emerging AI accelerators increasingly adopt wafer-scale manufacturing technologies, integrating hundreds of thousands of AI cores in a mesh architecture with large distributed on-chip memory (tens of GB in total) and ultra-high on-chip memory bandwidth (tens of PB/s). However, current LLM inference systems, optimized for shared memory architectures like GPUs, fail to exploit these accelerators fully.   We introduce WaferLLM, the first wafer-scale LLM inference system. WaferLLM is guided by a novel PLMR model (pronounced as \"Plummer\") that captures the unique hardware characteristics of wafer-scale architectures. Leveraging this model, WaferLLM pioneers wafer-scale LLM parallelism, optimizing the utilization of hundreds of thousands of on-chip cores. It also introduces MeshGEMM and MeshGEMV, the first GEMM and GEMV implementations designed to scale effectively on wafer-scale accelerators.   Evaluations show that WaferLLM achieves up to 200$\\times$ higher accelerator utilization than state-of-the-art methods. Leveraging a wafer-scale accelerator (Cerebras WSE2), WaferLLM delivers GEMV operations 606$\\times$ faster and 16$\\times$ more energy-efficient than on an NVIDIA A100 GPU. For full LLM inference, WaferLLM achieves 10-20$\\times$ speedups over A100 GPU clusters running SGLang and vLLM. These advantages are expected to grow as wafer-scale AI models, software, and hardware continue to mature. WaferLLM is open-sourced at https://github.com/MeshInfra/WaferLLM.",
        "authors": [
            "Congjie He",
            "Yeqi Huang",
            "Pei Mu",
            "Ziming Miao",
            "Jilong Xue",
            "Lingxiao Ma",
            "Fan Yang",
            "Luo Mai"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.AR",
            "cs.DC",
            "cs.ET"
        ],
        "submit_date": "2025-02-06"
    },
    "http://arxiv.org/abs/2502.03589v1": {
        "id": "http://arxiv.org/abs/2502.03589v1",
        "title": "HACK: Homomorphic Acceleration via Compression of the Key-Value Cache for Disaggregated LLM Inference",
        "link": "http://arxiv.org/abs/2502.03589v1",
        "tags": [
            "serving",
            "offloading",
            "disaggregation"
        ],
        "relevant": true,
        "indexed_date": "2025-02-07",
        "tldr": "Proposes HACK, a method to eliminate KV cache dequantization overhead in disaggregated LLM inference by directly performing computations on quantized data. Achieves up to 70.9% reduction in job completion time compared to baseline systems.",
        "abstract": "Disaggregated Large Language Model (LLM) inference has gained popularity as it separates the computation-intensive prefill stage from the memory-intensive decode stage, avoiding the prefill-decode interference and improving resource utilization. However, transmitting Key-Value (KV) data between the two stages can be a bottleneck, especially for long prompts. Additionally, the computation time overhead for prefill and decode is key for optimizing Job Completion Time (JCT), and KV data size can become prohibitive for long prompts and sequences. Existing KV quantization methods can alleviate the transmission bottleneck and reduce memory requirements, but they introduce significant dequantization overhead, exacerbating the computation time.   We propose Homomorphic Acceleration via Compression of the KV cache (HACK) for disaggregated LLM inference. HACK eliminates the heavy KV dequantization step, and directly performs computations on quantized KV data to approximate and reduce the cost of the expensive matrix-multiplication step. Extensive trace-driven experiments show that HACK reduces JCT by up to 70.9% compared to disaggregated LLM inference baseline and by up to 52.3% compared to state-of-the-art KV quantization methods.",
        "authors": [
            "Zeyu Zhang",
            "Haiying Shen",
            "Shay Vargaftik",
            "Ran Ben Basat",
            "Michael Mitzenmacher",
            "Minlan Yu"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-02-05"
    },
    "http://arxiv.org/abs/2502.03885v6": {
        "id": "http://arxiv.org/abs/2502.03885v6",
        "title": "InfiniteHBD: Building Datacenter-Scale High-Bandwidth Domain for LLM with Optical Circuit Switching Transceivers",
        "link": "http://arxiv.org/abs/2502.03885v6",
        "tags": [
            "training",
            "networking",
            "hardware"
        ],
        "relevant": true,
        "indexed_date": "2025-02-07",
        "tldr": "Proposes InfiniteHBD, a transceiver-centric HBD architecture with Optical Circuit Switching to enhance LLM training scalability, fault resiliency, and cost. Integrates OCS in transceivers for reconfigurable ring topologies. Reduces cost to 31% of NVL-72 and improves Model FLOPs Utilization by 3.37x vs DGX.",
        "abstract": "Scaling Large Language Model (LLM) training relies on multi-dimensional parallelism, where High-Bandwidth Domains (HBDs) are critical for communication-intensive parallelism like Tensor Parallelism. However, existing HBD architectures face fundamental limitations in scalability, cost, and fault resiliency: switch-centric HBDs (e.g., NVL-72) incur prohibitive scaling costs, while GPU-centric HBDs (e.g., TPUv3/Dojo) suffer from severe fault propagation. Switch-GPU hybrid HBDs (e.g., TPUv4) take a middle-ground approach, but the fault explosion radius remains large.   We propose InfiniteHBD, a transceiver-centric HBD architecture that integrates connectivity and dynamic switching at the transceiver level by embedding Optical Circuit Switching (OCS) within each transceiver. It enables reconfigurable point-to-multipoint communication and scalable variable-size ring topologies. InfiniteHBD achieves datacenter-scale scalability without cost explosion, fault isolation at the node level, and full bandwidth utilization for healthy GPUs. Key innovations include a Silicon Photonic-based OCS transceiver (OCSTrx), a reconfigurable k-hop ring topology, and an HBD-DCN orchestration algorithm. The evaluation demonstrates that InfiniteHBD reduces cost to 31% of NVL-72, achieves a near-zero GPU waste ratio (over 10x lower than NVL-72 and TPUv4), maintains near-zero cross-ToR traffic under 7% node fault ratio, and improves Model FLOPs Utilization by 3.37x compared to NVIDIA DGX (8 GPUs/node).",
        "authors": [
            "Chenchen Shou",
            "Guyue Liu",
            "Hao Nie",
            "Huaiyu Meng",
            "Yu Zhou",
            "Yimin Jiang",
            "Wenqing Lv",
            "Yelong Xu",
            "Yuanwei Lu",
            "Zhang Chen",
            "Yanbo Yu",
            "Yichen Shen",
            "Yibo Zhu",
            "Daxin Jiang"
        ],
        "categories": [
            "cs.NI",
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-02-06"
    },
    "http://arxiv.org/abs/2502.02406v3": {
        "id": "http://arxiv.org/abs/2502.02406v3",
        "title": "LV-XAttn: Distributed Cross-Attention for Long Visual Inputs in Multimodal Large Language Models",
        "link": "http://arxiv.org/abs/2502.02406v3",
        "tags": [
            "multi-modal",
            "training",
            "quantization"
        ],
        "relevant": true,
        "indexed_date": "2025-02-05",
        "tldr": "Addresses high communication overhead in distributed cross-attention for multimodal LLMs with large visual inputs. Proposes LV-XAttn, which retains key-value blocks locally and exchanges smaller query blocks. Achieves up to 10.62× end-to-end speedup on models like Llama 3-V.",
        "abstract": "Cross-attention is commonly adopted in multimodal large language models (MLLMs) for integrating visual information into the language backbone. However, in applications with large visual inputs, such as video understanding, processing a large number of visual tokens in cross-attention layers leads to high memory demands and often necessitates distributed computation across multiple GPUs. Existing distributed attention mechanisms face significant communication overheads, making cross-attention layers a critical bottleneck for efficient training and inference of MLLMs. To address this, we propose LV-XAttn, a distributed, exact cross-attention mechanism with minimal communication overhead. We observe that in applications involving large visual inputs, the size of the query block is typically much smaller than that of the key-value blocks. Thus, in LV-XAttn we keep the large key-value blocks locally on each GPU and exchange smaller query blocks across GPUs. We also introduce an efficient activation recomputation technique to support longer visual context. We theoretically analyze the communication benefits of LV-XAttn and show that it can achieve speedups for a wide range of models. Our evaluations with Llama 3-V, mPLUG-Owl3 and OpenFlamingo models find that LV-XAttn achieves up to 10.62$\\times$ end-to-end speedup compared to existing approaches.",
        "authors": [
            "Tzu-Tao Chang",
            "Shivaram Venkataraman"
        ],
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-02-04"
    },
    "http://arxiv.org/abs/2502.01659v2": {
        "id": "http://arxiv.org/abs/2502.01659v2",
        "title": "Longer Attention Span: Increasing Transformer Context Length with Sparse Graph Processing Techniques",
        "link": "http://arxiv.org/abs/2502.01659v2",
        "tags": [
            "kernel",
            "sparse"
        ],
        "relevant": true,
        "indexed_date": "2025-02-05",
        "tldr": "Proposes graph-based algorithms to implement sparse attention masks for Transformers, achieving work-optimal computation. Demonstrates speedups over FlashAttention and supports sequences up to 160M tokens on a single GPU.",
        "abstract": "Transformers have demonstrated great success in numerous domains including natural language processing and bioinformatics. This success stems from the use of the attention mechanism by these models in order to represent and propagate pairwise interactions between individual tokens of sequential data. However, the primary limitation of this operation is its quadratic memory and time complexity in relation to the input's context length - the length of a sequence over which the interactions need to be captured. This significantly limits the length of sequences that can be inferred upon by these models. Extensive research has been conducted to reduce the number of pairwise interactions to sub-quadratic in relation to the context length by introducing sparsity into the attention mechanism through the development of sparse attention masks. However, efficient implementations that achieve \"true sparsity\" are lacking.   In this work, we address this issue by proposing a graph computing view of attention where tokens are perceived as nodes of the graph and the attention mask determines the edges of the graph. Using this view, we develop graph processing algorithms to implement the attention mechanism. Both theoretically and empirically, we demonstrate that our algorithms only perform the needed computations, i.e., they are work optimal. We also perform extensive experimentation using popular attention masks to explore the impact of sparsity on execution time and achievable context length. Our experiments demonstrate significant speedups in execution times compared to state-of-the-art attention implementations such as FlashAttention for large sequence lengths. We also demonstrate that our algorithms are able to achieve extremely long sequence lengths of as high as 160 million on a single NVIDIA A100 GPU (SXM4 80GB).",
        "authors": [
            "Nathaniel Tomczak",
            "Sanmukh Kuppannagari"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC",
            "cs.PF"
        ],
        "submit_date": "2025-01-31"
    },
    "http://arxiv.org/abs/2502.01277v1": {
        "id": "http://arxiv.org/abs/2502.01277v1",
        "title": "OCTOPINF: Workload-Aware Inference Serving for Edge Video Analytics",
        "link": "http://arxiv.org/abs/2502.01277v1",
        "tags": [
            "edge",
            "serving",
            "sparse"
        ],
        "relevant": true,
        "indexed_date": "2025-02-04",
        "tldr": "Addresses efficient inference serving for edge video analytics pipelines under dynamic environments. Proposes fine-grained resource allocation, adaptive batching, and spatiotemporal scheduling for GPU co-location. Achieves up to 10× higher throughput versus baselines.",
        "abstract": "Edge Video Analytics (EVA) has gained significant attention as a major application of pervasive computing, enabling real-time visual processing. EVA pipelines, composed of deep neural networks (DNNs), typically demand efficient inference serving under stringent latency requirements, which is challenging due to the dynamic Edge environments (e.g., workload variability and network instability). Moreover, EVA pipelines also face significant resource contention caused by resource (e.g., GPU) constraints at the Edge. In this paper, we introduce OCTOPINF, a novel resource-efficient and workload-aware inference serving system designed for real-time EVA. OCTOPINF tackles the unique challenges of dynamic edge environments through fine-grained resource allocation, adaptive batching, and workload balancing between edge devices and servers. Furthermore, we propose a spatiotemporal scheduling algorithm that optimizes the co-location of inference tasks on GPUs, improving performance and ensuring service-level objectives (SLOs) compliance. Extensive evaluations on a real-world testbed demonstrate the effectiveness of our approach. It achieves an effective throughput increase of up to 10x compared to the baselines and shows better robustness in challenging scenarios. OCTOPINF can be used for any DNN-based EVA inference task with minimal adaptation and is available at https://github.com/tungngreen/PipelineScheduler.",
        "authors": [
            "Thanh-Tung Nguyen",
            "Lucas Liebe",
            "Nhat-Quang Tau",
            "Yuheng Wu",
            "Jinghan Cheng",
            "Dongman Lee"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-02-03"
    },
    "http://arxiv.org/abs/2502.00722v2": {
        "id": "http://arxiv.org/abs/2502.00722v2",
        "title": "Demystifying Cost-Efficiency in LLM Serving over Heterogeneous GPUs",
        "link": "http://arxiv.org/abs/2502.00722v2",
        "tags": [
            "serving",
            "autoscaling"
        ],
        "relevant": true,
        "indexed_date": "2025-02-04",
        "tldr": "Optimizes cost-efficiency of LLM serving on heterogeneous GPU clouds. Proposes a scheduling algorithm using mixed-integer linear programming to determine GPU composition, deployment, and workload assignments. Achieves superior cost-efficiency compared to homogeneous baselines under diverse scenarios.",
        "abstract": "Recent advancements in Large Language Models (LLMs) have led to increasingly diverse requests, accompanied with varying resource (compute and memory) demands to serve them. However, this in turn degrades the cost-efficiency of LLM serving as common practices primarily rely on homogeneous GPU resources. In response to this problem, this work conducts a thorough study about serving LLMs over heterogeneous GPU resources on cloud platforms. The rationale is that different GPU types exhibit distinct compute and memory characteristics, aligning well with the divergent resource demands of diverse requests. Particularly, through comprehensive benchmarking, we discover that the cost-efficiency of LLM serving can be substantially optimized by meticulously determining GPU composition, deployment configurations, and workload assignments. Subsequently, we design a scheduling algorithm via mixed-integer linear programming, aiming at deducing the most cost-efficient serving plan under the constraints of price budget and real-time GPU availability. Remarkably, our approach effectively outperforms homogeneous and heterogeneous baselines under a wide array of scenarios, covering diverse workload traces, varying GPU availablilities, and multi-model serving. This casts new light on more accessible and efficient LLM serving over heterogeneous cloud resources.",
        "authors": [
            "Youhe Jiang",
            "Fangcheng Fu",
            "Xiaozhe Yao",
            "Guoliang He",
            "Xupeng Miao",
            "Ana Klimovic",
            "Bin Cui",
            "Binhang Yuan",
            "Eiko Yoneki"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-02-02"
    },
    "http://arxiv.org/abs/2502.00645v1": {
        "id": "http://arxiv.org/abs/2502.00645v1",
        "title": "General Coded Computing in a Probabilistic Straggler Regime",
        "link": "http://arxiv.org/abs/2502.00645v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-02-04",
        "tldr": "",
        "abstract": "Coded computing has demonstrated promising results in addressing straggler resiliency in distributed computing systems. However, most coded computing schemes are designed for exact computation, requiring the number of responding servers to exceed a certain recovery threshold. Additionally, these schemes are tailored for highly structured functions. Recently, new coded computing schemes for general computing functions, where exact computation is replaced with approximate computation, have emerged. In these schemes, the availability of additional results corresponds to more accurate estimation of computational tasks. This flexibility introduces new questions that need to be addressed. This paper addresses the practically important scenario in the context of general coded computing, where each server may become a straggler with a probability $p$, independently from others. We theoretically analyze the approximation error of two existing general coded computing schemes: Berrut Approximate Coded Computing (BACC) and Learning Theoretic Coded Computing (LeTCC). Under the probabilistic straggler configuration, we demonstrate that the average approximation error for BACC and LeTCC converge to zero with the rate of at least $\\mathcal{O}(\\log^3_{\\frac{1}{p}}(N)\\cdot{N^{-3}})$ and $\\mathcal{O}(\\log^4_{\\frac{1}{p}}(N)\\cdot{N^{-2}})$, respectively. This is perhaps surprising, as earlier results does not indicate a convergence when the number of stragglers scales with the total number of servers $N$. However, in this case, despite the average number of stragglers being $Np$, the independence of servers in becoming stragglers allows the approximation error to converge to zero. These theoretical results are validated through experiments on various computing functions, including deep neural networks.",
        "authors": [
            "Parsa Moradi",
            "Mohammad Ali Maddah-Ali"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-02-02"
    },
    "http://arxiv.org/abs/2502.01214v1": {
        "id": "http://arxiv.org/abs/2502.01214v1",
        "title": "Leveraging InfiniBand Controller to Configure Deadlock-Free Routing Engines for Dragonflies",
        "link": "http://arxiv.org/abs/2502.01214v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-02-04",
        "tldr": "",
        "abstract": "The Dragonfly topology is currently one of the most popular network topologies in high-performance parallel systems. The interconnection networks of many of these systems are built from components based on the InfiniBand specification. However, due to some constraints in this specification, the available versions of the InfiniBand network controller (OpenSM) do not include routing engines based on some popular deadlock-free routing algorithms proposed theoretically for Dragonflies, such as the one proposed by Kim and Dally based on Virtual-Channel shifting. In this paper we propose a straightforward method to integrate this routing algorithm in OpenSM as a routing engine, explaining in detail the configuration required to support it. We also provide experiment results, obtained both from a real InfiniBand-based cluster and from simulation, to validate the new routing engine and to compare its performance and requirements against other routing engines currently available in OpenSM.",
        "authors": [
            "German Maglione-Mathey",
            "Jesus Escudero-Sahuquillo",
            "Pedro Javier Garcia",
            "Francisco J. Quiles",
            "Eitan Zahavi"
        ],
        "categories": [
            "cs.NI",
            "cs.DC"
        ],
        "submit_date": "2025-02-03"
    },
    "http://arxiv.org/abs/2501.18842v1": {
        "id": "http://arxiv.org/abs/2501.18842v1",
        "title": "Infer-EDGE: Dynamic DNN Inference Optimization in 'Just-in-time' Edge-AI Implementations",
        "link": "http://arxiv.org/abs/2501.18842v1",
        "tags": [
            "edge",
            "inference"
        ],
        "relevant": true,
        "indexed_date": "2025-02-03",
        "tldr": "Proposes Infer-EDGE, an RL-based framework for optimizing DNN inference parameters (e.g., latency, accuracy, energy) in edge environments. Uses A2C RL to dynamically adjust runtime settings based on application needs. Achieves energy savings, accuracy improvements, and latency reduction on video tasks.",
        "abstract": "Balancing mutually diverging performance metrics, such as end-to-end latency, accuracy, and device energy consumption, is a challenging undertaking for deep neural network (DNN) inference in Just-in-Time edge environments that are inherently resource-constrained and loosely coupled. In this paper, we design and develop the Infer-EDGE framework that seeks to strike such a balance for latency-sensitive video processing applications. First, using comprehensive benchmarking experiments, we develop intuitions about the trade-off characteristics, which are then used by the framework to develop an Advantage Actor-Critic (A2C) Reinforcement Learning (RL) approach that can choose optimal run-time DNN inference parameters aligning the performance metrics based on the application requirements. Using real-world DNNs and a hardware testbed, we evaluate the benefits of the Infer-EDGE framework in terms of device energy savings, inference accuracy improvement, and end-to-end inference latency reduction.",
        "authors": [
            "Motahare Mounesan",
            "Xiaojie Zhang",
            "Saptarshi Debroy"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-01-31"
    },
    "http://arxiv.org/abs/2501.17275v1": {
        "id": "http://arxiv.org/abs/2501.17275v1",
        "title": "Dual-Lagrange Encoding for Storage and Download in Elastic Computing for Resilience",
        "link": "http://arxiv.org/abs/2501.17275v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-01-30",
        "tldr": "",
        "abstract": "Coded elastic computing enables virtual machines to be preempted for high-priority tasks while allowing new virtual machines to join ongoing computation seamlessly. This paper addresses coded elastic computing for matrix-matrix multiplications with straggler tolerance by encoding both storage and download using Lagrange codes. In 2018, Yang et al. introduced the first coded elastic computing scheme for matrix-matrix multiplications, achieving a lower computational load requirement. However, this scheme lacks straggler tolerance and suffers from high upload cost. Zhong et al. (2023) later tackled these shortcomings by employing uncoded storage and Lagrange-coded download. However, their approach requires each machine to store the entire dataset. This paper introduces a new class of elastic computing schemes that utilize Lagrange codes to encode both storage and download, achieving a reduced storage size. The proposed schemes efficiently mitigate both elasticity and straggler effects, with a storage size reduced to a fraction $\\frac{1}{L}$ of Zhong et al.'s approach, at the expense of doubling the download cost. Moreover, we evaluate the proposed schemes on AWS EC2 by measuring computation time under two different tasks allocations: heterogeneous and cyclic assignments. Both assignments minimize computation redundancy of the system while distributing varying computation loads across machines.",
        "authors": [
            "Xi Zhong",
            "Samuel Lu",
            "Joerg Kliewer",
            "Mingyue Ji"
        ],
        "categories": [
            "cs.IT",
            "cs.DC"
        ],
        "submit_date": "2025-01-28"
    },
    "http://arxiv.org/abs/2501.16892v1": {
        "id": "http://arxiv.org/abs/2501.16892v1",
        "title": "On the Shape Containment Problem within the Amoebot Model with Reconfigurable Circuits",
        "link": "http://arxiv.org/abs/2501.16892v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-01-29",
        "tldr": "",
        "abstract": "In programmable matter, we consider a large number of tiny, primitive computational entities called particles that run distributed algorithms to control global properties of the particle structure. Shape formation problems, where the particles have to reorganize themselves into a desired shape using basic movement abilities, are particularly interesting. In the related shape containment problem, the particles are given the description of a shape $S$ and have to find maximally scaled representations of $S$ within the initial configuration, without movements. While the shape formation problem is being studied extensively, no attention has been given to the shape containment problem, which may have additional uses beside shape formation, such as detection of structural flaws.   In this paper, we consider the shape containment problem within the geometric amoebot model for programmable matter, using its reconfigurable circuit extension to enable the instantaneous transmission of primitive signals on connected subsets of particles. We first prove a lower runtime bound of $Ω(\\sqrt{n})$ synchronous rounds for the general problem, where $n$ is the number of particles. Then, we construct the class of snowflake shapes and its subclass of star convex shapes, and present solutions for both. Let $k$ be the maximum scale of the considered shape in a given amoebot structure. If the shape is star convex, we solve it within $\\mathcal{O}(\\log^2 k)$ rounds. If it is a snowflake but not star convex, we solve it within $\\mathcal{O}(\\sqrt{n} \\log n)$ rounds.",
        "authors": [
            "Matthias Artmann",
            "Andreas Padalkin",
            "Christian Scheideler"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-01-28"
    },
    "http://arxiv.org/abs/2501.16103v1": {
        "id": "http://arxiv.org/abs/2501.16103v1",
        "title": "Static Batching of Irregular Workloads on GPUs: Framework and Application to Efficient MoE Model Inference",
        "link": "http://arxiv.org/abs/2501.16103v1",
        "tags": [
            "MoE",
            "serving",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-01-28",
        "tldr": "Proposes a static batching framework for GPU irregular workloads, applied to MoE model inference. Implements optimized CUDA kernels achieving up to 91% of peak Tensor Core throughput on H800 GPU.",
        "abstract": "It has long been a problem to arrange and execute irregular workloads on massively parallel devices. We propose a general framework for statically batching irregular workloads into a single kernel with a runtime task mapping mechanism on GPUs. We further apply this framework to Mixture-of-Experts (MoE) model inference and implement an optimized and efficient CUDA kernel. Our MoE kernel achieves up to 91% of the peak Tensor Core throughput on NVIDIA H800 GPU and 95% on NVIDIA H20 GPU.",
        "authors": [
            "Yinghan Li",
            "Yifei Li",
            "Jiejing Zhang",
            "Bujiao Chen",
            "Xiaotong Chen",
            "Lian Duan",
            "Yejun Jin",
            "Zheng Li",
            "Xuanyu Liu",
            "Haoyu Wang",
            "Wente Wang",
            "Yajie Wang",
            "Jiacheng Yang",
            "Peiyang Zhang",
            "Laiwen Zheng",
            "Wenyuan Yu"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-01-27"
    },
    "http://arxiv.org/abs/2501.15829v1": {
        "id": "http://arxiv.org/abs/2501.15829v1",
        "title": "Aging-aware CPU Core Management for Embodied Carbon Amortization in Cloud LLM Inference",
        "link": "http://arxiv.org/abs/2501.15829v1",
        "tags": [
            "inference"
        ],
        "relevant": true,
        "indexed_date": "2025-01-28",
        "tldr": "Addresses sustainable expansion of cloud LLM inference by reducing embodied carbon. Proposes aging-aware CPU core management via selective deep idling and task allocation to extend hardware lifespan. Achieves 37.67% reduction in yearly embodied carbon emissions and 77% lower CPU underutilization with <10% impact on service.",
        "abstract": "Broad adoption of Large Language Models (LLM) demands rapid expansions of cloud LLM inference clusters, leading to accumulation of embodied carbon$-$the emissions from manufacturing and supplying IT assets$-$that mostly concentrate on inference server CPU. This paper delves into the challenges of sustainable growth of cloud LLM inference, emphasizing extended amortization of CPU embodied over an increased lifespan. Given the reliability risks of silicon aging, we propose an aging-aware CPU core management technique to delay CPU aging effects, allowing the cluster operator to safely increase CPU life. Our technique exploits CPU underutilization patterns that we uncover in cloud LLM inference by halting aging in unused cores and even-outing aging in active cores via selective deep idling and aging-aware inference task allocation. Through extensive simulations using real-world Azure inference traces and an extended LLM cluster simulator from Microsoft, we show superior performance of our technique over existing methods with an estimated 37.67\\% reduction in yearly embodied carbon emissions through p99 performance of managing CPU aging effects, a 77\\% reduction in CPU underutilization, and less than 10\\% impact to the inference service quality.",
        "authors": [
            "Tharindu B. Hewage",
            "Shashikant Ilager",
            "Maria Rodriguez Read",
            "Rajkumar Buyya"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-01-27"
    },
    "http://arxiv.org/abs/2501.14808v4": {
        "id": "http://arxiv.org/abs/2501.14808v4",
        "title": "HyGen: Efficient LLM Serving via Elastic Online-Offline Request Co-location",
        "link": "http://arxiv.org/abs/2501.14808v4",
        "tags": [
            "serving",
            "offline",
            "autoscaling"
        ],
        "relevant": true,
        "indexed_date": "2025-01-28",
        "tldr": "Proposes HyGen for efficient co-location of online and offline LLM inference workloads. Uses latency prediction, SLO-aware profiling, and scheduling policies to manage interference. Achieves 3.9-5.8x throughput gains while preserving latency SLOs.",
        "abstract": "Large language models (LLMs) have facilitated a wide range of applications with distinct service-level objectives (SLOs), from latency-sensitive online tasks like interactive chatbots to throughput-oriented offline workloads like data synthesis. The existing deployment model, which dedicates machines to each workload, simplifies SLO management but often leads to poor resource utilization. This paper introduces HyGen, an interference-aware LLM serving system that enables efficient co-location of online and offline workloads while preserving SLOs. HyGen incorporates two key innovations: (1) performance control mechanisms, including a latency predictor to estimate batch execution time and an SLO-aware profiler to quantify latency interference, and (2) SLO-aware offline scheduling policies that maximize serving throughput and prevent starvation. Our evaluation on production workloads shows that HyGen achieves up to 3.9-5.8x throughput gains over online and hybrid serving baselines, while ensuring latency SLOs. The code of HyGen is publicly available at https://github.com/UIUC-MLSys/HyGen.",
        "authors": [
            "Ting Sun",
            "Penghan Wang",
            "Fan Lai"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-01-15"
    },
    "http://arxiv.org/abs/2501.14794v2": {
        "id": "http://arxiv.org/abs/2501.14794v2",
        "title": "Characterizing Mobile SoC for Accelerating Heterogeneous LLM Inference",
        "link": "http://arxiv.org/abs/2501.14794v2",
        "tags": [
            "edge",
            "kernel",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-01-28",
        "tldr": "Proposes HeteroInfer, a mobile LLM inference engine optimizing GPU-NPU parallelization and synchronization. Leverages heterogeneous processors to boost throughput and memory bandwidth utilization. Achieves 1.34x-6.02x speedup over GPU/NPU-only engines.",
        "abstract": "With the rapid advancement of artificial intelligence technologies such as ChatGPT, AI agents, and video generation, contemporary mobile systems have begun integrating these AI capabilities on local devices to enhance privacy and reduce response latency. To meet the computational demands of AI tasks, current mobile SoCs are equipped with diverse AI accelerators, including GPUs and Neural Processing Units (NPUs). However, there has not been a comprehensive characterization of these heterogeneous processors, and existing designs typically only leverage a single AI accelerator for LLM inference, leading to suboptimal use of computational resources and memory bandwidth.   In this paper, we first summarize key performance characteristics of heterogeneous processors, SoC memory bandwidth, etc. Drawing on these observations, we propose different heterogeneous parallel mechanisms to fully exploit both GPU and NPU computational power and memory bandwidth. We further design a fast synchronization mechanism between heterogeneous processors that leverages the unified memory architecture. By employing these techniques, we present HeteroInfer, the fastest LLM inference engine in mobile devices which supports GPU-NPU heterogeneous execution. Evaluation shows that HeteroInfer delivers a 1.34x to 6.02x end-to-end speedup over state-of-the-art GPU-only and NPU-only LLM engines, while maintaining negligible interference with other applications.",
        "authors": [
            "Le Chen",
            "Dahu Feng",
            "Erhu Feng",
            "Yingrui Wang",
            "Rong Zhao",
            "Yubin Xia",
            "Pinjie Xu",
            "Haibo Chen"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.LG"
        ],
        "submit_date": "2025-01-11"
    },
    "http://arxiv.org/abs/2501.14784v1": {
        "id": "http://arxiv.org/abs/2501.14784v1",
        "title": "DeServe: Towards Affordable Offline LLM Inference via Decentralization",
        "link": "http://arxiv.org/abs/2501.14784v1",
        "tags": [
            "offline",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-01-28",
        "tldr": "Reduces cost and improves throughput of offline LLM inference using decentralized idle GPUs. Proposed DeServe system optimizes serving in high-latency networks with a scheduling strategy for batch processing. Achieves 6.7x-12.6x throughput increase over baselines in high-latency environments.",
        "abstract": "The rapid growth of generative AI and its integration into everyday workflows have significantly increased the demand for large language model (LLM) inference services. While proprietary models remain popular, recent advancements in open-source LLMs have positioned them as strong contenders. However, deploying these models is often constrained by the high costs and limited availability of GPU resources. In response, this paper presents the design of a decentralized offline serving system for LLM inference. Utilizing idle GPU resources, our proposed system, DeServe, decentralizes access to LLMs at a lower cost. DeServe specifically addresses key challenges in optimizing serving throughput in high-latency network environments. Experiments demonstrate that DeServe achieves a 6.7x-12.6x improvement in throughput over existing serving system baselines in such conditions.",
        "authors": [
            "Linyu Wu",
            "Xiaoyuan Liu",
            "Tianneng Shi",
            "Zhe Ye",
            "Dawn Song"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-01-04"
    },
    "http://arxiv.org/abs/2501.14771v2": {
        "id": "http://arxiv.org/abs/2501.14771v2",
        "title": "Dynamic Adaptation in Data Storage: Real-Time Machine Learning for Enhanced Prefetching",
        "link": "http://arxiv.org/abs/2501.14771v2",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-01-28",
        "tldr": "",
        "abstract": "The exponential growth of data storage demands has necessitated the evolution of hierarchical storage management strategies [1]. This study explores the application of streaming machine learning [3] to revolutionize data prefetching within multi-tiered storage systems. Unlike traditional batch-trained models, streaming machine learning [5] offers adaptability, real-time insights, and computational efficiency, responding dynamically to workload variations. This work designs and validates an innovative framework that integrates streaming classification models for predicting file access patterns, specifically the next file offset. Leveraging comprehensive feature engineering and real-time evaluation over extensive production traces, the proposed methodology achieves substantial improvements in prediction accuracy, memory efficiency, and system adaptability. The results underscore the potential of streaming models in real-time storage management, setting a precedent for advanced caching and tiering strategies.",
        "authors": [
            "Chiyu Cheng",
            "Chang Zhou",
            "Yang Zhao",
            "Jin Cao"
        ],
        "categories": [
            "cs.DC",
            "cs.LG",
            "cs.OS"
        ],
        "submit_date": "2024-12-29"
    },
    "http://arxiv.org/abs/2501.14312v1": {
        "id": "http://arxiv.org/abs/2501.14312v1",
        "title": "Locality-aware Fair Scheduling in LLM Serving",
        "link": "http://arxiv.org/abs/2501.14312v1",
        "tags": [
            "serving",
            "offloading",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-01-27",
        "tldr": "Proposes DLPM and D$^2$LPM scheduling algorithms to balance fairness and efficiency in LLM serving by leveraging locality-aware scheduling, achieving up to 2.87× higher throughput than VTC and up to 7.18× lower latency per-client.",
        "abstract": "Large language model (LLM) inference workload dominates a wide variety of modern AI applications, ranging from multi-turn conversation to document analysis. Balancing fairness and efficiency is critical for managing diverse client workloads with varying prefix patterns. Unfortunately, existing fair scheduling algorithms for LLM serving, such as Virtual Token Counter (VTC), fail to take prefix locality into consideration and thus suffer from poor performance. On the other hand, locality-aware scheduling algorithms in existing LLM serving frameworks tend to maximize the prefix cache hit rate without considering fair sharing among clients.   This paper introduces the first locality-aware fair scheduling algorithm, Deficit Longest Prefix Match (DLPM), which can maintain a high degree of prefix locality with a fairness guarantee. We also introduce a novel algorithm, Double Deficit LPM (D$^2$LPM), extending DLPM for the distributed setup that can find a balance point among fairness, locality, and load-balancing. Our extensive evaluation demonstrates the superior performance of DLPM and D$^2$LPM in ensuring fairness while maintaining high throughput (up to 2.87$\\times$ higher than VTC) and low per-client (up to 7.18$\\times$ lower than state-of-the-art distributed LLM serving system) latency.",
        "authors": [
            "Shiyi Cao",
            "Yichuan Wang",
            "Ziming Mao",
            "Pin-Lun Hsu",
            "Liangsheng Yin",
            "Tian Xia",
            "Dacheng Li",
            "Shu Liu",
            "Yineng Zhang",
            "Yang Zhou",
            "Ying Sheng",
            "Joseph Gonzalez",
            "Ion Stoica"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-01-24"
    },
    "http://arxiv.org/abs/2501.12388v1": {
        "id": "http://arxiv.org/abs/2501.12388v1",
        "title": "Accelerating End-Cloud Collaborative Inference via Near Bubble-free Pipeline Optimization",
        "link": "http://arxiv.org/abs/2501.12388v1",
        "tags": [
            "offloading",
            "edge",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-01-22",
        "tldr": "Proposes COACH for near bubble-free pipeline collaborative inference between end devices and cloud. Combines offline partitioning/quantization optimization with online adaptive quantization and caching for dynamic networks. Achieves 1.7x faster inference and 2.1x higher throughput than baselines with comparable accuracy.",
        "abstract": "End-cloud collaboration offers a promising strategy to enhance the Quality of Service (QoS) in DNN inference by offloading portions of the inference workload from end devices to cloud servers. Despite the potential, the complex model architectures and dynamic network conditions will introduce numerous bubbles (\\ie, idle waiting time) in pipeline execution, resulting in inefficient resource utilization and degraded QoS. To address these challenges, we introduce a novel framework named COACH, designed for near bubble-free pipeline collaborative inference, thereby achieving low inference latency and high system throughput. Initially, COACH employs an \\textit{offline} component that utilizes an efficient recursive divide-and-conquer algorithm to optimize both model partitioning and transmission quantization, aiming to minimize the occurrence of pipeline bubbles. Subsequently, the \\textit{online} component in COACH employs an adaptive quantization adjustment and a context-aware caching strategy to further stabilize pipeline execution. Specifically, COACH analyzes the correlation between intermediate data and label semantic centers in the cache, along with its influence on the quantization adjustment, thereby effectively accommodating network fluctuations. Our experiments demonstrate the efficacy of COACH in reducing inference latency and enhancing system throughput. Notably, while maintaining comparable accuracy, COACH achieves up to 1.7x faster inference and 2.1x higher system throughput than baselines.",
        "authors": [
            "Luyao Gao",
            "Jianchun Liu",
            "Hongli Xu",
            "Sun Xu",
            "Qianpiao Ma",
            "Liusheng Huang"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2024-12-17"
    },
    "http://arxiv.org/abs/2501.10375v2": {
        "id": "http://arxiv.org/abs/2501.10375v2",
        "title": "DAOP: Data-Aware Offloading and Predictive Pre-Calculation for Efficient MoE Inference",
        "link": "http://arxiv.org/abs/2501.10375v2",
        "tags": [
            "offloading",
            "MoE",
            "serving"
        ],
        "relevant": true,
        "indexed_date": "2025-01-22",
        "tldr": "Addresses inefficient MoE inference on memory-constrained GPU-CPU devices. Proposes DAOP, a system that dynamically allocates experts using predicted activations and pre-calculation. Achieves up to 8.20x speedup over caching methods and 1.35x over offloading techniques while maintaining accuracy.",
        "abstract": "Mixture-of-Experts (MoE) models, though highly effective for various machine learning tasks, face significant deployment challenges on memory-constrained devices. While GPUs offer fast inference, their limited memory compared to CPUs means not all experts can be stored on the GPU simultaneously, necessitating frequent, costly data transfers from CPU memory, often negating GPU speed advantages. To address this, we present DAOP, an on-device MoE inference engine to optimize parallel GPU-CPU execution. DAOP dynamically allocates experts between CPU and GPU based on per-sequence activation patterns, and selectively pre-calculates predicted experts on CPUs to minimize transfer latency. This approach enables efficient resource utilization across various expert cache ratios while maintaining model accuracy through a novel graceful degradation mechanism. Comprehensive evaluations across various datasets show that DAOP outperforms traditional expert caching and prefetching methods by up to 8.20x and offloading techniques by 1.35x while maintaining accuracy.",
        "authors": [
            "Yujie Zhang",
            "Shivam Aggarwal",
            "Tulika Mitra"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2024-12-16"
    },
    "http://arxiv.org/abs/2501.12162v2": {
        "id": "http://arxiv.org/abs/2501.12162v2",
        "title": "AdaServe: Accelerating Multi-SLO LLM Serving with SLO-Customized Speculative Decoding",
        "link": "http://arxiv.org/abs/2501.12162v2",
        "tags": [
            "serving",
            "quantization",
            "offloading"
        ],
        "relevant": true,
        "indexed_date": "2025-01-22",
        "tldr": "AdaServe accelerates multi-SLO LLM serving with customized speculative decoding. It formulates a constrained optimization and employs a hardware-aware algorithm to build request-specific speculation trees. Reduces SLO violations by up to 4.3× and improves goodput by 1.9× over baselines.",
        "abstract": "Modern large language model (LLM) applications exhibit diverse service-level objectives (SLOs), from low-latency requirements in interactive coding assistants to more relaxed constraints in data wrangling tasks. Existing LLM serving systems, which rely on uniform batching and scheduling strategies, often fail to meet these heterogeneous SLOs concurrently. We present AdaServe, the first LLM serving system designed to support efficient multi-SLO serving through SLO-customized speculative decoding. AdaServe formulates multi-SLO serving as a constrained optimization problem and introduces a hardware-aware algorithm that constructs a speculation tree tailored to each request's latency target. It features a speculate-select-verify pipeline that enables fine-grained control over decoding speed while maximizing system throughput. AdaServe further adapts to workload variation by dynamically adjusting speculation parameters. Evaluations across diverse workloads show that AdaServe reduces SLO violations by up to 4.3$\\times$ and improves goodput by up to 1.9$\\times$ compared to the best performing baselines, highlighting its effectiveness in multi-SLO serving.",
        "authors": [
            "Zikun Li",
            "Zhuofu Chen",
            "Remi Delacourt",
            "Gabriele Oliaro",
            "Zeyu Wang",
            "Qinghan Chen",
            "Shuhuai Lin",
            "April Yang",
            "Zhihao Zhang",
            "Zhuoming Chen",
            "Sean Lai",
            "Xinhao Cheng",
            "Xupeng Miao",
            "Zhihao Jia"
        ],
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-01-21"
    },
    "http://arxiv.org/abs/2501.11779v2": {
        "id": "http://arxiv.org/abs/2501.11779v2",
        "title": "Glinthawk: A Two-Tiered Architecture for Offline LLM Inference",
        "link": "http://arxiv.org/abs/2501.11779v2",
        "tags": [
            "offline",
            "offloading",
            "sparse"
        ],
        "relevant": true,
        "indexed_date": "2025-01-22",
        "tldr": "Proposes Glinthawk, an architecture for offline LLM inference using a two-tier system that offloads attention computation to lower-end hardware. Separates KV cache from model weights to enable larger batch sizes. Achieves 5.9× throughput improvement and 2.8× cost reduction over baselines.",
        "abstract": "We introduce Glinthawk, an architecture for offline Large Language Model (LLM) inference. By leveraging a two-tiered structure, Glinthawk optimizes the utilization of the high-end accelerators (\"Tier 1\") by offloading the attention mechanism to lower-end compute tier (\"Tier 2\"). This separation allows the memory demand of the attention, known as the key-value cache, to scale independently from the model weights, enabling larger batch sizes and more efficient accelerator usage. Prototyped with NVIDIA T4 GPUs and standard CPU VMs, Glinthawk improves throughput by $5.9\\times$ and reduces cost of generation by $2.8\\times$, compared to paged attention baselines. For long sequence lengths, it achieves $16.3\\times$ throughput improvement at $2.4\\times$ less cost. Our evaluation shows that this architecture can tolerate moderate network latency with minimal performance degradation, making it highly effective for latency-tolerant, throughput-focused applications such as batch processing. The prototype is publicly available at https://github.com/microsoft/glinthawk.",
        "authors": [
            "Pouya Hamadanian",
            "Sadjad Fouladi"
        ],
        "categories": [
            "cs.LG",
            "cs.DC",
            "cs.PF"
        ],
        "submit_date": "2025-01-20"
    },
    "http://arxiv.org/abs/2501.10245v1": {
        "id": "http://arxiv.org/abs/2501.10245v1",
        "title": "Over-the-Air Multi-Sensor Inference with Neural Networks Using Memristor-Based Analog Computing",
        "link": "http://arxiv.org/abs/2501.10245v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-01-20",
        "tldr": "",
        "abstract": "Deep neural networks provide reliable solutions for many classification and regression tasks; however, their application in real-time wireless systems with simple sensor networks is limited due to high energy consumption and significant bandwidth needs. This study proposes a multi-sensor wireless inference system with memristor-based analog computing. Given the sensors' limited computational capabilities, the features from the network's front end are transmitted to a central device where an $L_p$-norm inspired approximation of the maximum operation is employed to achieve transformation-invariant features, enabling efficient over-the-air transmission. We also introduce a trainable over-the-air sensor fusion method based on $L_p$-norm inspired combining function that customizes sensor fusion to match the network and sensor distribution characteristics, enhancing adaptability. To address the energy constraints of sensors, we utilize memristors, known for their energy-efficient in-memory computing, enabling analog-domain computations that reduce energy use and computational overhead in edge computing. This dual approach of memristors and $L_p$-norm inspired sensor fusion fosters energy-efficient computational and transmission paradigms and serves as a practical energy-efficient solution with minimal performance loss.",
        "authors": [
            "Busra Tegin",
            "Muhammad Atif Ali",
            "Tolga M Duman"
        ],
        "categories": [
            "cs.LG",
            "cs.DC",
            "cs.IT"
        ],
        "submit_date": "2025-01-17"
    },
    "http://arxiv.org/abs/2501.09367v1": {
        "id": "http://arxiv.org/abs/2501.09367v1",
        "title": "PICE: A Semantic-Driven Progressive Inference System for LLM Serving in Cloud-Edge Networks",
        "link": "http://arxiv.org/abs/2501.09367v1",
        "tags": [
            "serving",
            "networking"
        ],
        "relevant": true,
        "indexed_date": "2025-01-17",
        "tldr": "Proposes PICE, a semantic-driven cloud-edge progressive inference system for LLMs that offloads sketch completion to edge SLMs. Uses dynamic scheduling and ensemble learning to optimize throughput and latency. Achieves 1.5-2x throughput gain and up to 43% latency reduction compared to SOTA.",
        "abstract": "Large language models (LLMs), while driving a new wave of interactive AI applications across numerous domains, suffer from high inference costs and heavy cloud dependency. Motivated by the redundancy phenomenon in linguistics, we propose a progressive inference paradigm over cloud and edge, i.e., firstly generating the sketch of the answer by LLMs at cloud, and then conducting parallel extension to fill in details by small models (SLMs) at edge. Progressive inference offers potential benefits to improve throughput and reduce inference latency while facing key implementation challenges, including decreased response quality from SLMs, a tradeoff between the brevity and comprehensiveness of sketches, as well as increased latency caused by network transmission and edge inference. In this work, we propose and implement PICE, an LLM serving system with semantic-level cloud-edge collaboration, enhancing inference throughput and quality through dynamic inference task scheduling, ensemble learning, and parallel edge inference. Extensive testbed experiments illustrate that our approach achieves $1.5-2\\times$ throughput enhancement and up to 43% latency reduction, while also potentially enhancing the quality compared to SOTA systems.",
        "authors": [
            "Huiyou Zhan",
            "Xuan Zhang",
            "Haisheng Tan",
            "Han Tian",
            "Dongping Yong",
            "Junyang Zhang",
            "Xiang-Yang Li"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-01-16"
    },
    "http://arxiv.org/abs/2501.08192v2": {
        "id": "http://arxiv.org/abs/2501.08192v2",
        "title": "PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM Serving",
        "link": "http://arxiv.org/abs/2501.08192v2",
        "tags": [
            "serving",
            "offloading",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-01-15",
        "tldr": "Addresses communication overhead in distributed LLM inference. Proposes PRESERVE with prefetching of weights and KV-cache to on-chip cache during communication. Achieves up to 1.6x end-to-end speedup on commercial accelerators.",
        "abstract": "Large language models (LLMs) are typically served from clusters of GPUs/NPUs that consist of large number of devices. Unfortunately, communication between these devices incurs significant overhead, increasing the inference latency and cost while limiting the scalability. Prior work addressed this issue by overlapping communication with compute, but has severe limitations due to the data dependencies between these operations. In this paper, we propose PRESERVE, a novel framework that prefetches model weights and KV-cache from off-chip HBM memory to the on-chip cache of AI accelerators during the communication operations, which offers various advantages and performance improvements compared to prior methods.   Through extensive experiments conducted on commercial AI accelerators, we demonstrate up to 1.6x end-to-end speedup on state-of-the-art, open-source LLMs. Additionally, we perform a design space exploration that identifies the optimal hardware configuration for the proposed method, showing a further 1.25x improvement in performance per cost by selecting the optimal L2 cache size. Our results show that PRESERVE has the potential to mitigate the memory bottlenecks and communication overheads, offering a solution to improve the performance and scalability of the LLM inference systems.",
        "authors": [
            "Ahmet Caner Yüzügüler",
            "Jiawei Zhuang",
            "Lukas Cavigelli"
        ],
        "categories": [
            "cs.AI",
            "cs.AR",
            "cs.DC"
        ],
        "submit_date": "2025-01-14"
    },
    "http://arxiv.org/abs/2501.07767v1": {
        "id": "http://arxiv.org/abs/2501.07767v1",
        "title": "HgPCN: A Heterogeneous Architecture for E2E Embedded Point Cloud Inference",
        "link": "http://arxiv.org/abs/2501.07767v1",
        "tags": [
            "edge",
            "kernel",
            "inference"
        ],
        "relevant": true,
        "indexed_date": "2025-01-15",
        "tldr": "Proposes HgPCN, a heterogeneous architecture with spatial indexing methods to address memory-intensive down-sampling and data structuring bottlenecks for end-to-end point cloud inference on edge devices. Achieves real-time latency meeting sensor data generation speeds by optimizing both pre-processing and inference phases.",
        "abstract": "Point cloud is an important type of geometric data structure for many embedded applications such as autonomous driving and augmented reality. Current Point Cloud Networks (PCNs) have proven to achieve great success in using inference to perform point cloud analysis, including object part segmentation, shape classification, and so on. However, point cloud applications on the computing edge require more than just the inference step. They require an end-to-end (E2E) processing of the point cloud workloads: pre-processing of raw data, input preparation, and inference to perform point cloud analysis. Current PCN approaches to support end-to-end processing of point cloud workload cannot meet the real-time latency requirement on the edge, i.e., the ability of the AI service to keep up with the speed of raw data generation by 3D sensors. Latency for end-to-end processing of the point cloud workloads stems from two reasons: memory-intensive down-sampling in the pre-processing phase and the data structuring step for input preparation in the inference phase. In this paper, we present HgPCN, an end-to-end heterogeneous architecture for real-time embedded point cloud applications. In HgPCN, we introduce two novel methodologies based on spatial indexing to address the two identified bottlenecks. In the Pre-processing Engine of HgPCN, an Octree-Indexed-Sampling method is used to optimize the memory-intensive down-sampling bottleneck of the pre-processing phase. In the Inference Engine, HgPCN extends a commercial DLA with a customized Data Structuring Unit which is based on a Voxel-Expanded Gathering method to fundamentally reduce the workload of the data structuring step in the inference phase.",
        "authors": [
            "Yiming Gao",
            "Chao Jiang",
            "Wesley Piard",
            "Xiangru Chen",
            "Bhavesh Patel",
            "Herman Lam"
        ],
        "categories": [
            "cs.AR",
            "cs.DC"
        ],
        "submit_date": "2025-01-14"
    },
    "http://arxiv.org/abs/2501.06856v1": {
        "id": "http://arxiv.org/abs/2501.06856v1",
        "title": "CoCoI: Distributed Coded Inference System for Straggler Mitigation",
        "link": "http://arxiv.org/abs/2501.06856v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-01-14",
        "tldr": "",
        "abstract": "Convolutional neural networks (CNNs) are widely applied in real-time applications on resource-constrained devices. To accelerate CNN inference, prior works proposed to distribute the inference workload across multiple devices. However, they did not address stragglers and device failures in distributed inference, which is challenging due to the devices' time-varying and possibly unknown computation/communication capacities. To address this, we propose a distributed coded inference system, called CoCoI. It splits the convolutional layers of CNN, considering the data dependency of high-dimensional inputs and outputs, and then adapts coding schemes to generate task redundancy. With CoCoI, the inference results can be determined once a subset of devices complete their subtasks, improving robustness against stragglers and failures. To theoretically analyze the tradeoff between redundancy and subtask workload, we formulate an optimal splitting problem to minimize the expected inference latency. Despite its non-convexity, we determine an approximate strategy with minor errors, and prove that CoCoI outperforms uncoded benchmarks. For performance evaluation, we build a testbed with Raspberry Pi 4Bs. The experimental results show that the approximate strategy closely matches the optimal solution. When compared with uncoded benchmarks, CoCoI reduces inference latency by up to 34.2% in the presence of stragglers and device failures.",
        "authors": [
            "Xing Liu",
            "Chao Huang",
            "Ming Tang"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-01-12"
    },
    "http://arxiv.org/abs/2501.06589v5": {
        "id": "http://arxiv.org/abs/2501.06589v5",
        "title": "Ladder-residual: parallelism-aware architecture for accelerating large model inference with communication overlapping",
        "link": "http://arxiv.org/abs/2501.06589v5",
        "tags": [
            "serving",
            "kernel",
            "sparse"
        ],
        "relevant": true,
        "indexed_date": "2025-01-14",
        "tldr": "Proposes Ladder Residual, a model architecture modification for Transformer models to decouple communication from computation in tensor parallelism setups. Enables communication overlapping for model-parallel inference, reducing wall-clock time by 29% for a 70B parameter model on 8 GPUs.",
        "abstract": "Large language model inference is both memory-intensive and time-consuming, often requiring distributed algorithms to efficiently scale. Various model parallelism strategies are used in multi-gpu training and inference to partition computation across multiple devices, reducing memory load and computation time. However, using model parallelism necessitates communication of information between GPUs, which has been a major bottleneck and limits the gains obtained by scaling up the number of devices. We introduce Ladder Residual, a simple architectural modification applicable to all residual-based models that enables straightforward overlapping that effectively hides the latency of communication. Our insight is that in addition to systems optimization, one can also redesign the model architecture to decouple communication from computation. While Ladder Residual can allow communication-computation decoupling in conventional parallelism patterns, we focus on Tensor Parallelism in this paper, which is particularly bottlenecked by its heavy communication. For a Transformer model with 70B parameters, applying Ladder Residual to all its layers can achieve 29% end-to-end wall clock speed up at inference time with TP sharding over 8 devices. We refer the resulting Transformer model as the Ladder Transformer. We train a 1B and 3B Ladder Transformer from scratch and observe comparable performance to a standard dense transformer baseline. We also show that it is possible to convert parts of the Llama-3.1 8B model to our Ladder Residual architecture with minimal accuracy degradation by only retraining for 3B tokens. We release our code for training and inference for easier replication of experiments.",
        "authors": [
            "Muru Zhang",
            "Mayank Mishra",
            "Zhongzhu Zhou",
            "William Brandon",
            "Jue Wang",
            "Yoon Kim",
            "Jonathan Ragan-Kelley",
            "Shuaiwen Leon Song",
            "Ben Athiwaratkun",
            "Tri Dao"
        ],
        "categories": [
            "cs.LG",
            "cs.CL",
            "cs.DC"
        ],
        "submit_date": "2025-01-11"
    },
    "http://arxiv.org/abs/2501.05651v2": {
        "id": "http://arxiv.org/abs/2501.05651v2",
        "title": "A Bring-Your-Own-Model Approach for ML-Driven Storage Placement in Warehouse-Scale Computers",
        "link": "http://arxiv.org/abs/2501.05651v2",
        "tags": [
            "storage",
            "offline",
            "training"
        ],
        "relevant": true,
        "indexed_date": "2025-01-13",
        "tldr": "Proposes a BYOM approach for ML-driven storage placement, where workloads train lightweight models to guide a scheduling heuristic. Achieves up to 3.47× TCO savings compared to state-of-the-art methods in warehouse-scale computers.",
        "abstract": "Storage systems account for a major portion of the total cost of ownership (TCO) of warehouse-scale computers, and thus have a major impact on the overall system's efficiency. Machine learning (ML)-based methods for solving key problems in storage system efficiency, such as data placement, have shown significant promise. However, there are few known practical deployments of such methods. Studying this problem in the context of real-world hyperscale data centers at Google, we identify a number of challenges that we believe cause this lack of practical adoption. Specifically, prior work assumes a monolithic model that resides entirely within the storage layer, an unrealistic assumption in real-world deployments with frequently changing workloads. To address this problem, we introduce a cross-layer approach where workloads instead ''bring their own model''. This strategy moves ML out of the storage system and instead allows each workload to train its own lightweight model at the application layer, capturing the workload's specific characteristics. These small, interpretable models generate predictions that guide a co-designed scheduling heuristic at the storage layer, enabling adaptation to diverse online environments. We build a proof-of-concept of this approach in a production distributed computation framework at Google. Evaluations in a test deployment and large-scale simulation studies using production traces show improvements of as much as 3.47$\\times$ in TCO savings compared to state-of-the-art baselines.",
        "authors": [
            "Chenxi Yang",
            "Yan Li",
            "Martin Maas",
            "Mustafa Uysal",
            "Ubaid Ullah Hafeez",
            "Arif Merchant",
            "Richard McDougall"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-01-10"
    },
    "http://arxiv.org/abs/2501.05313v1": {
        "id": "http://arxiv.org/abs/2501.05313v1",
        "title": "Optimizing Distributed Deployment of Mixture-of-Experts Model Inference in Serverless Computing",
        "link": "http://arxiv.org/abs/2501.05313v1",
        "tags": [
            "serving",
            "MoE",
            "training"
        ],
        "relevant": true,
        "indexed_date": "2025-01-10",
        "tldr": "Optimizes serverless deployment of Mixture-of-Experts model inference to handle expert popularity skew and communication bottlenecks. Proposes a Bayesian optimization framework with pipelined communication and optimal model deployment. Reduces AWS Lambda billed cost by at least 75.67% compared to CPU clusters.",
        "abstract": "With the advancement of serverless computing, running machine learning (ML) inference services over a serverless platform has been advocated, given its labor-free scalability and cost effectiveness. Mixture-of-Experts (MoE) models have been a dominant type of model architectures to enable large models nowadays, with parallel expert networks. Serving large MoE models on serverless computing is potentially beneficial, but has been underexplored due to substantial challenges in handling the skewed expert popularity and scatter-gather communication bottleneck in MoE model execution, for cost-efficient serverless MoE deployment and performance guarantee. We study optimized MoE model deployment and distributed inference serving on a serverless platform, that effectively predict expert selection, pipeline communication with model execution, and minimize the overall billed cost of serving MoE models. Especially, we propose a Bayesian optimization framework with multi-dimensional epsilon-greedy search to learn expert selections and optimal MoE deployment achieving optimal billed cost, including: 1) a Bayesian decision-making method for predicting expert popularity; 2) flexibly pipelined scatter-gather communication; and 3) an optimal model deployment algorithm for distributed MoE serving. Extensive experiments on AWS Lambda show that our designs reduce the billed cost of all MoE layers by at least 75.67% compared to CPU clusters while maintaining satisfactory inference throughput. As compared to LambdaML in serverless computing, our designs achieves 43.41% lower cost with a throughput decrease of at most 18.76%.",
        "authors": [
            "Mengfan Liu",
            "Wei Wang",
            "Chuan Wu"
        ],
        "categories": [
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2025-01-09"
    },
    "http://arxiv.org/abs/2501.04489v1": {
        "id": "http://arxiv.org/abs/2501.04489v1",
        "title": "Collaborative Inference Acceleration with Non-Penetrative Tensor Partitioning",
        "link": "http://arxiv.org/abs/2501.04489v1",
        "tags": [
            "edge",
            "serving",
            "kernel"
        ],
        "relevant": true,
        "indexed_date": "2025-01-09",
        "tldr": "Proposes Non-Penetrative Tensor Partitioning (NPTP) to reduce communication latency in collaborative DNN inference for IoT devices on large images. Minimizes tile sharing communication load through fine-grained partitioning. Achieves 1.44-1.68x inference speedup versus state-of-the-art.",
        "abstract": "The inference of large-sized images on Internet of Things (IoT) devices is commonly hindered by limited resources, while there are often stringent latency requirements for Deep Neural Network (DNN) inference. Currently, this problem is generally addressed by collaborative inference, where the large-sized image is partitioned into multiple tiles, and each tile is assigned to an IoT device for processing. However, since significant latency will be incurred due to the communication overhead caused by tile sharing, the existing collaborative inference strategy is inefficient for convolutional computation, which is indispensable for any DNN. To reduce it, we propose Non-Penetrative Tensor Partitioning (NPTP), a fine-grained tensor partitioning method that reduces the communication latency by minimizing the communication load of tiles shared, thereby reducing inference latency. We evaluate NPTP with four widely-adopted DNN models. Experimental results demonstrate that NPTP achieves a 1.44-1.68x inference speedup relative to CoEdge, a state-of-the-art (SOTA) collaborative inference algorithm.",
        "authors": [
            "Zhibang Liu",
            "Chaonong Xu",
            "Zhenjie Lv",
            "Zhizhuo Liu",
            "Suyu Zhao"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-01-08"
    },
    "http://arxiv.org/abs/2501.04571v1": {
        "id": "http://arxiv.org/abs/2501.04571v1",
        "title": "Scalable Data Notarization Leveraging Hybrid DLTs",
        "link": "http://arxiv.org/abs/2501.04571v1",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-01-09",
        "tldr": "",
        "abstract": "Notarization is a procedure that enhance data management by ensuring the authentication of data during audits, thereby increasing trust in the audited data. Blockchain is frequently used as a secure, immutable, and transparent storage, contributing to make data notarization procedures more effective and trustable. Several blockchain-based data notarization protocols have been proposed in literature and commercial solutions. However, these implementations, whether on public or private blockchains, face inherent challenges: high fees on public blockchains and trust issues on private platforms, limiting the adoption of blockchains for data notarization or forcing several trade-offs. In this paper, we explore the use of hybrid blockchain architectures for data notarization, with a focus on scalability issues. Through the analysis of a real-world use case, the data notarization of product passports in supply chains, we propose a novel approach utilizing a data structure designed to efficiently manage the trade-offs in terms of storage occupation and costs involved in notarizing a large collection of data.",
        "authors": [
            "Domenico Tortola",
            "Claudio Felicioli",
            "Andrea Canciani",
            "Fabio Severino"
        ],
        "categories": [
            "cs.CR",
            "cs.DC"
        ],
        "submit_date": "2025-01-08"
    },
    "http://arxiv.org/abs/2501.02600v1": {
        "id": "http://arxiv.org/abs/2501.02600v1",
        "title": "TAPAS: Thermal- and Power-Aware Scheduling for LLM Inference in Cloud Platforms",
        "link": "http://arxiv.org/abs/2501.02600v1",
        "tags": [
            "serving",
            "training"
        ],
        "relevant": true,
        "indexed_date": "2025-01-07",
        "tldr": "Addresses thermal and power management challenges in LLM inference clusters. Proposes TAPAS, a framework for VM placement, request routing, and reconfiguration under cooling and power constraints. Reduces thermal/power throttling events, achieving TCO savings.",
        "abstract": "The rising demand for generative large language models (LLMs) poses challenges for thermal and power management in cloud datacenters. Traditional techniques often are inadequate for LLM inference due to the fine-grained, millisecond-scale execution phases, each with distinct performance, thermal, and power profiles. Additionally, LLM inference workloads are sensitive to various configuration parameters (e.g., model parallelism, size, and quantization) that involve trade-offs between performance, temperature, power, and output quality. Moreover, clouds often co-locate SaaS and IaaS workloads, each with different levels of visibility and flexibility. We propose TAPAS, a thermal- and power-aware framework designed for LLM inference clusters in the cloud. TAPAS enhances cooling and power oversubscription capabilities, reducing the total cost of ownership (TCO) while effectively handling emergencies (e.g., cooling and power failures). The system leverages historical temperature and power data, along with the adaptability of SaaS workloads, to: (1) efficiently place new GPU workload VMs within cooling and power constraints, (2) route LLM inference requests across SaaS VMs, and (3) reconfigure SaaS VMs to manage load spikes and emergency situations. Our evaluation on a large GPU cluster demonstrates significant reductions in thermal and power throttling events, boosting system efficiency.",
        "authors": [
            "Jovan Stojkovic",
            "Chaojie Zhang",
            "Íñigo Goiri",
            "Esha Choukse",
            "Haoran Qiu",
            "Rodrigo Fonseca",
            "Josep Torrellas",
            "Ricardo Bianchini"
        ],
        "categories": [
            "cs.DC",
            "cs.AI"
        ],
        "submit_date": "2025-01-05"
    },
    "http://arxiv.org/abs/2501.01792v1": {
        "id": "http://arxiv.org/abs/2501.01792v1",
        "title": "Efficient LLM Inference with Activation Checkpointing and Hybrid Caching",
        "link": "http://arxiv.org/abs/2501.01792v1",
        "tags": [
            "serving",
            "offloading",
            "quantization"
        ],
        "relevant": true,
        "indexed_date": "2025-01-06",
        "tldr": "Addresses the high cost and underutilization of LLM inference with host memory offloading. Introduces HybridServe with activation caching and checkpointing to recompute KV cache faster. Achieves 2.19x throughput improvement over state-of-the-art.",
        "abstract": "Recent large language models (LLMs) with enormous model sizes use many GPUs to meet memory capacity requirements incurring substantial costs for token generation. To provide cost-effective LLM inference with relaxed latency constraints, extensive research has focused on expanding GPU memory by leveraging the host memory. However, LLM inference engines that utilize the host memory often face underutilization of GPU compute units, as a considerable portion of inference time is spent in loading the model onto the GPU via host-GPU interconnect. To tackle these challenges of the host memory offloading for LLM, we introduce HybridServe, an LLM inference system with activation checkpointing based on activation caching. The activation cache stores activation checkpoints generated during intermediate inference stages, allowing the fast recomputation of KV cache while model parameters are transferred to GPU from host memory. Unlike conventional methods that recompute the KV cache from scratch using token IDs, the activation cache allows bypassing projection and FFN operations. To balance between the activation recomputation and parameter loading overhead, this study proposes a KV-activation hybrid caching scheme which finds the best ratio of the key-value and activation caches to adjust the recomputation time. Our system achieves 2.19x throughput improvement over the state-of-the-art prior work for offloading both model weights and KV cache.",
        "authors": [
            "Sanghyeon Lee",
            "Hongbeen Kim",
            "Soojin Hwang",
            "Guseul Heo",
            "Minwoo Noh",
            "Jaehyuk Huh"
        ],
        "categories": [
            "cs.DC"
        ],
        "submit_date": "2025-01-03"
    },
    "http://arxiv.org/abs/2501.01005v2": {
        "id": "http://arxiv.org/abs/2501.01005v2",
        "title": "FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving",
        "link": "http://arxiv.org/abs/2501.01005v2",
        "tags": [
            "serving",
            "kernel",
            "sparse"
        ],
        "relevant": true,
        "indexed_date": "2025-01-03",
        "tldr": "Proposes FlashInfer, an efficient attention engine for LLM inference serving. Utilizes block-sparse KV-cache and composable formats for memory optimization, with JIT-compiled attention customization and load-balanced scheduling. Achieves 13-69% latency reduction in various inference scenarios compared to state-of-the-art systems.",
        "abstract": "Transformers, driven by attention mechanisms, form the foundation of large language models (LLMs). As these models scale up, efficient GPU attention kernels become essential for high-throughput and low-latency inference. Diverse LLM applications demand flexible and high-performance attention solutions. We present FlashInfer: a customizable and efficient attention engine for LLM serving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse format and composable formats to optimize memory access and reduce redundancy. It also offers a customizable attention template, enabling adaptation to various settings through Just-In-Time (JIT) compilation. Additionally, FlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user requests while maintaining compatibility with CUDAGraph which requires static configuration. FlashInfer have been integrated into leading LLM serving frameworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and end-to-end evaluations demonstrate FlashInfer's ability to significantly boost kernel performance across diverse inference scenarios: compared to state-of-the-art LLM serving solutions, FlashInfer achieve 29-69% inter-token-latency reduction compared to compiler backends for LLM serving benchmark, 28-30% latency reduction for long-context inference, and 13-17% speedup for LLM serving with parallel generation.",
        "authors": [
            "Zihao Ye",
            "Lequn Chen",
            "Ruihang Lai",
            "Wuwei Lin",
            "Yineng Zhang",
            "Stephanie Wang",
            "Tianqi Chen",
            "Baris Kasikci",
            "Vinod Grover",
            "Arvind Krishnamurthy",
            "Luis Ceze"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.LG"
        ],
        "submit_date": "2025-01-02"
    },
    "http://arxiv.org/abs/2501.00068v2": {
        "id": "http://arxiv.org/abs/2501.00068v2",
        "title": "Dynamic Optimization of Storage Systems Using Reinforcement Learning Techniques",
        "link": "http://arxiv.org/abs/2501.00068v2",
        "tags": [],
        "relevant": false,
        "indexed_date": "2025-01-03",
        "tldr": "",
        "abstract": "The exponential growth of data-intensive applications has placed unprecedented demands on modern storage systems, necessitating dynamic and efficient optimization strategies. Traditional heuristics employed for storage performance optimization often fail to adapt to the variability and complexity of contemporary workloads, leading to significant performance bottlenecks and resource inefficiencies. To address these challenges, this paper introduces RL-Storage, a novel reinforcement learning (RL)-based framework designed to dynamically optimize storage system configurations. RL-Storage leverages deep Q-learning algorithms to continuously learn from real-time I/O patterns and predict optimal storage parameters, such as cache size, queue depths, and readahead settings[1].This work underscores the transformative potential of reinforcement learning techniques in addressing the dynamic nature of modern storage systems. By autonomously adapting to workload variations in real time, RL-Storage provides a robust and scalable solution for optimizing storage performance, paving the way for next-generation intelligent storage infrastructures.",
        "authors": [
            "Chiyu Cheng",
            "Chang Zhou",
            "Yang Zhao"
        ],
        "categories": [
            "cs.OS",
            "cs.DC",
            "cs.LG"
        ],
        "submit_date": "2024-12-29"
    },
    "http://arxiv.org/abs/2512.20573v1": {
        "title": "Fail Fast, Win Big: Rethinking the Drafting Strategy in Speculative Decoding via Diffusion LLMs",
        "link": "http://arxiv.org/abs/2512.20573v1",
        "abstract": "Diffusion Large Language Models (dLLMs) offer fast, parallel token generation, but their standalone use is plagued by an inherent efficiency-quality tradeoff. We show that, if carefully applied, the attributes of dLLMs can actually be a strength for drafters in speculative decoding with autoregressive (AR) verifiers. Our core insight is that dLLM's speed from parallel decoding drastically lowers the risk of costly rejections, providing a practical mechanism to effectively realize the (elusive) lengthy drafts that lead to large speedups with speculative decoding. We present FailFast, a dLLM-based speculative decoding framework that realizes this approach by dynamically adapting its speculation length. It \"fails fast\" by spending minimal compute in hard-to-speculate regions to shrink speculation latency and \"wins big\" by aggressively extending draft lengths in easier regions to reduce verification latency (in many cases, speculating and accepting 70 tokens at a time!). Without any fine-tuning, FailFast delivers lossless acceleration of AR LLMs and achieves up to 4.9$\\times$ speedup over vanilla decoding, 1.7$\\times$ over the best naive dLLM drafter, and 1.4$\\times$ over EAGLE-3 across diverse models and workloads. We open-source FailFast at https://github.com/ruipeterpan/failfast.",
        "authors": [
            "Rui Pan",
            "Zhuofu Chen",
            "Ravi Netravali"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.20573v1",
        "relevant": true,
        "tags": [
            "serving",
            "quantization",
            "diffusion"
        ],
        "tldr": "Proposes FailFast, a diffusion LLM-based speculative decoding framework that dynamically adapts draft length to accelerate autoregressive LLM inference. Achieves up to 4.9× speedup over vanilla decoding with lossless quality.",
        "indexed_date": "2025-12-24"
    },
    "http://arxiv.org/abs/2512.20485v1": {
        "title": "WOC: Dual-Path Weighted Object Consensus Made Efficient",
        "link": "http://arxiv.org/abs/2512.20485v1",
        "abstract": "Modern distributed systems face a critical challenge: existing consensus protocols optimize for either node heterogeneity or workload independence, but not both. For example, Cabinet leverages weighted quorums to handle node heterogeneity but serializes all operations through a global leader, limiting parallelism. EPaxos enables parallel execution for independent operations but treats all nodes uniformly, ignoring performance differences. To tackle this problem, we present WOC, a dual-path consensus protocol that dynamically routes operations into two paths based on their access patterns. Independent operations execute through a fast path that uses object-specific weighted quorums and completes in one network round-trip. Conflicting or shared objects route through a leader-coordinated slow path employing node-weighted consensus. Our evaluation demonstrates that WOC achieves up to 4X higher throughput than Cabinet for workloads with >70% independent objects, while maintaining equivalent performance under high contention.",
        "authors": [
            "Tanisha Fonseca",
            "Gengrui Zhang"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.20485v1",
        "relevant": true,
        "tags": [
            "networking",
            "storage"
        ],
        "tldr": "Addresses consensus protocols' inability to handle node heterogeneity and workload independence. Proposes WOC with dual-fast/slow paths using object-specific weighted quorums and leader coordination. Achieves 4× higher throughput than Cabinet for low-contention workloads.",
        "indexed_date": "2025-12-24"
    },
    "http://arxiv.org/abs/2512.20394v1": {
        "title": "Resilient Packet Forwarding: A Reinforcement Learning Approach to Routing in Gaussian Interconnected Networks with Clustered Faults",
        "link": "http://arxiv.org/abs/2512.20394v1",
        "abstract": "As Network-on-Chip (NoC) and Wireless Sensor Network architectures continue to scale, the topology of the underlying network becomes a critical factor in performance. Gaussian Interconnected Networks based on the arithmetic of Gaussian integers, offer attractive properties regarding diameter and symmetry. Despite their attractive theoretical properties, adaptive routing techniques in these networks are vulnerable to node and link faults, leading to rapid degradation in communication reliability. Node failures (particularly those following Gaussian distributions, such as thermal hotspots or physical damage clusters) pose severe challenges to traditional deterministic routing. This paper proposes a fault-aware Reinforcement Learning (RL) routing scheme tailored for Gaussian Interconnected Networks. By utilizing a PPO (Proximal Policy Optimization) agent with a specific reward structure designed to penalize fault proximity, the system dynamically learns to bypass faulty regions. We compare our proposed RL-based routing protocol against a greedy adaptive shortest-path routing algorithm. Experimental results demonstrate that the RL agent significantly outperforms the adaptive routing sustaining a Packet Delivery Ratio (PDR) of 0.95 at 40% fault density compared to 0.66 for the greedy. Furthermore, the RL approach exhibits effective delivery rates compared to the greedy adaptive routing, particularly under low network load of 20% at 0.57 vs. 0.43, showing greater proficiency in managing congestion, validating its efficacy in stochastic, fault-prone topologies",
        "authors": [
            "Mohammad Walid Charrwi",
            "Zaid Hussain"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.20394v1",
        "relevant": false,
        "indexed_date": "2025-12-24"
    },
    "http://arxiv.org/abs/2512.20363v1": {
        "title": "Clust-PSI-PFL: A Population Stability Index Approach for Clustered Non-IID Personalized Federated Learning",
        "link": "http://arxiv.org/abs/2512.20363v1",
        "abstract": "Federated learning (FL) supports privacy-preserving, decentralized machine learning (ML) model training by keeping data on client devices. However, non-independent and identically distributed (non-IID) data across clients biases updates and degrades performance. To alleviate these issues, we propose Clust-PSI-PFL, a clustering-based personalized FL framework that uses the Population Stability Index (PSI) to quantify the level of non-IID data. We compute a weighted PSI metric, $WPSI^L$, which we show to be more informative than common non-IID metrics (Hellinger, Jensen-Shannon, and Earth Mover's distance). Using PSI features, we form distributionally homogeneous groups of clients via K-means++; the number of optimal clusters is chosen by a systematic silhouette-based procedure, typically yielding few clusters with modest overhead. Across six datasets (tabular, image, and text modalities), two partition protocols (Dirichlet with parameter $α$ and Similarity with parameter S), and multiple client sizes, Clust-PSI-PFL delivers up to 18% higher global accuracy than state-of-the-art baselines and markedly improves client fairness by a relative improvement of 37% under severe non-IID data. These results establish PSI-guided clustering as a principled, lightweight mechanism for robust PFL under label skew.",
        "authors": [
            "Daniel M. Jimenez-Gutierrez",
            "Mehrdad Hassanzadeh",
            "Aris Anagnostopoulos",
            "Ioannis Chatzigiannakis",
            "Andrea Vitaletti"
        ],
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.DC",
            "stat.AP",
            "stat.ML"
        ],
        "id": "http://arxiv.org/abs/2512.20363v1",
        "relevant": false,
        "indexed_date": "2025-12-24"
    },
    "http://arxiv.org/abs/2512.20210v1": {
        "title": "Predictive-LoRA: A Proactive and Fragmentation-Aware Serverless Inference System for LLMs",
        "link": "http://arxiv.org/abs/2512.20210v1",
        "abstract": "The serverless computing paradigm offers compelling advantages for deploying Large Language Model (LLM) inference services, including elastic scaling and pay-per-use billing. However, serving multiple fine-tuned LLMs via Low-Rank Adaptation (LoRA) in serverless environments faces critical challenges: reactive adapter loading causes significant cold start latency, and frequent adapter swapping leads to severe GPU memory fragmentation. In this paper, we present Predictive-LoRA (P-LoRA), a proactive and fragmentation-aware serverless inference system for LoRA-based LLMs. P-LoRA introduces two key innovations: (1) a lightweight LSTM-based traffic predictor that forecasts adapter demand and proactively prefetches hot adapters from host memory to GPU, reducing cold start latency by up to 68%; and (2) a page-based adapter memory management mechanism inspired by operating system virtual memory, which keeps GPU memory utilization above 87% even under heterogeneous adapter ranks. We evaluate P-LoRA using production-like workloads derived from the Azure Functions trace. Experimental results demonstrate that P-LoRA achieves 1.52x higher throughput than S-LoRA while reducing the average Time-To-First-Token (TTFT) by 35% under high concurrency scenarios.",
        "authors": [
            "Yinan Ni",
            "Xiao Yang",
            "Yuqi Tang",
            "Zhimin Qiu",
            "Chen Wang",
            "Tingzhou Yuan"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.20210v1",
        "relevant": true,
        "tags": [
            "LoRA",
            "serving"
        ],
        "tldr": "Addresses latency and fragmentation in serverless LLM serving with multiple LoRA adapters. Proposes proactive adapter prefetching and page-based memory management. Achieves 35% reduction in average TTFT and 1.52x throughput improvement over prior system.",
        "indexed_date": "2025-12-24"
    },
    "http://arxiv.org/abs/2512.20184v1": {
        "title": "Reaching Agreement Among Reasoning LLM Agents",
        "link": "http://arxiv.org/abs/2512.20184v1",
        "abstract": "Multi-agent systems have extended the capability of agentic AI. Instead of single inference passes, multiple agents perform collective reasoning to derive high quality answers. However, existing multi-agent orchestration relies on static heuristic workflows such as fixed loop limits and barrier synchronization. These ad-hoc approaches waste computational resources, incur high latency due to stragglers, and risk finalizing transient agreements. We argue that reliable multi-agent reasoning requires a formal foundation analogous to classical distributed consensus problem.\n  To that end, we propose a formal model of the multi-agent refinement problem. The model includes definitions of the correctness guarantees and formal semantics of agent reasoning. We then introduce Aegean, a consensus protocol designed for stochastic reasoning agents that solves multi-agent refinement. We implement the protocol in Aegean-Serve, a consensus-aware serving engine that performs incremental quorum detection across concurrent agent executions, enabling early termination when sufficient agents converge. Evaluation using four mathematical reasoning benchmarks shows that Aegean provides provable safety and liveness guarantees while reducing latency by 1.2--20$\\times$ compared to state-of-the-art baselines, maintaining answer quality within 2.5%. Consistent gains across both local GPU deployments and commercial API providers validate that consensus-based orchestration eliminates straggler delays without sacrificing correctness.",
        "authors": [
            "Chaoyi Ruan",
            "Yiliang Wang",
            "Ziji Shi",
            "Jialin Li"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.20184v1",
        "relevant": true,
        "tags": [
            "serving",
            "thinking",
            "RL"
        ],
        "tldr": "Proposes Aegean, a consensus protocol for multi-agent reasoning to ensure reliable agreement while reducing latency. Implements a serving engine with incremental quorum detection for early termination. Cuts latency by 1.2-20× compared to baselines while maintaining answer quality within 2.5%.",
        "indexed_date": "2025-12-24"
    },
    "http://arxiv.org/abs/2512.20178v1": {
        "title": "SHIRO: Near-Optimal Communication Strategies for Distributed Sparse Matrix Multiplication",
        "link": "http://arxiv.org/abs/2512.20178v1",
        "abstract": "Distributed Sparse Matrix-Matrix Multiplication (SpMM) is a fundamental operation in numerous high-performance computing and deep learning applications. The major performance bottleneck in distributed SpMM lies in the substantial communication overhead, which limits both performance and scalability. In this paper, we identify and analyze sources of inefficient communication in existing distributed SpMM implementations at two levels and address these inefficiencies by proposing: (1) a fine-grained, sparsity-aware communication strategy that reduces communication overhead by exploiting the sparsity pattern of the sparse matrix, and (2) a hierarchical communication strategy that integrates the sparsity-aware strategy with the common two-tier network architectures in GPU-accelerated systems, to reduce redundant communication across slow network links. We implement these optimizations in a comprehensive distributed SpMM framework, \\method{}. Extensive evaluations on real-world datasets show that our framework demonstrates strong scalability up to 128 GPUs, achieving geometric mean speedups of 221.5$\\times$, 56.0$\\times$, 23.4$\\times$, and 8.8$\\times$ over four state-of-the-art baselines (CAGNET, SPA, BCL, and CoLa, respectively) at this scale.",
        "authors": [
            "Chen Zhuang",
            "Lingqi Zhang",
            "Benjamin Brock",
            "Du Wu",
            "Peng Chen",
            "Toshio Endo",
            "Satoshi Matsuoka",
            "Mohamed Wahib"
        ],
        "categories": [
            "cs.DC",
            "cs.PF"
        ],
        "id": "http://arxiv.org/abs/2512.20178v1",
        "relevant": true,
        "tags": [
            "sparse",
            "training",
            "kernel"
        ],
        "tldr": "Addresses high communication overhead in distributed sparse matrix multiplication. Proposes fine-grained sparsity-aware communication and hierarchical strategies leveraging GPU network topologies. Achieves up to 221.5× speedup over baselines at 128-GPU scale.",
        "indexed_date": "2025-12-24"
    },
    "http://arxiv.org/abs/2512.20163v1": {
        "title": "Population Protocols Revisited: Parity and Beyond",
        "link": "http://arxiv.org/abs/2512.20163v1",
        "abstract": "For nearly two decades, population protocols have been extensively studied, yielding efficient solutions for central problems in distributed computing, including leader election, and majority computation, a predicate type in Presburger Arithmetic closely tied to population protocols. Surprisingly, no protocols have achieved both time- and space-efficiency for congruency predicates, such as parity computation, which are complementary in this arithmetic framework. This gap highlights a significant challenge in the field. To address this gap, we explore the parity problem, where agents are tasked with computing the parity of the given sub-population size. Then we extend the solution for parity to compute congruences modulo an arbitrary $m$.\n  Previous research on efficient population protocols has focused on protocols that minimise both stabilisation time and state utilisation for specific problems. In contrast, this work slightly relaxes this expectation, permitting protocols to place less emphasis on full optimisation and more on universality, robustness, and probabilistic guarantees. This allows us to propose a novel computing paradigm that integrates population weights (or simply weights), a robust clocking mechanism, and efficient anomaly detection coupled with a switching mechanism (which ensures slow but always correct solutions). This paradigm facilitates universal design of efficient multistage stable population protocols. Specifically, the first efficient parity and congruence protocols introduced here use both $O(\\log^3 n)$ states and achieve silent stabilisation in $O(\\log^3 n)$ time. We conclude by discussing the impact of implicit conversion between unary and binary representations enabled by the weight system, with applications to other problems, including the computation and representation of (sub-)population sizes.",
        "authors": [
            "Leszek Gąsieniec",
            "Tytus Grodzicki",
            "Tomasz Jurdziński",
            "Jakub Kowalski",
            "Grzegorz Stachowiak"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.20163v1",
        "relevant": false,
        "indexed_date": "2025-12-24"
    },
    "http://arxiv.org/abs/2512.20064v1": {
        "title": "FastMPS: Revisit Data Parallel in Large-scale Matrix Product State Sampling",
        "link": "http://arxiv.org/abs/2512.20064v1",
        "abstract": "Matrix Product State (MPS) is a versatile tensor network representation widely applied in quantum physics, quantum chemistry, and machine learning, etc. MPS sampling serves as a critical fundamental operation in these fields. As the problems become more complex, the scale of MPS is rapidly increasing. Traditional data parallelism is limited by memory and heavy I/O in large-scale MPS. Model parallelism that can handle large-scale MPS imposes rigid process bindings and lacks scalability. This work proposes Fast-MPS, a multi-level parallel framework for scalable MPS sampling. Our design combines data parallelism across samples with tensor parallelism along bond dimensions. We eliminate memory and I/O pressure through compression and overlapping, and revive data parallel in large-scale MPS sampling. We evaluate our approach on Gaussian Boson Sampling, a representative and demanding application. Fast-MPS achieves over 10x speedup compared to existing simulators, scales to thousands of processes, and enables simulations with 8,176 sites and bond dimension chi = 10^4, significantly outperforming the state of the art. Fast-MPS has demonstrated great potential in high-performance tensor network applications.",
        "authors": [
            "Yaojian Chen",
            "Si-Qiu Gong",
            "Lin Gan",
            "Yanfei Liu",
            "An Yang",
            "Yinuo Wang",
            "Chao-yang Lu",
            "Guangwen Yang"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.20064v1",
        "relevant": true,
        "tags": [
            "offline",
            "scaling",
            "quantization"
        ],
        "tldr": "Addresses high memory and I/O overhead in large-scale Matrix Product State (MPS) sampling. Proposes Fast-MPS, combining data and tensor parallelism with compression and overlapping techniques. Achieves over 10× speedup and scales to thousands of processes, handling 8,176 sites.",
        "indexed_date": "2025-12-24"
    },
    "http://arxiv.org/abs/2512.20017v1": {
        "title": "Scaling Point-based Differentiable Rendering for Large-scale Reconstruction",
        "link": "http://arxiv.org/abs/2512.20017v1",
        "abstract": "Point-based Differentiable Rendering (PBDR) enables high-fidelity 3D scene reconstruction, but scaling PBDR to high-resolution and large scenes requires efficient distributed training systems. Existing systems are tightly coupled to a specific PBDR method. And they suffer from severe communication overhead due to poor data locality. In this paper, we present Gaian, a general distributed training system for PBDR. Gaian provides a unified API expressive enough to support existing PBDR methods, while exposing rich data-access information, which Gaian leverages to optimize locality and reduce communication. We evaluated Gaian by implementing 4 PBDR algorithms. Our implementations achieve high performance and resource efficiency: across six datasets and up to 128 GPUs, it reduces communication by up to 91% and improves training throughput by 1.50x-3.71x.",
        "authors": [
            "Hexu Zhao",
            "Xiaoteng Liu",
            "Xiwen Min",
            "Jianhao Huang",
            "Youming Deng",
            "Yanfei Li",
            "Ang Li",
            "Jinyang Li",
            "Aurojit Panda"
        ],
        "categories": [
            "cs.DC",
            "cs.GR"
        ],
        "id": "http://arxiv.org/abs/2512.20017v1",
        "relevant": true,
        "tags": [
            "training",
            "storage",
            "networking"
        ],
        "tldr": "Addresses distributed training inefficiencies in Point-based Differentiable Rendering (PBDR) for large-scale 3D reconstruction. Introduces Gaian, a system unifying PBDR APIs and optimizing data locality to reduce communication. Achieves up to 91% communication reduction and 1.50x-3.71x throughput improvement.",
        "indexed_date": "2025-12-24"
    },
    "http://arxiv.org/abs/2512.19972v1": {
        "title": "Rethinking Knowledge Distillation in Collaborative Machine Learning: Memory, Knowledge, and Their Interactions",
        "link": "http://arxiv.org/abs/2512.19972v1",
        "abstract": "Collaborative learning has emerged as a key paradigm in large-scale intelligent systems, enabling distributed agents to cooperatively train their models while addressing their privacy concerns. Central to this paradigm is knowledge distillation (KD), a technique that facilitates efficient knowledge transfer among agents. However, the underlying mechanisms by which KD leverages memory and knowledge across agents remain underexplored. This paper aims to bridge this gap by offering a comprehensive review of KD in collaborative learning, with a focus on the roles of memory and knowledge. We define and categorize memory and knowledge within the KD process and explore their interrelationships, providing a clear understanding of how knowledge is extracted, stored, and shared in collaborative settings. We examine various collaborative learning patterns, including distributed, hierarchical, and decentralized structures, and provide insights into how memory and knowledge dynamics shape the effectiveness of KD in collaborative learning. Particularly, we emphasize task heterogeneity in distributed learning pattern covering federated learning (FL), multi-agent domain adaptation (MADA), federated multi-modal learning (FML), federated continual learning (FCL), federated multi-task learning (FMTL), and federated graph knowledge embedding (FKGE). Additionally, we highlight model heterogeneity, data heterogeneity, resource heterogeneity, and privacy concerns of these tasks. Our analysis categorizes existing work based on how they handle memory and knowledge. Finally, we discuss existing challenges and propose future directions for advancing KD techniques in the context of collaborative learning.",
        "authors": [
            "Pengchao Han",
            "Xi Huang",
            "Yi Fang",
            "Guojun Han"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.19972v1",
        "relevant": false,
        "indexed_date": "2025-12-24"
    },
    "http://arxiv.org/abs/2512.19851v1": {
        "title": "An Adaptive Distributed Stencil Abstraction for GPUs",
        "link": "http://arxiv.org/abs/2512.19851v1",
        "abstract": "The scientific computing ecosystem in Python is largely confined to single-node parallelism, creating a gap between high-level prototyping in NumPy and high-performance execution on modern supercomputers. The increasing prevalence of hardware accelerators and the need for energy efficiency have made resource adaptivity a critical requirement, yet traditional HPC abstractions remain rigid. To address these challenges, we present an adaptive, distributed abstraction for stencil computations on multi-node GPUs. This abstraction is built using CharmTyles, a framework based on the adaptive Charm++ runtime, and features a familiar NumPy-like syntax to minimize the porting effort from prototype to production code. We showcase the resource elasticity of our abstraction by dynamically rescaling a running application across a different number of nodes and present a performance analysis of the associated overheads. Furthermore, we demonstrate that our abstraction achieves significant performance improvements over both a specialized, high-performance stencil DSL and a generalized NumPy replacement.",
        "authors": [
            "Aditya Bhosale",
            "Laxmikant Kale"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.19851v1",
        "relevant": false,
        "indexed_date": "2025-12-24"
    },
    "http://arxiv.org/abs/2512.19849v1": {
        "title": "UCCL-EP: Portable Expert-Parallel Communication",
        "link": "http://arxiv.org/abs/2512.19849v1",
        "abstract": "Mixture-of-Experts (MoE) workloads rely on expert parallelism (EP) to achieve high GPU efficiency. State-of-the-art EP communication systems such as DeepEP demonstrate strong performance but exhibit poor portability across heterogeneous GPU and NIC platforms. The poor portability is rooted in architecture: GPU-initiated token-level RDMA communication requires tight vertical integration between GPUs and NICs, e.g., GPU writes to NIC driver/MMIO interfaces.\n  We present UCCL-EP, a portable EP communication system that delivers DeepEP-level performance across heterogeneous GPU and NIC hardware. UCCL-EP replaces GPU-initiated RDMA with a high-throughput GPU-CPU control channel: compact token-routing commands are transferred to multithreaded CPU proxies, which then issue GPUDirect RDMA operations on behalf of GPUs. UCCL-EP further emulates various ordering semantics required by specialized EP communication modes using RDMA immediate data, enabling correctness on NICs that lack such ordering, e.g., AWS EFA. We implement UCCL-EP on NVIDIA and AMD GPUs with EFA and Broadcom NICs. On EFA, it outperforms the best existing EP solution by up to $2.1\\times$ for dispatch and combine throughput. On NVIDIA-only platform, UCCL-EP achieves comparable performance to the original DeepEP. UCCL-EP also improves token throughput on SGLang by up to 40% on the NVIDIA+EFA platform, and improves DeepSeek-V3 training throughput over the AMD Primus/Megatron-LM framework by up to 45% on a 16-node AMD+Broadcom platform.",
        "authors": [
            "Ziming Mao",
            "Yihan Zhang",
            "Chihan Cui",
            "Kaichao You",
            "Zhongjie Chen",
            "Zhiying Xu",
            "Scott Shenker",
            "Costin Raiciu",
            "Yang Zhou",
            "Ion Stoica"
        ],
        "categories": [
            "cs.DC",
            "cs.AI",
            "cs.LG",
            "cs.NI"
        ],
        "id": "http://arxiv.org/abs/2512.19849v1",
        "relevant": true,
        "tags": [
            "MoE",
            "training",
            "networking"
        ],
        "tldr": "Addresses poor portability of expert-parallel (EP) communication systems across heterogeneous GPU/NIC platforms. Proposes UCCL-EP with GPU-CPU control channel and RDMA emulation for ordering semantics. Achieves up to 2.1× dispatch/combine throughput and 45% training throughput improvement.",
        "indexed_date": "2025-12-24"
    },
    "http://arxiv.org/abs/2512.19842v1": {
        "title": "Holoscope: Open and Lightweight Distributed Telescope & Honeypot Platform",
        "link": "http://arxiv.org/abs/2512.19842v1",
        "abstract": "The complexity and scale of Internet attacks call for distributed, cooperative observatories capable of monitoring malicious traffic across diverse networks. Holoscope is a lightweight, cloud-native platform designed to simplify the deployment and management of distributed telescope (passive) and honeypot (active) sensors, used to collect and analyse attack traffic by exposing or simulating vulnerable systems. Built upon K3s and WireGuard, Holoscope offers secure connectivity, automated node onboarding, and resilient operation even in resource-constrained environments. Through modular design and Infrastructure-as-Code principles, it supports dynamic sensor orchestration, automated recovery and processing. We build, deploy and operate Holoscope across multiple institutions and cloud networks in Europe and Brazil, enabling unified visibility into large-scale attack phenomena while maintaining ease of integration and security compliance.",
        "authors": [
            "Andrea Sordello",
            "Marco Mellia",
            "Idilio Drago",
            "Rodolfo Valentim",
            "Francesco Musumeci",
            "Massimo Tornatore",
            "Federico Cerutti",
            "Martino Trevisan",
            "Alessio Botta",
            "Willen Borges Coelho"
        ],
        "categories": [
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.19842v1",
        "relevant": false,
        "indexed_date": "2025-12-24"
    },
    "http://arxiv.org/abs/2512.19777v1": {
        "title": "Learned Digital Codes for Over-the-Air Computation in Federated Edge Learning",
        "link": "http://arxiv.org/abs/2512.19777v1",
        "abstract": "Federated edge learning (FEEL) enables wireless devices to collaboratively train a centralised model without sharing raw data, but repeated uplink transmission of model updates makes communication the dominant bottleneck. Over-the-air (OTA) aggregation alleviates this by exploiting the superposition property of the wireless channel, enabling simultaneous transmission and merging communication with computation. Digital OTA schemes extend this principle by incorporating the robustness of conventional digital communication, but current designs remain limited in low signal-to-noise ratio (SNR) regimes. This work proposes a learned digital OTA framework that improves recovery accuracy, convergence behaviour, and robustness to challenging SNR conditions while maintaining the same uplink overhead as state-of-the-art methods. The design integrates an unsourced random access (URA) codebook with vector quantisation and AMP-DA-Net, an unrolled approximate message passing (AMP)-style decoder trained end-to-end with the digital codebook and parameter server local training statistics. The proposed design extends OTA aggregation beyond averaging to a broad class of symmetric functions, including trimmed means and majority-based rules. Experiments on highly heterogeneous device datasets and varying numbers of active devices show that the proposed design extends reliable digital OTA operation by more than 10 dB into low SNR regimes while matching or improving performance across the full SNR range. The learned decoder remains effective under message corruption and nonlinear aggregation, highlighting the broader potential of end-to-end learned design for digital OTA communication in FEEL.",
        "authors": [
            "Antonio Tarizzo",
            "Mohammad Kazemi",
            "Deniz Gündüz"
        ],
        "categories": [
            "cs.IT",
            "cs.AI",
            "cs.DC"
        ],
        "id": "http://arxiv.org/abs/2512.19777v1",
        "relevant": false,
        "indexed_date": "2025-12-24"
    }
}