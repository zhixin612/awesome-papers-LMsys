name: Arxiv Paper Crawler

on:
  schedule:
    - cron: '0 0 */1 * *'  # Runs at UTC 00:00, every day. --> UTC+8 08:00
  workflow_dispatch:
    
# A workflow run is made up of one or more jobs that can run sequentially or in parallel
jobs:
  crawl:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.x'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4 pytz arxiv openai dotenv

      # Run the crawler script
      - name: Run crawler and update daily arxiv
        run: |
          python tools/main.py
        env:
          API_KEY: ${{ secrets.SILICONFLOW_API_KEY }}

      # Commit and push changes if any
      - name: Commit changes
        run: |
          git config --local user.name "zhixin612"
          git config --local user.email "zhao612@tju.edu.cn"
          git commit -a -m "Update daily arxiv papers" || echo "No changes to commit"
          git push
        env: 
         GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      # ------------------------------------------------------
      # Setup Node.js for frontend build
      - name: Setup Node.js
        uses: actions/setup-node@v3
        with:
          node-version: '18'

      # Build the frontend
      - name: Build Frontend
        run: |
          cd web
          npm install
          npm run build
          cp ../tools/index.json dist/index.json

      # Deploy to GitHub Pages
      - name: Deploy to GitHub Pages
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./web/dist
          force_orphan: true # Ensure a fresh history for gh-pages branch
